{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"assess/assess-checks/","text":"Assessment Checks Reference \u00b6 Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. It contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards. For each assessment tool that it supports, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. These checks are categorized under the out-of-the-box policies in Assess Accelerator. This topic lists the set of checks provided for each assessment tool. You must use the Assess Check IDs while creating assessment templates . Contents \u00b6 Prowler checks HCAP Assess checks Azure CIS Blueprint checks Prowler checks \u00b6 Prowler is an open-source third-party tool that can be used to run security assessments of your AWS cloud infrastructure. The Prowler checks that are included in Assess Accelerator adhere to the Centre for Internet Security (CIS) standard. Assess Accelerator provides the following checks for the Prowler tool. These checks are categorized under the Security policy in Assess Accelerator . Identity and Access Management checks Logging checks Monitoring checks Networking checks The format of check IDs in the Prowler tool is check followed by a number (for example, check11 and check12 ). However, the reports in Assess Accelerator use a different format for the Prowler checks (for example, CIS1.1 and CIS1.2 ). In the following tables, the Prowler Check ID column lists the original check IDs; whereas the Assess Check ID column lists the corresponding check IDs that are used in the reports generated by Assess Accelerator. Important: You must use the Prowler Check IDs while creating assessment templates for the Prowler tool. Identity and Access Management checks \u00b6 Prowler Check ID Assess Check ID Description check11 CIS1.1 Avoid the use of the \"root\" account check12 CIS1.2 Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password check13 CIS1.3 Ensure credentials unused for 90 days or greater are disabled check14 CIS1.4 Ensure access keys are rotated every 90 days or less check15 CIS1.5 Ensure IAM password policy requires at least one uppercase letter check16 CIS1.6 Ensure IAM password policy require at least one lowercase letter check17 CIS1.7 Ensure IAM password policy require at least one symbol check18 CIS1.8 Ensure IAM password policy require at least one number check19 CIS1.9 Ensure IAM password policy requires minimum length of 14 or greater check110 CIS1.10 Ensure IAM password policy prevents password reuse: 24 or greater check111 CIS1.11 Ensure IAM password policy expires passwords within 90 days or less check112 CIS1.12 Ensure no root account access key exists check113 CIS1.13 Ensure MFA is enabled for the root account check114 CIS1.14 Ensure hardware MFA is enabled for the root account check115 CIS1.15 Ensure security questions are registered in the AWS account check116 CIS1.16 Ensure IAM policies are attached only to groups or roles check117 CIS1.17 Maintain current contact details check118 CIS1.18 Ensure security contact information is registered check119 CIS1.19 Ensure IAM instance roles are used for AWS resource access from instances check120 CIS1.20 Ensure a support role has been created to manage incidents with AWS Support check121 CIS1.21 Do not setup access keys during initial user setup for all IAM users that have a console password check122 CIS 1.22 Ensure IAM policies that allow full \" : \" administrative privileges are not created Logging checks \u00b6 Prowler Check ID Assess Check ID Description check21 CIS2.1 Ensure CloudTrail is enabled in all regions check22 CIS2.2 Ensure CloudTrail log file validation is enabled check23 CIS2.3 Ensure the S3 bucket CloudTrail logs to is not publicly accessible check24 CIS2.4 Ensure CloudTrail trails are integrated with CloudWatch Logs check25 CIS2.5 Ensure AWS Config is enabled in all regions check26 CIS2.6 Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket check27 CIS2.7 Ensure CloudTrail logs are encrypted at rest using KMS CMKs check28 CIS2.8 Ensure rotation for customer created CMKs is enabled check29 CIS2.9 Ensure VPC Flow Logging is Enabled in all VPCs Monitoring checks \u00b6 Prowler Check ID Assess Check ID Description check31 CIS3.1 Ensure a log metric filter and alarm exist for unauthorized API calls check32 CIS3.2 Ensure a log metric filter and alarm exist for Management Console sign-in without MFA check33 CIS3.3 Ensure a log metric filter and alarm exist for usage of root account check34 CIS3.4 Ensure a log metric filter and alarm exist for IAM policy changes check35 CIS3.5 Ensure a log metric filter and alarm exist for CloudTrail configuration changes check36 CIS3.6 Ensure a log metric filter and alarm exist for AWS Management Console authentication failures check37 CIS3.7 Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs check38 CIS3.8 Ensure a log metric filter and alarm exist for S3 bucket policy changes check39 CIS3.9 Ensure a log metric filter and alarm exist for AWS Config configuration changes check310 CIS3.10 Ensure a log metric filter and alarm exist for security group changes check311 CIS3.11 Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL) check312 CIS3.12 Ensure a log metric filter and alarm exist for changes to network gateways check313 CIS3.13 Ensure a log metric filter and alarm exist for route table changes check314 CIS3.14 Ensure a log metric filter and alarm exist for VPC changes Networking checks \u00b6 Prowler Check ID Assess Check ID Description check41 CIS4.1 Ensure no security groups allow ingress from 0.0.0.0/0 or ::/0 to port 22 check42 CIS4.2 Ensure no security groups allow ingress from 0.0.0.0/0 or ::/0 to port 3389 check43 CIS4.3 Ensure the default security group of every VPC restricts all traffic check44 CIS4.4 Ensure routing tables for VPC peering are \"least access\" HCAP Assess checks \u00b6 HCAP Assess is an inbuilt tool that can be used to run security, operations, and cost optimization assessments of your AWS cloud infrastructure. Assess Accelerator provides the following checks for the HCAP Assess tool. These checks are categorized under multiple policies in Assess Accelerator . Assess Accelerator Policy HCAP Assess Checks Security -- Identity and Access Management checks -- Logging checks -- Monitoring checks -- Networking checks -- Other checks Operations -- Ops checks -- Performance checks -- Fault Tolerance checks Cost Optimization -- Cost Optimization checks The check IDs in the HCAP Assess tool use one of the following prefixes: CIS (Centre for Internet Security): This prefix is used for checks that adhere to the CIS standard. Example: CIS1.1 and CIS1.2 CAP (Custom Assess Policies): This prefix is used for checks that are custom checks provided by the Assess Accelerator. The CAP prefix is available only with the HCAP Assess tool. Example: CAP4.2 and CAP4.3 Important: You must use the Assess Check IDs listed in the following tables while creating assessment templates for the HCAP Assess tool. Identity and Access Management checks (Security Policy) \u00b6 Assess Check ID Description CIS1.1 Avoid the use of the \"root\" account CIS1.2 Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password CIS1.3A Disable all console passwords that are unused for 90 days or more CIS1.3B Disable all access keys that are unused for 90 days or more CIS1.4 Ensure access keys are rotated every 90 days or less CIS1.5 Ensure IAM password policy requires at least one uppercase letter CIS1.6 Ensure IAM password policy require at least one lowercase letter CIS1.7 Ensure IAM password policy require at least one symbol CIS1.8 Ensure IAM password policy require at least one number CIS1.9 Ensure IAM password policy requires minimum password length of 14 or greater CIS1.10 Ensure IAM password policy prevents password reuse CIS1.11 Ensure IAM password policy expires passwords within 90 days or less CIS1.12 Ensure no root account access key exists CIS1.13 Ensure MFA is enabled for the \"root\" account CIS1.16 Ensure IAM policies are attached only to groups or roles CIS1.19 Ensure IAM instance roles are used for AWS resource access from instances CIS1.22 Ensure IAM policies that allow full administrative privileges (i.e. \" : \") are not created CAP1.1 Verify that there are no IAM users without password CAP1.2 Verify that no IAM user have access keys Logging checks (Security Policy) \u00b6 Assess Check ID Description CIS2.1 Ensure CloudTrail is enabled in all regions CIS2.2 Ensure CloudTrail log file validation is enabled CIS2.3 Ensure the S3 bucket used to store CloudTrail logs is not publicly accessible CIS2.4 Ensure CloudTrail trails are integrated with CloudWatch Logs CIS2.5 Ensure AWS Config is enabled in all regions CIS2.6 Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket CIS2.7 Ensure CloudTrail logs are encrypted at rest using KMS CMKs CIS2.8 Ensure rotation for customer created CMKs is enabled CAP2.1 Verify that the Cloudtrail SNS topic is not missing CAP2.2 Verify that an alert is displayed if CloudTrail Global Service Events is not enabled CAP2.3 Verify that the SNS Notifications being sent from CloudTrail is not failing Monitoring checks (Security Policy) \u00b6 Assess Check ID Description CIS3.1 Ensure a log metric filter and alarm exist for unauthorized API calls CIS3.2 Ensure a log metric filter and alarm exist for Management Console sign-in without MFA CIS3.3 Ensure a log metric filter and alarm exist for usage of \"root\" account CIS3.4 Ensure a log metric filter and alarm exist for IAM policy changes CIS3.5 Ensure a log metric filter and alarm exist for CloudTrail configuration changes CIS3.6 Ensure a log metric filter and alarm exist for AWS Management Console authentication failures CIS3.7 Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs CIS3.8 Ensure a log metric filter and alarm exist for S3 bucket policy changes CIS3.9 Ensure a log metric filter and alarm exist for AWS Config configuration changes CIS3.10 Ensure a log metric filter and alarm exist for security group changes CIS3.11 Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL) CIS3.12 Ensure a log metric filter and alarm exist for changes to network gateways CIS3.13 Ensure a log metric filter and alarm exist for route table changes CIS3.14 Ensure a log metric filter and alarm exist for VPC changes Networking checks (Security Policy) \u00b6 Assess Check ID Description CIS4.1 Ensure no security groups allow ingress from 0.0.0.0/0 to port 22 CIS4.2 Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 CAP4.1 Ensure VPC flow logging is enabled in all VPCs CIS4.3 Ensure the default security group of every VPC restricts all traffic CIS4.4 Ensure routing tables for VPC peering are \"least access\" CAP4.2 Verify that Telnet protocol is denied in the NACL rules of VPC CAP4.3 Verify that POP3 protocol is denied in the NACL rules of VPC CAP4.4 Verify that IMAP protocol is denied in the NACL rules of VPC CAP4.5 Verify that ELB Back-end Authentication is enabled CAP4.6 Verify that Cross Zone Load Balancing is enabled CAP4.7 Verify that ELB Security Groups ports matches ELB Listener Configuration CAP4.8 Verify that ELB SSL configuration is enabled CAP4.9 Verify that ELB Cipher policies are the latest CAP4.10 Verify that no S3 buckets are public CAP4.11 Verify that no ICMP port is open to public CAP4.12 Verify that the RDS DB Instances are not running On default Ports Other checks (Security Policy) \u00b6 Assess Check ID Description CAP5.1 Verify that DB servers are not Public CAP5.2 Verify that secondary volumes are encrypted CAP5.3 Verify that CloudFront SSL Certificate on the Origin Server is expired CAP5.4 Verify that CloudFront SSL Certificate on the Origin Server are about to expire Ops checks (Operations Policy) \u00b6 Assess Check ID Description CAP6.1 Verify that the use of generic user names are avoided for IAM Users CAP6.2 Verify that all IAM groups are associated with IAM Users CAP6.3 Verify that every Instance should have Name, Environment, Owner, CostCenter, ExpirationDate, Role, Application and Monitoring tags CAP6.4 Verify that S3 VPC Endpoints are configured for VPC CAP6.5 Verify that Cross-zone Load Balancing is enabled to ELB to experience better performance CAP6.6 Verify that AutoMinorVersionUpgrade is enabled for all RDS Instances CAP6.7 Verify that you are using new VPC for your servers. Default VPC is not a best practice CAP6.8 Verify that S3 Bucket life cycle rules are provided for log buckets CAP6.9 Verify that S3 bucket versioning is enabled for log buckets CAP6.10 Verify that Multi-AZ deployment is used for production RDS instances CAP6.11 Verify that for production RDS instances backups are enabled CAP6.12 Verify that there are no unused network interfaces CAP6.13 Verify that IAM-Instance-Profiles not used for more than 90 days are removed CAP6.14 Verify that unused IAM certificates for more than 90 days are removed CAP6.15 Verify that backups are taken of instances with Deregistered AMIs CAP6.16 Verify that instances are not launched with Public AMIs CAP6.17 Verify that data stored in RDS instances is encrypted CAP6.18 Verify that production ELBs attached with more than one EC2 instance CAP6.19 Verify that there are no unused route tables CAP6.20 Verify the failed state for RDS DB instances CAP6.21 Verify that the instances used in ELB are active with status as InService CAP6.22 Verify that instances in stopped state for more than two weeks are deleted CAP6.23 Verify IAM-Roles not used for more than 90 days are removed Performance checks (Operations Policy) \u00b6 Assess Check ID Description CAP7.1 Verify that provisioned IOPS volume is not attached to a non-EBS optimized instance CAP7.2 Verify that the CloudFront distributions having alternate domain names are resolvable CAP7.3 Verify that there are no security groups with more than 50 rules CAP7.4 Verify that all instances are enabled with termination protection Fault Tolerance checks (Operations Policy) \u00b6 Assess Check ID Description CAP8.1 Verify that Connection draining is enabled for ELB for better user experience Cost Optimization checks (Cost Optimization Policy) \u00b6 Assess Check ID Description CAP9.1 Verify that reserved instance is used, if there is any EC2 instance running for more than 30 days CAP9.2 Verify that all auto-scaling launch configurations are associated with an auto-scaling group CAP9.3 Verify that there are no unused Elastic Load Balancers (ELBs) CAP9.4 Verify that there are no unused Elastic IPs (EIP) CAP9.5 Verify that there are no unused available EBS Volumes CAP9.6 Verify that there are no snapshots older than 90 days Azure CIS Blueprint checks \u00b6 Microsoft Azure assessments use CIS Microsoft Azure Foundations Benchmark 1.1.0 policy, which gets created by assigning CIS Microsoft Azure Foundations Benchmark blueprint. The Azure assessment checks that are included in Assess Accelerator adhere to the Centre for Internet Security (CIS) standard. Assess Accelerator provides the following checks for Azure assessments. These checks are categorized under the Security policy in Assess Accelerator . Identity and Access Management checks Security Center checks Storage Accounts checks Database Services checks Logging and Monitoring checks Networking checks Virtual Machines checks Other Security Applications checks AppService checks Identity and Access Management \u00b6 Check ID Description CIS1.1 Ensure that multi-factor authentication is enabled for all privileged users CIS1.2 Ensure that multi-factor authentication is enabled for all non-privileged users CIS1.3 Ensure that there are no guest users CIS1.23 Ensure that no custom subscription owner roles are created Security Center checks \u00b6 Check ID Description CIS2.1 Ensure that standard pricing tier is selected CIS2.2 Ensure that 'Automatic provisioning of monitoring agent' is set to 'On' CIS2.3 Ensure ASC Default policy setting \"Monitor System Updates\" is not \"Disabled\" CIS2.4 Ensure ASC Default policy setting \"Monitor OS Vulnerabilities\" is not \"Disabled\" CIS2.5 Ensure ASC Default policy setting \"Monitor Endpoint Protection\" is not \"Disabled\" CIS2.6 Ensure ASC Default policy setting \"Monitor Disk Encryption\" is not \"Disabled\" CIS2.7 Ensure ASC Default policy setting \"Monitor Network Security Groups\" is not \"Disabled\" CIS2.9 Ensure ASC Default policy setting \"Enable Next Generation Firewall(NGFW) Monitoring\" is not \"Disabled\" CIS2.10 Ensure ASC Default policy setting \"Monitor Vulnerability Assessment\" is not \"Disabled\" CIS2.12 Ensure ASC Default policy setting \"Monitor JIT Network Access\" is not \"Disabled\" CIS2.13 Ensure ASC Default policy setting \"Monitor Adaptive Application Whitelisting\" is not \"Disabled\" CIS2.14 Ensure ASC Default policy setting \"Monitor SQL Auditing\" is not \"Disabled\" CIS2.15 Ensure ASC Default policy setting \"Monitor SQL Encryption\" is not \"Disabled\" CIS2.16 Ensure that 'Security contact emails' is set CIS2.18 Ensure that 'Send email notification for high severity alerts' is set to 'On' CIS2.19 Ensure that 'Send email also to subscription owners' is set to 'On' Storage Accounts checks \u00b6 Check ID Description CIS3.1 Ensure that 'Secure transfer required' is set to 'Enabled' CIS3.6 Ensure that 'Public access level' is set to Private for blob containers CIS3.7 Ensure default network access rule for Storage Accounts is set to deny CIS3.8 Ensure 'Trusted Microsoft Services' is enabled for Storage Account access Database Services checks \u00b6 Check ID Description CIS4.1 Ensure that 'Auditing' is set to 'On' CIS4.2 Ensure that 'AuditActionGroups' in 'auditing' policy for a SQL server is set properly CIS4.3 Ensure that 'Auditing' Retention is 'greater than 90 days' CIS4.4 Ensure that 'Advanced Data Security' on a SQL server is set to 'On' CIS4.8 Ensure that Azure Active Directory Admin is configured CIS4.9 Ensure that 'Data encryption' is set to 'On' on a SQL Database CIS4.10 Ensure SQL server's TDE protector is encrypted with BYOK (Use your own key) CIS4.11 Ensure 'Enforce SSL connection' is set to 'ENABLED' for MySQL Database Server CIS4.12 Ensure server parameter 'log_checkpoints' is set to 'ON' for PostgreSQL Database Server CIS4.13 Ensure 'Enforce SSL connection' is set to 'ENABLED' for PostgreSQL Database Server CIS4.14 Ensure server parameter 'log_connections' is set to 'ON' for PostgreSQL Database Server CIS4.15 Ensure server parameter 'log_disconnections' is set to 'ON' for PostgreSQL Database Server CIS4.17 Ensure server parameter 'connection_throttling' is set to 'ON' for PostgreSQL Database Server Logging and Monitoring checks \u00b6 Check ID Description CIS 5.1.1 Ensure that a Log Profile exists CIS5.1.2 Ensure that Activity Log Retention is set 365 days or greater CIS5.1.3 Ensure audit profile captures all the activities CIS 5.1.4 Ensure the log profile captures activity logs for all regions including global CIS5.1.5 Ensure the storage container storing the activity logs is not publicly accessible CIS5.1.6 Ensure the storage account containing the container with activity logs is encrypted with BYOK (Use Your Own Key) CIS5.1.7 Ensure that logging for Azure KeyVault is 'Enabled' CIS5.2.1 Ensure that Activity Log Alert exists for Create Policy Assignment CIS5.2.2 Ensure that Activity Log Alert exists for Create or Update Network Security Group CIS5.2.3 Ensure that Activity Log Alert exists for Delete Network Security Group CIS5.2.4 Ensure that Activity Log Alert exists for Create or Update Network Security Group Rule CIS5.2.5 Ensure that activity log alert exists for the Delete Network Security Group Rule CIS5.2.6 Ensure that Activity Log Alert exists for Create or Update Security Solution CIS5.2.7 Ensure that Activity Log Alert exists for Delete Security Solution CIS5.2.8 Ensure that Activity Log Alert exists for Create or Update or Delete SQL Server Firewall Rule CIS5.2.9 Ensure that Activity Log Alert exists for Update Security Policy Networking checks \u00b6 Check ID Description CIS6.1 Ensure that RDP access is restricted from the internet CIS6.2 Ensure that SSH access is restricted from the internet CIS6.5 Ensure that Network Watcher is 'Enabled' Virtual Machines checks \u00b6 Check ID Description CIS7.1 Ensure that 'OS disk' are encrypted CIS7.2 Ensure that 'Data disks' are encrypted CIS7.3 Ensure that 'Unattached disks' are encrypted CIS7.4 Ensure that only approved extensions are installed CIS7.5 Ensure that the latest OS Patches for all Virtual Machines are applied CIS7.6 Ensure that the endpoint protection for all Virtual Machines is installed Other Security Considerations checks \u00b6 Check ID Description CIS8.1 Ensure that the expiration date is set on all keys CIS8.2 Ensure that the expiration date is set on all Secrets CIS8.4 Ensure the key vault is recoverable CIS8.5 Enable role-based access control (RBAC) within Azure Kubernetes Services AppService checks \u00b6 Check ID Description CIS9.1 Ensure App Service Authentication is set on Azure App Service CIS9.2 Ensure web app redirects all HTTP traffic to HTTPS in Azure App Service CIS9.3 Ensure web app is using the latest version of TLS encryption CIS9.4 Ensure the web app has 'Client Certificates (Incoming client certificates)' set to 'On' CIS9.5 Ensure that Register with Azure Active Directory is enabled on App Service CIS9.7 Ensure that 'PHP version' is the latest, if used to run the web app CIS9.8 Ensure that 'Python version' is the latest, if used to run the web app CIS9.9 Ensure that 'Java version' is the latest, if used to run the web app CIS9.10 Ensure that 'HTTP Version' is the latest, if used to run the web app","title":"Assessment checks reference"},{"location":"assess/assess-checks/#assessment-checks-reference","text":"Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. It contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards. For each assessment tool that it supports, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. These checks are categorized under the out-of-the-box policies in Assess Accelerator. This topic lists the set of checks provided for each assessment tool. You must use the Assess Check IDs while creating assessment templates .","title":"Assessment Checks Reference"},{"location":"assess/assess-checks/#contents","text":"Prowler checks HCAP Assess checks Azure CIS Blueprint checks","title":"Contents"},{"location":"assess/assess-checks/#prowler-checks","text":"Prowler is an open-source third-party tool that can be used to run security assessments of your AWS cloud infrastructure. The Prowler checks that are included in Assess Accelerator adhere to the Centre for Internet Security (CIS) standard. Assess Accelerator provides the following checks for the Prowler tool. These checks are categorized under the Security policy in Assess Accelerator . Identity and Access Management checks Logging checks Monitoring checks Networking checks The format of check IDs in the Prowler tool is check followed by a number (for example, check11 and check12 ). However, the reports in Assess Accelerator use a different format for the Prowler checks (for example, CIS1.1 and CIS1.2 ). In the following tables, the Prowler Check ID column lists the original check IDs; whereas the Assess Check ID column lists the corresponding check IDs that are used in the reports generated by Assess Accelerator. Important: You must use the Prowler Check IDs while creating assessment templates for the Prowler tool.","title":"Prowler checks"},{"location":"assess/assess-checks/#identity-and-access-management-checks","text":"Prowler Check ID Assess Check ID Description check11 CIS1.1 Avoid the use of the \"root\" account check12 CIS1.2 Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password check13 CIS1.3 Ensure credentials unused for 90 days or greater are disabled check14 CIS1.4 Ensure access keys are rotated every 90 days or less check15 CIS1.5 Ensure IAM password policy requires at least one uppercase letter check16 CIS1.6 Ensure IAM password policy require at least one lowercase letter check17 CIS1.7 Ensure IAM password policy require at least one symbol check18 CIS1.8 Ensure IAM password policy require at least one number check19 CIS1.9 Ensure IAM password policy requires minimum length of 14 or greater check110 CIS1.10 Ensure IAM password policy prevents password reuse: 24 or greater check111 CIS1.11 Ensure IAM password policy expires passwords within 90 days or less check112 CIS1.12 Ensure no root account access key exists check113 CIS1.13 Ensure MFA is enabled for the root account check114 CIS1.14 Ensure hardware MFA is enabled for the root account check115 CIS1.15 Ensure security questions are registered in the AWS account check116 CIS1.16 Ensure IAM policies are attached only to groups or roles check117 CIS1.17 Maintain current contact details check118 CIS1.18 Ensure security contact information is registered check119 CIS1.19 Ensure IAM instance roles are used for AWS resource access from instances check120 CIS1.20 Ensure a support role has been created to manage incidents with AWS Support check121 CIS1.21 Do not setup access keys during initial user setup for all IAM users that have a console password check122 CIS 1.22 Ensure IAM policies that allow full \" : \" administrative privileges are not created","title":"Identity and Access Management checks"},{"location":"assess/assess-checks/#logging-checks","text":"Prowler Check ID Assess Check ID Description check21 CIS2.1 Ensure CloudTrail is enabled in all regions check22 CIS2.2 Ensure CloudTrail log file validation is enabled check23 CIS2.3 Ensure the S3 bucket CloudTrail logs to is not publicly accessible check24 CIS2.4 Ensure CloudTrail trails are integrated with CloudWatch Logs check25 CIS2.5 Ensure AWS Config is enabled in all regions check26 CIS2.6 Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket check27 CIS2.7 Ensure CloudTrail logs are encrypted at rest using KMS CMKs check28 CIS2.8 Ensure rotation for customer created CMKs is enabled check29 CIS2.9 Ensure VPC Flow Logging is Enabled in all VPCs","title":"Logging checks"},{"location":"assess/assess-checks/#monitoring-checks","text":"Prowler Check ID Assess Check ID Description check31 CIS3.1 Ensure a log metric filter and alarm exist for unauthorized API calls check32 CIS3.2 Ensure a log metric filter and alarm exist for Management Console sign-in without MFA check33 CIS3.3 Ensure a log metric filter and alarm exist for usage of root account check34 CIS3.4 Ensure a log metric filter and alarm exist for IAM policy changes check35 CIS3.5 Ensure a log metric filter and alarm exist for CloudTrail configuration changes check36 CIS3.6 Ensure a log metric filter and alarm exist for AWS Management Console authentication failures check37 CIS3.7 Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs check38 CIS3.8 Ensure a log metric filter and alarm exist for S3 bucket policy changes check39 CIS3.9 Ensure a log metric filter and alarm exist for AWS Config configuration changes check310 CIS3.10 Ensure a log metric filter and alarm exist for security group changes check311 CIS3.11 Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL) check312 CIS3.12 Ensure a log metric filter and alarm exist for changes to network gateways check313 CIS3.13 Ensure a log metric filter and alarm exist for route table changes check314 CIS3.14 Ensure a log metric filter and alarm exist for VPC changes","title":"Monitoring checks"},{"location":"assess/assess-checks/#networking-checks","text":"Prowler Check ID Assess Check ID Description check41 CIS4.1 Ensure no security groups allow ingress from 0.0.0.0/0 or ::/0 to port 22 check42 CIS4.2 Ensure no security groups allow ingress from 0.0.0.0/0 or ::/0 to port 3389 check43 CIS4.3 Ensure the default security group of every VPC restricts all traffic check44 CIS4.4 Ensure routing tables for VPC peering are \"least access\"","title":"Networking checks"},{"location":"assess/assess-checks/#hcap-assess-checks","text":"HCAP Assess is an inbuilt tool that can be used to run security, operations, and cost optimization assessments of your AWS cloud infrastructure. Assess Accelerator provides the following checks for the HCAP Assess tool. These checks are categorized under multiple policies in Assess Accelerator . Assess Accelerator Policy HCAP Assess Checks Security -- Identity and Access Management checks -- Logging checks -- Monitoring checks -- Networking checks -- Other checks Operations -- Ops checks -- Performance checks -- Fault Tolerance checks Cost Optimization -- Cost Optimization checks The check IDs in the HCAP Assess tool use one of the following prefixes: CIS (Centre for Internet Security): This prefix is used for checks that adhere to the CIS standard. Example: CIS1.1 and CIS1.2 CAP (Custom Assess Policies): This prefix is used for checks that are custom checks provided by the Assess Accelerator. The CAP prefix is available only with the HCAP Assess tool. Example: CAP4.2 and CAP4.3 Important: You must use the Assess Check IDs listed in the following tables while creating assessment templates for the HCAP Assess tool.","title":"HCAP Assess checks"},{"location":"assess/assess-checks/#identity-and-access-management-checks-security-policy","text":"Assess Check ID Description CIS1.1 Avoid the use of the \"root\" account CIS1.2 Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password CIS1.3A Disable all console passwords that are unused for 90 days or more CIS1.3B Disable all access keys that are unused for 90 days or more CIS1.4 Ensure access keys are rotated every 90 days or less CIS1.5 Ensure IAM password policy requires at least one uppercase letter CIS1.6 Ensure IAM password policy require at least one lowercase letter CIS1.7 Ensure IAM password policy require at least one symbol CIS1.8 Ensure IAM password policy require at least one number CIS1.9 Ensure IAM password policy requires minimum password length of 14 or greater CIS1.10 Ensure IAM password policy prevents password reuse CIS1.11 Ensure IAM password policy expires passwords within 90 days or less CIS1.12 Ensure no root account access key exists CIS1.13 Ensure MFA is enabled for the \"root\" account CIS1.16 Ensure IAM policies are attached only to groups or roles CIS1.19 Ensure IAM instance roles are used for AWS resource access from instances CIS1.22 Ensure IAM policies that allow full administrative privileges (i.e. \" : \") are not created CAP1.1 Verify that there are no IAM users without password CAP1.2 Verify that no IAM user have access keys","title":"Identity and Access Management checks (Security Policy)"},{"location":"assess/assess-checks/#logging-checks-security-policy","text":"Assess Check ID Description CIS2.1 Ensure CloudTrail is enabled in all regions CIS2.2 Ensure CloudTrail log file validation is enabled CIS2.3 Ensure the S3 bucket used to store CloudTrail logs is not publicly accessible CIS2.4 Ensure CloudTrail trails are integrated with CloudWatch Logs CIS2.5 Ensure AWS Config is enabled in all regions CIS2.6 Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket CIS2.7 Ensure CloudTrail logs are encrypted at rest using KMS CMKs CIS2.8 Ensure rotation for customer created CMKs is enabled CAP2.1 Verify that the Cloudtrail SNS topic is not missing CAP2.2 Verify that an alert is displayed if CloudTrail Global Service Events is not enabled CAP2.3 Verify that the SNS Notifications being sent from CloudTrail is not failing","title":"Logging checks (Security Policy)"},{"location":"assess/assess-checks/#monitoring-checks-security-policy","text":"Assess Check ID Description CIS3.1 Ensure a log metric filter and alarm exist for unauthorized API calls CIS3.2 Ensure a log metric filter and alarm exist for Management Console sign-in without MFA CIS3.3 Ensure a log metric filter and alarm exist for usage of \"root\" account CIS3.4 Ensure a log metric filter and alarm exist for IAM policy changes CIS3.5 Ensure a log metric filter and alarm exist for CloudTrail configuration changes CIS3.6 Ensure a log metric filter and alarm exist for AWS Management Console authentication failures CIS3.7 Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs CIS3.8 Ensure a log metric filter and alarm exist for S3 bucket policy changes CIS3.9 Ensure a log metric filter and alarm exist for AWS Config configuration changes CIS3.10 Ensure a log metric filter and alarm exist for security group changes CIS3.11 Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL) CIS3.12 Ensure a log metric filter and alarm exist for changes to network gateways CIS3.13 Ensure a log metric filter and alarm exist for route table changes CIS3.14 Ensure a log metric filter and alarm exist for VPC changes","title":"Monitoring checks (Security Policy)"},{"location":"assess/assess-checks/#networking-checks-security-policy","text":"Assess Check ID Description CIS4.1 Ensure no security groups allow ingress from 0.0.0.0/0 to port 22 CIS4.2 Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 CAP4.1 Ensure VPC flow logging is enabled in all VPCs CIS4.3 Ensure the default security group of every VPC restricts all traffic CIS4.4 Ensure routing tables for VPC peering are \"least access\" CAP4.2 Verify that Telnet protocol is denied in the NACL rules of VPC CAP4.3 Verify that POP3 protocol is denied in the NACL rules of VPC CAP4.4 Verify that IMAP protocol is denied in the NACL rules of VPC CAP4.5 Verify that ELB Back-end Authentication is enabled CAP4.6 Verify that Cross Zone Load Balancing is enabled CAP4.7 Verify that ELB Security Groups ports matches ELB Listener Configuration CAP4.8 Verify that ELB SSL configuration is enabled CAP4.9 Verify that ELB Cipher policies are the latest CAP4.10 Verify that no S3 buckets are public CAP4.11 Verify that no ICMP port is open to public CAP4.12 Verify that the RDS DB Instances are not running On default Ports","title":"Networking checks (Security Policy)"},{"location":"assess/assess-checks/#other-checks-security-policy","text":"Assess Check ID Description CAP5.1 Verify that DB servers are not Public CAP5.2 Verify that secondary volumes are encrypted CAP5.3 Verify that CloudFront SSL Certificate on the Origin Server is expired CAP5.4 Verify that CloudFront SSL Certificate on the Origin Server are about to expire","title":"Other checks (Security Policy)"},{"location":"assess/assess-checks/#ops-checks-operations-policy","text":"Assess Check ID Description CAP6.1 Verify that the use of generic user names are avoided for IAM Users CAP6.2 Verify that all IAM groups are associated with IAM Users CAP6.3 Verify that every Instance should have Name, Environment, Owner, CostCenter, ExpirationDate, Role, Application and Monitoring tags CAP6.4 Verify that S3 VPC Endpoints are configured for VPC CAP6.5 Verify that Cross-zone Load Balancing is enabled to ELB to experience better performance CAP6.6 Verify that AutoMinorVersionUpgrade is enabled for all RDS Instances CAP6.7 Verify that you are using new VPC for your servers. Default VPC is not a best practice CAP6.8 Verify that S3 Bucket life cycle rules are provided for log buckets CAP6.9 Verify that S3 bucket versioning is enabled for log buckets CAP6.10 Verify that Multi-AZ deployment is used for production RDS instances CAP6.11 Verify that for production RDS instances backups are enabled CAP6.12 Verify that there are no unused network interfaces CAP6.13 Verify that IAM-Instance-Profiles not used for more than 90 days are removed CAP6.14 Verify that unused IAM certificates for more than 90 days are removed CAP6.15 Verify that backups are taken of instances with Deregistered AMIs CAP6.16 Verify that instances are not launched with Public AMIs CAP6.17 Verify that data stored in RDS instances is encrypted CAP6.18 Verify that production ELBs attached with more than one EC2 instance CAP6.19 Verify that there are no unused route tables CAP6.20 Verify the failed state for RDS DB instances CAP6.21 Verify that the instances used in ELB are active with status as InService CAP6.22 Verify that instances in stopped state for more than two weeks are deleted CAP6.23 Verify IAM-Roles not used for more than 90 days are removed","title":"Ops checks (Operations Policy)"},{"location":"assess/assess-checks/#performance-checks-operations-policy","text":"Assess Check ID Description CAP7.1 Verify that provisioned IOPS volume is not attached to a non-EBS optimized instance CAP7.2 Verify that the CloudFront distributions having alternate domain names are resolvable CAP7.3 Verify that there are no security groups with more than 50 rules CAP7.4 Verify that all instances are enabled with termination protection","title":"Performance checks (Operations Policy)"},{"location":"assess/assess-checks/#fault-tolerance-checks-operations-policy","text":"Assess Check ID Description CAP8.1 Verify that Connection draining is enabled for ELB for better user experience","title":"Fault Tolerance checks (Operations Policy)"},{"location":"assess/assess-checks/#cost-optimization-checks-cost-optimization-policy","text":"Assess Check ID Description CAP9.1 Verify that reserved instance is used, if there is any EC2 instance running for more than 30 days CAP9.2 Verify that all auto-scaling launch configurations are associated with an auto-scaling group CAP9.3 Verify that there are no unused Elastic Load Balancers (ELBs) CAP9.4 Verify that there are no unused Elastic IPs (EIP) CAP9.5 Verify that there are no unused available EBS Volumes CAP9.6 Verify that there are no snapshots older than 90 days","title":"Cost Optimization checks (Cost Optimization Policy)"},{"location":"assess/assess-checks/#azure-cis-blueprint-checks","text":"Microsoft Azure assessments use CIS Microsoft Azure Foundations Benchmark 1.1.0 policy, which gets created by assigning CIS Microsoft Azure Foundations Benchmark blueprint. The Azure assessment checks that are included in Assess Accelerator adhere to the Centre for Internet Security (CIS) standard. Assess Accelerator provides the following checks for Azure assessments. These checks are categorized under the Security policy in Assess Accelerator . Identity and Access Management checks Security Center checks Storage Accounts checks Database Services checks Logging and Monitoring checks Networking checks Virtual Machines checks Other Security Applications checks AppService checks","title":"Azure CIS Blueprint checks"},{"location":"assess/assess-checks/#identity-and-access-management","text":"Check ID Description CIS1.1 Ensure that multi-factor authentication is enabled for all privileged users CIS1.2 Ensure that multi-factor authentication is enabled for all non-privileged users CIS1.3 Ensure that there are no guest users CIS1.23 Ensure that no custom subscription owner roles are created","title":"Identity and Access Management"},{"location":"assess/assess-checks/#security-center-checks","text":"Check ID Description CIS2.1 Ensure that standard pricing tier is selected CIS2.2 Ensure that 'Automatic provisioning of monitoring agent' is set to 'On' CIS2.3 Ensure ASC Default policy setting \"Monitor System Updates\" is not \"Disabled\" CIS2.4 Ensure ASC Default policy setting \"Monitor OS Vulnerabilities\" is not \"Disabled\" CIS2.5 Ensure ASC Default policy setting \"Monitor Endpoint Protection\" is not \"Disabled\" CIS2.6 Ensure ASC Default policy setting \"Monitor Disk Encryption\" is not \"Disabled\" CIS2.7 Ensure ASC Default policy setting \"Monitor Network Security Groups\" is not \"Disabled\" CIS2.9 Ensure ASC Default policy setting \"Enable Next Generation Firewall(NGFW) Monitoring\" is not \"Disabled\" CIS2.10 Ensure ASC Default policy setting \"Monitor Vulnerability Assessment\" is not \"Disabled\" CIS2.12 Ensure ASC Default policy setting \"Monitor JIT Network Access\" is not \"Disabled\" CIS2.13 Ensure ASC Default policy setting \"Monitor Adaptive Application Whitelisting\" is not \"Disabled\" CIS2.14 Ensure ASC Default policy setting \"Monitor SQL Auditing\" is not \"Disabled\" CIS2.15 Ensure ASC Default policy setting \"Monitor SQL Encryption\" is not \"Disabled\" CIS2.16 Ensure that 'Security contact emails' is set CIS2.18 Ensure that 'Send email notification for high severity alerts' is set to 'On' CIS2.19 Ensure that 'Send email also to subscription owners' is set to 'On'","title":"Security Center checks"},{"location":"assess/assess-checks/#storage-accounts-checks","text":"Check ID Description CIS3.1 Ensure that 'Secure transfer required' is set to 'Enabled' CIS3.6 Ensure that 'Public access level' is set to Private for blob containers CIS3.7 Ensure default network access rule for Storage Accounts is set to deny CIS3.8 Ensure 'Trusted Microsoft Services' is enabled for Storage Account access","title":"Storage Accounts checks"},{"location":"assess/assess-checks/#database-services-checks","text":"Check ID Description CIS4.1 Ensure that 'Auditing' is set to 'On' CIS4.2 Ensure that 'AuditActionGroups' in 'auditing' policy for a SQL server is set properly CIS4.3 Ensure that 'Auditing' Retention is 'greater than 90 days' CIS4.4 Ensure that 'Advanced Data Security' on a SQL server is set to 'On' CIS4.8 Ensure that Azure Active Directory Admin is configured CIS4.9 Ensure that 'Data encryption' is set to 'On' on a SQL Database CIS4.10 Ensure SQL server's TDE protector is encrypted with BYOK (Use your own key) CIS4.11 Ensure 'Enforce SSL connection' is set to 'ENABLED' for MySQL Database Server CIS4.12 Ensure server parameter 'log_checkpoints' is set to 'ON' for PostgreSQL Database Server CIS4.13 Ensure 'Enforce SSL connection' is set to 'ENABLED' for PostgreSQL Database Server CIS4.14 Ensure server parameter 'log_connections' is set to 'ON' for PostgreSQL Database Server CIS4.15 Ensure server parameter 'log_disconnections' is set to 'ON' for PostgreSQL Database Server CIS4.17 Ensure server parameter 'connection_throttling' is set to 'ON' for PostgreSQL Database Server","title":"Database Services checks"},{"location":"assess/assess-checks/#logging-and-monitoring-checks","text":"Check ID Description CIS 5.1.1 Ensure that a Log Profile exists CIS5.1.2 Ensure that Activity Log Retention is set 365 days or greater CIS5.1.3 Ensure audit profile captures all the activities CIS 5.1.4 Ensure the log profile captures activity logs for all regions including global CIS5.1.5 Ensure the storage container storing the activity logs is not publicly accessible CIS5.1.6 Ensure the storage account containing the container with activity logs is encrypted with BYOK (Use Your Own Key) CIS5.1.7 Ensure that logging for Azure KeyVault is 'Enabled' CIS5.2.1 Ensure that Activity Log Alert exists for Create Policy Assignment CIS5.2.2 Ensure that Activity Log Alert exists for Create or Update Network Security Group CIS5.2.3 Ensure that Activity Log Alert exists for Delete Network Security Group CIS5.2.4 Ensure that Activity Log Alert exists for Create or Update Network Security Group Rule CIS5.2.5 Ensure that activity log alert exists for the Delete Network Security Group Rule CIS5.2.6 Ensure that Activity Log Alert exists for Create or Update Security Solution CIS5.2.7 Ensure that Activity Log Alert exists for Delete Security Solution CIS5.2.8 Ensure that Activity Log Alert exists for Create or Update or Delete SQL Server Firewall Rule CIS5.2.9 Ensure that Activity Log Alert exists for Update Security Policy","title":"Logging and Monitoring checks"},{"location":"assess/assess-checks/#networking-checks_1","text":"Check ID Description CIS6.1 Ensure that RDP access is restricted from the internet CIS6.2 Ensure that SSH access is restricted from the internet CIS6.5 Ensure that Network Watcher is 'Enabled'","title":"Networking checks"},{"location":"assess/assess-checks/#virtual-machines-checks","text":"Check ID Description CIS7.1 Ensure that 'OS disk' are encrypted CIS7.2 Ensure that 'Data disks' are encrypted CIS7.3 Ensure that 'Unattached disks' are encrypted CIS7.4 Ensure that only approved extensions are installed CIS7.5 Ensure that the latest OS Patches for all Virtual Machines are applied CIS7.6 Ensure that the endpoint protection for all Virtual Machines is installed","title":"Virtual Machines checks"},{"location":"assess/assess-checks/#other-security-considerations-checks","text":"Check ID Description CIS8.1 Ensure that the expiration date is set on all keys CIS8.2 Ensure that the expiration date is set on all Secrets CIS8.4 Ensure the key vault is recoverable CIS8.5 Enable role-based access control (RBAC) within Azure Kubernetes Services","title":"Other Security Considerations checks"},{"location":"assess/assess-checks/#appservice-checks","text":"Check ID Description CIS9.1 Ensure App Service Authentication is set on Azure App Service CIS9.2 Ensure web app redirects all HTTP traffic to HTTPS in Azure App Service CIS9.3 Ensure web app is using the latest version of TLS encryption CIS9.4 Ensure the web app has 'Client Certificates (Incoming client certificates)' set to 'On' CIS9.5 Ensure that Register with Azure Active Directory is enabled on App Service CIS9.7 Ensure that 'PHP version' is the latest, if used to run the web app CIS9.8 Ensure that 'Python version' is the latest, if used to run the web app CIS9.9 Ensure that 'Java version' is the latest, if used to run the web app CIS9.10 Ensure that 'HTTP Version' is the latest, if used to run the web app","title":"AppService checks"},{"location":"assess/assess-templates/","text":"Create and manage assessment templates \u00b6 Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. This topic describes how to create and manage assessment templates, which are required to run assessment jobs . Contents \u00b6 Assessment templates Connecting to the Access Accelerator backend pod Creating and uploading assessment templates Viewing uploaded assessment templates Updating assessment templates Deleting assessment templates Assessment templates \u00b6 Assess Accelerator supports multiple tools that can be used for assessments of your cloud infrastructure. For each assessment tool, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. An assessment template is a JSON template that defines the configurations of an assessment tool. It contains the list of checks to use and to ignore. Based on your requirements, you can create multiple assessment templates with different sets of checks. The following sections help you to understand the structure and format of the assessment templates. Objects in an assessment template Format of an assessment template Assessment template for HCAP Assess Assessment template for Prowler Objects in an assessment template \u00b6 Parameter Type Description name String Unique name of the assessment template. This name will appear in the list of assessment templates on the Assess Accelerator UI. This name is also used in the Read|Update|Delete operations of assessment templates in Elasticsearch. Note: Template JSONs are pushed to Elasticsearch using the APIs. Make sure the templates have unique names. description String Short description of the assessment template. This description appears in the UI by placing the mouse pointer on the Information icon. cloud_provider_type String Cloud provider type specific to the assessment template and the configurations defined in it. assessment_framework_provider List The assessment tool that we want to associate with the assessment template. Enter one of the following values: - For Prowler tool: \" assessment.provider.prowler.2.2.0 \" - For HCAP Assess tool: \" assessment.provider.hcap.assess \" - For Azure CIS Blueprint tool: \" assessment.provider.azure_blueprint.1.1 \" data_collection_configuration List Each object in this list describes assessment tool-specific customization configurations required to be done in the Data Collection phase of Assess DataPipeline. These configurations (if specified) will override the default configurations defined by the \"assessment_framework_provider\" parameter. customization Object Tool specific customizations to specify either set of Rules to skip or set of Rules to execute. - Prowler checks - HCAP Assess checks The Azure CIS Blueprint tool does not currently support customization. rules_to_include List Tool specific list of IDs of the checks that are to be included in the assessment. rules_to_exclude List Tool specific list of IDs of the checks that are to be excluded from the assessment. Format of an assessment template \u00b6 { \"name\" : \"<assessment_template_unique_name>\" , \"description\" : \"Assessment template description\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [], \"data_collection_configuration\" :[ { \"image_name\" : \"<Image_Name>\" , \"image_tag\" : \"<Image_Tag>\" , \"customisation\" :{ \"rules_to_include\" :[ \"<check_ID1>\" , \"<check_ID2>\" ], \"rules_to_exclude\" :[ \"<check_ID1>\" , \"<check_ID2>\" ] } } ], } Notes If both **rules_to_include* and rules_to_exclude lists are empty, all the checks will be executed.* If some checks are specified in the **rules_to_include* list and the rules_to_exclude list is empty, only the checks that are specified in rules_to_include list will get executed.* If some checks are specified in the **rules_to_exclude* list and the rules_to_include list is empty, all the checks will be executed except the ones that are specified in rules_to_exclude list.* If some checks are specified in both **rules_to_include* and rules_to_exclude lists, the checks specified in rules_to_include list will be ignored. In this case, all the checks will be executed except the ones that are specified in rules_to_exclude list.* The **rule_IDs* must be same as mentioned in Prowler checks and HCAP Assess checks .* Assessment template for HCAP Assess \u00b6 The HCAP Assess tool includes checks for the Security, Operations, and Cost Optimization policies in Assess Accelerator. When users select an HCAP Assess template for an assessment job, Assess Accelerator uses the HCAP Assess tool to run the checks for all the selected policies in the job. The checks that are run are based on the configurations in the selected assessment template. For example, say a user selects the following sample assessment template and all policies (Security, Operations, and Cost Optimization) in an assessment job. In this case, Assess Accelerator will use the HCAP Assess tool to run all checks in the Operations and Cost Optimization policies but will run only the CIS1.1, CIS1.3A, and CIS1.5 checks in the Security policy. Sample assessment template for HCAP Assess { \"name\" : \"template.hcap.assess\" , \"description\" : \"Assessment template is HCAP specific with customisations configured.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.hcap.assess\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_include\" :[ \"CIS1.1\" , \"CIS1.3A\" , \"CIS1.5\" ], \"rules_to_exclude\" :[] } } ], } Assessment template for Prowler \u00b6 The Prowler tool includes checks for only the Security policy in Assess Accelerator. When users select a Prowler template for an assessment job, Assess Accelerator uses the Prowler tool to run the CIS checks in the Security policy. It uses the HCAP Assess tool to run the CAP checks in the Security policy and all checks in the Operations and Cost Optimization policies. The checks that are run are based on the configurations in the selected assessment template. For example, say a user selects the following sample assessment template and all policies (Security, Operations, and Cost Optimization) in an assessment job. In this case, Assess Accelerator will use the Prowler tool to run the check11, check12, and check13 checks in the Security policy and the HCAP Assess tool to run all checks in the Operations and Cost Optimization policies. Sample assessment template for Prowler { \"name\" : \"Prowler template\" , \"description\" : \"This Assessment Template is Prowler specific enforcing only check11, check12, check13 checks to execute.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.prowler.2.2.0\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"\" , \"image_name\" : \"toniblyx/prowler\" , \"image_tag\" : \"2.2.0\" , \"customisation\" :{ \"rules_to_include\" :[ \"check11\" , \"check12\" , \"check13\" ], \"rules_to_exclude\" :[] } } ], } Assessment template for Azure CIS Blueprint \u00b6 The Azure CIS Blueprint includes the checks for Security policy in Assess Accelerator. When you select Azure template for assessment, it runs all the checks in the Security policy for Azure. For the list of checks supported by Azure CIS blueprint, see Azure CIS Blueprint checks . Note : The features in the template like data_collection_configuration, data_curation_configuration, rules_to_exclude, and rules_to_include are currently not supported in this version. Sample assessment template for Azure CIS Blueprint { \"name\" : \"template.azure_blueprint.1.1\" , \"description\" : \"Assessment template description\" , \"cloud_provider_type\" : \"AZURE\" , \"assessment_framework_provider\" : [ \"assessment.provider.azure_blueprint.1.1\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"azure_blueprint\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_exclude\" :[], \"rules_to_include\" :[] } } ], \"data_curation_configuration\" :[ { \"assessment_tool\" : \"azure_blueprint\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_exclude\" :[], \"rules_to_include\" :[] } } ] } Connecting to the Assess Accelerator backend pod \u00b6 To manually add assessment templates, you have to upload a JSON file to the Elasticsearch pod in the Assess Accelerator kubernetes setup. To upload a JSON file, the first step is to connect to the Assess Accelerator backend pod ( reanassess-backend ). This step is also required if you want to later view, update, or delete the uploaded JSON file. Make sure that you have set up kube-config and are able to connect to the cluster from your local machine. To get the pod name of the Assess backend-service, use the following command: kubectl get pod -l app=reanassess-backend -o=Name Output : pod/<assess_backend_pod_name> Example Output Value: pod/reanassess-backend-78bcfcf47-mdfgr To connect to the Assess backend container, use the above pod name in the following command: kubectl exec --stdin --tty <assess_backend_pod_name> -- /bin/bash Creating and uploading assessment templates \u00b6 To manually add assessment templates, you have to upload a JSON file to the Elasticsearch pod in the Assess Accelerator kubernetes setup. Create a JSON document using the pre-defined structure and format for the assessment template. For detailed information, see Assessment templates . You must save the JSON document with the .json extension. Connect to the Assess Accelerator backend kubernetes pod . Upload the assessment template JSON file that you have created to Elasticsearch by using the following command. curl -XPOST \"<ES_Host>:9200/assessnow_custom/assessment_template?pretty\" -d @<JSON_file_name> <ES_Host> : Use the value reanassess-es . <JSON_file_name> : Use the exact file name of the JSON file. Expected Output { \"_index\" : \"assessnow_custom\", \"_type\" : \"assessment_template\", \"_id\" : \"<some_autogenerated_id>\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 3, \"successful\" : 1, \"failed\" : 0 }, \"created\" : true } Make a note of the auto-generated ID of the template. You will need the ID to view, update, or delete the assessment template. Alternatively, you can later search for the template, go through the output, and get the ID. Viewing uploaded assessment templates \u00b6 To view an assessment template that you had previously uploaded to Elasticsearch, connect to the Assess Accelerator backend kubernetes pod and use the following command. curl -XGET \"<ES_Host>:9200/assessnow_custom/assessment_template/_search?pretty\" -d '{\"query\": {\"match\": {\"name\": \"<assessment_template_unique_name>\"}}}' <ES_Host> : Use the value reanassess-es . <assessment_template_unique_name> : Use the name of the assessment template that you want to view. Expected Output { \"took\" : 2 , \"timed_out\" : false , \"_shards\" : { \"total\" : 5 , \"successful\" : 5 , \"failed\" : 0 }, \"hits\" : { \"total\" : 1 , \"max_score\" : 0.6931472 , \"hits\" : [ { \"_index\" : \"assessnow_custom\" , \"_type\" : \"assessment_template\" , \"_id\" : \"<some_autogenerated_id>\" , \"_score\" : 0.6931472 , \"_source\" : { \"name\" : \"<assessment_template_unique_name>\" , \"description\" : \"Description of the assessment template.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.hcap.assess.python\" ], \"data_collection_configuration\" : [ { \"assessment_tool\" : \"hcap.assess\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" : { \"rules_to_include\" : [ ], \"rules_to_exclude\" : [ ] } } ], \"data_curation_configuration\" : [ ] } } ] } } Notes The actual contents of the assessment template are within the * \"_source\"** field.* The results of the search query can return multiple templates since it returns all the matches in the keyword. Note the * _id** of the template to delete or update it later.* Updating assessment templates \u00b6 Connect to the Assess Accelerator backend kubernetes pod . If you have not previously noted the ID of the assessment template you want to update, use the command to view the assessment template . To update the assessment template, use the following command. curl -XPOST \"<ES_Host>:9200/assessnow_custom/assessment_template/<assessment_template_id>?pretty\"-d @<updated_JSON_file_name> <ES_Host> : Use the value reanassess-es . <assessment_template_id> : From the output of the view command, use the value of the \"_id\" field. <updated_JSON_file_name> : Use the exact file name of the JSON file. Expected Output { \"_index\" : \"assessnow_custom\" , \"_type\" : \"assessment_template\" , \"_id\" : \"default_assessment_template\" , \"_version\" : 2 , \"result\" : \"updated\" , \"_shards\" : { \"total\" : 3 , \"successful\" : 1 , \"failed\" : 0 }, \"created\" : false } Deleting assessment templates \u00b6 Connect to the Assess Accelerator backend kubernetes pod . If you have not previously noted the ID of the assessment template you want to delete, use the command to view the assessment template . To delete the assessment template, use the following command. curl -XDELETE \"<ES_Host>:9200/assessnow_custom/assessment_template/<assessment_template_id>\" <ES_Host> : Use the value reanassess-es . <assessment_template_id> : From the output of the view command, use the value of the \"_id\" field. Expected Output { \"took\" : 21 , \"timed_out\" : false , \"total\" : 1 , \"deleted\" : 1 , \"batches\" : 1 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [ ] }","title":"Create and manage assessment templates"},{"location":"assess/assess-templates/#create-and-manage-assessment-templates","text":"Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. This topic describes how to create and manage assessment templates, which are required to run assessment jobs .","title":"Create and manage assessment templates"},{"location":"assess/assess-templates/#contents","text":"Assessment templates Connecting to the Access Accelerator backend pod Creating and uploading assessment templates Viewing uploaded assessment templates Updating assessment templates Deleting assessment templates","title":"Contents"},{"location":"assess/assess-templates/#assessment-templates","text":"Assess Accelerator supports multiple tools that can be used for assessments of your cloud infrastructure. For each assessment tool, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. An assessment template is a JSON template that defines the configurations of an assessment tool. It contains the list of checks to use and to ignore. Based on your requirements, you can create multiple assessment templates with different sets of checks. The following sections help you to understand the structure and format of the assessment templates. Objects in an assessment template Format of an assessment template Assessment template for HCAP Assess Assessment template for Prowler","title":"Assessment templates"},{"location":"assess/assess-templates/#objects-in-an-assessment-template","text":"Parameter Type Description name String Unique name of the assessment template. This name will appear in the list of assessment templates on the Assess Accelerator UI. This name is also used in the Read|Update|Delete operations of assessment templates in Elasticsearch. Note: Template JSONs are pushed to Elasticsearch using the APIs. Make sure the templates have unique names. description String Short description of the assessment template. This description appears in the UI by placing the mouse pointer on the Information icon. cloud_provider_type String Cloud provider type specific to the assessment template and the configurations defined in it. assessment_framework_provider List The assessment tool that we want to associate with the assessment template. Enter one of the following values: - For Prowler tool: \" assessment.provider.prowler.2.2.0 \" - For HCAP Assess tool: \" assessment.provider.hcap.assess \" - For Azure CIS Blueprint tool: \" assessment.provider.azure_blueprint.1.1 \" data_collection_configuration List Each object in this list describes assessment tool-specific customization configurations required to be done in the Data Collection phase of Assess DataPipeline. These configurations (if specified) will override the default configurations defined by the \"assessment_framework_provider\" parameter. customization Object Tool specific customizations to specify either set of Rules to skip or set of Rules to execute. - Prowler checks - HCAP Assess checks The Azure CIS Blueprint tool does not currently support customization. rules_to_include List Tool specific list of IDs of the checks that are to be included in the assessment. rules_to_exclude List Tool specific list of IDs of the checks that are to be excluded from the assessment.","title":"Objects in an assessment template"},{"location":"assess/assess-templates/#format-of-an-assessment-template","text":"{ \"name\" : \"<assessment_template_unique_name>\" , \"description\" : \"Assessment template description\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [], \"data_collection_configuration\" :[ { \"image_name\" : \"<Image_Name>\" , \"image_tag\" : \"<Image_Tag>\" , \"customisation\" :{ \"rules_to_include\" :[ \"<check_ID1>\" , \"<check_ID2>\" ], \"rules_to_exclude\" :[ \"<check_ID1>\" , \"<check_ID2>\" ] } } ], } Notes If both **rules_to_include* and rules_to_exclude lists are empty, all the checks will be executed.* If some checks are specified in the **rules_to_include* list and the rules_to_exclude list is empty, only the checks that are specified in rules_to_include list will get executed.* If some checks are specified in the **rules_to_exclude* list and the rules_to_include list is empty, all the checks will be executed except the ones that are specified in rules_to_exclude list.* If some checks are specified in both **rules_to_include* and rules_to_exclude lists, the checks specified in rules_to_include list will be ignored. In this case, all the checks will be executed except the ones that are specified in rules_to_exclude list.* The **rule_IDs* must be same as mentioned in Prowler checks and HCAP Assess checks .*","title":"Format of an assessment template"},{"location":"assess/assess-templates/#assessment-template-for-hcap-assess","text":"The HCAP Assess tool includes checks for the Security, Operations, and Cost Optimization policies in Assess Accelerator. When users select an HCAP Assess template for an assessment job, Assess Accelerator uses the HCAP Assess tool to run the checks for all the selected policies in the job. The checks that are run are based on the configurations in the selected assessment template. For example, say a user selects the following sample assessment template and all policies (Security, Operations, and Cost Optimization) in an assessment job. In this case, Assess Accelerator will use the HCAP Assess tool to run all checks in the Operations and Cost Optimization policies but will run only the CIS1.1, CIS1.3A, and CIS1.5 checks in the Security policy. Sample assessment template for HCAP Assess { \"name\" : \"template.hcap.assess\" , \"description\" : \"Assessment template is HCAP specific with customisations configured.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.hcap.assess\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_include\" :[ \"CIS1.1\" , \"CIS1.3A\" , \"CIS1.5\" ], \"rules_to_exclude\" :[] } } ], }","title":"Assessment template for HCAP Assess"},{"location":"assess/assess-templates/#assessment-template-for-prowler","text":"The Prowler tool includes checks for only the Security policy in Assess Accelerator. When users select a Prowler template for an assessment job, Assess Accelerator uses the Prowler tool to run the CIS checks in the Security policy. It uses the HCAP Assess tool to run the CAP checks in the Security policy and all checks in the Operations and Cost Optimization policies. The checks that are run are based on the configurations in the selected assessment template. For example, say a user selects the following sample assessment template and all policies (Security, Operations, and Cost Optimization) in an assessment job. In this case, Assess Accelerator will use the Prowler tool to run the check11, check12, and check13 checks in the Security policy and the HCAP Assess tool to run all checks in the Operations and Cost Optimization policies. Sample assessment template for Prowler { \"name\" : \"Prowler template\" , \"description\" : \"This Assessment Template is Prowler specific enforcing only check11, check12, check13 checks to execute.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.prowler.2.2.0\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"\" , \"image_name\" : \"toniblyx/prowler\" , \"image_tag\" : \"2.2.0\" , \"customisation\" :{ \"rules_to_include\" :[ \"check11\" , \"check12\" , \"check13\" ], \"rules_to_exclude\" :[] } } ], }","title":"Assessment template for Prowler"},{"location":"assess/assess-templates/#assessment-template-for-azure-cis-blueprint","text":"The Azure CIS Blueprint includes the checks for Security policy in Assess Accelerator. When you select Azure template for assessment, it runs all the checks in the Security policy for Azure. For the list of checks supported by Azure CIS blueprint, see Azure CIS Blueprint checks . Note : The features in the template like data_collection_configuration, data_curation_configuration, rules_to_exclude, and rules_to_include are currently not supported in this version. Sample assessment template for Azure CIS Blueprint { \"name\" : \"template.azure_blueprint.1.1\" , \"description\" : \"Assessment template description\" , \"cloud_provider_type\" : \"AZURE\" , \"assessment_framework_provider\" : [ \"assessment.provider.azure_blueprint.1.1\" ], \"data_collection_configuration\" :[ { \"assessment_tool\" : \"azure_blueprint\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_exclude\" :[], \"rules_to_include\" :[] } } ], \"data_curation_configuration\" :[ { \"assessment_tool\" : \"azure_blueprint\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" :{ \"rules_to_exclude\" :[], \"rules_to_include\" :[] } } ] }","title":"Assessment template for Azure CIS Blueprint"},{"location":"assess/assess-templates/#connecting-to-the-assess-accelerator-backend-pod","text":"To manually add assessment templates, you have to upload a JSON file to the Elasticsearch pod in the Assess Accelerator kubernetes setup. To upload a JSON file, the first step is to connect to the Assess Accelerator backend pod ( reanassess-backend ). This step is also required if you want to later view, update, or delete the uploaded JSON file. Make sure that you have set up kube-config and are able to connect to the cluster from your local machine. To get the pod name of the Assess backend-service, use the following command: kubectl get pod -l app=reanassess-backend -o=Name Output : pod/<assess_backend_pod_name> Example Output Value: pod/reanassess-backend-78bcfcf47-mdfgr To connect to the Assess backend container, use the above pod name in the following command: kubectl exec --stdin --tty <assess_backend_pod_name> -- /bin/bash","title":"Connecting to the Assess Accelerator backend pod"},{"location":"assess/assess-templates/#creating-and-uploading-assessment-templates","text":"To manually add assessment templates, you have to upload a JSON file to the Elasticsearch pod in the Assess Accelerator kubernetes setup. Create a JSON document using the pre-defined structure and format for the assessment template. For detailed information, see Assessment templates . You must save the JSON document with the .json extension. Connect to the Assess Accelerator backend kubernetes pod . Upload the assessment template JSON file that you have created to Elasticsearch by using the following command. curl -XPOST \"<ES_Host>:9200/assessnow_custom/assessment_template?pretty\" -d @<JSON_file_name> <ES_Host> : Use the value reanassess-es . <JSON_file_name> : Use the exact file name of the JSON file. Expected Output { \"_index\" : \"assessnow_custom\", \"_type\" : \"assessment_template\", \"_id\" : \"<some_autogenerated_id>\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 3, \"successful\" : 1, \"failed\" : 0 }, \"created\" : true } Make a note of the auto-generated ID of the template. You will need the ID to view, update, or delete the assessment template. Alternatively, you can later search for the template, go through the output, and get the ID.","title":"Creating and uploading assessment templates"},{"location":"assess/assess-templates/#viewing-uploaded-assessment-templates","text":"To view an assessment template that you had previously uploaded to Elasticsearch, connect to the Assess Accelerator backend kubernetes pod and use the following command. curl -XGET \"<ES_Host>:9200/assessnow_custom/assessment_template/_search?pretty\" -d '{\"query\": {\"match\": {\"name\": \"<assessment_template_unique_name>\"}}}' <ES_Host> : Use the value reanassess-es . <assessment_template_unique_name> : Use the name of the assessment template that you want to view. Expected Output { \"took\" : 2 , \"timed_out\" : false , \"_shards\" : { \"total\" : 5 , \"successful\" : 5 , \"failed\" : 0 }, \"hits\" : { \"total\" : 1 , \"max_score\" : 0.6931472 , \"hits\" : [ { \"_index\" : \"assessnow_custom\" , \"_type\" : \"assessment_template\" , \"_id\" : \"<some_autogenerated_id>\" , \"_score\" : 0.6931472 , \"_source\" : { \"name\" : \"<assessment_template_unique_name>\" , \"description\" : \"Description of the assessment template.\" , \"cloud_provider_type\" : \"AWS\" , \"assessment_framework_provider\" : [ \"assessment.provider.hcap.assess.python\" ], \"data_collection_configuration\" : [ { \"assessment_tool\" : \"hcap.assess\" , \"image_name\" : \"\" , \"image_tag\" : \"\" , \"customisation\" : { \"rules_to_include\" : [ ], \"rules_to_exclude\" : [ ] } } ], \"data_curation_configuration\" : [ ] } } ] } } Notes The actual contents of the assessment template are within the * \"_source\"** field.* The results of the search query can return multiple templates since it returns all the matches in the keyword. Note the * _id** of the template to delete or update it later.*","title":"Viewing uploaded assessment templates"},{"location":"assess/assess-templates/#updating-assessment-templates","text":"Connect to the Assess Accelerator backend kubernetes pod . If you have not previously noted the ID of the assessment template you want to update, use the command to view the assessment template . To update the assessment template, use the following command. curl -XPOST \"<ES_Host>:9200/assessnow_custom/assessment_template/<assessment_template_id>?pretty\"-d @<updated_JSON_file_name> <ES_Host> : Use the value reanassess-es . <assessment_template_id> : From the output of the view command, use the value of the \"_id\" field. <updated_JSON_file_name> : Use the exact file name of the JSON file. Expected Output { \"_index\" : \"assessnow_custom\" , \"_type\" : \"assessment_template\" , \"_id\" : \"default_assessment_template\" , \"_version\" : 2 , \"result\" : \"updated\" , \"_shards\" : { \"total\" : 3 , \"successful\" : 1 , \"failed\" : 0 }, \"created\" : false }","title":"Updating assessment templates"},{"location":"assess/assess-templates/#deleting-assessment-templates","text":"Connect to the Assess Accelerator backend kubernetes pod . If you have not previously noted the ID of the assessment template you want to delete, use the command to view the assessment template . To delete the assessment template, use the following command. curl -XDELETE \"<ES_Host>:9200/assessnow_custom/assessment_template/<assessment_template_id>\" <ES_Host> : Use the value reanassess-es . <assessment_template_id> : From the output of the view command, use the value of the \"_id\" field. Expected Output { \"took\" : 21 , \"timed_out\" : false , \"total\" : 1 , \"deleted\" : 1 , \"batches\" : 1 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [ ] }","title":"Deleting assessment  templates"},{"location":"assess/getting-started/","text":"Overview of Assess Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against relevant security and compliance standards. It contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards and requirements for security, operations, and cost-optimization. Assess Accelerator supports the following assessment tools: - HCAP Assess: This is an inbuilt tool, which can be used for security, operations, and cost optimization assessments of AWS cloud infrastructure. - Prowler: This is an open-source third-party tool, which can be used for security assessments of AWS cloud infrastructure. - Azure CIS Blueprint : This tool is an Azure service which uses an Azure blueprint to execute CIS checks on Azure cloud accounts. This tool is used for running security assessments of Azure cloud infrastructure. For each assessment tool, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. These checks are categorized under the out-of-the-box policies in Assess Accelerator. You can create an assessment template for each tool and add all or some of the supported checks in the template. Based on your requirements, you can create multiple assessment templates with different sets of checks. Assess Accelerator enables you to create an assessment job for one or more providers (or accounts) by selecting the appropriate assessment template and policies. The assessment report that Assess Accelerator generates for each provider, enables you to take action and fix any security and compliance issues in your environment. You can also generate a report that compares the assessment results across multiple jobs or providers. For more information, see Run assessment policies and Create and manage assessment templates . The following image shows the Home page of Assess Accelerator:","title":"Overview"},{"location":"assess/getting-started/#overview-of-assess-accelerator","text":"Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against relevant security and compliance standards. It contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards and requirements for security, operations, and cost-optimization. Assess Accelerator supports the following assessment tools: - HCAP Assess: This is an inbuilt tool, which can be used for security, operations, and cost optimization assessments of AWS cloud infrastructure. - Prowler: This is an open-source third-party tool, which can be used for security assessments of AWS cloud infrastructure. - Azure CIS Blueprint : This tool is an Azure service which uses an Azure blueprint to execute CIS checks on Azure cloud accounts. This tool is used for running security assessments of Azure cloud infrastructure. For each assessment tool, Assess Accelerator provides a set of checks against which the cloud accounts are evaluated. These checks are categorized under the out-of-the-box policies in Assess Accelerator. You can create an assessment template for each tool and add all or some of the supported checks in the template. Based on your requirements, you can create multiple assessment templates with different sets of checks. Assess Accelerator enables you to create an assessment job for one or more providers (or accounts) by selecting the appropriate assessment template and policies. The assessment report that Assess Accelerator generates for each provider, enables you to take action and fix any security and compliance issues in your environment. You can also generate a report that compares the assessment results across multiple jobs or providers. For more information, see Run assessment policies and Create and manage assessment templates . The following image shows the Home page of Assess Accelerator:","title":"Overview of Assess Accelerator"},{"location":"assess/using/","text":"Run assessment policies \u00b6 Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. This topic describes how you can use Assess Accelerator to run assessment jobs and view or download assessment reports. Contents \u00b6 Get started Overview of assessment policies Accessing Assess Accelerator Run assessment jobs Prerequisites for AWS Assessments Prerequisites for Azure Assessments Creating AWS assessment jobs Creating Azure assessment jobs View and download reports Viewing assessment reports Downloading assessment reports Viewing and downloading comparison reports Overview of assessment policies \u00b6 Assess Accelerator contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards based on different checks. Assessment reports that are generated provide the status for each assessment check and possible solutions for resolving issues. Assess Accelerator supports multiple tools that can be used for assessments of your environment. For each assessment tool, Assess Accelerator provides a specific set of checks. These checks are categorized under the assessment policies. For more information, see Assessment Checks Reference . The following table describes the out-of-the-box policies that are available to assess your cloud infrastructure. Policy name Description Cost Optimization Policy This policy enables you to assess how optimally you are managing the cost of your cloud infrastructure. It basically assesses cost optimization for your Amazon Web Services (AWS) environment by checking for unused Amazon Machine Images (AMIs), Snapshots, route tables, Elastic Load Balancers (ELBs), security groups, network interfaces, and much more. Operations Policy This policy enables you to assess the operational efficiency of your AWS environment based on best practices. It evaluates your environment based on criteria such as creation of your own VPC, data encryption, creation of the required tags for instances, and use of appropriate IAM user names. Security Policy This policy enables you to assess the security of your AWS and Azure environment based on industry standards and best practices. It consists of security assessment for network compliance, monitoring compliance, logging compliance, and IAM compliance. This policy is based on the best-practice security guidelines developed by the Center for Internet Security (CIS) for Amazon Web Services and Microsoft Azure environments. Accessing Assess Accelerator \u00b6 Assess Accelerator is deployed as part of Hitachi Cloud Accelerator Platform. Sign in to Hitachi Cloud Accelerator Platform . For information on creating a Hitachi Cloud Accelerator Platform account, see Create & access account . Click the Module selector icon ( ) in the top-left corner. The Assess Accelerator home page appears. Note: You can access Assess Accelerator and perform various actions only if your Hitachi Cloud Accelerator Platform administrator has granted you the appropriate permissions. Prerequisites for AWS Assessments \u00b6 The prerequisites for AWS assessment includes creating a provider. Following are the prerequisites you need to complete before running assessments for AWS: Creating IAM user and role with required permission Configuring a provider Creating IAM user and role with required permission \u00b6 Creating AWS user \u00b6 Before configuring a provider, you must create an Identity and Access Management (IAM) user or role with permissions to run the different assessment checks on your AWS infrastructure. Download the following custom inline policies: rean-assess-aws-iam-policy-1.json rean-assess-aws-iam-policy-2.json rean-assess-aws-iam-policy-3.json In the AWS account where you want to run the assessment policies, perform the following actions: Create IAM policies by using the custom inline policies that you have downloaded. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Important: To enable Assess Accelerator to upload assessment reports to an S3 bucket, you must add the s3:PutObject permission to one of the IAM policies. To enable Assess Accelerator to create a new S3 bucket, you must also add the s3:CreateBucket permission, as shown in the following example: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:CreateBucket\" ], \"Resource\": \"<s3_bucket_ARN>/*\" } ] } If you plan to use the Instance Profile with Assume Role or Static Credentials with Assume Role method while configuring a provider , perform the following actions: Create an IAM role that Assess Accelerator can assume to run the assessment policies. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Attach the IAM policies that you have created to this role. While creating the AWS provider, you must specify this IAM role. If you plan to use the Static Credentials method while configuring a provider , perform the following actions: Create an IAM group and attach the IAM policies that you have created to this group. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Create an IAM user and add the user to the IAM group that you have created. For step-by-step instructions, see the AWS Identity and Access Management User Guide . While creating the AWS provider, you must provide credentials of this IAM user. Important* : Make sure that the IAM user/role does not have administrator access. As per AWS CIS Foundations Benchmark the use of administrator access is highly discouraged. This is applicable for both Instance Profile and Static Credentials.* Creating Kubernetes user \u00b6 Following are the steps to create a provider for instance profile when Assess is running on EKS. Before configuring a provider, you must create an Identity and Access Management (IAM) user or role with permissions. Download the following custom inline policies: rean-assess-aws-iam-policy-1.json rean-assess-aws-iam-policy-2.json rean-assess-aws-iam-policy-3.json Create an IAM role specific to Assess pod. In the AWS account where you want to run the assessment policies, perform the following actions: Create an IAM Role (or use an existing role with the required name) for the Assess pod specific IAM Role**.** For the IAM role, you can enter the name as hcap-assess-iam-role , or use a custom name specified in the customer.yml file before deploying Assess. The custom name must have the value \" assess_pods_iam_role_name \". Once the Assess is deployed, make sure to use only this Role for various Instance profile use-cases. For different use-cases, you can change the policies attached to this Role as per requirement. After creating a new Role, select Type of trusted entity as AWS Service - ec2 . Add EKS worker node's IAM Role in the Trust relationships for the above role: Under the Role, go to \" Trust relationships \" tab and click on \" Edit trust relationships \". Replace the policy JSON with the following JSON. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"<EKS_Worker_ec2_Node_IAM_Role_ARN>\" }, \"Action\": \"sts:AssumeRole\" } ] } For , use the Role ARN of the IAM Role attached to the EKS Worker node ec2 instance, where Assess is running. Save changes by clicking on \" Update Trust Policy \". Attach the IAM policies to the above created role. To create a provider with an instance profile without name use the following JSON: { \"region\": \"<AWS_Region>\" } To create a provider with an instance profile without name with assume role use the following JSON: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"sts:DecodeAuthorizationMessage\", \"sts:GetCallerIdentity\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"sts:*\", \"Resource\": \"<Assume_Role_ARN>\" } ] } Configuring a provider \u00b6 Assess Accelerator uses the AWS providers that are available in Deploy Accelerator. Open Deploy Accelerator, on the home page, click the More options ( ) icon in the top-right corner and then click Providers . On the Provider page, click New . Enter the provider name and select the AWS provider type. In the Provider Details section, use one of the following methods to specify authentication details of the AWS account in which Assess Accelerator must run the assessment policies. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Hitachi Cloud Accelerator Platform is deployed in an AWS account that is different from the accounts in which Assess Accelerator must run the assessment policies. Instead of storing the access credentials for all those accounts in Assess Accelerator, you can attach a role to the instance in which Hitachi Cloud Accelerator Platform is deployed. Assess Accelerator can then assume a role in the other accounts and run the assessment policies. For more information about assuming roles, see the AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Hitachi Cloud Accelerator Platform is deployed. For the other accounts, you must specify a role that Assess Accelerator can assume to run the assessment policies. The role that you specify must have permissions to run the different assessment checks on the AWS infrastructure. It must also define the account in which Hitachi Cloud Accelerator Platform is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\": \"xx-xxxx-x\", \"assume_role\": { \"role_arn\": \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\", \"session_name\": \"SESSION-NAME\", \"external_id\": \"assume_role\" } } Instance Profile This method provides a more secure way of accessing the account in which Assess Accelerator must run the assessment policies. However, Assess Accelerator can use this method only if a role is attached to the instance on which Hitachi Cloud Accelerator Platform is deployed. This role must have permissions to run the different assessment checks on the AWS infrastructure. To use the Instance Profile method, you must specify only the region in which the assessment policies must be run, as shown in the following example: { \"region\": \"xx-xxx-x\" } Static Credentials with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Assess Accelerator needs to run the assessment policies across multiple AWS accounts. Instead of storing the access credentials for all these accounts in Assess Accelerator, you can specify long-term access credentials for only the parent account. Assess Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Assess Accelerator can assume to run the assessment policies in that account. The role that you specify must have access to run the different assessment checks on the AWS infrastructure. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\": \"xxxxxxxxxx\", \"secret_key\": \"xxxxxxxxxx\", \"region\": \"xx-xxxx-x\", \"assume_role\": { \"role_arn\": \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\", \"session_name\": \"SESSION-NAME\", \"external_id\": \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Assess Accelerator must run the assessment policies, as shown in the following example: { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECRET-KEY\", \"region\": \"xx-xxxx-x\" } Note: The IAM user whose credentials you specify must have permissions to run the different assessment checks on the AWS infrastructure. For more information, see Before you begin . To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click Save . A new provider appears in the Provider List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details. Prerequisites for Azure assessments \u00b6 The Azure assessment in Assess Accelerator is executed by the Microsoft Azure blueprint service. The blueprint used for Azure assessment is a sample of the CIS Microsoft Azure Foundations Benchmark blueprint. For more details see, Microsoft Azure Documentation . The prerequisite steps must be performed in the Azure account on which the assessment is to be run. The prerequisite involves the following steps: Deploy the Azure Blueprints CIS Microsoft Azure Foundations Benchmark blueprint sample Copy CIS Benchmark policy ID Deploy the Azure Blueprints CIS Microsoft Azure Foundations Benchmark blueprint sample \u00b6 The deployment steps include the following three steps: Create a new blueprint from the sample Mark your copy of the sample as Published Assign your copy of the blueprint to an existing subscription For more information on how to deploy the sample blueprint, see Microsoft Azure Documentation . Copy CIS Benchmark policy ID \u00b6 After a successful assignment of the blueprint, you must copy the policy ID for using while running the assessment job. To get the policy ID, perform the following actions: In the Azure account, go to Policy Service . Select CIS Microsoft Azure Foundations Benchmark 1.1.0 policy From the Assignment ID parameter, click Copy to clipboard. Save the last section of the assignment ID. For example, if complete policy assignment id is /subscriptions/ /providers/Microsoft.Authorization/policyAssignments/qwertyuiop1234567 then policy assignment ID to be saved is qwertyuiop1234567 . Creating AWS assessment jobs \u00b6 On the Home page of Assess Accelerator, select Assess Now . For Select your Cloud type , select AWS . Enter a name for the assessment job. Tip: You might run different types of assessment policies for one or more providers multiple times. Therefore, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. In the Select Providers list, perform one of the following actions: To run the assessment policies for one or more providers, select the check boxes next to those providers. To run the assessment policies for all providers, select All . Note: The assessment policies are run on all resources that are owned by the account that is specified in the provider. However, the account credentials that are specified must have the required access to run these policies on the resources. Otherwise, no data or incomplete data is displayed in the summary and detailed reports. In the Select Assessment Template list, select a template for assessment. You can create multiple templates with different checks for each template. For more information, see Creating and managing assessment templates . Select the assessment policies that you want to run for the selected providers. For information about the available policies, see Overview of assessment policies . (Optional) To upload the assessment report for each provider to an AWS S3 bucket, perform the following actions: Select the Upload report to S3 check box. Enter the S3 bucket name. For each selected provider, Assess Accelerator appends this S3 bucket name with the account ID ( BucketName-AccountID ). If the BucketName-AccountID bucket already exists for a provider, Assess Accelerator uploads the assessment reports to this existing bucket. Otherwise, Assess Accelerator creates a new S3 bucket and then uploads the assessment report to this bucket. Important: To enable Assess Accelerator to upload assessment reports to an S3 bucket, you must add the s3:PutObject permission to the IAM policies that are used in each selected provider. To enable Assess Accelerator to create a new S3 bucket, you must also add the s3:CreateBucket permission, as shown in the following example: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:CreateBucket\" ], \"Resource\": \"<s3_bucket_ARN>/*\" } ] } (Optional) To receive email notifications for assessments, perform the following actions: Select Enable Notification . In the Customer Name field, enter your company name and enter your email address that was used for creating the login credentials. You will receive an assessment start email and an email with a detailed report after assessment completion. You can also enter multiple email addresses separated by a comma. The email addresses apart from the logged in user will only receive an assessment start email. Click START ASSESSMENT . The Assessment Started message appears. Notes: If the user has administrator access inherited from the IAM user in AWS, a validation message appears asking if you want to proceed with the assessment. You can choose to continue or cancel the assessment and check t he policies assigned to the IAM user. If the user is missing some required permissions, a validation message appears with a list of missing permissions. You can choose to continue or cancel the assessment and check the permissions assigned to the IAM user. To start another assessment, click ASSESS MORE . To view the progress of the assessment, click VIEW PROGRESS . Under the My Assessment tab, click Refresh ( ) to view the recently started assessment. If an assessment job contains multiple providers, Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . Following are the columns: Provider : provider name Job Name : job name for the provider. Group : assessment job name that you have specified. The status icons indicate whether the job is in progress ( ), has failed ( ), or has completed successfully ( ). Place mouse-pointer over the status icons to see the job start date and time. Creating Azure assessment jobs \u00b6 On the Home page of Assess Accelerator, select Assess Now . For Select your Cloud type , select Azure . Enter a name for the assessment job. Tip: You might run different types of assessment policies for one or more providers multiple times. Therefore, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. In the Select Providers list, select a provider for running the assessment on that account. For Azure assessment job, you can currently select only one provider. Note: The assessment policies are run on all resources that are owned by the account that is specified in the provider. However, the account credentials that are specified must have the required access to run these policies on the resources. Otherwise, no data or incomplete data is displayed in the summary and detailed reports. In the Select Assessment Template list, select a template for assessment. You can create multiple templates with different checks for each template. For more information, see Creating and managing assessment templates . In the Azure CIS Policy Assignment ID field, enter the assignment ID acquired by performing the prerequisite steps. For more information, see Prerequisites for Azure assessments . Select Security Policy . For information about the available policies, see Overview of assessment policies . (Optional) To receive email notifications for assessments, perform the following actions: Select Enable Notification . In the Customer Name field, enter your company name and enter your email address that was used for creating the login credentials. You will receive an assessment start email and an email with a detailed report after assessment completion. You can also enter multiple email addresses separated by a comma. The email addresses apart from the logged in user will only receive an assessment start email. Click START ASSESSMENT . The Assessment Started message appears. To start another assessment, click ASSESS MORE . To view the progress of the assessment, click VIEW PROGRESS . Under the My Assessment tab, click Refresh ( ) to view the recently started assessment. If an assessment job contains multiple providers, Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . Following are the columns: Provider : provider name Job Name : job name for the provider Group : assessment job name that you have specified. The status icons indicate whether the job is in progress ( ), has failed ( ), or has completed successfully ( ). Place mouse-pointer over the status icons to see the job start date and time. Viewing assessment reports \u00b6 On the Home page, select My Assessment . From the list of assessment jobs, select the job for which you want to view a report. In the right panel, you can see a sunburst chart that provides a graphical summary of the policies that were run for the selected job, along with their status. The sunburst chart consists of the following layers: The center layer provides a summary of the assessment job. You can see the total number of checks that were run and the number of checks that failed. The second layer represents the policies that were run for the selected job. For example, security policy, cost optimization policy, and operations policy. The third layer represents different categories of assessment checks within each policy. For example, Security Assessment for Monitoring Compliance is a category of assessment check within the security policy. The outermost layer represents each check that is included in a policy. For example, \"Ensure no S3 buckets are public\" is a specific check within the security policy. To drill down into a specific set of data, click on a layer in the sunburst chart. When you click on the chart, the right panel further expands to display a report. The information displayed in the report is filtered based on the policy, assessment category, or assessment check that you click in the chart: If you click on a policy in the second layer of the chart, you can view the different categories of assessment checks for that policy. If you click on a specific category of assessment checks in the third layer of the chart, you can view the different assessment checks that are included in that category. If you click on an assessment check in the outermost layer of the chart, you can view details about that check. For each assessment check, you can view the status (failed or passed), severity level (CRITICAL, MAJOR, or WARNING), and possible solutions for resolving the identified issues. Note: If the provider that you have selected does not have the permissions required to run a specific policy, no data is displayed for that policy. (Optional) To navigate to the previous layer, click on the center layer in the chart. (Optional) To rerun a failed job from the list of assessment jobs, click the ReRun link next to that job. Note: If AWS credentials are not configured correctly, you cannot re-run the failed job from the list of assessment jobs. Downloading assessment reports \u00b6 On the Home page, select My Assessment . From the list of assessment jobs, click the job for which you want to download a report. In the right panel, click Download Report . In the Download Report window, perform the following actions: In Customer Name , enter your company name. In Summary , type a summary of the report. The text that you specify in this box is displayed in the Executive Summary section of the report. In the Template list, select the type of report as Custom_Assess_Template . Click Download . Assess Accelerator generates the assessment report in the .docx format. The assessment report contains detailed tables that list the multiple assessment criteria, their status and severity level, and possible solutions. The report contains the following columns: Check ID: The check ID for the assessment check. For more information, see Assessment Checks Reference . Assessment Check: The check being performed on the resources. Status: The result of the check after it is run. The status could either be Passed or Failed . Severity: The severity of the result of the check. The severity could Critical , Low , or Medium . Output: The output of the check after it is run. Possible Solutions: The recommendations after running the checks. Viewing and downloading comparison reports \u00b6 Assess Accelerator enables you to view and download a report that compares the assessment results across multiple jobs or providers. This report is especially useful in the following scenarios: If you have created an assessment job with multiple providers, you can generate a comparison report to view the assessment summary across all the providers. If you have created multiple jobs for the same provider over time, you can generate a comparison report to view the assessment summary across all the jobs for that provider. To view and download a comparison report, perform the following actions: On the Home page, select Compare . Click Select Jobs . In the Select your Cloud type list, select AWS or Azure . In the Select job list, perform one of the following actions: To compare the assessment results across multiple jobs, select the check boxes next to those assessment jobs. To compare the assessment results across all jobs, click Select all jobs . The selected jobs are displayed in the Summary table. To remove a job from the Summary table, click Delete ( ). Note: You can view the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If you select more than 6 jobs, you have to download the comparison report. (Only if you have selected a maximum of 6 jobs) To view and download the comparison report, perform the following actions: In the Add Assessment jobs to compare window, click OK . On the Compare tab, you can view the comparison report for the selected jobs. If you move your mouse over a column header, you can see the provider name and the date and time when the assessment job was started. To download the comparison report to your local computer, click Export and then select the Comparison Report option. The comparison report contains the following sheets: Dashboard: This sheet provides an overall status of all the jobs or providers in charts and the graph format. It also displays the assessment status based on account, policy and checkset, and assessment checks (for example, Cost Optimization Assessment, Performance Assessment, and other Security Assessment) across all jobs or providers. Summary View: This sheet lists all the assessment checks that were run and displays the total number of jobs (or providers) for which each check passed, failed, or was not applicable. It also displays the pass, fail, and not applicable percentage across all jobs or providers. Policy Raw Data: This sheet provides all the assessment data that is gathered across all selected jobs or providers. You can use this data to generate your own reports and charts. (Optional) To download a TAR file that contains the assessment summary report ( .docx format) for each selected job, select the Document Report option. In the Download Report window, enter your company name and report summary, select the report type, and then click DOWNLOAD . (Only if you have selected more than 6 jobs) In the Add Assessment jobs to compare window, perform one of the following actions based on your requirements: To download the comparison report to your local computer, click Export Comparison Report . To download a TAR file that contains the assessment summary report (docx format) for each selected job, click Export Document Report . In the Download Report window, enter your company name and report summary, select the report type, and then click DOWNLOAD .","title":"Run assessment policies"},{"location":"assess/using/#run-assessment-policies","text":"Hitachi Cloud Accelerator Platform - Assess ( Assess Accelerator) is a tool that automates the assessment of your cloud environment against the relevant security and compliance standards. This topic describes how you can use Assess Accelerator to run assessment jobs and view or download assessment reports.","title":"Run assessment policies"},{"location":"assess/using/#contents","text":"Get started Overview of assessment policies Accessing Assess Accelerator Run assessment jobs Prerequisites for AWS Assessments Prerequisites for Azure Assessments Creating AWS assessment jobs Creating Azure assessment jobs View and download reports Viewing assessment reports Downloading assessment reports Viewing and downloading comparison reports","title":"Contents"},{"location":"assess/using/#overview-of-assessment-policies","text":"Assess Accelerator contains out-of-the-box policies that help you to assess if your cloud infrastructure meets the best-practice standards based on different checks. Assessment reports that are generated provide the status for each assessment check and possible solutions for resolving issues. Assess Accelerator supports multiple tools that can be used for assessments of your environment. For each assessment tool, Assess Accelerator provides a specific set of checks. These checks are categorized under the assessment policies. For more information, see Assessment Checks Reference . The following table describes the out-of-the-box policies that are available to assess your cloud infrastructure. Policy name Description Cost Optimization Policy This policy enables you to assess how optimally you are managing the cost of your cloud infrastructure. It basically assesses cost optimization for your Amazon Web Services (AWS) environment by checking for unused Amazon Machine Images (AMIs), Snapshots, route tables, Elastic Load Balancers (ELBs), security groups, network interfaces, and much more. Operations Policy This policy enables you to assess the operational efficiency of your AWS environment based on best practices. It evaluates your environment based on criteria such as creation of your own VPC, data encryption, creation of the required tags for instances, and use of appropriate IAM user names. Security Policy This policy enables you to assess the security of your AWS and Azure environment based on industry standards and best practices. It consists of security assessment for network compliance, monitoring compliance, logging compliance, and IAM compliance. This policy is based on the best-practice security guidelines developed by the Center for Internet Security (CIS) for Amazon Web Services and Microsoft Azure environments.","title":"Overview of assessment policies"},{"location":"assess/using/#accessing-assess-accelerator","text":"Assess Accelerator is deployed as part of Hitachi Cloud Accelerator Platform. Sign in to Hitachi Cloud Accelerator Platform . For information on creating a Hitachi Cloud Accelerator Platform account, see Create & access account . Click the Module selector icon ( ) in the top-left corner. The Assess Accelerator home page appears. Note: You can access Assess Accelerator and perform various actions only if your Hitachi Cloud Accelerator Platform administrator has granted you the appropriate permissions.","title":"Accessing Assess Accelerator"},{"location":"assess/using/#prerequisites-for-aws-assessments","text":"The prerequisites for AWS assessment includes creating a provider. Following are the prerequisites you need to complete before running assessments for AWS: Creating IAM user and role with required permission Configuring a provider","title":"Prerequisites for AWS Assessments"},{"location":"assess/using/#creating-iam-user-and-role-with-required-permission","text":"","title":"Creating IAM user and role with required permission"},{"location":"assess/using/#creating-aws-user","text":"Before configuring a provider, you must create an Identity and Access Management (IAM) user or role with permissions to run the different assessment checks on your AWS infrastructure. Download the following custom inline policies: rean-assess-aws-iam-policy-1.json rean-assess-aws-iam-policy-2.json rean-assess-aws-iam-policy-3.json In the AWS account where you want to run the assessment policies, perform the following actions: Create IAM policies by using the custom inline policies that you have downloaded. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Important: To enable Assess Accelerator to upload assessment reports to an S3 bucket, you must add the s3:PutObject permission to one of the IAM policies. To enable Assess Accelerator to create a new S3 bucket, you must also add the s3:CreateBucket permission, as shown in the following example: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:CreateBucket\" ], \"Resource\": \"<s3_bucket_ARN>/*\" } ] } If you plan to use the Instance Profile with Assume Role or Static Credentials with Assume Role method while configuring a provider , perform the following actions: Create an IAM role that Assess Accelerator can assume to run the assessment policies. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Attach the IAM policies that you have created to this role. While creating the AWS provider, you must specify this IAM role. If you plan to use the Static Credentials method while configuring a provider , perform the following actions: Create an IAM group and attach the IAM policies that you have created to this group. For step-by-step instructions, see the AWS Identity and Access Management User Guide . Create an IAM user and add the user to the IAM group that you have created. For step-by-step instructions, see the AWS Identity and Access Management User Guide . While creating the AWS provider, you must provide credentials of this IAM user. Important* : Make sure that the IAM user/role does not have administrator access. As per AWS CIS Foundations Benchmark the use of administrator access is highly discouraged. This is applicable for both Instance Profile and Static Credentials.*","title":"Creating AWS user"},{"location":"assess/using/#creating-kubernetes-user","text":"Following are the steps to create a provider for instance profile when Assess is running on EKS. Before configuring a provider, you must create an Identity and Access Management (IAM) user or role with permissions. Download the following custom inline policies: rean-assess-aws-iam-policy-1.json rean-assess-aws-iam-policy-2.json rean-assess-aws-iam-policy-3.json Create an IAM role specific to Assess pod. In the AWS account where you want to run the assessment policies, perform the following actions: Create an IAM Role (or use an existing role with the required name) for the Assess pod specific IAM Role**.** For the IAM role, you can enter the name as hcap-assess-iam-role , or use a custom name specified in the customer.yml file before deploying Assess. The custom name must have the value \" assess_pods_iam_role_name \". Once the Assess is deployed, make sure to use only this Role for various Instance profile use-cases. For different use-cases, you can change the policies attached to this Role as per requirement. After creating a new Role, select Type of trusted entity as AWS Service - ec2 . Add EKS worker node's IAM Role in the Trust relationships for the above role: Under the Role, go to \" Trust relationships \" tab and click on \" Edit trust relationships \". Replace the policy JSON with the following JSON. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"<EKS_Worker_ec2_Node_IAM_Role_ARN>\" }, \"Action\": \"sts:AssumeRole\" } ] } For , use the Role ARN of the IAM Role attached to the EKS Worker node ec2 instance, where Assess is running. Save changes by clicking on \" Update Trust Policy \". Attach the IAM policies to the above created role. To create a provider with an instance profile without name use the following JSON: { \"region\": \"<AWS_Region>\" } To create a provider with an instance profile without name with assume role use the following JSON: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"sts:DecodeAuthorizationMessage\", \"sts:GetCallerIdentity\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"sts:*\", \"Resource\": \"<Assume_Role_ARN>\" } ] }","title":"Creating Kubernetes user"},{"location":"assess/using/#configuring-a-provider","text":"Assess Accelerator uses the AWS providers that are available in Deploy Accelerator. Open Deploy Accelerator, on the home page, click the More options ( ) icon in the top-right corner and then click Providers . On the Provider page, click New . Enter the provider name and select the AWS provider type. In the Provider Details section, use one of the following methods to specify authentication details of the AWS account in which Assess Accelerator must run the assessment policies. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Hitachi Cloud Accelerator Platform is deployed in an AWS account that is different from the accounts in which Assess Accelerator must run the assessment policies. Instead of storing the access credentials for all those accounts in Assess Accelerator, you can attach a role to the instance in which Hitachi Cloud Accelerator Platform is deployed. Assess Accelerator can then assume a role in the other accounts and run the assessment policies. For more information about assuming roles, see the AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Hitachi Cloud Accelerator Platform is deployed. For the other accounts, you must specify a role that Assess Accelerator can assume to run the assessment policies. The role that you specify must have permissions to run the different assessment checks on the AWS infrastructure. It must also define the account in which Hitachi Cloud Accelerator Platform is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\": \"xx-xxxx-x\", \"assume_role\": { \"role_arn\": \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\", \"session_name\": \"SESSION-NAME\", \"external_id\": \"assume_role\" } } Instance Profile This method provides a more secure way of accessing the account in which Assess Accelerator must run the assessment policies. However, Assess Accelerator can use this method only if a role is attached to the instance on which Hitachi Cloud Accelerator Platform is deployed. This role must have permissions to run the different assessment checks on the AWS infrastructure. To use the Instance Profile method, you must specify only the region in which the assessment policies must be run, as shown in the following example: { \"region\": \"xx-xxx-x\" } Static Credentials with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Assess Accelerator needs to run the assessment policies across multiple AWS accounts. Instead of storing the access credentials for all these accounts in Assess Accelerator, you can specify long-term access credentials for only the parent account. Assess Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Assess Accelerator can assume to run the assessment policies in that account. The role that you specify must have access to run the different assessment checks on the AWS infrastructure. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\": \"xxxxxxxxxx\", \"secret_key\": \"xxxxxxxxxx\", \"region\": \"xx-xxxx-x\", \"assume_role\": { \"role_arn\": \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\", \"session_name\": \"SESSION-NAME\", \"external_id\": \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Assess Accelerator must run the assessment policies, as shown in the following example: { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECRET-KEY\", \"region\": \"xx-xxxx-x\" } Note: The IAM user whose credentials you specify must have permissions to run the different assessment checks on the AWS infrastructure. For more information, see Before you begin . To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click Save . A new provider appears in the Provider List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details.","title":"Configuring a provider"},{"location":"assess/using/#prerequisites-for-azure-assessments","text":"The Azure assessment in Assess Accelerator is executed by the Microsoft Azure blueprint service. The blueprint used for Azure assessment is a sample of the CIS Microsoft Azure Foundations Benchmark blueprint. For more details see, Microsoft Azure Documentation . The prerequisite steps must be performed in the Azure account on which the assessment is to be run. The prerequisite involves the following steps: Deploy the Azure Blueprints CIS Microsoft Azure Foundations Benchmark blueprint sample Copy CIS Benchmark policy ID","title":"Prerequisites for Azure assessments"},{"location":"assess/using/#deploy-the-azure-blueprints-cis-microsoft-azure-foundations-benchmark-blueprint-sample","text":"The deployment steps include the following three steps: Create a new blueprint from the sample Mark your copy of the sample as Published Assign your copy of the blueprint to an existing subscription For more information on how to deploy the sample blueprint, see Microsoft Azure Documentation .","title":"Deploy the Azure Blueprints CIS Microsoft Azure Foundations Benchmark blueprint sample"},{"location":"assess/using/#copy-cis-benchmark-policy-id","text":"After a successful assignment of the blueprint, you must copy the policy ID for using while running the assessment job. To get the policy ID, perform the following actions: In the Azure account, go to Policy Service . Select CIS Microsoft Azure Foundations Benchmark 1.1.0 policy From the Assignment ID parameter, click Copy to clipboard. Save the last section of the assignment ID. For example, if complete policy assignment id is /subscriptions/ /providers/Microsoft.Authorization/policyAssignments/qwertyuiop1234567 then policy assignment ID to be saved is qwertyuiop1234567 .","title":"Copy CIS Benchmark policy ID"},{"location":"assess/using/#creating-aws-assessment-jobs","text":"On the Home page of Assess Accelerator, select Assess Now . For Select your Cloud type , select AWS . Enter a name for the assessment job. Tip: You might run different types of assessment policies for one or more providers multiple times. Therefore, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. In the Select Providers list, perform one of the following actions: To run the assessment policies for one or more providers, select the check boxes next to those providers. To run the assessment policies for all providers, select All . Note: The assessment policies are run on all resources that are owned by the account that is specified in the provider. However, the account credentials that are specified must have the required access to run these policies on the resources. Otherwise, no data or incomplete data is displayed in the summary and detailed reports. In the Select Assessment Template list, select a template for assessment. You can create multiple templates with different checks for each template. For more information, see Creating and managing assessment templates . Select the assessment policies that you want to run for the selected providers. For information about the available policies, see Overview of assessment policies . (Optional) To upload the assessment report for each provider to an AWS S3 bucket, perform the following actions: Select the Upload report to S3 check box. Enter the S3 bucket name. For each selected provider, Assess Accelerator appends this S3 bucket name with the account ID ( BucketName-AccountID ). If the BucketName-AccountID bucket already exists for a provider, Assess Accelerator uploads the assessment reports to this existing bucket. Otherwise, Assess Accelerator creates a new S3 bucket and then uploads the assessment report to this bucket. Important: To enable Assess Accelerator to upload assessment reports to an S3 bucket, you must add the s3:PutObject permission to the IAM policies that are used in each selected provider. To enable Assess Accelerator to create a new S3 bucket, you must also add the s3:CreateBucket permission, as shown in the following example: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:CreateBucket\" ], \"Resource\": \"<s3_bucket_ARN>/*\" } ] } (Optional) To receive email notifications for assessments, perform the following actions: Select Enable Notification . In the Customer Name field, enter your company name and enter your email address that was used for creating the login credentials. You will receive an assessment start email and an email with a detailed report after assessment completion. You can also enter multiple email addresses separated by a comma. The email addresses apart from the logged in user will only receive an assessment start email. Click START ASSESSMENT . The Assessment Started message appears. Notes: If the user has administrator access inherited from the IAM user in AWS, a validation message appears asking if you want to proceed with the assessment. You can choose to continue or cancel the assessment and check t he policies assigned to the IAM user. If the user is missing some required permissions, a validation message appears with a list of missing permissions. You can choose to continue or cancel the assessment and check the permissions assigned to the IAM user. To start another assessment, click ASSESS MORE . To view the progress of the assessment, click VIEW PROGRESS . Under the My Assessment tab, click Refresh ( ) to view the recently started assessment. If an assessment job contains multiple providers, Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . Following are the columns: Provider : provider name Job Name : job name for the provider. Group : assessment job name that you have specified. The status icons indicate whether the job is in progress ( ), has failed ( ), or has completed successfully ( ). Place mouse-pointer over the status icons to see the job start date and time.","title":"Creating AWS assessment jobs"},{"location":"assess/using/#creating-azure-assessment-jobs","text":"On the Home page of Assess Accelerator, select Assess Now . For Select your Cloud type , select Azure . Enter a name for the assessment job. Tip: You might run different types of assessment policies for one or more providers multiple times. Therefore, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. In the Select Providers list, select a provider for running the assessment on that account. For Azure assessment job, you can currently select only one provider. Note: The assessment policies are run on all resources that are owned by the account that is specified in the provider. However, the account credentials that are specified must have the required access to run these policies on the resources. Otherwise, no data or incomplete data is displayed in the summary and detailed reports. In the Select Assessment Template list, select a template for assessment. You can create multiple templates with different checks for each template. For more information, see Creating and managing assessment templates . In the Azure CIS Policy Assignment ID field, enter the assignment ID acquired by performing the prerequisite steps. For more information, see Prerequisites for Azure assessments . Select Security Policy . For information about the available policies, see Overview of assessment policies . (Optional) To receive email notifications for assessments, perform the following actions: Select Enable Notification . In the Customer Name field, enter your company name and enter your email address that was used for creating the login credentials. You will receive an assessment start email and an email with a detailed report after assessment completion. You can also enter multiple email addresses separated by a comma. The email addresses apart from the logged in user will only receive an assessment start email. Click START ASSESSMENT . The Assessment Started message appears. To start another assessment, click ASSESS MORE . To view the progress of the assessment, click VIEW PROGRESS . Under the My Assessment tab, click Refresh ( ) to view the recently started assessment. If an assessment job contains multiple providers, Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . Following are the columns: Provider : provider name Job Name : job name for the provider Group : assessment job name that you have specified. The status icons indicate whether the job is in progress ( ), has failed ( ), or has completed successfully ( ). Place mouse-pointer over the status icons to see the job start date and time.","title":"Creating Azure assessment jobs"},{"location":"assess/using/#viewing-assessment-reports","text":"On the Home page, select My Assessment . From the list of assessment jobs, select the job for which you want to view a report. In the right panel, you can see a sunburst chart that provides a graphical summary of the policies that were run for the selected job, along with their status. The sunburst chart consists of the following layers: The center layer provides a summary of the assessment job. You can see the total number of checks that were run and the number of checks that failed. The second layer represents the policies that were run for the selected job. For example, security policy, cost optimization policy, and operations policy. The third layer represents different categories of assessment checks within each policy. For example, Security Assessment for Monitoring Compliance is a category of assessment check within the security policy. The outermost layer represents each check that is included in a policy. For example, \"Ensure no S3 buckets are public\" is a specific check within the security policy. To drill down into a specific set of data, click on a layer in the sunburst chart. When you click on the chart, the right panel further expands to display a report. The information displayed in the report is filtered based on the policy, assessment category, or assessment check that you click in the chart: If you click on a policy in the second layer of the chart, you can view the different categories of assessment checks for that policy. If you click on a specific category of assessment checks in the third layer of the chart, you can view the different assessment checks that are included in that category. If you click on an assessment check in the outermost layer of the chart, you can view details about that check. For each assessment check, you can view the status (failed or passed), severity level (CRITICAL, MAJOR, or WARNING), and possible solutions for resolving the identified issues. Note: If the provider that you have selected does not have the permissions required to run a specific policy, no data is displayed for that policy. (Optional) To navigate to the previous layer, click on the center layer in the chart. (Optional) To rerun a failed job from the list of assessment jobs, click the ReRun link next to that job. Note: If AWS credentials are not configured correctly, you cannot re-run the failed job from the list of assessment jobs.","title":"Viewing assessment reports"},{"location":"assess/using/#downloading-assessment-reports","text":"On the Home page, select My Assessment . From the list of assessment jobs, click the job for which you want to download a report. In the right panel, click Download Report . In the Download Report window, perform the following actions: In Customer Name , enter your company name. In Summary , type a summary of the report. The text that you specify in this box is displayed in the Executive Summary section of the report. In the Template list, select the type of report as Custom_Assess_Template . Click Download . Assess Accelerator generates the assessment report in the .docx format. The assessment report contains detailed tables that list the multiple assessment criteria, their status and severity level, and possible solutions. The report contains the following columns: Check ID: The check ID for the assessment check. For more information, see Assessment Checks Reference . Assessment Check: The check being performed on the resources. Status: The result of the check after it is run. The status could either be Passed or Failed . Severity: The severity of the result of the check. The severity could Critical , Low , or Medium . Output: The output of the check after it is run. Possible Solutions: The recommendations after running the checks.","title":"Downloading assessment reports"},{"location":"assess/using/#viewing-and-downloading-comparison-reports","text":"Assess Accelerator enables you to view and download a report that compares the assessment results across multiple jobs or providers. This report is especially useful in the following scenarios: If you have created an assessment job with multiple providers, you can generate a comparison report to view the assessment summary across all the providers. If you have created multiple jobs for the same provider over time, you can generate a comparison report to view the assessment summary across all the jobs for that provider. To view and download a comparison report, perform the following actions: On the Home page, select Compare . Click Select Jobs . In the Select your Cloud type list, select AWS or Azure . In the Select job list, perform one of the following actions: To compare the assessment results across multiple jobs, select the check boxes next to those assessment jobs. To compare the assessment results across all jobs, click Select all jobs . The selected jobs are displayed in the Summary table. To remove a job from the Summary table, click Delete ( ). Note: You can view the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If you select more than 6 jobs, you have to download the comparison report. (Only if you have selected a maximum of 6 jobs) To view and download the comparison report, perform the following actions: In the Add Assessment jobs to compare window, click OK . On the Compare tab, you can view the comparison report for the selected jobs. If you move your mouse over a column header, you can see the provider name and the date and time when the assessment job was started. To download the comparison report to your local computer, click Export and then select the Comparison Report option. The comparison report contains the following sheets: Dashboard: This sheet provides an overall status of all the jobs or providers in charts and the graph format. It also displays the assessment status based on account, policy and checkset, and assessment checks (for example, Cost Optimization Assessment, Performance Assessment, and other Security Assessment) across all jobs or providers. Summary View: This sheet lists all the assessment checks that were run and displays the total number of jobs (or providers) for which each check passed, failed, or was not applicable. It also displays the pass, fail, and not applicable percentage across all jobs or providers. Policy Raw Data: This sheet provides all the assessment data that is gathered across all selected jobs or providers. You can use this data to generate your own reports and charts. (Optional) To download a TAR file that contains the assessment summary report ( .docx format) for each selected job, select the Document Report option. In the Download Report window, enter your company name and report summary, select the report type, and then click DOWNLOAD . (Only if you have selected more than 6 jobs) In the Add Assessment jobs to compare window, perform one of the following actions based on your requirements: To download the comparison report to your local computer, click Export Comparison Report . To download a TAR file that contains the assessment summary report (docx format) for each selected job, click Export Document Report . In the Download Report window, enter your company name and report summary, select the report type, and then click DOWNLOAD .","title":"Viewing and downloading comparison reports"},{"location":"deploy/administering/","text":"Administer Deploy Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to configure Deploy Accelerator. Contents \u00b6 Configuring the Blueprint Gallery Configuring the environment package types Configuring the default access token for the packages repository Registering a Chef Server in Deploy Accelerator Sharing Chef Server packages and roles Synching with a registered Chef Server Configuring Deploy Accelerator Customizing Deploy Accelerator email messages Configuring the Blueprint Gallery \u00b6 The Blueprint Gallery is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . Users can export their environments as blueprints and add the blueprints in the Blueprint Gallery . To configure the Blueprint Gallery, perform the following actions: Connect to the EC2 instance on which Hitachi Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. To display the Blueprint Gallery option when users click the More options icon ( ) icon in the top-right corner of the Deploy Accelerator Home page, add the com.reancloud.platform.show_blueprints property and set its value to true , as shown below. com.reancloud.platform.show_blueprints=true Note: To once again hide the Blueprint Gallery option, you must update the value of the com.reancloud.platform.show_blueprints property to false . To configure the Blueprint Gallery repository in the Artifactory that is used for Deploy Accelerator, add the dnow.blueprintsGallery.artifactory_repo_name property and set its value to the appropriate repository name. Only blueprints that are uploaded to the specified repository appear in the Blueprint Gallery. For more information, see adding blueprints in the Blueprint Gallery . Note: You must ensure that Deploy Accelerator users who are allowed to add blueprints in the Blueprint Gallery have access to the Artifactory and the specified repository. (Optional) To modify the default interval between two successive syncs of the blueprint metadata that is shown in the Blueprint Gallery with the Artifactory, add the dnow.blueprintsGallery.sync.interval property and specify the appropriate value in minutes. The default interval between two successive syncs is 30 minutes. Example: To modify the sync interval to an hour, you must update the property value to 60 (unit of measurement for this property is minutes), as shown in the example below: dnow.blueprintsGallery.sync.interval=60 Note: The blueprint metadata that is synchronized contains the blueprint name, description, version, and image name. Updated image files with the same name are not synchronized. Therefore, the old image continues to be shown in the Blueprint Gallery. Save the dnow.properties file. Restart the rean-deploy service. Configuring the environment package types \u00b6 Deploy Accelerator provides out-of-the-box support for Chef Solo, Chef Server, Ansible, and Puppet. However, only Chef Solo and Chef Server are configured as the default environment package types. While creating an environment, users can select either Ansible Solo, Chef Solo, or Chef Server as the environment package type. Based on the requirements, administrators can choose to individually configure Chef Solo, Chef Server, Ansible Solo, Puppet, or a combination of these three as the environment package types. Important: To configure the environment package types, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To configure the environment package type, perform the following actions: Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. If the com.reancloud.platform.deploy.environment.configuration_types property is not available in the dnow.properties file, add the property. Set the value of the com.reancloud.platform.deploy.environment.configuration_types property to the appropriate environment package types, as shown in the example below: Example for Chef package: com.reancloud.platform.deploy.environment.configuration_types=chef-solo,chef-server Example for Ansible package: com.reancloud.platform.deploy.environment.configuration_types=ansible-solo The supported values for this property are chef-solo , chef-server , ansible-solo and puppet . By default, Deploy Accelerator supports Ansible Solo, Chef Solo and Chef Server as the environment package types. (Optional) If Chef Server is configured as the environment package type, update the following additional properties. To access the Chef Server without self-signed SSL certificates, add the chefserver.no_verify_ssl property and set its value to true , as shown below: chefserver.no_verify_ssl=true Note: To once again enable access to the Chef Server with self-signed SSL certificates, you must update the value of the chefserver.no_verify_ssl property to false . To modify the interval between two successive syncs with the Chef Server, add the hcap.chefserver.sync.interval property and set its value (in minutes), as shown in the example below: hcap.chefserver.sync.interval=30 The default interval between two successive syncs is 10 minutes. After each successful sync, any cookbooks, roles, and groups that are added or updated in the Chef Server become available in Deploy Accelerator. Save the dnow.properties file. Restart the rean-deploy service. Configuring the default access token for the packages repository \u00b6 Users can add their own custom (or user-defined) packages based on the environment package types that are configured. In the package definition, they have to define the repository type, package download URL, and the access token for the repository. Administrators must define the default access token for the packages repository (GitHub or GitLab). If users do not specify an access token while adding a user-defined package, Deploy Accelerator uses this default access token to download the package. To configure the default access token for the packages repository, perform the following actions: Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate one of the following properties based on the repository type in which packages are stored: dnow.git.access.token dnow.gitlab.access.token If the property is not available in the dnow.properties file, add the property. Set the value of the property to the default access token for accessing the packages repository. Save the dnow.properties file. Restart the rean-deploy service. Registering a Chef Server in Deploy Accelerator \u00b6 By default, Deploy Accelerator supports Chef Solo and Chef Server as the environment package types. While creating an environment, users can choose whether they want to use Chef Solo or Chef Server packages to configure compute instances. While users can add their own custom (or user-defined) Chef Solo packages, only administrators can register a Chef Server in Deploy Accelerator. When you register a Chef Server, cookbooks (along with all the recipes it contains) and roles from that Chef Server become available in Deploy Accelerator. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. You can then manually share each package (or cookbook) and role with these groups and assign appropriate permissions. When users select the Chef Server package type while creating an environment, they also have to select the registered Chef Server. For that environment, the Packages tab displays packages (or cookbooks) and roles from the selected Chef Server only. However, users can see only the packages and roles that are shared with a group to which they belong. While adding a Chef Server package (or cookbook) to a resource, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Note: Deploy Accelerator polls the registered Chef Server every 10 minutes for new and updated cookbooks and roles. However, new groups that are added in the registered Chef Server are not automatically shown in Cloud Accelerator Platform. To view the latest cookbooks, roles, and groups at any given time, you can manually sync with the registered Chef Server . Before you begin Before you register a Chef Server, you must map the IP address and host name of that Chef Server in the /etc/hosts file of the Deploy Accelerator docker container. Connect to the EC2 instance on which Deploy Accelerator is deployed. To connect to the Deploy Accelerator docker container, run the following command: docker exec -it containerID bash In this command, specify the ID of the Deploy Accelerator docker container. To open the etc/hosts file, run the following command vi /etc/hosts In the etc/hosts file, add an entry that maps the IP address and hostname of the Chef Server. Note: If you have configured a load balancer for the Chef Server, you must map the IP address and host name of that load balancer. To register a Chef Server in Deploy Accelerator, perform the following actions: In the Chef Server organization that you want to configure in Deploy Accelerator, ensure that the clients group is a member of all other groups in that organization. If you do not perform this action, groups are not automatically created in Deploy Accelerator. On the Home page, click the More options icon ( ) icon in the top-right corner and then click Chef Servers . On the Chef Server page, click NEW . Enter the following details about the Chef Server that you want to configure in Deploy Accelerator: Field Description Name Enter a unique name for the Chef Server. Type Select the type of Chef Server that you want to configure. The available options are Chef , OpsWorks , and Enterprise Chef . Host Name Enter the host name of the Chef Server that you want to configure in Deploy Accelerator. Example: chef-server.example.com Description Enter details about the Chef Server that you are configuring. Organization Enter the organization from which you want to display packages in Deploy Accelerator. Client Enter the name of the Chef client that Deploy Accelerator must use to connect to the Chef Server. This client must be installed on the Chef Server. Client Key Enter the key that Deploy Accelerator must use to connect to the Chef client that you have specified. Click SAVE . A new Chef Server appears under the Chef Server List section. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. The naming convention used for these groups is organizationName-groupName . Add users to the groups (from the Chef Server organization) that have been created in Cloud Accelerator Platform. For more information, see Managing groups . Share each Chef Server package and role with the appropriate groups . By default, only administrators can view the Chef Server packages and roles. (Optional) To configure access to the Chef Server without self-signed SSL certificates, add the chefserver.no_verify_ssl property in the dnow.properties file and set its value to true . For more information, see Configuring the environment package types . Sharing Chef Server packages and roles \u00b6 By default, Chef Server packages (or cookbooks) and roles are shared with only administrators. You can control access to Chef Server packages and roles by sharing them with specific groups and assigning appropriate permissions. On the Home page, click the More options icon ( icon in the top-right corner and then click Packages . The Package List page displays packages and roles from the Chef Server that is registered in Deploy Accelerator. The Package Type column enables you to identify a Chef Server package or role and the Chef Server column enables you to identify the Chef Server from which the package or role is displayed. To share one or more Chef Server packages or roles, select the check box next to those packages and roles and then click SHARE . In the Share Packages window, click the Share icon ( ) icon for the Group with which you want to share the selected packages and roles. Note: When a Chef Server is registered, all groups in the selected Chef Server organization are also automatically created in Cloud Accelerator Platform. The naming convention used for these groups is organizationName-groupName . However, you can add new groups or manage users in these groups. For more information, see Managing groups . New groups that are added in the registered Chef Server are not automatically displayed in Cloud Accelerator Platform. To view the new groups, you must manually sync with the registered Chef Server . Select the permissions that you want to grant to the group and click DONE . Note: If you have selected multiple packages and roles, any permissions that were previously assigned to the packages and roles are not displayed. (Optional) To share the packages and roles with additional groups, repeat steps 3 and 4. Read the warning message and perform one of the following actions: If you want to proceed with sharing the packages, select the Warning check box and click SUBMIT . All users who are members of the selected groups can now view the packages and roles in Deploy Accelerator. The actions that they can perform on the packages and roles are based on the selected permissions. Note: Sharing of Chef Server packages and roles with the selected groups might take several minutes If you no longer want to share the packages, click CANCEL . Synching with a registered Chef Server \u00b6 When you register a Chef Server with Deploy Accelerator, cookbooks (along with all the recipes it contains) and roles from that Chef Server become available in Deploy Accelerator. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. Deploy Accelerator polls the registered Chef Server every 10 minutes for new and updated cookbooks and roles. However, new groups that are added in the selected Chef Server organization are not automatically shown in Cloud Accelerator Platform. To view the latest cookbooks, roles, and groups at any given time, you can manually sync with the registered Chef Server. Based on your requirements, you can also modify the default interval between two successive syncs with the Chef Server. For more information, see Configuring the environment package types . Note: Any new cookbooks, roles, and groups that are added in the Chef Server also become available when you manually update the Chef Server details in Deploy Accelerator. To sync with a registered Chef Server, perform the following actions: On the Home page, click the More options icon ( ) icon in the top-right corner and then click Chef Servers . On the ChefServer List page, click the Sync Chef Server ( ) icon next to the Chef Server. In the Chef Server Last Sync window, you can view the start time, end time, and status (InProgress, Success, or Failed) of the most recent sync, as shown in the following image. Click Sync Chef Server . If the previous sync is in progress, you have to wait for it to complete before starting another sync with the registered Chef Server. Configuring Deploy Accelerator \u00b6 To update the Deploy Accelerator configuration properties, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. If the configuration properties that you want to update are not available in the dnow.properties file, add those properties. Set the value of the appropriate properties in the dnow.properties file. Save the dnow.properties file. Restart the rean-deploy service. Deploy Accelerator configuration properties \u00b6 The following sections list the Deploy Accelerator configuration properties that you can update based on your requirements: General properties Email properties Package properties Artifactory properties Blueprint Gallery properties Helm Provider properties General properties \u00b6 Property Description Default value dnow.url Defines the URL to access Deploy Accelerator. The URL specified must be https:// ipAddress , where ipAddress refers to the public IP address of the instance on which Deploy Accelerator is installed. Email properties \u00b6 Property Description Default value send_status_mails Enables or disables the sending of status emails for an environment. true com.hcap.deploy.notification.deployment-initiation Enables or disables the sending of destroy initiation email for all environments in Deploy Accelerator. Note : The environment-level setting has the highest precedence, followed by the user-level setting, and finally the application-level setting. true com.hcap.deploy.notification.deployment-completion Enables or disables the sending of deploy or destroy completion emails for all environments in Deploy Accelerator. Note : The environment-level setting has the highest precedence, followed by the user-level setting, and finally the application-level setting. true smtp.host Defines the host name of the SMTP server from which deployment status emails must be sent to users. email-smtp.us-east-1.amazonaws.com smtp.port Defines the port on which the SMTP server can be accessed. 587 smtp.username Defines the user name that must be used to access the SMTP server from which emails are sent to users. This property is required only if you have enabled authentication for your SMTP server. smtp.password Defines the encrypted password for the user name that must be used to access the SMTP server. This property is required only if you have enabled authentication for your SMTP server. from.email Defines the From email address that is used in status emails that are sent to users. rean.noreply@hitachivantara.com mail.destroying_started.subject Defines the subject of the email that is sent when an environment starts being destroyed. Environment destroying started mail.destroy_failed.subject Defines the subject of the email that is sent when an environment is not destroyed successfully. Environment deployment failed mail.deploy_successful.subject Defines the subject of the email that is sent when an environment is successfully deployed. Environment deployment successful mail.environment_status.subject Defines the subject of the email that is sent to inform users about the deployment status of an environment. Environment deployment status Package properties \u00b6 Property Description Default value com.reancloud.platform.deploy.environment.configuration_types Defines the environment package types that are displayed when users create an environment. The supported values for this property are chef-server , chef-solo , and puppet . chef-solo,chef-server chefserver.no_verify_ssl To disable SSL certificate validation when a self-signed certificate is configured for the registered Chef Server, set the value of this property as true . false hcap.chefserver.sync.interval Defines the interval (in minutes) between two successive syncs with the registered Chef Server. Defines the interval (in minutes) between two successive syncs with the registered Chef Server. After each successful sync, any new cookbooks, roles, and groups that are added in the Chef Server become available in Deploy Accelerator. 10 dnow.git.access.token Defines the access token for the GitHub repository from which Deploy Accelerator must download packages. If users do not provide an access token while adding user-defined packages, Deploy Accelerator uses this default access token to download the packages from the GitHub repository. dnow.gitlab.access.token Defines the access token for the Git Lab repository from which Deploy Accelerator must download packages. If users do not provide an access token while adding user-defined packages, Deploy Accelerator uses this default access token to download the packages from the Git Lab repository. Artifactory properties \u00b6 Property Description Default value com.reancloud.platform.artifactory_support Defines whether a repository is configured for Deploy Accelerator. true com.reancloud.platform.artifactory_repo_type Defines the type of repository that is configured for Deploy Accelerator. Jfrog com.reancloud.platform.artifactory_url Defines the URL to access the artifactory. com.reancloud.platform.artifactory_username Defines the username to access the artifactory. com.reancloud.platform.artifactory_api_key Defines the API key of the username that you have specified. This API key must have all required permissions to access the artifactory. com.reancloud.platform.super_market_repo Defines the repository in which Chef cookbooks are stored. In an offline deployment, Deploy Accelerator downloads Chef cookbooks from this repository. virtual-supermarket Blueprint Gallery properties \u00b6 Property Description Default value com.reancloud.platform.show_blueprints Shows or hides the Blueprint Gallery option when users click the More options icon ( ) icon in the top-right corner of the Deploy Accelerator Home page. false dnow.blueprintsGallery.artifactory_repo_name Defines the repository in which blueprints are stored. Deploy Accelerator shows blueprints from this repository in the Blueprint Gallery. dnow.blueprintsGallery.sync.interval Defines the interval (in minutes) between two successive syncs of the blueprint metadata that is shown in the Blueprint Gallery with the Artifactory. The blueprint metadata that is synchronized contains the blueprint name, description, version, and image name. 30 Helm Provider properties \u00b6 These properties are for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm provider type . Users do not have to add the Helm Repository data source in their environment if their helm chart is available in this repository. Property Description Default value com.reancloud.deploy.helm.repository.name Chart repository name true com.reancloud.deploy.helm.repository.url Chart repository URL true com.reancloud.deploy.helm.repository.key_file Identify HTTPS client using this SSL key file false com.reancloud.deploy.helm.repository.cert_file Identify HTTPS client using this SSL certificate file false com.reancloud.deploy.helm.repository.ca_file Verify certificates of HTTPS-enabled servers using this CA bundle false com.reancloud.deploy.helm.repository.username Username for HTTP basic authentication false com.reancloud.deploy.helm.repository.password Password for HTTP basic authentication false Customizing Deploy Accelerator email messages \u00b6 When the deployment of an environment has succeeded or failed, an email is sent to the user whose email address is specified at the time of deploying the environment. Similarly, when an environment starts being destroyed, an email is sent to the specified email address. All emails related to an environment are also sent to the registered email address of the user who has created the environment, also known as the owner of the environment. You can choose to configure the sending of status emails for environments . The content of these emails is based on the default email templates that are selected. You can choose to create custom email templates and even customize the subject of status emails . Important: To customize Deploy Accelerator email messages, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. Configure the sending of status emails for environments \u00b6 Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate the send_status_mails property. If this property is not available in the dnow.properties file, add the property. To stop sending status emails for environments, change the value of the send_status_mails property to false , as shown in the example below: send_status_mails=false Save the dnow.properties file. Restart the rean-deploy service. Note: To once again send status emails for environments, update the value of the send_status_mails property to true . Customize the subject of status emails \u00b6 Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate the following properties: mail.destroying_started.subject mail.destroy_failed.subject mail.deploy_successful.subject mail.environment_status.subject If these properties are not available in the dnow.properties file, add the properties. Update the value of these properties. Save the dnow.properties file. Restart the rean-deploy service. Create a custom email template \u00b6 (Optional) To add data about an environment in the email template, perform the following actions: In Deploy Accelerator, open the environment for which you are creating a custom email template. From the left panel, click the Resources tab, and drag the Output resource to the canvas. Rename the resource and click it to open the right panel. On the RESOURCE tab, select the Set Json link for the output attribute. In the Set Json value for : output window, define the output variables that you want to use in the email template, as shown in the example below: { \"ip\" : { \"value\" : \"${output_1.public_ip}\" }, \"privateip\" : { \"value\" : \"${output_1.private_ip}\" }, \"first_name\" : { \"value\" : \"peter\" }, \"last_name\" : { \"value\" : \"morgan\" } } Click Set Value . To update your changes to the environment, click Save . Create a new HTML file. In the HTML file, add the content to be included in the email. (Optional) Based on your requirements, use the custom output variables that you defined in step 1. You can also use any of the following output variables in the email content: Output variable Description $status Displays the status of the deployment $envName Displays the name of the environment for which an email is being sent to the user $url Displays the URL to access Deploy Accelerator Save the HTML file. Ensure that the file name enables users to easily identify the purpose of the template. For example, you can use the name customerName_deploysuccess.html . Connect to the EC2 instance on which Cloud Accelerator Platform is installed. Locate the ${PLATFORM_HOME}/rean-deploy/dnow-data-dir/email-templates folder. Note: If the email-templates folder is not available, you must create the folder. Copy the custom email template file that you have created to the email-templates folder. Note: The custom email templates are not displayed in the Deploy Accelerator UI. Therefore, you must share the list of custom email templates with other Deploy Accelerator users as required. Users have to manually enter the names of these email templates before starting new deployments of an environment . Alternatively, users can configure an environment to use these templates for all its deployments.","title":"Administer"},{"location":"deploy/administering/#administer-deploy-accelerator","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to configure Deploy Accelerator.","title":"Administer Deploy Accelerator"},{"location":"deploy/administering/#contents","text":"Configuring the Blueprint Gallery Configuring the environment package types Configuring the default access token for the packages repository Registering a Chef Server in Deploy Accelerator Sharing Chef Server packages and roles Synching with a registered Chef Server Configuring Deploy Accelerator Customizing Deploy Accelerator email messages","title":"Contents"},{"location":"deploy/administering/#configuring-the-blueprint-gallery","text":"The Blueprint Gallery is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . Users can export their environments as blueprints and add the blueprints in the Blueprint Gallery . To configure the Blueprint Gallery, perform the following actions: Connect to the EC2 instance on which Hitachi Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. To display the Blueprint Gallery option when users click the More options icon ( ) icon in the top-right corner of the Deploy Accelerator Home page, add the com.reancloud.platform.show_blueprints property and set its value to true , as shown below. com.reancloud.platform.show_blueprints=true Note: To once again hide the Blueprint Gallery option, you must update the value of the com.reancloud.platform.show_blueprints property to false . To configure the Blueprint Gallery repository in the Artifactory that is used for Deploy Accelerator, add the dnow.blueprintsGallery.artifactory_repo_name property and set its value to the appropriate repository name. Only blueprints that are uploaded to the specified repository appear in the Blueprint Gallery. For more information, see adding blueprints in the Blueprint Gallery . Note: You must ensure that Deploy Accelerator users who are allowed to add blueprints in the Blueprint Gallery have access to the Artifactory and the specified repository. (Optional) To modify the default interval between two successive syncs of the blueprint metadata that is shown in the Blueprint Gallery with the Artifactory, add the dnow.blueprintsGallery.sync.interval property and specify the appropriate value in minutes. The default interval between two successive syncs is 30 minutes. Example: To modify the sync interval to an hour, you must update the property value to 60 (unit of measurement for this property is minutes), as shown in the example below: dnow.blueprintsGallery.sync.interval=60 Note: The blueprint metadata that is synchronized contains the blueprint name, description, version, and image name. Updated image files with the same name are not synchronized. Therefore, the old image continues to be shown in the Blueprint Gallery. Save the dnow.properties file. Restart the rean-deploy service.","title":"Configuring the Blueprint Gallery"},{"location":"deploy/administering/#configuring-the-environment-package-types","text":"Deploy Accelerator provides out-of-the-box support for Chef Solo, Chef Server, Ansible, and Puppet. However, only Chef Solo and Chef Server are configured as the default environment package types. While creating an environment, users can select either Ansible Solo, Chef Solo, or Chef Server as the environment package type. Based on the requirements, administrators can choose to individually configure Chef Solo, Chef Server, Ansible Solo, Puppet, or a combination of these three as the environment package types. Important: To configure the environment package types, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To configure the environment package type, perform the following actions: Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. If the com.reancloud.platform.deploy.environment.configuration_types property is not available in the dnow.properties file, add the property. Set the value of the com.reancloud.platform.deploy.environment.configuration_types property to the appropriate environment package types, as shown in the example below: Example for Chef package: com.reancloud.platform.deploy.environment.configuration_types=chef-solo,chef-server Example for Ansible package: com.reancloud.platform.deploy.environment.configuration_types=ansible-solo The supported values for this property are chef-solo , chef-server , ansible-solo and puppet . By default, Deploy Accelerator supports Ansible Solo, Chef Solo and Chef Server as the environment package types. (Optional) If Chef Server is configured as the environment package type, update the following additional properties. To access the Chef Server without self-signed SSL certificates, add the chefserver.no_verify_ssl property and set its value to true , as shown below: chefserver.no_verify_ssl=true Note: To once again enable access to the Chef Server with self-signed SSL certificates, you must update the value of the chefserver.no_verify_ssl property to false . To modify the interval between two successive syncs with the Chef Server, add the hcap.chefserver.sync.interval property and set its value (in minutes), as shown in the example below: hcap.chefserver.sync.interval=30 The default interval between two successive syncs is 10 minutes. After each successful sync, any cookbooks, roles, and groups that are added or updated in the Chef Server become available in Deploy Accelerator. Save the dnow.properties file. Restart the rean-deploy service.","title":"Configuring the environment package types"},{"location":"deploy/administering/#configuring-the-default-access-token-for-the-packages-repository","text":"Users can add their own custom (or user-defined) packages based on the environment package types that are configured. In the package definition, they have to define the repository type, package download URL, and the access token for the repository. Administrators must define the default access token for the packages repository (GitHub or GitLab). If users do not specify an access token while adding a user-defined package, Deploy Accelerator uses this default access token to download the package. To configure the default access token for the packages repository, perform the following actions: Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate one of the following properties based on the repository type in which packages are stored: dnow.git.access.token dnow.gitlab.access.token If the property is not available in the dnow.properties file, add the property. Set the value of the property to the default access token for accessing the packages repository. Save the dnow.properties file. Restart the rean-deploy service.","title":"Configuring the default access token for the packages repository"},{"location":"deploy/administering/#registering-a-chef-server-in-deploy-accelerator","text":"By default, Deploy Accelerator supports Chef Solo and Chef Server as the environment package types. While creating an environment, users can choose whether they want to use Chef Solo or Chef Server packages to configure compute instances. While users can add their own custom (or user-defined) Chef Solo packages, only administrators can register a Chef Server in Deploy Accelerator. When you register a Chef Server, cookbooks (along with all the recipes it contains) and roles from that Chef Server become available in Deploy Accelerator. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. You can then manually share each package (or cookbook) and role with these groups and assign appropriate permissions. When users select the Chef Server package type while creating an environment, they also have to select the registered Chef Server. For that environment, the Packages tab displays packages (or cookbooks) and roles from the selected Chef Server only. However, users can see only the packages and roles that are shared with a group to which they belong. While adding a Chef Server package (or cookbook) to a resource, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Note: Deploy Accelerator polls the registered Chef Server every 10 minutes for new and updated cookbooks and roles. However, new groups that are added in the registered Chef Server are not automatically shown in Cloud Accelerator Platform. To view the latest cookbooks, roles, and groups at any given time, you can manually sync with the registered Chef Server . Before you begin Before you register a Chef Server, you must map the IP address and host name of that Chef Server in the /etc/hosts file of the Deploy Accelerator docker container. Connect to the EC2 instance on which Deploy Accelerator is deployed. To connect to the Deploy Accelerator docker container, run the following command: docker exec -it containerID bash In this command, specify the ID of the Deploy Accelerator docker container. To open the etc/hosts file, run the following command vi /etc/hosts In the etc/hosts file, add an entry that maps the IP address and hostname of the Chef Server. Note: If you have configured a load balancer for the Chef Server, you must map the IP address and host name of that load balancer. To register a Chef Server in Deploy Accelerator, perform the following actions: In the Chef Server organization that you want to configure in Deploy Accelerator, ensure that the clients group is a member of all other groups in that organization. If you do not perform this action, groups are not automatically created in Deploy Accelerator. On the Home page, click the More options icon ( ) icon in the top-right corner and then click Chef Servers . On the Chef Server page, click NEW . Enter the following details about the Chef Server that you want to configure in Deploy Accelerator: Field Description Name Enter a unique name for the Chef Server. Type Select the type of Chef Server that you want to configure. The available options are Chef , OpsWorks , and Enterprise Chef . Host Name Enter the host name of the Chef Server that you want to configure in Deploy Accelerator. Example: chef-server.example.com Description Enter details about the Chef Server that you are configuring. Organization Enter the organization from which you want to display packages in Deploy Accelerator. Client Enter the name of the Chef client that Deploy Accelerator must use to connect to the Chef Server. This client must be installed on the Chef Server. Client Key Enter the key that Deploy Accelerator must use to connect to the Chef client that you have specified. Click SAVE . A new Chef Server appears under the Chef Server List section. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. The naming convention used for these groups is organizationName-groupName . Add users to the groups (from the Chef Server organization) that have been created in Cloud Accelerator Platform. For more information, see Managing groups . Share each Chef Server package and role with the appropriate groups . By default, only administrators can view the Chef Server packages and roles. (Optional) To configure access to the Chef Server without self-signed SSL certificates, add the chefserver.no_verify_ssl property in the dnow.properties file and set its value to true . For more information, see Configuring the environment package types .","title":"Registering a Chef Server in Deploy Accelerator"},{"location":"deploy/administering/#sharing-chef-server-packages-and-roles","text":"By default, Chef Server packages (or cookbooks) and roles are shared with only administrators. You can control access to Chef Server packages and roles by sharing them with specific groups and assigning appropriate permissions. On the Home page, click the More options icon ( icon in the top-right corner and then click Packages . The Package List page displays packages and roles from the Chef Server that is registered in Deploy Accelerator. The Package Type column enables you to identify a Chef Server package or role and the Chef Server column enables you to identify the Chef Server from which the package or role is displayed. To share one or more Chef Server packages or roles, select the check box next to those packages and roles and then click SHARE . In the Share Packages window, click the Share icon ( ) icon for the Group with which you want to share the selected packages and roles. Note: When a Chef Server is registered, all groups in the selected Chef Server organization are also automatically created in Cloud Accelerator Platform. The naming convention used for these groups is organizationName-groupName . However, you can add new groups or manage users in these groups. For more information, see Managing groups . New groups that are added in the registered Chef Server are not automatically displayed in Cloud Accelerator Platform. To view the new groups, you must manually sync with the registered Chef Server . Select the permissions that you want to grant to the group and click DONE . Note: If you have selected multiple packages and roles, any permissions that were previously assigned to the packages and roles are not displayed. (Optional) To share the packages and roles with additional groups, repeat steps 3 and 4. Read the warning message and perform one of the following actions: If you want to proceed with sharing the packages, select the Warning check box and click SUBMIT . All users who are members of the selected groups can now view the packages and roles in Deploy Accelerator. The actions that they can perform on the packages and roles are based on the selected permissions. Note: Sharing of Chef Server packages and roles with the selected groups might take several minutes If you no longer want to share the packages, click CANCEL .","title":"Sharing Chef Server packages and roles"},{"location":"deploy/administering/#synching-with-a-registered-chef-server","text":"When you register a Chef Server with Deploy Accelerator, cookbooks (along with all the recipes it contains) and roles from that Chef Server become available in Deploy Accelerator. In addition, groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. Deploy Accelerator polls the registered Chef Server every 10 minutes for new and updated cookbooks and roles. However, new groups that are added in the selected Chef Server organization are not automatically shown in Cloud Accelerator Platform. To view the latest cookbooks, roles, and groups at any given time, you can manually sync with the registered Chef Server. Based on your requirements, you can also modify the default interval between two successive syncs with the Chef Server. For more information, see Configuring the environment package types . Note: Any new cookbooks, roles, and groups that are added in the Chef Server also become available when you manually update the Chef Server details in Deploy Accelerator. To sync with a registered Chef Server, perform the following actions: On the Home page, click the More options icon ( ) icon in the top-right corner and then click Chef Servers . On the ChefServer List page, click the Sync Chef Server ( ) icon next to the Chef Server. In the Chef Server Last Sync window, you can view the start time, end time, and status (InProgress, Success, or Failed) of the most recent sync, as shown in the following image. Click Sync Chef Server . If the previous sync is in progress, you have to wait for it to complete before starting another sync with the registered Chef Server.","title":"Synching with a registered Chef Server"},{"location":"deploy/administering/#configuring-deploy-accelerator","text":"To update the Deploy Accelerator configuration properties, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. If the configuration properties that you want to update are not available in the dnow.properties file, add those properties. Set the value of the appropriate properties in the dnow.properties file. Save the dnow.properties file. Restart the rean-deploy service.","title":"Configuring Deploy Accelerator"},{"location":"deploy/administering/#deploy-accelerator-configuration-properties","text":"The following sections list the Deploy Accelerator configuration properties that you can update based on your requirements: General properties Email properties Package properties Artifactory properties Blueprint Gallery properties Helm Provider properties","title":"Deploy Accelerator configuration properties"},{"location":"deploy/administering/#general-properties","text":"Property Description Default value dnow.url Defines the URL to access Deploy Accelerator. The URL specified must be https:// ipAddress , where ipAddress refers to the public IP address of the instance on which Deploy Accelerator is installed.","title":"General properties"},{"location":"deploy/administering/#email-properties","text":"Property Description Default value send_status_mails Enables or disables the sending of status emails for an environment. true com.hcap.deploy.notification.deployment-initiation Enables or disables the sending of destroy initiation email for all environments in Deploy Accelerator. Note : The environment-level setting has the highest precedence, followed by the user-level setting, and finally the application-level setting. true com.hcap.deploy.notification.deployment-completion Enables or disables the sending of deploy or destroy completion emails for all environments in Deploy Accelerator. Note : The environment-level setting has the highest precedence, followed by the user-level setting, and finally the application-level setting. true smtp.host Defines the host name of the SMTP server from which deployment status emails must be sent to users. email-smtp.us-east-1.amazonaws.com smtp.port Defines the port on which the SMTP server can be accessed. 587 smtp.username Defines the user name that must be used to access the SMTP server from which emails are sent to users. This property is required only if you have enabled authentication for your SMTP server. smtp.password Defines the encrypted password for the user name that must be used to access the SMTP server. This property is required only if you have enabled authentication for your SMTP server. from.email Defines the From email address that is used in status emails that are sent to users. rean.noreply@hitachivantara.com mail.destroying_started.subject Defines the subject of the email that is sent when an environment starts being destroyed. Environment destroying started mail.destroy_failed.subject Defines the subject of the email that is sent when an environment is not destroyed successfully. Environment deployment failed mail.deploy_successful.subject Defines the subject of the email that is sent when an environment is successfully deployed. Environment deployment successful mail.environment_status.subject Defines the subject of the email that is sent to inform users about the deployment status of an environment. Environment deployment status","title":"Email properties"},{"location":"deploy/administering/#package-properties","text":"Property Description Default value com.reancloud.platform.deploy.environment.configuration_types Defines the environment package types that are displayed when users create an environment. The supported values for this property are chef-server , chef-solo , and puppet . chef-solo,chef-server chefserver.no_verify_ssl To disable SSL certificate validation when a self-signed certificate is configured for the registered Chef Server, set the value of this property as true . false hcap.chefserver.sync.interval Defines the interval (in minutes) between two successive syncs with the registered Chef Server. Defines the interval (in minutes) between two successive syncs with the registered Chef Server. After each successful sync, any new cookbooks, roles, and groups that are added in the Chef Server become available in Deploy Accelerator. 10 dnow.git.access.token Defines the access token for the GitHub repository from which Deploy Accelerator must download packages. If users do not provide an access token while adding user-defined packages, Deploy Accelerator uses this default access token to download the packages from the GitHub repository. dnow.gitlab.access.token Defines the access token for the Git Lab repository from which Deploy Accelerator must download packages. If users do not provide an access token while adding user-defined packages, Deploy Accelerator uses this default access token to download the packages from the Git Lab repository.","title":"Package properties"},{"location":"deploy/administering/#artifactory-properties","text":"Property Description Default value com.reancloud.platform.artifactory_support Defines whether a repository is configured for Deploy Accelerator. true com.reancloud.platform.artifactory_repo_type Defines the type of repository that is configured for Deploy Accelerator. Jfrog com.reancloud.platform.artifactory_url Defines the URL to access the artifactory. com.reancloud.platform.artifactory_username Defines the username to access the artifactory. com.reancloud.platform.artifactory_api_key Defines the API key of the username that you have specified. This API key must have all required permissions to access the artifactory. com.reancloud.platform.super_market_repo Defines the repository in which Chef cookbooks are stored. In an offline deployment, Deploy Accelerator downloads Chef cookbooks from this repository. virtual-supermarket","title":"Artifactory properties"},{"location":"deploy/administering/#blueprint-gallery-properties","text":"Property Description Default value com.reancloud.platform.show_blueprints Shows or hides the Blueprint Gallery option when users click the More options icon ( ) icon in the top-right corner of the Deploy Accelerator Home page. false dnow.blueprintsGallery.artifactory_repo_name Defines the repository in which blueprints are stored. Deploy Accelerator shows blueprints from this repository in the Blueprint Gallery. dnow.blueprintsGallery.sync.interval Defines the interval (in minutes) between two successive syncs of the blueprint metadata that is shown in the Blueprint Gallery with the Artifactory. The blueprint metadata that is synchronized contains the blueprint name, description, version, and image name. 30","title":"Blueprint Gallery properties"},{"location":"deploy/administering/#helm-provider-properties","text":"These properties are for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm provider type . Users do not have to add the Helm Repository data source in their environment if their helm chart is available in this repository. Property Description Default value com.reancloud.deploy.helm.repository.name Chart repository name true com.reancloud.deploy.helm.repository.url Chart repository URL true com.reancloud.deploy.helm.repository.key_file Identify HTTPS client using this SSL key file false com.reancloud.deploy.helm.repository.cert_file Identify HTTPS client using this SSL certificate file false com.reancloud.deploy.helm.repository.ca_file Verify certificates of HTTPS-enabled servers using this CA bundle false com.reancloud.deploy.helm.repository.username Username for HTTP basic authentication false com.reancloud.deploy.helm.repository.password Password for HTTP basic authentication false","title":"Helm Provider properties"},{"location":"deploy/administering/#customizing-deploy-accelerator-email-messages","text":"When the deployment of an environment has succeeded or failed, an email is sent to the user whose email address is specified at the time of deploying the environment. Similarly, when an environment starts being destroyed, an email is sent to the specified email address. All emails related to an environment are also sent to the registered email address of the user who has created the environment, also known as the owner of the environment. You can choose to configure the sending of status emails for environments . The content of these emails is based on the default email templates that are selected. You can choose to create custom email templates and even customize the subject of status emails . Important: To customize Deploy Accelerator email messages, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed.","title":"Customizing Deploy Accelerator email messages"},{"location":"deploy/administering/#configure-the-sending-of-status-emails-for-environments","text":"Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate the send_status_mails property. If this property is not available in the dnow.properties file, add the property. To stop sending status emails for environments, change the value of the send_status_mails property to false , as shown in the example below: send_status_mails=false Save the dnow.properties file. Restart the rean-deploy service. Note: To once again send status emails for environments, update the value of the send_status_mails property to true .","title":"Configure the sending of status emails for environments"},{"location":"deploy/administering/#customize-the-subject-of-status-emails","text":"Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-deploy/conf folder. If the dnow.properties file is not available in this folder, create the file. In the dnow.properties file, locate the following properties: mail.destroying_started.subject mail.destroy_failed.subject mail.deploy_successful.subject mail.environment_status.subject If these properties are not available in the dnow.properties file, add the properties. Update the value of these properties. Save the dnow.properties file. Restart the rean-deploy service.","title":"Customize the subject of status emails"},{"location":"deploy/administering/#create-a-custom-email-template","text":"(Optional) To add data about an environment in the email template, perform the following actions: In Deploy Accelerator, open the environment for which you are creating a custom email template. From the left panel, click the Resources tab, and drag the Output resource to the canvas. Rename the resource and click it to open the right panel. On the RESOURCE tab, select the Set Json link for the output attribute. In the Set Json value for : output window, define the output variables that you want to use in the email template, as shown in the example below: { \"ip\" : { \"value\" : \"${output_1.public_ip}\" }, \"privateip\" : { \"value\" : \"${output_1.private_ip}\" }, \"first_name\" : { \"value\" : \"peter\" }, \"last_name\" : { \"value\" : \"morgan\" } } Click Set Value . To update your changes to the environment, click Save . Create a new HTML file. In the HTML file, add the content to be included in the email. (Optional) Based on your requirements, use the custom output variables that you defined in step 1. You can also use any of the following output variables in the email content: Output variable Description $status Displays the status of the deployment $envName Displays the name of the environment for which an email is being sent to the user $url Displays the URL to access Deploy Accelerator Save the HTML file. Ensure that the file name enables users to easily identify the purpose of the template. For example, you can use the name customerName_deploysuccess.html . Connect to the EC2 instance on which Cloud Accelerator Platform is installed. Locate the ${PLATFORM_HOME}/rean-deploy/dnow-data-dir/email-templates folder. Note: If the email-templates folder is not available, you must create the folder. Copy the custom email template file that you have created to the email-templates folder. Note: The custom email templates are not displayed in the Deploy Accelerator UI. Therefore, you must share the list of custom email templates with other Deploy Accelerator users as required. Users have to manually enter the names of these email templates before starting new deployments of an environment . Alternatively, users can configure an environment to use these templates for all its deployments.","title":"Create a custom email template"},{"location":"deploy/api-reference/","text":"Deploy Accelerator API Reference \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) provides a set of APIs (Application Programming Interface) for various components such as environments, providers, connections, resources, and more. For example, you can use the Environment APIs to get a list of all user environments, save and update environments, and share all versions of an environment based on the assigned permissions. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API. Accessing the API documentation \u00b6 To view the API documentation for the version of Deploy Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html . To view the API documentation for the latest version of Deploy Accelerator, see the Deploy Accelerator API Reference website .","title":"API reference"},{"location":"deploy/api-reference/#deploy-accelerator-api-reference","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) provides a set of APIs (Application Programming Interface) for various components such as environments, providers, connections, resources, and more. For example, you can use the Environment APIs to get a list of all user environments, save and update environments, and share all versions of an environment based on the assigned permissions. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API.","title":"Deploy Accelerator API Reference"},{"location":"deploy/api-reference/#accessing-the-api-documentation","text":"To view the API documentation for the version of Deploy Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html . To view the API documentation for the latest version of Deploy Accelerator, see the Deploy Accelerator API Reference website .","title":"Accessing the API documentation"},{"location":"deploy/getting-started/","text":"Overview of Deploy Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. Installation that used to take weeks can now happen in hours. It supports many cloud providers out-of-the-box, including Amazon Web Services (AWS) and Microsoft Azure. Deploy Accelerator also supports parallel deployments and leverages all the major infrastructure automation tools such as Chef (Chef Server and Chef Solo), Ansible Solo, and Puppet. Deploy Accelerator can be deployed in the online or offline mode based on your requirements. In the offline mode, Deploy Accelerator does not require Internet connectivity to perform various operations. All required artifacts are accessed from the Artifactory that is configured while deploying Deploy Accelerator. For information about using Deploy Accelerator to deploy and manage environments, see the Deploy and manage environments topic. For information about configuring Deploy Accelerator, see the Administer Deploy Accelerator topic.","title":"Overview"},{"location":"deploy/getting-started/#overview-of-deploy-accelerator","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. Installation that used to take weeks can now happen in hours. It supports many cloud providers out-of-the-box, including Amazon Web Services (AWS) and Microsoft Azure. Deploy Accelerator also supports parallel deployments and leverages all the major infrastructure automation tools such as Chef (Chef Server and Chef Solo), Ansible Solo, and Puppet. Deploy Accelerator can be deployed in the online or offline mode based on your requirements. In the offline mode, Deploy Accelerator does not require Internet connectivity to perform various operations. All required artifacts are accessed from the Artifactory that is configured while deploying Deploy Accelerator. For information about using Deploy Accelerator to deploy and manage environments, see the Deploy and manage environments topic. For information about configuring Deploy Accelerator, see the Administer Deploy Accelerator topic.","title":"Overview of Deploy Accelerator"},{"location":"deploy/howto-guides/","text":"How-to guides for Deploy Accelerator \u00b6 The How-to guides for Deploy Accelerator focus on achieving specific goals, performing common tasks, or resolving specific issues. These guides include high-levels steps or detailed procedures with examples to help you to quickly accomplish your goals. For step-by-step instructions on performing various tasks in Deploy Accelerator, see Deploy and manage environments . Contents \u00b6 How do I import a new version of an existing environment? How do I upgrade an existing environment? How do I collaborate with other users? How do I access entities of a user who has left the organization? How do I create a template file? How do I dynamically generate a key pair for an EC2 instance? How do I import a new version of an existing environment? \u00b6 Deploy Accelerator allows you to import a blueprint as a new version of an existing environment. For example, say that you created a new environment in Deploy Accelerator by importing a blueprint from the Blueprint Gallery or your computer. And an updated version of this blueprint is now available. Deploy Accelerator allows you to import this updated blueprint as a new version of the existing environment. While you can import a blueprint with the same name as an existing environment, the version must be unique. Important: Importing a blueprint as a new version of an existing environment does not affect any of the existing deployments of the environment. High-level steps to import a new version of an existing environment Open the existing environment and note the name and existing version numbers of this environment. Based on your requirements, perform one of the following actions: To import a blueprint from your computer, click the More icon ( ) on the canvas and then click Import . To import a blueprint from the Blueprint Gallery, click the More options icon ( ) in the top-right corner of the Home page and then click Blueprint Gallery . On the Blueprint Gallery page, click IMPORT for the new blueprint version that you want to import. For detailed information about the different ways in which you can import a blueprint in Deploy Accelerator see, Creating new environments from blueprints . In the Import Environments window, perform the following actions: Enter the same name as your existing environment. Enter a unique version up to a maximum of 40 characters. It is recommended that the version you specify is greater than all versions of your existing environment. However, you can also specify an existing version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Select the appropriate connection and provider. If Chef Server is set as the environment package type, select the Chef Server and chef environment. Click IMPORT ALL . The blueprint is imported as a new version of your existing environment. Click the version list on the canvas. You can see the previous versions of the existing environment in this list. (Optional) Compare differences between the version that you have imported and previous versions of the existing environment. (Optional) Start a new deployment of the environment version that you have imported. None of the existing deployments of the environment are impacted. To see the list of deployments across all versions of the existing environment, see Viewing deployments . If required, you can also redeploy an existing deployment with the new environment version. For more information, see Redeploying existing deployments . How do I upgrade an existing environment? \u00b6 When you create an environment and start a deployment, Deploy Accelerator creates the resources using the configured provider. After an environment is deployed in production, you can choose to continue to make updates in the same environment. However, this approach makes it difficult to keep a track of changes to an environment over time. Instead, it is recommended that you release the environment to freeze any updates to that environment. When you want to upgrade this existing environment (add or update resources), you can create a new environment version. High-level steps to upgrade an existing environment Create a new environment and add resources as required. To provision any compute resources in the environment, also add appropriate packages to those resources. Important: Deploy Accelerator cannot deploy packages on compute resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. Start a deployment of the environment. Each environment can have multiple deployments, such as Staging and Production. (Optional) After the production deployment of the environment completes successfully, release the environment . Releasing an environment ensures that no further updates can be made to the existing environment. If you redeploy existing deployments of a released environment, already deployed resources are not impacted. This is because during a redeployment, only updates to deployed resources are considered. To upgrade the existing environment that has been released, perform the following actions: Create a new environment version . In the new environment version, add or update resources as required. Save the environment. (Optional) Compare differences between the two environment versions. To upgrade the existing (production) deployment based on the new environment version, perform the following actions: From the deployments list on the canvas, select the existing deployment that you want to upgrade. To view the plan for the existing deployment, click the Plan icon ( ) and select Plan Selected Deployment . In the Review and Plan: Environment Name window, select the new environment version that you have created and PLAN . The deployment plan shows the new or updated resources that will be deployed. The already deployed resources that were not updated in the new environment version are not impacted. To redeploy the existing deployment, click the Re-Deploy icon ( ). On the Deployment tab, from the Environment Version list, select the new environment version that you have created. Select the Confirmation check box and click UPGRADE . The production deployment gets associated with the new environment version. How do I collaborate with other users? \u00b6 Deploy Accelerator allows you to collaborate with other users on environments and related entities. You can share environments that you have created with multiple groups. When you share an environment, you can also define share permissions for groups. Based on the assigned share permissions, users in these groups can view, edit, export, and delete the environment. You can also enable these users to start and destroy their own deployments of the shared environment. The provider, connection, and deployments associated with the shared environment are not automatically shared. You can choose to separately share the provider, connection, and deployments and assign the appropriate permissions to the same set of groups. Otherwise, users in the groups with the deploy permission can use their own providers to start new deployments of the shared environment. Examples The following examples highlight the different benefits of collaborating with other users: The networking team can share the network environment and its Production deployment with the database and application teams and assign only the View permission. These teams can view and use the network deployment as the parent deployment for their own deployments. The application team can comprise multiple members who need to start deployments of the application environment. For example, different QA team members might need to start QA deployments of the application. The owner can share the environment with these users and assign permissions to deploy and destroy their own deployments. High-level steps to collaborate with other users Request your Deploy administrator to create a new group , assign the appropriate policies (access level) to the group, and add users to the group. (Optional) If you plan to use the registered Chef Server in your environment, request the administrator to share the appropriate Chef Server packages and roles with the new group. Create a new environment and select the appropriate provider and connection for the environment. While creating an environment, you can choose to share the environment with appropriate groups. However, you can also create the environment and later share that environment with the appropriate groups. By default, only the current environment version is shared with the selected groups. While creating a new version of this environment , you can choose to apply the share permissions from the base environment. (Optional) Share the provider and connections used in the environment with the same group. (Optional) When you start a new deployment of the environment, share this deployment with the appropriate group. When you share the environment, its deployments are not automatically shared with the group. You have to separately share each deployment with the appropriate groups and assign share permissions. Note: If multiple users in a group are working simultaneously on a shared environment or another shared entity, Deploy Accelerator shows an error message in case of any conflicts. For example, say User A has added a resource named vpc to a shared environment and User B also tries to add a resource named vpc , an error message is shown to User B . This error message is shown even if User A has not saved the environment after adding the vpc resource. How do I access entities of a user who has left the organization? \u00b6 If a user who leaves an organization has shared environments and other entities with a group, all members of that group can continue to access these shared entities. However, if the user has not shared any environments, deployments, providers, and connections, no other user can automatically access these entities. In this case, the Deploy administrator can disable the user and share all the user's entities with one or more groups. For detailed step-by-step instructions, see Disable an existing user . Example User A , User B , and User C are members of the Application Blueprints group in Deploy Accelerator. The three users share all their environments and deployments with this group, making it possible for them to access each other's entities. User C leaves the organization without sharing his most recent environments and deployments with the Application Blueprints group. Now no one (including User A and User B ) can access these environments and deployments. User A and User B have to spend time recreating User C's environments. Also, they cannot destroy any resources that User C has deployed. To resolve these issues, the Deploy administrator can disable User C and share all the user's entities with the Application Blueprint group. Now, User A and User B can access and work with User C's environments, deployments, providers, and connections. How do I create a template file? \u00b6 Deploy Accelerator enables you to render a template file from a template string by using the Template data source. Ideally, this template string should be loaded from an external file ( Local file resource). However, you can also use an inline template string in the Template data source. To create a template file by using the Template resource, perform the following actions: Create or open the environment in which you want to generate a template file. (Optional) To load the template string from an external file, perform the following actions: From the Resources tab in the left panel, drag the Local file resource to the canvas. In the Resource Name window, enter a name for the resource and click CREATE . In the name attribute, enter the name of the template file you want to create (for example, init.tpl ). In the content attribute, click Set text , enter the template string in the Set String value for window, and click SET VALUE . To escape interpolation, use double dollar signs ($$). From the Resources tab in the left panel, drag the Template data source to the canvas. In the vars attribute, click Set Json , enter the variables in the Set Json value for window, and click SET VALUE . Example: { \"keystore_password\" : \"password\" } In the template attribute, click Set text , and perform one of the following actions: If you have already added the template string in the Local file resource, specify the file path, as shown in the example below. \" ${ file ( \" ${ path .module } /init.tpl\" ) } \" Add an inline template string, as shown in the example in the image below. Consider the following points when using dollar signs ($) in the inline template string: To indicate a variable defined in the vars attribute of the Template data source, you must use double dollar signs ($$). In the example, double dollar signs are used to refer to the keystore_password variable. When you start a deployment, $ \\({keystore_password}** will be rendered as **\\) in the template file. To escape interpolation, the template file needs to use double dollar signs. However, if you are using an inline template string to generate the template file, you must use four dollar signs ($$$$). In the example, $$ ( \\({my_pass}** will be rendered as **\\) \\) in the template file. Save the environment. How do I dynamically generate a key pair for an EC2 instance? \u00b6 The following procedure describes an example of how you can use the TLS Private Key Generator and Key Pair resources to dynamically generate a key pair and associate it with an AWS EC2 instance. On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter the name as TLS_PrivateKey , add a description for this environment, specify the version as 01.00.00, select the appropriate environment package type, connection, and provider, and click CREATE . To add the TLS Private Key Generator resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the TLS Private Key Generator resource to the canvas. In the Resource Name window, enter the name as AppTLSPrivateKeyGen and click CREATE . To add the Key Pair resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the Key Pair resource to the canvas. In the Resource Name window, enter the name as AppKeypair and click CREATE . Select the AppKeypair resource and on the Resource tab in the right panel that opens, enter the following attribute values: key_name: AppKeypair public_key: ${AppTLSPrivateKeyGen.public_key_openssh} In the public_key attribute, you must use the interpolation syntax to reference the public_key_openssh attribute value of the TLS Private Key Generator resource that you created in step 3. To add the Instance resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the Instance resource to the canvas. In the Resource Name window, enter the name as AppServer and click CREATE . Select the AppServer resource and on the Resource tab in the right panel that opens, enter the following value for the key_name attribute: ${AppKeypair.key_name} In the key_name attribute, you must use the interpolation syntax to reference the key_name attribute value of the Key Pair resource that you created in step 4. In the Connection attribute, select Use Custom Connection and then click Set Json for the Custom Connection attribute. In the Set Json value for: customConnection window, enter the content shown below and click SET VALUE . { \"private_key\": \"${AppTLSPrivateKeyGen.private_key_pem}\", \"user\": \"ubuntu\", \"type\": \"ssh\" } In the interpolation syntax for the private_key parameter, you must use the interpolation syntax to reference the private_key_pem attribute value of the TLS Private Key Generator resource that you created in step 3. In the ami and instance_type attributes, specify the appropriate values. Click the Save icon ( ). (Optional) To use this dynamically generated key pair in another environment, perform the following actions: From the Resources tab in the left panel, drag the Output resource to the canvas. Select the output resource and on the Resource tab in the right panel that opens, click Set Json for the output attribute. In the Set Json value for: output window, enter the content as shown below and click SET VALUE . { \"ssh_private_key\": { \"value\": \"${AppTLSPrivateKeyGen.private_key_pem}\" }, \"key_name\": { \"value\": \"${AppKeypair.key_name}\" } } To save the environment and define the output, click the Save icon ( ). To start a new deployment, perform the following actions: Click the Deploy icon ( ). On the Deployment tab, enter the deployment name and description. Click START NEW DEPLOYMENT . (Optional) Open the other environment in which you want to use the dynamically generated key pair and perform the following actions: From the Resources tab in the left panel, drag the Depends On resource to the canvas. In the Resource Name window, enter the name as depends-on and click CREATE . Click the depends-on resource. In the right panel that opens, on the Resource tab, enter TLS_PrivateKey in the Depends_On attribute. From the Resources tab in the left panel, drag the Instance resource to the canvas. In the Resource Name window, enter the name as instance-dynamic-key and click CREATE . Select the instance-dynamic-key resource and on the RESOURCE tab in the right panel that opens, enter ${depends-on.key_name} in the key_name attribute. In this interpolation syntax, key_name is the variable that you have defined in the output resource of the TLS_PrivateKey environment in step 7. In the Connection attribute, select Use Custom Connection and then click Set Json for the Custom Connection attribute. In the Set Json value for: customConnection window, enter the content shown below, and click SET VALUE . { \"private_key\": \"${depends-on.ssh_private_key}\", \"user\": \"ubuntu\", \"type\": \"ssh\" } In the private_key parameter, use the interpolation syntax to reference the ssh_private_key variable that you have defined in the output resource of the TLS_PrivateKey environment in step 7. Click the Save icon ( ). Click the Deploy icon ( ) to open the Review and Deploy: EnvironmentName window. On the Deployment tab, enter the deployment name and description, select the parent deployment name, and then click START NEW DEPLOYMENT .","title":"How-to guides"},{"location":"deploy/howto-guides/#how-to-guides-for-deploy-accelerator","text":"The How-to guides for Deploy Accelerator focus on achieving specific goals, performing common tasks, or resolving specific issues. These guides include high-levels steps or detailed procedures with examples to help you to quickly accomplish your goals. For step-by-step instructions on performing various tasks in Deploy Accelerator, see Deploy and manage environments .","title":"How-to guides for Deploy Accelerator"},{"location":"deploy/howto-guides/#contents","text":"How do I import a new version of an existing environment? How do I upgrade an existing environment? How do I collaborate with other users? How do I access entities of a user who has left the organization? How do I create a template file? How do I dynamically generate a key pair for an EC2 instance?","title":"Contents"},{"location":"deploy/howto-guides/#how-do-i-import-a-new-version-of-an-existing-environment","text":"Deploy Accelerator allows you to import a blueprint as a new version of an existing environment. For example, say that you created a new environment in Deploy Accelerator by importing a blueprint from the Blueprint Gallery or your computer. And an updated version of this blueprint is now available. Deploy Accelerator allows you to import this updated blueprint as a new version of the existing environment. While you can import a blueprint with the same name as an existing environment, the version must be unique. Important: Importing a blueprint as a new version of an existing environment does not affect any of the existing deployments of the environment. High-level steps to import a new version of an existing environment Open the existing environment and note the name and existing version numbers of this environment. Based on your requirements, perform one of the following actions: To import a blueprint from your computer, click the More icon ( ) on the canvas and then click Import . To import a blueprint from the Blueprint Gallery, click the More options icon ( ) in the top-right corner of the Home page and then click Blueprint Gallery . On the Blueprint Gallery page, click IMPORT for the new blueprint version that you want to import. For detailed information about the different ways in which you can import a blueprint in Deploy Accelerator see, Creating new environments from blueprints . In the Import Environments window, perform the following actions: Enter the same name as your existing environment. Enter a unique version up to a maximum of 40 characters. It is recommended that the version you specify is greater than all versions of your existing environment. However, you can also specify an existing version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Select the appropriate connection and provider. If Chef Server is set as the environment package type, select the Chef Server and chef environment. Click IMPORT ALL . The blueprint is imported as a new version of your existing environment. Click the version list on the canvas. You can see the previous versions of the existing environment in this list. (Optional) Compare differences between the version that you have imported and previous versions of the existing environment. (Optional) Start a new deployment of the environment version that you have imported. None of the existing deployments of the environment are impacted. To see the list of deployments across all versions of the existing environment, see Viewing deployments . If required, you can also redeploy an existing deployment with the new environment version. For more information, see Redeploying existing deployments .","title":"How do I import a new version of an existing environment?"},{"location":"deploy/howto-guides/#how-do-i-upgrade-an-existing-environment","text":"When you create an environment and start a deployment, Deploy Accelerator creates the resources using the configured provider. After an environment is deployed in production, you can choose to continue to make updates in the same environment. However, this approach makes it difficult to keep a track of changes to an environment over time. Instead, it is recommended that you release the environment to freeze any updates to that environment. When you want to upgrade this existing environment (add or update resources), you can create a new environment version. High-level steps to upgrade an existing environment Create a new environment and add resources as required. To provision any compute resources in the environment, also add appropriate packages to those resources. Important: Deploy Accelerator cannot deploy packages on compute resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. Start a deployment of the environment. Each environment can have multiple deployments, such as Staging and Production. (Optional) After the production deployment of the environment completes successfully, release the environment . Releasing an environment ensures that no further updates can be made to the existing environment. If you redeploy existing deployments of a released environment, already deployed resources are not impacted. This is because during a redeployment, only updates to deployed resources are considered. To upgrade the existing environment that has been released, perform the following actions: Create a new environment version . In the new environment version, add or update resources as required. Save the environment. (Optional) Compare differences between the two environment versions. To upgrade the existing (production) deployment based on the new environment version, perform the following actions: From the deployments list on the canvas, select the existing deployment that you want to upgrade. To view the plan for the existing deployment, click the Plan icon ( ) and select Plan Selected Deployment . In the Review and Plan: Environment Name window, select the new environment version that you have created and PLAN . The deployment plan shows the new or updated resources that will be deployed. The already deployed resources that were not updated in the new environment version are not impacted. To redeploy the existing deployment, click the Re-Deploy icon ( ). On the Deployment tab, from the Environment Version list, select the new environment version that you have created. Select the Confirmation check box and click UPGRADE . The production deployment gets associated with the new environment version.","title":"How do I upgrade an existing environment?"},{"location":"deploy/howto-guides/#how-do-i-collaborate-with-other-users","text":"Deploy Accelerator allows you to collaborate with other users on environments and related entities. You can share environments that you have created with multiple groups. When you share an environment, you can also define share permissions for groups. Based on the assigned share permissions, users in these groups can view, edit, export, and delete the environment. You can also enable these users to start and destroy their own deployments of the shared environment. The provider, connection, and deployments associated with the shared environment are not automatically shared. You can choose to separately share the provider, connection, and deployments and assign the appropriate permissions to the same set of groups. Otherwise, users in the groups with the deploy permission can use their own providers to start new deployments of the shared environment. Examples The following examples highlight the different benefits of collaborating with other users: The networking team can share the network environment and its Production deployment with the database and application teams and assign only the View permission. These teams can view and use the network deployment as the parent deployment for their own deployments. The application team can comprise multiple members who need to start deployments of the application environment. For example, different QA team members might need to start QA deployments of the application. The owner can share the environment with these users and assign permissions to deploy and destroy their own deployments. High-level steps to collaborate with other users Request your Deploy administrator to create a new group , assign the appropriate policies (access level) to the group, and add users to the group. (Optional) If you plan to use the registered Chef Server in your environment, request the administrator to share the appropriate Chef Server packages and roles with the new group. Create a new environment and select the appropriate provider and connection for the environment. While creating an environment, you can choose to share the environment with appropriate groups. However, you can also create the environment and later share that environment with the appropriate groups. By default, only the current environment version is shared with the selected groups. While creating a new version of this environment , you can choose to apply the share permissions from the base environment. (Optional) Share the provider and connections used in the environment with the same group. (Optional) When you start a new deployment of the environment, share this deployment with the appropriate group. When you share the environment, its deployments are not automatically shared with the group. You have to separately share each deployment with the appropriate groups and assign share permissions. Note: If multiple users in a group are working simultaneously on a shared environment or another shared entity, Deploy Accelerator shows an error message in case of any conflicts. For example, say User A has added a resource named vpc to a shared environment and User B also tries to add a resource named vpc , an error message is shown to User B . This error message is shown even if User A has not saved the environment after adding the vpc resource.","title":"How do I collaborate with other users?"},{"location":"deploy/howto-guides/#how-do-i-access-entities-of-a-user-who-has-left-the-organization","text":"If a user who leaves an organization has shared environments and other entities with a group, all members of that group can continue to access these shared entities. However, if the user has not shared any environments, deployments, providers, and connections, no other user can automatically access these entities. In this case, the Deploy administrator can disable the user and share all the user's entities with one or more groups. For detailed step-by-step instructions, see Disable an existing user . Example User A , User B , and User C are members of the Application Blueprints group in Deploy Accelerator. The three users share all their environments and deployments with this group, making it possible for them to access each other's entities. User C leaves the organization without sharing his most recent environments and deployments with the Application Blueprints group. Now no one (including User A and User B ) can access these environments and deployments. User A and User B have to spend time recreating User C's environments. Also, they cannot destroy any resources that User C has deployed. To resolve these issues, the Deploy administrator can disable User C and share all the user's entities with the Application Blueprint group. Now, User A and User B can access and work with User C's environments, deployments, providers, and connections.","title":"How do I access entities of a user who has left the organization?"},{"location":"deploy/howto-guides/#how-do-i-create-a-template-file","text":"Deploy Accelerator enables you to render a template file from a template string by using the Template data source. Ideally, this template string should be loaded from an external file ( Local file resource). However, you can also use an inline template string in the Template data source. To create a template file by using the Template resource, perform the following actions: Create or open the environment in which you want to generate a template file. (Optional) To load the template string from an external file, perform the following actions: From the Resources tab in the left panel, drag the Local file resource to the canvas. In the Resource Name window, enter a name for the resource and click CREATE . In the name attribute, enter the name of the template file you want to create (for example, init.tpl ). In the content attribute, click Set text , enter the template string in the Set String value for window, and click SET VALUE . To escape interpolation, use double dollar signs ($$). From the Resources tab in the left panel, drag the Template data source to the canvas. In the vars attribute, click Set Json , enter the variables in the Set Json value for window, and click SET VALUE . Example: { \"keystore_password\" : \"password\" } In the template attribute, click Set text , and perform one of the following actions: If you have already added the template string in the Local file resource, specify the file path, as shown in the example below. \" ${ file ( \" ${ path .module } /init.tpl\" ) } \" Add an inline template string, as shown in the example in the image below. Consider the following points when using dollar signs ($) in the inline template string: To indicate a variable defined in the vars attribute of the Template data source, you must use double dollar signs ($$). In the example, double dollar signs are used to refer to the keystore_password variable. When you start a deployment, $ \\({keystore_password}** will be rendered as **\\) in the template file. To escape interpolation, the template file needs to use double dollar signs. However, if you are using an inline template string to generate the template file, you must use four dollar signs ($$$$). In the example, $$ ( \\({my_pass}** will be rendered as **\\) \\) in the template file. Save the environment.","title":"How do I create a template file?"},{"location":"deploy/howto-guides/#how-do-i-dynamically-generate-a-key-pair-for-an-ec2-instance","text":"The following procedure describes an example of how you can use the TLS Private Key Generator and Key Pair resources to dynamically generate a key pair and associate it with an AWS EC2 instance. On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter the name as TLS_PrivateKey , add a description for this environment, specify the version as 01.00.00, select the appropriate environment package type, connection, and provider, and click CREATE . To add the TLS Private Key Generator resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the TLS Private Key Generator resource to the canvas. In the Resource Name window, enter the name as AppTLSPrivateKeyGen and click CREATE . To add the Key Pair resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the Key Pair resource to the canvas. In the Resource Name window, enter the name as AppKeypair and click CREATE . Select the AppKeypair resource and on the Resource tab in the right panel that opens, enter the following attribute values: key_name: AppKeypair public_key: ${AppTLSPrivateKeyGen.public_key_openssh} In the public_key attribute, you must use the interpolation syntax to reference the public_key_openssh attribute value of the TLS Private Key Generator resource that you created in step 3. To add the Instance resource to the environment, perform the following actions: From the Resources tab in the left panel, drag the Instance resource to the canvas. In the Resource Name window, enter the name as AppServer and click CREATE . Select the AppServer resource and on the Resource tab in the right panel that opens, enter the following value for the key_name attribute: ${AppKeypair.key_name} In the key_name attribute, you must use the interpolation syntax to reference the key_name attribute value of the Key Pair resource that you created in step 4. In the Connection attribute, select Use Custom Connection and then click Set Json for the Custom Connection attribute. In the Set Json value for: customConnection window, enter the content shown below and click SET VALUE . { \"private_key\": \"${AppTLSPrivateKeyGen.private_key_pem}\", \"user\": \"ubuntu\", \"type\": \"ssh\" } In the interpolation syntax for the private_key parameter, you must use the interpolation syntax to reference the private_key_pem attribute value of the TLS Private Key Generator resource that you created in step 3. In the ami and instance_type attributes, specify the appropriate values. Click the Save icon ( ). (Optional) To use this dynamically generated key pair in another environment, perform the following actions: From the Resources tab in the left panel, drag the Output resource to the canvas. Select the output resource and on the Resource tab in the right panel that opens, click Set Json for the output attribute. In the Set Json value for: output window, enter the content as shown below and click SET VALUE . { \"ssh_private_key\": { \"value\": \"${AppTLSPrivateKeyGen.private_key_pem}\" }, \"key_name\": { \"value\": \"${AppKeypair.key_name}\" } } To save the environment and define the output, click the Save icon ( ). To start a new deployment, perform the following actions: Click the Deploy icon ( ). On the Deployment tab, enter the deployment name and description. Click START NEW DEPLOYMENT . (Optional) Open the other environment in which you want to use the dynamically generated key pair and perform the following actions: From the Resources tab in the left panel, drag the Depends On resource to the canvas. In the Resource Name window, enter the name as depends-on and click CREATE . Click the depends-on resource. In the right panel that opens, on the Resource tab, enter TLS_PrivateKey in the Depends_On attribute. From the Resources tab in the left panel, drag the Instance resource to the canvas. In the Resource Name window, enter the name as instance-dynamic-key and click CREATE . Select the instance-dynamic-key resource and on the RESOURCE tab in the right panel that opens, enter ${depends-on.key_name} in the key_name attribute. In this interpolation syntax, key_name is the variable that you have defined in the output resource of the TLS_PrivateKey environment in step 7. In the Connection attribute, select Use Custom Connection and then click Set Json for the Custom Connection attribute. In the Set Json value for: customConnection window, enter the content shown below, and click SET VALUE . { \"private_key\": \"${depends-on.ssh_private_key}\", \"user\": \"ubuntu\", \"type\": \"ssh\" } In the private_key parameter, use the interpolation syntax to reference the ssh_private_key variable that you have defined in the output resource of the TLS_PrivateKey environment in step 7. Click the Save icon ( ). Click the Deploy icon ( ) to open the Review and Deploy: EnvironmentName window. On the Deployment tab, enter the deployment name and description, select the parent deployment name, and then click START NEW DEPLOYMENT .","title":"How do I dynamically generate a key pair for an EC2 instance?"},{"location":"deploy/using/","text":"Deploy and manage environments \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to create, deploy, and manage environments in Deploy Accelerator. Contents \u00b6 Overview * Overview of environments * Overview of layered environments * Overview of blueprints * End-to-end process for multiple deployments of an environment Create environments * Configuring providers * Configuring connections * Creating environments * Creating layered environments * Creating new environments from blueprints * Copying environments Deploy environments * Starting new deployments * Viewing deployments * Viewing the plan for deployments * Redeploying existing deployments * Sharing deployments * Destroying deployments Release environments Releasing environment versions Creating new environment versions Manage environments Upgrading the provider version of environments Viewing dependency between environments Configuring custom tags Comparing differences between environments Searching environments Renaming environments Sharing environments Deleting environments Restoring deleted environments Accessing environments and deployments of other users Create and manage blueprints Exporting environments as blueprints Best practices for creating blueprints Adding blueprints in the Blueprint Gallery Manage resources Deploy Accelerator resources Copying resources Renaming resources Manage packages Adding user-defined packages Managing user-defined packages Creating new package versions Perform post-upgrade tasks Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 Note: To access Deploy Accelerator, you must sign in to Hitachi Cloud Accelerator Platform by using your Cloud Accelerator Platform account. You can access Deploy Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions. For information about creating a Cloud Accelerator Platform account, see Create & access account . Overview of environments \u00b6 Environments in Deploy Accelerator help you to design the infrastructure that you need to deploy different applications in the cloud. Environments are visual representations of your infrastructure and can be easily created by dragging resources and packages to a canvas. You can also add dependencies between the various resources in an environment. The following image shows an example of an environment in Deploy Accelerator: Each environment contains the following elements: Resources: Entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. Packages: Entities that use open-source infrastructure automation tools to configure compute resources in an environment. Deploy Accelerator provides multiple out-of-the-box packages. If required, you can also create your own user-defined packages by using Chef (Chef Solo or Chef Server) or Puppet . Links: Visual representation of dependencies between various resources in an environment. Scripts: Set of specific actions that must be performed before or after deploying and destroying an environment. Environment versions and multiple deployments \u00b6 You can release and version environments in Deploy Accelerator. Releasing an environment enables you to freeze any updates to the environment. Typically, when an environment is ready to be deployed in production, you can release that environment. To add or update resources in a released environment, you must create a new environment version. Environment versions enable you to keep a track of changes to an environment over time. After you have designed the infrastructure in an environment version, you can start a deployment to actually create the infrastructure. Based on your requirements, you can also start multiple deployments of the same environment version. Multiple deployments help you to create an environment once and then reuse it to create new infrastructures. For example, you can start QA and Production deployments of an environment version or you can start two Production deployments of that environment version. For each new deployment of an environment version, you can define different values for the input variables (or parameters) that are defined in the environment. For example, you can select different instance types (such as t2. micro or t2.large) for the AWS EC2 instances in your QA and Production deployments. For more information, see end-to-end process for multiple deployments of an environment . You can also redeploy an existing deployment with the same environment version or with another environment version. Deployments that you no longer need can easily be destroyed. Environments that you no longer need can also be deleted. If an environment has multiple versions, each version must be separately deleted. However, before you can delete an environment version, you must destroy all deployments for that version. Overview of layered environments \u00b6 A layered environment is basically a collection of multiple interdependent environments . For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between them, as shown in the following image. While deploying a child environment in a layered environment, you have to select a specific deployment of the parent environment. If parent environments have multiple deployments, you have to select a specific deployment while deploying the child environment. If you do not specify any deployment, the parent deployment with the name default is used to deploy the child environment. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other dependent environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. Overview of blueprints \u00b6 Blueprints in Deploy Accelerator are templates of commonly used IT infrastructures. If you have designed modular solutions (layered environments) or simple solutions (standalone environments) that need to be replicated multiple times, you can export the environments as blueprints . If only you plan to create environments from your blueprints, you can maintain the blueprints on your computer. However, if you want other users to create environments from your blueprints, you can add the blueprints in the Blueprint Gallery . Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . For example, you can create an environment for a high-availability deployment of Chef Server and export that environment as a blueprint. If another team needs to deploy its own high-availability Chef Server, a user in that team can import your blueprint, customize the deployment parameters, and start a new deployment. The following image describes the end-to-end lifecycle of a blueprint, from creating and exporting an environment as a blueprint to creating new environments from that blueprint. End-to-end process for multiple deployments of an environment \u00b6 The following procedure describes the end-to-end process that you can follow to implement multiple deployments of a standalone environment. Create a new environment and design your infrastructure. For example, create an environment with the version 01.00.00. Start a new deployment of the environment that you have created. For example, create a Staging deployment to test your design. (Optional) Based on your requirements, add or update resources in this environment and redeploy the existing deployment . Release the environment version . No changes can be made to a released environment version. However, you can start new deployments or redeploy existing deployments with this environment version. Start another new deployment of the environment. For example, create a Production deployment to deploy your applications. Note: For each new deployment, you can change the input variables based on your requirements. For example, you can use different VPC IDs for your Staging and Productions deployments. (Optional) View all deployments for this environment. To make any updates to the environment, create a new environment version and then add or update resources in this environment version. For example, create an environment version 02.00.00 that is based on version 01.00.00. Based on your requirements, perform one of the following options: Start a new deployment of the environment version that you have created. Plan and redeploy an existing deployment with this environment version. For example, you can redeploy the Staging deployment of environment version 01.00.00 with this new environment version 02.00.00. In this case, only the delta between the two versions is deployed. Resources that have not been changed between the two versions are not impacted. Note: When you redeploy an existing deployment with a different environment version, the deployment gets associated with that environment version. Release the environment version . Note: In the case of layered environments, you must follow these steps for each environment within the layered environment. However, in addition to using the Depends On resource to create a dependency between the environments, you must also create a dependency between specific deployments of the environments. For example, the Staging deployment of a child environment must be dependent on the Staging deployment of the parent environment. Configuring providers \u00b6 Providers in Deploy Accelerator allow you to deploy and manage different types of infrastructure resources. They also contain the authentication details required to deploy and manage the resources. For example, an AWS provider contains credentials for accessing the Amazon Web Services (AWS) account in which you want to deploy and manage resources, such as EC2 instances, subnets, and S3 buckets. Deploy Accelerator supports many different types of providers, such as AWS, Microsoft Azure, and Kubernetes. When you create a new environment in Deploy Accelerator, you have to select a provider. The resources that are available for the environment are based on the selected provider. By default, only you can use the providers that you create. However, you can choose to share your own providers with one or more groups. Supported providers \u00b6 The following table lists the providers that are supported in the latest version of Deploy Accelerator. It also lists supported versions of the Terraform providers that Deploy Accelerator uses to create and manage resources. Provider Supported Terraform Provider Version ACME 1.6.3 Artifactory 2.2.4 AWS 3.23.0 Azure 2.41.0 Azure Active Directory 1.1.1 Azure DevOps 0.1.0 Azure Stack (Beta) 0.9.0 Consul 2.10.1 Databricks 0.2.9 Datadog 2.18.1 DNS 3.0.0 Docker 2.8.0 Google Cloud Platform 3.51.0 HEC-CP None HEC-VM None Helm2 0.10.6 Helm3 1.3.2 Kibana 0.7.1 Kubernetes 1.13.3 Oracle 1.4.0 Oracle Cloud Infrastructure 4.7.0 vSphere 1.24.2 vSphere NSX-T (Beta) 3.1.0 Create a provider \u00b6 On the Home page, click the More options icon ( ) in the top-right corner. Click Providers . On the Provider page, click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). For a complete list of provider types that are available in Deploy Accelerator, see Supported providers . In Provider Details , enter the authentication details that Deploy Accelerator must use to deploy and manage resources. The provider details that you have to specify differ based on the selected provider type. For more information, see Provider details . You can also click the Terraform Provider Link below the Provider Details box to view the Terraform documentation on the selected provider. (Only for AWS and Kubernetes provider types) To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click SAVE . A new provider appears in the Providers List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details. (Optional) Share the provider with one or more groups. Important: If you later edit this provider, you must re-enter all values in the Provider Details section before you click UPDATE . Otherwise, an error message might be shown for deployments that use this provider. Delete a provider \u00b6 On the Home page, click the More options icon ( ) in the top-right corner, and then click Providers . Under the Provider List , click Delete ( ), and in the confirmation dialog box, click Yes . Once a provider is deleted the provider credentials are also removed from the database. Before deleting a provider, make sure that no environment is using the provider for deployments. If an environment uses a provider, the provider cannot be deleted. Provider details \u00b6 Deploy Accelerator supports multiple providers, such as AWS, Azure, and Kubernetes. While creating a provider, you have to specify authentication details that Deploy Accelerator can use to deploy and manage the resources. The following topics provide information about the required details for each provider type: ACME provider details Artifactory provider details AWS provider details Azure provider details Azure DevOps provider details Consul provider details Databricks provider details Datadog provider details Google provider details Helm provider details Kubernetes provider details Kibana provider details Oracle Cloud Infrastructure provider details ACME provider details \u00b6 The Automated Certificate Management Environment (ACME) is an evolving standard for the automation of a domain-validated certificate authority. The ACME provider allows you to acquire a valid SSL certificate from Let's Encrypt. The ACME provider supports a wide list of DNS challenge types, for example gcloud, azure, digitalocean, etc. For the complete list, see the Terraform documentation . Following are the parameters in an ACME provider: Parameter Details server_url The URL to the ACME endpoint's directory. This is a mandatory parameter. config The list of key-value pair required according to the provider. You can enter a key parameter value here to override the values from provider_reference_id. This is an optional parameter. provider_reference_id The reference ID of the reference provider. This is an optional parameter. If you enter this parameter, you will not have to specify the credentials of the cloud provider used as dns_challenge . Provider Details JSON Example { \"server_url\": \"\", \"config\": { AWS_ACCESS_KEY_ID = \"XXXXXX\" AWS_SECRET_ACCESS_KEY = \"XXXXXXX\" AWS_DEFAULT_REGION = \"us-east-1\" }, \"provider_reference_id\": \"1\" } Artifactory provider details \u00b6 The Artifactory provider is to manage the resources for Artifactory. Following are the parameters in Artifactory provider: Parameter Details url The URL for customer artifactory. This is a mandatory parameter. username The username for accessing customer artifactory. password The password for logging in to customer artifactory. access_token The access token for accessing the artifactory. api_key The API key for accessing the artifactory. Note: You require only one type of parameter for authentication. Enter a username/password, or an access token, or an API key. Provider Details JSON Example { \"url\": \"http://www.example.com\", \"username\": \"XXXXX\", \"password\": \"XXXXX\", \"access_token\": \"XXXXX\", \"api_key\": \"XXXXXXXXXXX\" } AWS provider details \u00b6 For the AWS provider type, you must select either Basic Credentials or Instance Profile to specify authentication details of the AWS account that Deploy Accelerator must use to deploy an environment. To use a more secure way of cross-account access, you must select Assume Role . Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can attach a role to the instance on which Deploy Accelerator is deployed. Deploy Accelerator can then assume a role in the other accounts and deploy the resources. For more information about assuming roles, see AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Deploy Accelerator is deployed. For the other accounts, you must specify a role that Deploy Accelerator can assume to deploy the resources. The role that you specify must have permissions to deploy the required resources. It must also define the account in which Deploy Accelerator is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Deploy Accelerator must deploy resources. However, Deploy Accelerator can use this method only if a role is attached to the instance on which Deploy Accelerator is deployed. This role must have the permissions to deploy the required resources. To use the Instance Profile method, you must specify only the region in which the resources must be deployed, as shown in the following example: { \"region\" : \"xx-xxxx-x\" } Static Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can specify long-term access credentials for only the parent account. Deploy Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Deploy Accelerator can assume to deploy resources in that account. The role that you specify must have access to deploy the required resources. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Deploy Accelerator must deploy resources, as shown in the following example: { \"access_key\" : \"ACCESS-KEY\" , \"secret_key\" : \"SECRET-KEY\" , \"region\" : \"xx-xxxx-x\" } The IAM user whose credentials you specify must have access to deploy the required resources. Azure provider details \u00b6 The Azure provider is used to interact with the resources supported by Azure. Following are the parameters in an Azure provider: Parameter Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated. Azure DevOps provider details \u00b6 The Azure DevOps provider is used to interact with the resources supported by Azure DevOps. Following are the parameters in an Azure DevOps provider: Parameter Details org_service_url (Required) This is the Azure DevOps organization URL. personal_access_token (Required) This is the Azure DevOps organization personal access token. The account corresponding to the token will need \"owner\" privileges for this organization. Consul provider details \u00b6 The Consul provider is used to interact with the resources supported by Consul. Following are the parameters in an Consul provider: Parameter Details address Public IP address of the instance on which Consul server is installed. datacenter The datacenter that is configured for the Consul server you have specified. The following is an example of the provider details section for a Consul provider. { \"address\" : \"123.0.57.189\" , \"datacenter\" : \"dc2\" } Databricks provider details \u00b6 The Databricks provider is used to interact with the resources supported by Databricks. Following are the parameters in an Databricks provider: Parameters Details host (optional) This is the host of the Databricks workspace. It is a URL that you use to login to your workspace. token (optional) This is the API token to authenticate into the workspace. username (optional) This is the username of the user that can log into the workspace. password (optional) This is the user's password that can log into the workspace. profile (optional) Connection profile specified within ~/.databrickscfg . To learn more about connect profiles, see Databricks documentation. Datadog provider details \u00b6 The Datadog provider is used to interact with the resources supported by Datadog. Following are the parameters in an Datadog provider: Parameters Details api_key Datadog API key. This is a mandatory parameter if the validate parameter is set to true . app_key Datadog APP key. This is a mandatory parameter if the validate parameter is set to true . api_url (optional) The Datagog API URL. validate (optional) Enables validation of the provided API and APP keys during provider initialization. Default is true. When false, api_key and app_key is not checked. Google provider details \u00b6 The Google provider is used to connect to Google Cloud Platform infrastructure products. You can also use Google beta resources in Google environments. Following are the parameters in the google beta provider: Parameters Details Credentials The credentials field is for entering the service account key in JSON format. You can generate a service account key using various methods, for more information, see Google Cloud Platform documentation . Project The project field is your personal project id. The project indicates the default GCP project all of your resources will be created in. Most Terraform resources will have a project field. Region The region is used to choose the default location for regional resources. Regional resources are spread across several zones. Sample { \"credentials\" :{ \"type\" : \"service_account\" , \"project_id\" : \"project-id\" , \"private_key_id\" : \"key-id\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"service-account-email\" , \"client_id\" : \"client-id\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\" }, \"project\" : \"my-project-id\" , \"region\" : \"us-central1\" } Helm provider details \u00b6 Helm2 and Helm3 providers allow you to specify details of the Kubernetes cluster in which you want to deploy Helm charts. The difference between the Helm2 and Helm3 provider types is the Terraform Helm provider version that is supported. Helm2 provider supports version 0.10.4 Helm3 provider supports version 1.2.1 or later Based on your requirements, you can choose the appropriate Helm provider type. The provider details that you have to specify for both provider types is the same. Note: Your administrator might have set the properties for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm2 or Helm3 provider type. In this case, you do not have to add the Helm Repository data source in your environment if your helm chart is available in this repository. Pre-requisites Before configuring a Helm2 or Helm3 provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. (Only for Helm 2 provider) Ensure that Tiller is running in the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Helm2 or Helm3 provider, you must populate the config_path_Content sub-attribute under the kubernetes attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the kubernetes attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 . Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file entered below the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } } ] } Example 3 The following is an example of the provider details section for a Helm2 or Helm3 provider for AWS EKS. { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"aws\" : [ { \"access_key\" : \"\" , \"secret_key\" : \"\" , \"region\" : \"\" } ], \"provider_reference_id\" : \"88\" } Example 4 The following is an example of the provider details section for a Helm2 or Helm3 provider for GKE. { \"google\": [ { \"zone\": \"XXXXX\", \"region\": \"XXXXXX\", \"project\": \"XXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXXXX\" } ] } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter. Kibana provider details \u00b6 The Kibana provider is used to interact with the resources supported by Kibana. Following are the parameters in an Kibana provider: Parameters Details elastic_search_path The path at which elastic search is hosted. kibana_uri The URI at which Kibana is hosted. kibana_version (Optional) The version of Kibana configuration. For the list of supported versions, see Kibana Terraform Provider documentation . In case of empty field the default value is 6.0.0 . kibana_type (Optional) The type of Kibana in the back-end. The default value is KibanaTypeVanilla , which supports the standard open-source kibana distribution. For more information, see Kibana Terraform Provider documentation . To configure logz.io kibana, use KibanaTypeLogzio . kibana_username (Optional) Username for Kibana API authentication. kibana_password (Optional) Password for Kibana API authentication. logzio_client_id (Optional) The client ID used for authentication with logzio. For more information, see Kibana Terraform Provider documentation . logzio_account_id (Optional)The logz.io account id. logzio_mfa_secret (Optional) MFA shared secret, create when signing up user account with MFA. Kubernetes provider details \u00b6 Kubernetes provider allows you to specify details of the Kubernetes cluster in which you want to deploy various resources. Pre-requisites Before configuring a Kubernetes provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Kubernetes provider, you must populate the config_path_Content attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the config_path_Content attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 and Example 5 Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Kubernetes provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"config_path_Content\" : \"\" , \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Kubernetes provider (JSON file entered below the config_path_Content attribute). { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } \"provider_reference_id\" : \"1\" } Example 3 The following is an example of the provider details section for a Kubernetes provider for AWS EKS. { \"config_path_Content\" : \"XXXXXXXXXXXXXXXXXXXXXXXXX\" , \"aws\" : [ { \"access_key\" : \"XXXX\" , \"secret_key\" : \"XXX\" , \"region\" : \"XXXX\" } ] } Example 4 The following is an example of the provider details section for a Kubernetes provider for GKE. { \"google\": [ { \"zone\": \"XXXXXXXX\", \"region\": \"XXXXXXX\", \"project\": \"XXXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXX\" } ] } Example 5 The following is an example of the provider details section for a Kubernetes provider for GKE using a reference provider. { \"google\": [ { \"zone\": \"\", \"region\": \"\", \"project\": \"\", \"service_key_Content\": \"\", \"cluster\": \"\" } ], \"provider_reference_id\": \"1\" } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter. Oracle Cloud Infrastructure provider details \u00b6 The Oracle Cloud Infrastructure is for managing the resources of the Oracle Cloud Infrastructure. Following are the parameters in the Oracle Cloud Infrastructure provider: Parameter Details tenancy_ocid OCID of your tenancy. For more information, see Oracle Cloud Infrastructure documentation . user_ocid OCID of the user calling the API. For more information, see Oracle Cloud Infrastructure documentation . fingerprint Fingerprint for the key pair being used. For more information, see Oracle Cloud Infrastructure documentation . private_key The path (including filename) of the private key stored on your computer, required if private_key is not defined. For information on how to create and configure keys, see Oracle Cloud Infrastructure documentation . region An Oracle Cloud Infrastructure region. For more information, see Oracle Cloud Infrastructure documentation . Provider Details JSON Example { \"tenancy_ocid\": \"XXXXXXXXXXX\", \"user_ocid\": \"XXXXXXXXXX\", \"fingerprint\": \"XXXXXXXXX\", \"private_key\": \"XXXXXXXXX\", \"region\": \"XXXXXXXXXXX\" } Share a provider \u00b6 Deploy Accelerator enables you to share your provider with other users. The ability to share providers is useful when multiple users in a group need to use the same provider to deploy environments. These users can view, edit, share, or delete providers that are shared with them based on the permissions assigned to their group. On the Home page, click the icon in the top-right corner. Click Providers . On the Providers List page, click the icon for the provider that you want to share with other users. You can identify the providers that are shared with you based on the color of the provider name. You can share a shared provider only if your group has been assigned the Share permission for that provider. In the Share Provider : ProviderName window, click the icon for the Group with which you want to share the provider. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group. Select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the provider, select the Warning check box and click SUBMIT . If you no longer want to share the provider, click CANCEL . Configuring connections \u00b6 Amazon Elastic Compute Cloud (EC2) uses public-key cryptography to encrypt and decrypt login information. When Deploy Accelerator launches an instance as part of deploying an environment, it specifies the name of a key pair (set of public and private keys) that must be used to connect to the instance. Each connection that you create contains the private key that can be used to connect to one or more instances in your environments. While creating an environment, you must specify a connection that is applied to all instances by default. You can also share your own connections with one or more groups. Note: If you do not want to use the default connection for all instances in an environment, you can create multiple connections. You can also choose to dynamically generate a key pair and associate it with one or more instances while the environment is being deployed. For an example of the step-by-step procedure to dynamically generate and associate a key pair with an instance, see Configure the dynamic generation of a key pair . Create a connection \u00b6 Create the required key pair by using either Amazon EC2 or a third-party tool. If you use a third-party tool, you must also import the public keys to Amazon EC2. For more information, see the Amazon Elastic Compute Cloud User Guide for Linux Instances or the Amazon Elastic Compute Cloud User Guide for Windows Instances . Important: To create a key pair by using Amazon EC2, you must have your own AWS cloud account. On the Home page, click the More options icon ( ) in the top-right corner and then click Connections . On the Add/Edit Connection page, click NEW . Enter the connection name and connection type. If you have selected the WinRM connection type, perform the following actions: In SSH/WinRM User , enter administrator . In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. (Optional) To set up a secure connection, select Https and enter a CA certificate in Cacert Note: If you want Deploy Accelerator to skip validation of the CA certificate, you can select Insecure . (Optional) To use NTLM authentication for remote connection, select NTLM . Important: When you select a WinRM connection for an instance, you must also specify a PowerShell script in the user_data attribute of that EC2 Instance resource. The user and password that you specify in the PowerShell script and the selected WinRM connection must match. Otherwise, you cannot deploy packages on that instance. If you have selected the SSH connection type, enter the user and perform one of the following actions: In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. In SSH Key , enter the SSH key instead of a password. (Optional) To specify details of a Bastion Host in the connection, select Bastion Connection , enter the following details, and then click SET PARAMETER . User Password (enter either the password or the key) Host (IP address of the Bastion host, which must be available in the network) Port (By default, the port number is selected based on the connection type.) Key You can use a Bastion connection based on how your network is configured. If the connection that you are creating is for instances in a private network and a Bastion Host is configured to connect to those instances, you must also specify details for connecting to the Bastion host. Note: The Bastion Connection link is enabled only for the SSH connection type. Click SAVE . A new connection appears under the Connection List section. (Optional) Share the connection with one or more groups. Share a connection \u00b6 Deploy Accelerator enables you to share your connection with other users. The ability to share connections is useful when multiple users in a group need to use the same connection for the instances that they are deploying. These users can view, edit, or delete connections that are shared with them based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top-right corner. Click Connections . On the Connection List page, click the Share icon ( ) for the connection that you want to share with other users. Note: You can identify the connections that are shared with you based on the color of the connection name. In the Share Connection: ConnectionName window, click the icon for the Group with which you want to share the connection. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the connection, select the Warning check box and click SUBMIT . If you no longer want to share the connection, click CANCEL . Creating environments \u00b6 The process to create an environment includes multiple steps, as shown in the following image. Create an environment Add resources Add packages Configure the environment To create a layered environment , you must perform these steps for each environment that is a part of the layered environment. For more information, see Creating layered environments . Before you begin \u00b6 Before you create an environment, ensure that you have performed the following actions: Configured the appropriate cloud account (for example, AWS account) in which you want to deploy an environment. For more information, see Configure providers . Created the connections that Deploy Accelerator can use to connect to the instances in your deployed environment. For more information, see Configure connections . Create an environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter a unique name and description for the environment that you want to create. Enter a version for the environment. By default, 01.00.00 is specified as the version for a new environment. If required, you can specify a different version. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Environment Package Type list, select the package type that you want to use for this environment. Under the Packages tab, the Private Packages list is populated depending on the Environment Package Type selected for creating the environment. Note: Deploy Accelerator supports the Chef Solo, Chef Server, Ansible Solo, and Puppet package types. However, the options that are shown in the Environment Package Type list are based on the configuration by your administrator . From the Connection list, select the default connection for all instances in this environment. You can identify shared connections in the list based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the appropriate account for accessing the cloud provider that you want to use to deploy the environment. You can identify shared providers in the list based on the color of the provider name. (Optional) From the AWS Region list, select the appropriate AWS region where you want to deploy the environment. Note: If you select an AWS Region while creating an environment, any predefined region in the Provider is overwritten with the new AWS Region. (Only if Chef Server is selected as the environment package type) To associate a Chef Server with the environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share the environment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the same groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections . Add resources to the environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment Resources are an important element of environments in Deploy Accelerator. They represent entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. You can also add Terraform data sources, Terraform modules , and Terraform Random Provider resources in your environment. You can add resources and data sources to the environment by using either the resource attributes form or the HCL editor . Notes: After you start a deployment of the environment , you can view various attributes of a resource for that deployment, such as its public IP address, on the Resource Details tab in the right panel. You can also add resources to an environment version after it has been deployed. These new resources get created when you start a new deployment or redeploy an existing deployment. However, no changes can be made to a released environment version. If you rearrange a resource on the canvas, you must save the environment to retain the changed position of that resource. Add resources and data sources to the environment \u00b6 In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name and click CREATE . Click the resource or data source that you have created. In the right panel that opens, on the Resource tab, enter the required details. Consider the following points while configuring the resource attributes. Goal Action Configure the connection and key pair that Deploy Accelerator must use to connect to a non-Windows instance. In the Connection attribute, select the appropriate SSH connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the key_name attribute, specify the corresponding key pair that Deploy Accelerator must associate with the instance when it is launched. Based on your requirements, you can also specify multiple key pairs. Note: To use a key pair that is dynamically generated while the environment is being deployed, you can select Use Custom Connection . The procedure to dynamically generate and associate a key pair with an instance includes multiple steps. For more information, see Configure the dynamic generation of a key pair . Configure the connection and user data that Deploy Accelerator must use to connect to a Windows instance In the Connection attribute, select the appropriate WinRM connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the user_data attribute, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. (See sample PowerShell script ). Important: You must ensure that the security group associated with the instance allows inbound traffic (ingress) from the port that you specify in the PowerShell script. Create multiple resources based on the same configuration In the Count attribute, specify the number of identical resources that you want to create while deploying the environment. Create a dependency on another resource in this environment In the Depends On attribute, specify the name of the resource on which you want to create a dependency. Use a variable that is defined in an Input Variable resource in this environment In any attribute, use the appropriate interpolation syntax based on the type of input variable that you are referring. For more information, see Formats for using input variables in resources . Use a local value that is defined in a locals resource in this environment. In any attribute, use the following interpolation syntax to refer to a local variable that is defined in the locals resource. ${ localResourceName . variableName } A local value can be a simple constant or an expression that can be defined once and used multiple times in different resources within the same environment. For more information, see the Terraform documentation . Reference attributes of another resource in the same environment For information about the supported attributes, select the resource whose attributes you want to reference and then click the resource type link at the top of the Resource tab in the right panel. Use the following interpolation syntax: ${ ResourceName . Attribute } Reference attributes of a data source in the same environment Use the following interpolation syntax: ${ DataSourceName . Attribute } Set array for attributes of the Text type While entering array values in attributes, specify each value on a separate line. The following image shows an example of array values for the vpc_security_groups_ids attribute of an AWS Instance : Apply Flatten flag for string array in JSON, or JSON array of the resources. To apply a Flatten flag on string arrays or nested string arrays in a JSON, or JSON array, select the Flatten toggle switch. The Flatten function is for replacing a nested string array list with a single flattened array. The array with the Flatten flag, returns a flattened sequence while migrating or importing the blueprint. Example -- JSON and JSON array -- String array Add a timeout for creating, reading, updating, or deleting resources. Use the Timeouts attribute for adding a timeout for creating, reading, updating, or deleting resources. You can define timeout for the following actions: - Create : To add a timeout while creating resources. - Update : To add a timeout while updating resources. - Read : To add a timeout while reading resources. - Delete : To add a timeout while deleting resources. The timeout is added in the following JSON format: To check the actions supported by a resource, click the Timeouts attribute in resource attributes form to open its relevant Hashicorp resource documentation. Sample PowerShell script for a Windows instance While creating a Windows instance, you must select a WinRM connection. In the user_data attribute of the instance, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. The following is an example of the PowerShell script that can be specified in the user_data attribute. < powershell > if ( [Environment] :: OSVersion . Version -ge ( new-object 'Version' 6 , 1 )) { New-NetFirewallRule -DisplayName \"Allow WinRM\" -Direction Inbound -Action Allow -Protocol TCP -EdgeTraversalPolicy Allow -LocalPort 5985 } else { netsh advfirewall firewall add rule name = \"Allow WinRM\" dir = in protocol = TCP localport = 5985 action = allow remoteip = any localip = any profile = any } winrm set winrm / config / service / auth '@{Basic=\"true\"}' winrm set winrm / config / service '@{AllowUnencrypted=\"true\"}' $admin = [adsi] ( \"WinNT://./administrator, user\" ) $admin . psbase . invoke ( \"SetPassword\" , \"password@123\" ) </ powershell > To add other resources or data sources, repeat steps 2 to 5. (Optional) Copy an existing resource from either the same environment or a different environment. (Optional) To remove an extra resource or data source from the canvas, click the x icon on that resource. (Optional) To view the links that are created when a resource uses the Depends On attribute or the interpolation syntax to reference another resource, click the Links icon ( ). Click the Save icon ( ). (Optional) To switch from resource attributes form to HCL code, select Switch to Hashicorp Configuration Language . The switch confirmation dialog box appears. Switching from attributes form to HCL code will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the HCL code, see Add resources and data sources using Hashicorp Configuration Language . Add resources and data sources using Hashicorp Configuration Language \u00b6 In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name, turn on the Switch to Hashicorp Configuration Language toggle switch, and click CREATE . Click the resource or data source that you have created. In the right panel that opens, you will see an HCL Editor on the Resource tab. In the HCL Editor, insert the HCL code for your resource. Click the Save icon ( ). ( Optional ) To switch the resource definition from HCL code to the default resource attributes form, turn off the Switch to Hashicorp Configuration Language toggle. The switch confirmation dialog box appears. Switching from HCL code to attributes form will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the default resource attributes form, see Add resources and data sources to the environment . Add Terraform modules to the environment \u00b6 Deploy Accelerator allows you to use an existing Terraform module in the environment. Terraform defines a module as a container for multiple resources that are used together . For more information about creating modules, see the Terraform documentation . To use an existing Terraform module, you can add the Terraform Module resource to the environment and specify the source from where the module can be downloaded. To add a Terraform module to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the Terraform Module resource to the canvas. In the Resource Name window, enter a unique name and click CREATE . Click the Terraform Module resource that you have created. In the right panel that opens, on the Resource tab, enter the following details: source: Enter the source from where to download the Terraform Module. Deploy Accelerator supports the following sources for Terraform Modules. Source type Supported format Terraform public registry <NAMESPACE>/<NAME>/<PROVIDER><br> Example: terraform-aws-modules/vpc/aws GitHub repository (public) github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION><br> Example: github.com/terraform-aws-modules/terraform-aws-vpc.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. GitHub repository (private) git::https://<USERNAME>:<PASSWORD>@github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION> Example: git::https://sampleuser:samplepassword@github.com/sample-demo/terraform-module.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. Artifactory ( configured in Deploy Accelerator ) artifactory::<OBJECT-RELATIVE-PATH> Example: artifactory::local-demo-sample/module/aws_vpc.zip Note: The Artifactory URL that is configured in the dnow.properties file is prefixed to the path that you specify in the source attribute. input: Input parameters for the Terraform Module. You can use the interpolation syntax to reference the output of other resources in the same environment or a parent environment as input to the Terraform module. tags: Tags that you want to assign to all resources in the Terraform Module. However, ensure that the Terraform Module supports tags. version: Version of the Terraform Module to download. Use this attribute when you specify a Terraform public registry in the source attribute. Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module from the Terraform public registry. Click the Save icon ( ). Note: You can use the interpolation syntax to reference the output of this Terraform module in other resources within the same environment or in child environments. Add Terraform Random Provider resources to the environment \u00b6 Deploy Accelerator allows you to use Terraform Random Provider resources in the environment. These resources are always available on the Resources tab irrespective of the provider type that is selected for the environment. Terraform defines the Random Provider as a logical provider that allows the use of randomness within Terraform configurations...it provides resources that generate random values during their creation and then hold those values steady until the inputs are changed. For more information about the Random Provider, see the Terraform documentation . To add a Random Provider resource to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the appropriate Random Provider resource to the canvas. A few examples of the Random Provider resources are Random Id , Random Password , and Random String . In the Resource Name window, enter a unique name and click CREATE . Click the Random resource that you have created. In the right panel that opens, on the Resource tab, enter the required details. For more information about the resource attributes, click the resource type link at the top of the Resource tab, as shown in the following image. Click the Save icon ( ). Add packages to the resources \u00b6 Create an environment > Add resources > Add packages > Configure the environment Packages in Deploy Accelerator are entities that use supported infrastructure automation tools to configure compute resources in an environment. Chef Solo and Chef Server are configured by default in Deploy Accelerator. However, your administrator might have also configured the use of Ansible and Puppet packages. Each package references a Chef cookbook, Ansible playbook, or Puppet manifest file or module. You can add user-defined packages to install your own custom applications on compute resources. Important: Deploy Accelerator cannot deploy packages on resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a new deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. To add packages to the resources, perform the following actions: In the new environment that you have created, from the left panel, click the Packages tab, and drag the appropriate package to a compute resource (for example, an EC2 Instance resource for AWS) on the canvas. The Packages tab displays packages of only the environment package type (Chef Solo, Chef Server, Ansible Solo or Puppet) that you have selected. If you have selected Chef Server as the environment package type, the Packages tab also displays roles from the selected Chef Server. You can add both Chef Server packages and roles to a compute resource. The following utility packages are always shown irrespective of the environment package type that is selected. Utility package Details file Uploads a file on the resource to which it is added. execute-script Runs a script on the resource to which it is added. You can specify whether the script must be run after the resource is created or before it is destroyed. local-exec Runs a command on the instance on which Deploy Accelerator is deployed. You can specify whether the command must be run after the environment is deployed or before it is destroyed. The local-exec package can be added to any resource in the environment. chef_configuration Restricts Deploy Accelerator from installing the ChefDK client on instances before deploying packages. You must set the skip_install attribute of this package to true . Note: This package is available only in environments for which Chef Server is selected as the environment package type. Click the package or role. In the right panel, on the PACKAGES tab, select the package or role that you have added to the resource. (Optional) From the Package Version list, select another version of the package or role. By default, the latest released version of the package or role is selected. Based on your requirements, you can select another version of the package or role. Ideally, you should add an unreleased version of a package or role to an environment only if you want to test that version. (Chef Solo and Chef Server packages only) From the Recipes list, select one or more recipes that you want to run on the resource. All recipes that the selected cookbook (or package) contains appear in the list. You must select the recipes in the order in which they must be run on the resource. If you do not select any recipe, Deploy Accelerator runs the default recipe that is defined in the cookbook. Enter other details about the package or role. Note: The details that you must specify for each package are different. For example, in case of the execute-script package, you can specify whether the script must be run after the resource is created or before it is destroyed by selecting the appropriate option from the when list. (Optional) To drag additional packages or roles to the resource, repeat steps 1 to 6. If you add multiple packages and roles to a resource, the packages and roles are arranged from top to bottom by default. However, you can drag and rearrange the packages and roles in the order in which you want to run them on the resource. Note: The utility packages are always run on the resource before the other selected packages and roles. However, if you add multiple utility packages to a resource, you must arrange these packages in the order in which you want to run them on the resource. To add packages or roles to other resources, repeat steps 3 to 6. (Optional) To remove a package or role from a resource, click the x icon on that package or role. Click the Save icon ( ). Configure the environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment Based on your requirements, you can configure additional settings for the environment. This step is optional in the end-to-end process of creating environments. In the new environment that you have created, click the Configure icon ( ). Based on your requirements, perform the following actions in the Configuration window: On the Environment tab, edit the following details for the selected environment version: Edit the description of the environment. Edit the connection, provider, or AWS Region for the environment. For information about configuring connections and providers, see Configure connection and Configure provider . To assign a custom tag to all resources in the environment, select that custom tag from the Custom Tag list. To view the key-pair values in the selected custom tag, move your cursor on the icon. For information about creating or editing a custom tag, see <a href=\"\" ui-sref=\"rean-platform-docs.accelerator({viewAccelerator: 'rean-deploy', viewPage: 'deploy-and-manage-environments', viewSection: 'tags'})\" style=\"text-decoration:none\">Configuring custom tags</a>. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that is selected for the environment. On the Deployment tab, configure the following details that are applied by default to all deployments of this environment version: Under Deploy , specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. By default, Deploy Accelerator does not destroy any deployment. On the Notification tab, specify the email notification details that are applied to all deployments of this environment version: Email: By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts or destroys a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. Deploy Template: Deploy Accelerator uses the default template for sending an email when a deployment is started. However, you can specify your own custom template. Destroy Template: Deploy Accelerator uses the default template for sending an email when a deployment is destroyed. However, you can specify your own custom template. Deployment Initiation: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users as soon as they start new deployments or destroy existing deployments of this environment. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Deployment Complete: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users after new deployments of this environment either succeed or fail, and existing deployments are successfully destroyed. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Note: Contact your system administrator for a list of custom email templates that might be available. Click SAVE . Where to go next \u00b6 When you are ready to create the infrastructure that you have designed in your environment, you can start a new deployment . Creating a layered environment \u00b6 Layered environments in Deploy Accelerator are a collection of environments in which some environments (child) are dependent on other environments (parent). You can create layered environments for complex IT infrastructures. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. To create layered environments, you require the following two resources: Output: You must add this resource in the parent environment. This resource enables you to expose some resource attribute values from the parent environment in the form of variables. These variables can then be referenced in other dependent (child) environments. Depends On: You must add this resource in the child environment. This resource enables you to create a dependency on another environment. Based on your requirements, you can add multiple Depends On resources in a child environment. To create a layered environment, perform the following actions: To create the parent environment, perform the following actions: Create an environment . Add the required resources in the environment . To reference attributes of the parent environment in one or more child environments, add the Output resource. In the Output resource, create a JSON file and specify variables that can be used in child environments. The value of these output variables can use the interpolation syntax to reference attributes of other resources in the parent environment. If you have added the Input Variable resource, the output variables can also reference the input variables. Save the environment. To create the child environment, perform the following actions: Create an environment . Add the Depends On resource in the environment. The name of the Depends On resource should ideally indicate the environment on which there is a dependency (for example: dependson_network ). In the reference_type attribute of the Depends On resource, select Environment Name . In the Depends_On attribute, select the appropriate parent environment version. Note: You can also select S3 in the reference_type attribute of the Depends On resource. For more information, see Deploy Accelerator resources . Add other required resources in the child environment . To reference attribute values of resources in the parent environment, use the following interpolation syntax: Syntax: ${ DependsOnResourceName.OutputVariableName } Example: ${ dependson_network.subnet } Save the environment. To confirm that the dependency on the parent environment, click the Environment Dependency View icon ( ). Creating new environments from blueprints \u00b6 Blueprints are templates of commonly used IT infrastructures. Deploy Accelerator users can export their environments as blueprints and add them in the Blueprint Gallery . You can leverage blueprints to quickly create infrastructure for solutions. Deploy Accelerator allows you to create new environments from blueprints by using any of the following methods: Import blueprints from the Blueprint Gallery Import blueprints from the Artifactory by using an API Import blueprints from your computer Important: If you create a layered environment from a blueprint, you must deploy the environments in a particular order. You must first deploy the environment in the top layer, then the environment in the next layer, and so on until the environment in the lowest layer. You must follow this order because child environments can be deployed only if their parent environments are already deployed. Before you begin \u00b6 Before you create an environment from a blueprint, ensure that you have configured the appropriate providers and connections that can be used for each environment in the blueprint. Import blueprints from the Blueprint Gallery \u00b6 The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable and configure the Blueprint Gallery . Also, blueprints are not available out of the box and have to be added in the Blueprint Gallery . The following image shows an example of the Blueprint Gallery. To import blueprints from the Blueprint Gallery, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . (Optional) To manually refresh the Blueprint Gallery to display all latest updates, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint data from the Artifactory. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. On the Blueprint Gallery page, browse or search for a blueprint that meets your requirements. The search results include blueprints that contain the search keyword in the blueprint name, description, owner name, or owner email. Searches are not case-sensitive and use partial keyword matching. For example, if you specify layer as the search keyword, the search results include blueprints with the name NetworkLayer and MultilayerEnvironment . Note: You can also sort the blueprints based on the blueprint name or owner name. By default, blueprints are sorted based on the blueprint names in the descending order. From the version list of the blueprint that you want to import, select the appropriate version. For details about the selected blueprint version, hover over Description . To access the ReadMe file of the selected blueprint version, click README . To view the email address of the blueprint owner, hover over the owner name. However, the email address is not shown if the blueprint owner is admin . Also, the owner name and email address is shown only if these values are available in the blueprint metadata. To import the selected blueprint version, click IMPORT . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as a part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. (Optional) Update the description for the environment. By default, the description from the blueprint is shown. From the Connection list, select the appropriate connection. You can identify shared connections based on the color of the connection name. From the Provider list, select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. When you import a blueprint from the Blueprint Gallery, all environments in that blueprint are in the Released state. If input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments . Import blueprints from the Artifactory by using an API \u00b6 The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable the Blueprint Gallery and specify a repository in the Artifactory for storing the blueprints. Based on your requirements, you can either import blueprints from the Blueprint Gallery or import blueprints from the Artifactory by using an API. The importBlueprintFromArtifactory API allows you to import blueprints from the specified repository in the Artifactory. You can specify the following parameters for importing a blueprint: blueprintName blueprintVersion environmentNamePrefix This parameter is optional. The value that you specify for this parameter is added as a prefix to the names of all environments that are imported as a part of the blueprint. groupName This parameter is optional. The group that you specify for this parameter is assigned the View and Deploy share permission for all the imported environments. For more information about the API, see the Deploy Accelerator API documentation: https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html#/Environment/importBlueprintFromArtifactory Import blueprints from your computer \u00b6 On the canvas, click the More icon ( ) and then click Import . In the Import Environment/Blueprint window, perform the following actions: Click Choose File and then select the blueprint (JSON file) that you want to import. Click Upload . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Enter the description for the environment that you are importing. From Connection , select the appropriate connection. You can identify shared connections based on the color of the connection name. From Provider , select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From Chef Server , select the Chef Server from which you want to display packages. From Chef Environment , select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. When you import a blueprint from your computer, the state of environments in that blueprint are dependent on their state when they were exported. For example, if parent and child environments in the Released state were exported as a blueprint, they are in the Released State when the blueprint is imported. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. If the imported environments are in a Released state and if input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments . Copying environments \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment that you want to copy. On the canvas, click the More icon ( ) and then click Copy . The environment version that is currently selected is copied. In the Create Environment window, enter a unique name, description, and version for the environment that you are creating. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Connection list, select the default connection for all instances in the environment. You can identify shared connections in the drop-down based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the cloud provider that you want to use to deploy the environment. You can identify shared providers in the drop-down based on the color of the provider name. (Optional) After selecting the provider, select the appropriate AWS region where you want to deploy an environment. Note: If you select an AWS region while creating an environment, any predefined region in the provider is overwritten with the new AWS region. If Chef Server is selected as the environment package type in the environment that you are copying, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, the share permissions of the environment that you have copied are automatically applied to this environment. (Optional) Modify the share permissions for this environment by selecting new or clearing existing permission check boxes for the appropriate groups. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the selected groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections . Starting new deployments \u00b6 Each environment can have multiple versions. When you are ready to create the infrastructure that you have designed in an environment version, you can start a new deployment. If required, each environment version can also have multiple deployments, such as Staging and Production. Important: You can deploy child environments only if their parent environments are already deployed. Therefore, in the case of a layered environment , you must deploy the environments it contains in a particular order. You must first deploy the parent environment in the top layer, then the child environment in the next layer, and so on until the child environment in the lowest layer. To start a new deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to deploy. To start a new deployment, click the Deploy icon ( ). The Review and Deploy: EnvironmentName window displays the default values that are configured for the environment. For more information, see Configure the environment . (Optional) To start a quick deployment with default values, click QUICK DEPLOY . (Optional) On the Environment tab, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for this deployment. Note: If you select an AWS region while starting a deployment, any predefined region in the provider is overwritten with the new AWS region. From the Custom Tag list, select the custom tag that you want to assign to all resources that are deployed. To view the key-pair values in the selected custom tag, hover over the icon. Note: If you have also specified key-pair values in the tags attribute of a resource in the environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for this deployment. Following additional information is displayed: Provider version: The version of the provider used for deploying the environment. Package Type: The package types used for deploying the environment. Created By: The user who created the environment. On the Deployment tab, perform the following actions based on your requirements: Under Deploy , enter the following details: Enter the deployment name and description. Note: The deployment name must be unique across all versions of the selected environment. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while deploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Parent deployments of the same name as the child deployment name are selected by default. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. (Optional) On the Notification tab, specify the email and template details for this deployment. By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. If required, you can specify custom templates for sending an email when a deployment is started or destroyed. Contact your system administrator for a list of custom email templates that might be available. Note: You cannot customize the Deployment Initiation and Deployment Complete settings for a deployment. These settings are configured at the environment level . To start a new deployment, click START NEW DEPLOYMENT . The icon appears on resources that are successfully deployed. The icon appears on resources that could not be successfully deployed. (Optional) To view the resource logs, click the Logs icon ( ). (Optional) To stop a deployment that is in progress, perform the following actions: Click the Stop icon next to the Deploying status. In the confirmation box, click YES STOP IT . The status of the deployment changes to Stopping . Deploy Accelerator deploys all resources in progress and then stops the deployment. The status of the deployment changes to Stopped . Note: If Deploy Accelerator is unable to successfully deploy any of the resources in progress, the status changes to Failed instead of Stopped . Viewing deployments \u00b6 Each environment version can have multiple deployments, such as Staging and Production. You can view the list of your deployments for an environment across all versions. You can also view the deployments that other users have shared with you. The deployment list also displays the status of each deployment -- deployed ( ) or failed ( ). On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of your own and shared deployments across all versions of the environment, click the deployment list on the canvas. Note: You can identify the deployments that are shared with you based on the color of the deployment name. In the list of deployments, click the deployment name that you want to open on the canvas. To view additional information related to the deployment after it is complete, select Deployment Details on the Canvas. The Deployment Details displays information about the following parameters: Deployment ID: The ID for deploying the environment. Deployment run ID: The run ID for querying the database for troubleshooting. Deployment Owner: User who had run the environment deployment. Deployment Input Variables: Additional deployment variables. Viewing the plan for deployments \u00b6 Before you start a new deployment or redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list, select the appropriate environment version. To view the plan for a deployment, perform one of the following actions: Goal Action To view the plan for a new deployment of the environment. Click the Plan icon ( ) and select Plan Environment . To view the plan for an existing deployment of the environment. Click the Plan icon ( ) and select Plan Selected Deployment . Note: You can view the plan for an existing deployment that is shared with you only if your group has been given the Redeploy permission. (Optional) To view the plan with default values, click QUICK PLAN . (Only to view the plan for a new deployment) In the Review and Plan: EnvironmentName window, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for the plan. The connection, provider, and AWS region that is configured for the environment version is shown by default. Note: If you select an AWS region for the plan, any predefined region in the provider is overwritten with the new AWS region. To plan the deployment with a different environment version, from the Environment Version list, select that version. In Input Variables , add input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while planning is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for the plan. (Only to view the plan for an existing deployment) In the Review and Plan: EnvironmentName window, update the environment version, input variables, and parent deployments mapping based on your requirements. Click PLAN . Note: In the case of a layered environment , the plan for child deployments is available only if the parent deployments are available. Redeploying existing deployments \u00b6 You can redeploy an existing deployment with the same environment version. If required, you can also redeploy an existing deployment with a different environment version. In both cases, only the changes made in the selected environment version are implemented. Deployed resources that have not been updated in the selected environment version are not impacted. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployments list on the canvas. From the deployments list, click the deployment name that you want to redeploy. Click the Re-Deploy icon ( ) to open the Review and Deploy window. Note: The Re-Deploy icon is available only if the environment version has been previously deployed. (Optional) On the Environment tab, perform the following actions based on your requirements: From the Connection list, select the connection that you want to use to connect to all resources in the environment. From the Custom Tag list, select the custom tag that you want to assign to all resources in the environment. To view the key-pair values in the selected custom tag, hover over the icon. (Optional) On the Deployment tab, perform the following actions based on your requirements: To redeploy with a different environment version, from the Environment Version list, select that version. By default, the environment version that was originally deployed is selected. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while redeploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Select the Confirmation check box. Click Upgrade . If you have redeployed with a different environment version, the deployment is now associated with that version. Sharing deployments \u00b6 Deploy Accelerator enables you to collaborate with other users on your deployments of an environment. The ability to share deployments is useful when multiple users need to view or manage the same deployment. All these users can view, redeploy, destroy, or stop the deployment based on the permissions assigned to their group. Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for the deployments. To share a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment whose deployment you want to share. Important: Before you share deployments of an environment version with selected groups, ensure that the environment version is also shared with those groups. Otherwise, users cannot view the shared deployments. From the deployments list, click the deployment name that you want to share. Click the Share icon ( ) and select Share Selected Deployment . You can share a deployment when it is in the Deploying , Deployed , Stopping , or Failed state. In the Share Deployment window, perform the following actions: Click the Share icon ( ) for the Group with which you want to share the deployment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the deployment, select the Warning check box and click SUBMIT . The deployment is shared with all users who are members of the selected groups If you no longer want to share the deployment, click CANCEL . (Optional) To enable users to redeploy the shared deployment by using the provider that is configured in that deployment, share the provider with the same groups. (Optional) To enable users to redeploy the shared deployment by using the connections that are configured in that deployment, share the connections with the same groups. Note: In case of layered environments, ensure that you share both parent and child deployments with the same groups. You must assign at least the View permission for the parent deployments. Otherwise, users cannot redeploy the child deployment. Destroying deployments \u00b6 Each environment version can have multiple associated deployments, such as Staging and Production. You can destroy a deployment that is no longer required. This action ensures that you are not paying for resources that are not being used. Destroying a deployment deletes all the deployed resources that it contains. However, the environment version and its other deployments continue to be available. Typically, you destroy test, development, or other non-production deployments. Note: You can destroy parent deployments only if their child deployments are already destroyed. Therefore, in the case of a layered environment , you must destroy the deployments in a particular order. You must first destroy the child deployment in the lowest layer, then the deployment in the next layer, and so on until the parent deployment in the top layer. To destroy a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployment list on the canvas. From the deployments list, click the deployment name that you want to destroy. Click the Destroy icon ( ). Note: The Destroy icon is available only if the environment version has been previously deployed. In the confirmation message box, type Yes and then click SUBMIT . After the deployment is destroyed, an email is sent to your registered email address and to the additional email address that might be configured for the deployment. (Optional) To view the resource logs, click the Logs icon ( ). The Logs icon continues to be available until the deployment is successfully destroyed. Releasing environment versions \u00b6 After an environment version is finalized for production, you can choose to release that version. No changes can be made to a released version. On the Home page, click the More options icon ( ) in the top-right corner, and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to release. From the version list, select Release Version . In the confirmation message box, enter Yes and then click SUBMIT . You can no longer make any changes to the released version of an environment. However, you can start new deployments or redeploy existing deployments with this released version. Creating new environment versions \u00b6 Each environment can have multiple versions. After an environment version is released, no changes can be made to that version. To add or update resources to the environment, you must create a new environment version. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the version on which you want to base the new environment version. From the version list, select Create New Version . In the Create New Version From base version window, enter a unique version up to a maximum of 40 characters. The new version must be greater than the base version that you have selected. Alternatively, you can retain the base version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). (Optional) To not apply share permissions from the base environment to the new environment version, clear the Use share permission from base environment check box. By default, share permissions configured for the base environment are applied to the new environment version. Click CREATE . A new version of the environment appears on the canvas. This version also appears on the Environment List page and in the Search Environments box. (Optional) In the new environment version, add or modify resources and packages based on your requirements. (Optional) Update the description and configure additional settings for the environment version based on your requirements. When you are ready to create the infrastructure that you have designed in the new environment version, you can start a new deployment or redeploy an existing deployment with this version. Upgrading the provider version of environments \u00b6 Deploy Accelerator uses Terraform providers to create, manage, and update resources in AWS, Azure, Google Cloud Platform, and other supported providers. When you create a new environment or a new version of an existing environment, resources are created based on the most recent provider version that is supported in Deploy Accelerator. When you open an environment whose provider version has been automatically upgraded, the Provider Version Upgraded Changes window appears. For each resource in the environment, it lists the attributes that are deprecated or cannot be upgraded. The resources that require user action are marked in red. The resources for which Deploy Accelerator has automatically performed the changes are marked in green. To acknowledge the upgrade changes, click the Confirmation checkbox, and click CONFIRM. However, the list of provider upgrade changes will not be shown after you submit your acknowledgement. To retain the list of provider upgrade changes for some time, you can click CLOSE. To reopen the window, click Provider upgrade changes on the canvas. Important : If the upgrade changes are not acknowledged, the Provider Version Upgraded Changes window appears while deploying the environment. If the upgrade changes are acknowledged but the manual changes are not made to the resources, you will see an error in the deployment log file. Viewing dependency between environments \u00b6 The Environment Dependency View enables you to view the dependency of an environment on other environments (parent environments). You can also view other environments (child environments) that are dependent on an environment. If the environment has deployments, this view displays the parent or child deployments of the selected deployment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. If the selected environment has deployments, the deployment list on the canvas displays the latest deployment of the selected environment version. Click the Environment Dependency View icon ( ). The Environment Dependency View window appears, as shown in the following image: If the environment has a deployment, the Environment Dependency window displays the connection between the selected deployment and its parent or child deployments. The color of the deployment represents its current state. For example, black indicates Not Started, orange indicates Running, green indicates Deployed, and red indicates Failed. Click the toggle switch to see the dependent parent and child environment(s). If the toggle switch is towards Child, the Environment Dependency window displays the connection between the current environment and its child environment(s). If the toggle switch is towards Parent, the Environment Dependency window displays the connection between the current environment and its parent environment(s). (Optional) To switch to another environment or deployment shown in the Environment Dependency View window, select that environment or deployment name. Configuring custom tags \u00b6 You can define tags for individual resources in an environment but this process is repetitive and time consuming. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can create a custom tag, which contains a set of AWS key-pair tags, and attach the custom tag to an environment. Deploy Accelerator attaches this custom tag to all resources in the environment. For example, to control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. To configure a custom tag, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Custom Tags . Under Custom Tag , click NEW . Enter the custom tag name and description. Under Tags , define the tags that you want to assign to all resources in the environment. The following image shows an example of how you must specify tags for your resources. Note: Tag values for all the resources that have tag support in your environment are set to the default values from the tags defined in the selected custom tag. Click SAVE . A new custom tag appears under the Custom Tag List section. Note: You can also add tags to a resource by setting the tags attribute in the RESOURCE panel. If users select a custom tag for an environment and specify values in the tags attribute of a resource in that environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. Comparing differences between environments \u00b6 Deploy Accelerator enables you to compare differences between two environments or two versions of the same environment. Comparing two versions of the same environment helps you to track changes to an environment over time. Comparing environments is also useful when you have two similar environments and you want to retain only one of them. You can compare the differences between these environments, ensure that one of the environments has all the required resources and packages, and then delete the other environment. The comparison detects the following differences between the base and target environments. Comparison Details Environment configuration-level comparison This comparison displays a list of configuration details that have changed in the target environment version: -- Connection, provider, region, and environment package type -- Chef server and Chef environment -- Environment owner -- Custom tag -- Released status -- Notification email address and email templates -- Deploy and Destroy pre-scripts, post-scripts, and destroy after interval Resource-level comparison This comparison displays a list of resources that are added, edited, or removed in the target environment version. For each edited resource, you can see a list of attributes and packages that are added, edited, or removed. If a package has been edited, you can also see the differences in the package attributes. To compare differences between environments, perform the following actions: On the canvas, click the More icon ( ) and then click Compare . The environment version that is currently selected is considered as the Base Environment for comparison. (Optional) In the Compare Resources and Packages window, from the Base Environment and Version lists, select a different environment and environment version. You can identify environment versions that are shared with you based on the color of the environment version. From the Target Environment and Version lists, select the environment and environment version with which you want compare the base environment. The RESOURCES tab is selected by default. In the Resources panel, you can see a list of resources that are added, edited, or removed in the target environment. To help you identify the changes, the resources are highlighted in different colors, as shown in the following table. Highlighted color Meaning Green Added resource Red Deleted resource Blue Edited resource To see the resource-level comparison, click an edited resource in the Resources panel. On the RESOURCE ATTRIBUTES and PACKAGES tabs, you can see the differences in attribute values and packages. Green background color indicates added content while red background color indicates deleted content. To see the environment configuration-level comparison, click the CONFIGURATION tab. On the ATTRIBUTES tab, you can see a list of configuration details that have changed in the target environment. Green background color indicates added content while red background color indicates deleted content. Click CLOSE . Searching environments \u00b6 On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, in the Search Environment tab, enter a search string to find an environment. The search string filters the environments and displays results with the string. The search string filters environments across all columns on the Environment List page. Renaming environments \u00b6 On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to rename. On the canvas, click the More icon ( ) and then click Rename . In the Rename Environment window, enter a unique name for the environment and click SUBMIT . All versions of the environment are renamed. You can see the updated environment name in the Search Environment box, Environment List page, and all other instances. The environment name is also updated in any child environments that reference this environment. Note: You can rename an environment that has been shared with you only if the root environment version is shared with the Edit permission. Otherwise an error message is shown when you click SUBMIT . Sharing environments \u00b6 Deploy Accelerator enables you to collaborate with other users on an environment. The ability to share environments is useful when multiple users need permissions to perform different tasks such as edit, deploy, destroy, delete, view, and export on the same environment. All these users can work and collaborate on a shared environment at the same time. Users can perform one or more tasks based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to share with other users. Click the Share icon ( ) and select Share Environment . In the Share Environment window, perform the following actions: (Optional) To share all existing versions of the environment with other users, select Share All Versions . By default, only the currently selected environment version is shared with other users. Note: If you later create a new version of this environment , you must separately share that environment version with users. While creating the new version, you can choose to apply the share permissions from the base environment. Click the icon for the Group with which you want to share the environment version. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign share permissions to users, select the appropriate permission check boxes and click DONE . You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the environment, select the Warning check box and click SUBMIT . The environment version is shared with all users who are members of the selected groups. When you share an environment version, its deployments are not automatically shared with the specified groups. You must separately share each deployment with the appropriate groups and assign the required permissions. If you no longer want to share the environment, click CANCEL . Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for these deployments. (Optional) To enable users to start a deployment of the shared environment version by using the provider that is configured in that version, share the provider with the same groups. (Optional) To enable users to successfully use the connections that are specified in the shared environment version, share the connections with the same groups. Deleting environments \u00b6 You can choose to delete environments that you no longer need. If an environment has multiple versions, each version must be separately deleted. Deleting an environment version also permanently deletes all configurations related to that version. On the Home page, in the Search Environment box, enter the name of the environment that you want to delete. If the environment has multiple versions, each version is listed separately in the Search Environment box. You can identify environments that have been shared with you based on the color of the environment name. Next to the environment version that you want to delete, click the Delete icon ( ). In the confirmation message box, click YES . The environment version is no longer shown in the list of environments. Note: You cannot delete environment versions that have deployments. Similarly, you cannot delete an environment version that is being used as a parent in one or more child environments. Restoring deleted environments \u00b6 You can restore a deleted environment which was deleted by you, or the environments for which you have the Delete permission. On the Home page, click the More options ( ) icon in the top right corner and then click Environments . In the Environment List , select Show Deleted Environments . The deleted environments appear at the end of the list. Under the Actions column, click Restore to restore the environment. In the confirmation message box, click YES . The environment is restored in the list of environments. Accessing environments and deployments of other users \u00b6 By default, Deploy Accelerator allows you to access only the following environments and deployments: Environments and deployments that you own. For more information, see Creating environments and Starting new deployments . Environments and deployments that other users have shared with you. For more information, see Sharing environments and Sharing deployments . However, sometimes you might need to access the environments and deployments of other users even though they are not shared with you. Your administrator can grant you this ability by adding you to the VIEW_ALL_USER'S_ENTITIES group. This group allows you to view all environments and deployments of all users in Deploy Accelerator. Note: To perform any actions (such as delete environments and destroy deployments) on the entities of other users, you must also be a member of the ADMIN group. To access environments and deployments of other users, perform the following actions: To access an environment or a deployment of another user, ensure that you have the ID of the environment version. Note: When an environment version is opened on the canvas, its ID is shown in the URL. Open a browser and enter the Deploy Accelerator URL in the following format: https:// YOUR-PLATFORM-BASE-URL /hcapdeploy/#/home/dnow/ ENVIRONMENT-VERSION-ID To view the list of all deployments across all versions of the environment, click the deployment tab on the canvas. Select the deployment that you want to view. Exporting environments as blueprints \u00b6 If you create a standalone or layered environment that needs to be deployed multiple times, you can export that environment as a blueprint. Ensure that you have followed the best practices for creating blueprints . If you are exporting a layered environment, also ensure that you export the child environment that is in the lowest layer. This action ensures that parent environments in all other layers are also included in the blueprint. You can also export multiple environments. To export multiple environments, you can select a child environment and all the parent environments in the hierarchy are automatically included. Example of exporting a layered environment Consider a scenario in which you create a Jenkins environment and an APP-VPC environment that is dependent on the Jenkins environment. You then create environments for different applications - Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator), and Hitachi Cloud Accelerator - Repair. Each of these environments has a dependency on the APP-VPC environment, as shown in the following image: When you export one of the application environments as a blueprint, the Jenkins and APP-VPC environments are also included in the blueprint and the dependency between them is retained. To replicate the application environment that you have exported, you can create new environments from the blueprint , make customizations if required, and then start a deployment of the environments. To export an environment as a blueprint, perform the following actions: On the canvas, click the More Options icon ( ) and then click Export . In the Environment Export window, from the list select the environment(s) to export. The environment which is open on the canvas is selected by default. The environments created by you are listed in black color. The environments that other users have shared with you are listed in a different color. Important: If you select a child environment, all its parent environments are also included in the exported file. To filter a long list of environments, enter the search keywords in the Search Environments box. Enter a name for the blueprint. Select the warning checkbox to acknowledge the warning message, and click EXPORT . A JSON file is saved as ***fileName*.blueprint.reandeploy** to your local computer. In this JSON file, the environments, the resources within each environment, and the packages within each resource are alphabetically sorted. The packages that are listed after the environments are also alphabetically sorted. Best practices for creating blueprints \u00b6 Consider the following best practices while creating environments that you want to export as blueprints: Blueprints must ideally use a layered environment. You should create an environment with network resources, such as VPCs and CIDRs. In all other environments within the blueprint, you should get the VPC IDs, CIDRs, and other infrastructure resources from this environment by using the Depends On resource. The name of the Depends On resource in each environment must ideally indicate the environment on which there is a dependency. For example, depends_on_app_vpc . Blueprints must enable users to change resource attribute values by using input variables. Blueprints must output important information, such as Server URLs and IDs of important resources, to outputs. Blueprints must use name attributes that are descriptive and unique. For example, you must use ${var.environment_name}-myserver instead of myserver . Blueprints must not hard-code critical things, such as AWS regions, account IDs, VPCs, owner information, product information, environment information, buckets, user names, and passwords. For things that should be automatically detected, blueprints must use Terraform data sources instead of input variables. The following table lists a few examples of data sources that you can use in your environment. To know all the data sources that you can add to your environment, see the Data Sources section on the Resources tab in the left panel on the Deploy Accelerator Home page. Goal Data source Usage Get the current AWS Account ID aws_caller_identity ${ DataSourceName .current.account_id} Get the name of the current region aws_region ${ DataSourceName .current.name} Get the current partition and use it to build an Amazon Resource Name (ARN) aws_partition ${ DataSourceName .current.partition} Get a list of availability zones aws_availability_zones ${ DataSourceName .available.names} Get the current Elastic Load Balancer (ELB) service account ID and use it in the bucket policy for ELB logs aws_elb_service_account ${ DataSourceName .current} Adding blueprints in the Blueprint Gallery \u00b6 If you have created an environment that needs to be replicated multiple times by different users, you can export that environment as a blueprint and then add the blueprint in the Blueprint Gallery. Users can then create a new environment by importing that blueprint from the Blueprint Gallery . Before you begin Ensure that you can access the Artifactory that is configured for Deploy Accelerator. In the Artifactory, ensure that you can access the repository that the Administrator has configured for storing the blueprints that are displayed in the Blueprint Gallery. Export your environment as a blueprint . To add a blueprint in the Blueprint Gallery, perform the following actions: Create additional files that are required for the blueprint. Create a metadata.yml file for the blueprint. This file must contain the attributes that are listed in the following table. Attribute Details name The blueprint name specified in this file is shown in the Blueprint Gallery. Ensure that the blueprint name in this file matches the actual file name of the blueprint (which you provided while exporting the environment as a blueprint from Deploy Accelerator). For example, if you exported an environment from Deploy Accelerator as Chef-Server-HA , the blueprint file name is Chef-Server-HA.blueprint.reandeploy and the name that you must specify in the metadata.yml file must be Chef-Server-HA . description The blueprint description specified in this file is shown in the Blueprint Gallery. version The blueprint version specified in this file is shown in the Blueprint Gallery. image To display an image for the blueprint version in the Blueprint Gallery, specify that image in this file and add the image file in the images folder that you will create in a later step. You must specify the image path relative to the BlueprintName and Version folder in the Artifactory. For example: images/chef.png The image must be in the PNG format. Also, it is recommended that the image dimensions are 130 x 40 pixels. Note: If you later want to update the image for a blueprint version, it is recommended that you use a different image file name and update the metadata.yml file. If you just replace the image file with the same name in the images folder, the updated image is not shown in the Blueprint Gallery. ownerName The owner name specified in this file is shown in the Blueprint Gallery. If you add this attribute in the file but do not specify a value, admin is shown as the owner of this blueprint. ownerName is an optional attribute. ownerEmail The owner email specified in this file is shown in the Blueprint Gallery. ownerEmail is an optional attribute. The following image shows an example of a metadata.yml file. (Optional) Create a README.md file for the blueprint. This file (Markdown format) should ideally include information such as deployment architecture, details about input and output variables used in the environment, and any dependencies. To add an image in the README.md file, you must add the image file in the images folder that you will create in the next step. Also, you must use the IMAGE_BASE_PATH prefix for the image path in the README.md file, as shown in the example below: IMAGE_BASE_PATH/images/chef.png (Optional) Create an images folder and add the image that you want to show for the blueprint version in the Blueprint Gallery. Also, add any images that are used in the README.md file in this folder. Add the blueprint in the Blueprint Gallery. Sign in to the Artifactory that is configured for Deploy Accelerator. Navigate to the repository in which the blueprints must be stored. Create a folder with the same name as the blueprint file name (for example: Chef-Server-HA). Important: The blueprint is displayed in the Blueprint Gallery only if the folder name, blueprint file name, and the blueprint name defined in the metadata.yml file are the same. In the BlueprintName folder, create a folder with the blueprint version (for example: 01.00.00). Ensure that the folder name matches the blueprint version that is defined in the metadata.yml file. In the version folder, add the following files and folder: metadata.yml file ***BlueprintName*.blueprint.reandeploy** file (Optional) README.md file (Optional) images folder Verify that the blueprint is available in the Blueprint Gallery and can be successfully imported and deployed. Sign in to Cloud Accelerator Platform . On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . On the Blueprint Gallery page, search for the blueprint that you have just added in the Artifactory. (Optional) If you cannot see your blueprint in the Blueprint Gallery, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint metadata from the Artifactory. This metadata includes the blueprint name, description, version, and image. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. Confirm that the name, description, version, and image of your blueprint are displayed correctly. Import the blueprint and start a new deployment . Deploy Accelerator resources \u00b6 The Resources tab displays many resources that you can add to your environment. The following table lists the additional resources that Deploy Accelerator provides to address specific scenarios : Resource name Details Depends On Use this resource to create a dependency on another environment. The name of the Depends On resource must ideally indicate the environment on which there is a dependency. In the reference_type attribute, you can select either Environment Name or S3 . -- If you select Environment Name , click the Depends On box and select the parent environment version on which the environment must be dependent. The selected parent environment appears in the environmentName:environmentVersion format (for example: networkEnv:01.00.00). While exporting the child environment, Deploy Accelerator exports this version of the parent environment. -- If you select S3 , enter the S3 bucket name and the remote state file name of the environment that is referred by the Depends On resource. For example: s3://bucket_name/tfstatefile_name , in which bucket_name is the name of the S3 bucket and tfstatefile_name is the Terraform remote state file name of the environment that is to be referred. If required, you can define variables for the S3 bucket and Terraform state file names in the Input Variables resource. To reference these variables in the Depends On resource, you must use the following interpolation syntax: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } For example: If you have defined the S3bucket and tfstatefile variables in the Input Variables resource, specify the following URL in the Depends On resource. s3:// \\({var.s3bucket}/\\) Ensure that the provider that you use to deploy the environment has the appropriate access to the S3 bucket that you have specified. Note: For information about how you can create layered environments by using the Depends On resource, see Creating layered environments . Input Variable Use this resource to create a JSON file in which you can specify multiple variables and set their values. You can later use these variables in other resources within the same environment. For information about the different ways in which you can define and use input variables, see Formats for defining input variables and Formats for using input variables in resources . Locals Use this resource to create a JSON file in which you can specify multiple local values. You can later use these local values in other resources within the same environment. A local value can be a simple constant or a complex expression that transforms or combines values from other resources in the environment, as shown in the following example. For more information, see the Terraform documentation . Output Use this resource to create a JSON file in which you can specify multiple variables that can be used in other dependent environments. The value of these variables can reference either variables defined in the Input Variable resource or attributes of other resources in the environment by using the interpolation syntax. Note: If you do not want to display an output value in the resource logs of an environment, set the sensitive parameter for that output to true . Existing VM Use this resource to add an existing instance to the environment. Formats for defining input variables \u00b6 In the Input Variable resource, you can use the following formats to define input variables based on your requirements: String: Use the following syntax to define input variables by using the String format: \"string_input\": { \"type\": \"string\", \"default\": \"sample_value\", \"description\": \"this is sample string value for hcap deploy\" } Example: \"string_input\": { \"type\": \"string\", \"first_name\": \"John\", \"description\": \"First name of the user\" } List: Use the following syntax to define input variables by using the List format: \"list_input\": { \"type\": \"list\", \"default\": [ \"sample_value1\", \"sample_value2\" ], \"description\": \"this is sample list value for hcap deploy\" } Example: \"list_input\": { \"type\": \"list\", \"Region\": [ \"us-west-2\", \"us-east-1\" ], \"description\": \"List of regions to be used\" } Map: Use the following syntax to define input variables by using the Map format: \"map_input\": { \"type\": \"map\", \"default\": { \"key\": \"sample_value\" }, \"description\": \"this is sample map value for hcap deploy\" } Example: \"map_input\": { \"type\": \"map\", \"ec2details\": { \"app\": { \"ami\": \"ami-718c6909\", \"type\": \"t2.micro\" }, }, \"description\": \"this is sample map value for hcap deploy\" } The following image shows an example of using these different formats in the Input Variable resource. Formats for using input variables in resources \u00b6 In any environment, instead of hard coding the attribute value of a resource, you can use the interpolation syntax to refer to the input variables that you have defined. The interpolation syntax is based on the type of input variable that you are referring. String: Use the following interpolation syntax to refer to an input variable of the String type: ${var.variableName} Example: ${var.string_input} List: Use the following interpolation syntax to refer to an input variable of the List type: To refer to all the values in a list, use the following syntax: ${var.variableName} To refer to a specific value in a list, use the following syntax: ${var.variableName[index]} Examples: ${var.list_input} ${var.list_input[0]} Map: Use one of the following interpolation syntax to refer to an input variable of the Map type: ${var.variableName} ${var.variableName[\"Key\"]} ``` ${lookup(var.variableName[\"Key\"], \"InnerKey\")} ``` Examples: ``` ${var.map_input} ``` ``` ${var.map_input[\"ec2details\"]} ``` ``` ${lookup(var.map_input[\"ec2details\"], \"app\"} ``` Copying resources \u00b6 You can copy one or more existing resources from either the same environment or a different environment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment from which you want to copy a resource. On the canvas, right-click the resource that you want to copy, and click Copy . To copy multiple resources, repeat this step for each resource. Based on your requirements, perform one of the following actions: To create a copy of the resource in the same environment, right-click the canvas and then select the resource from the Paste Resource list. To create a copy of the resource in another environment, open that environment in a new tab, right-click the canvas and then select the resource from the Paste Resource list. All the resources that you have copied appear in the Paste Resource list. You can add a copied resource to any environment only once. ![](/images/rean-deploy/rd_resourcepaste.PNG) From the Paste Resource list, click the appropriate resource and then perform the following actions: In the Resource Name window, enter a name for the resource and click CREATE . In the right panel that opens, on the Resource tab, you can see that the attribute values are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The Packages tab also displays the same packages, along with their attributes, as the original resource. On the Resource tab, add or update the attribute values based on your requirements. Important: If you have copied a resource from another environment, you must update attribute values that use the interpolation syntax to reference input variables or other resources. You must also update the Depends On attribute if it references another resource in the original environment. Otherwise, the new resource might not be successfully deployed. (Optional) On the Packages tab, add or remove packages based on your requirements. Note: After you select a copied resource from the Paste Resource list, it no longer appears in the list. (Optional) Repeat step 5 for the other resources in the Paste Resource list. (Optional) To clear all resources from the Paste Resource list, click clearAll . To save the environment, click the Save icon ( ). Renaming resources \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment version. On the canvas, select the resource that you want to rename. In the right panel that opens, on the Resource tab, click the resource name link at the top. In the Resource Name window, update the resource name and click SAVE . The resource name is automatically updated in other resources within the environment that reference it by using either the interpolation syntax or the Depends On attribute. To save the environment, click the Save icon ( ). Important: If you rename a deployed resource, that resource is destroyed and a new resource is created with the updated name when you redeploy existing deployments. Also, until you redeploy the deployments, the deployed icon is not shown on the resource and the deployed values are not shown on the Resource Details tab. Adding user-defined packages \u00b6 Packages in Deploy Accelerator enable you to leverage your existing infrastructure automation tool (Chef Solo, Chef Server, Ansible Solo, or Puppet) to configure compute resources in your environments. Each package definition includes the infrastructure automation tool and the Chef cookbook or Puppet module or manifest file that Deploy Accelerator must use to configure the compute resource. Managed packages are available by default after Deploy Accelerator is successfully deployed. You can leverage these managed packages if they meet your requirements. Otherwise, you can add your own custom (or user-defined) packages. In the case of user-defined packages, you must define a versioning scheme for the packages and ensure that each combination of package type, name, and version is unique. The following diagram describes the high-level process for creating and releasing a user-defined package: After a user-defined package is released, you can neither edit nor delete that package. To include additional updates, you must create a new package version. Add a user-defined Chef, Ansible, or Puppet package \u00b6 To prepare the user-defined package that you want to add in Deploy Accelerator, perform the following actions: Based on your infrastructure automation tool, perform one of the following actions: Create a Chef cookbook and test it well using the chef-client. For more information, see the Chef documentation . Create an Ansible playbook and test it well using the Ansible-client. For more information, see the Ansible documentation . Create a Puppet module or manifest file and test it well using the puppet-agent. For more information, see the Puppet documentation . Create a ZIP file of the Chef cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the tar.gz format. Commit the ZIP file (tar.gz format) in GitHub, GitLab, Artifactory, or any other repository. You can use a public repository for open source software. Important: If Deploy Accelerator is deployed in the offline mode, you must commit the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the Artifactory. Decide the attributes that you want to define for this package. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . On the Package List page, the Package Type column enables you to identify the package type. In case of Chef Server packages, the Chef Server column enables you to identify the Chef Server from which the packages are shown. To add a user-defined package, click ADD . On the Add Package page, perform the following actions: On the Basic tab, enter the following details: Field Details Package Name Enter the name of the package. This name appears on the Packages tab on the Home page. The name that you enter must be unique across all packages of that type. However, if you are adding a new version of an existing package, ensure that the package name is the same as the existing package name. Note: You cannot enter a name that matches a managed package. Package Type Select chef or ansible as the package type. Depending on the option selected here the package is listed, under the Packages tab, in the Private Packages list. Package Version Enter the version of the package based on your own versioning scheme. The value of this field must be unique across all existing versions of the package that you are adding. Note: Each combination of package type, name, and version must be unique. Description Enter details about the package. Image Url Enter the URL from which Deploy Accelerator must download the image file that is shown on the Packages tab on the Home page. On the Repository tab, enter the following details for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located: Fields Details Download URL Enter the URL from which Deploy Accelerator must download the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Note: If Deploy Accelerator is deployed in the offline mode, you must specify the Artifactory URL from where the Chef Solo cookbook, Ansible playbook or Puppet module or manifest file can be downloaded. Repository Type Select the repository type where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have stored the files in a different repository or location (such as Amazon S3), select the plain option. Access Token Enter the access token for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have selected Artifactory as the repository type, you must enter the API key. Ensure that the access token or API key that you enter is authorized to download the ZIP file from the repository. Note: If you do not enter the access token or API key, Deploy Accelerator takes the default access token or API key that your your administrator has configured . ZIP File Name Enter the name of the ZIP file that contains the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Unzipped Name Enter the name of the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file that is extracted from the ZIP file. Dependent packages Select the other packages that are a prerequisite for installing this package on a resource. On the Attributes tab, specify the attribute values as the array of JSON. The attributes help to parameterize the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file with some dynamic values. All the values that you enter here are shown on the Deploy Accelerator UI to ask the user to provide dynamic values before deployment. A few default attributes are shown for your reference. You can choose to use these attributes or replace them with other attributes that are relevant to your package. Important: The package will be visible to other users, so ideally you should avoid specifying default values for sensitive attributes. Click SAVE . Confirm that this package version appears on the Package List page. The state of this package version is unreleased. Note: In the case of Chef Solo cookbooks, all recipes that the cookbook contains also become available in Deploy Accelerator. While adding a Chef Solo package (or cookbook) to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook Test the user-defined package . Release the user-defined package . Test a user-defined package \u00b6 Create a test environment of the appropriate environment package type. After the environment is created, confirm that the user-defined package that you want to test appears on the Packages tab in the left panel. From the Resources tab in the left panel, drag a compute resource (For AWS an EC2 instance, or a null VM) to the canvas. From the Packages tab in the left panel, drag the user-defined package to this resource. In the right panel that opens, on the Packages tab, select the unreleased version that you want to test, and enter other package details as required. In case of a Chef Solo package (or cookbook), confirm that all recipes that the cookbook contains are available for selection in the Recipes box. Note: While adding a Chef Solo package to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Save the environment. Start a new deployment of the environment . (Optional) To resolve any issues that are detected during the testing, edit the user-defined package and then redeploy the existing deployment . After all issues with the user-defined package are successfully resolved, release the user-defined package . Release a user-defined package \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To release a package, in the Actions column for that package, click Edit . On the Attributes tab, click RELEASE . Important: You can neither edit nor delete a released package. In case you need to make any changes, you must create a new package version. Managing user-defined packages \u00b6 You can edit, download, or delete user-defined packages only if they are not yet released. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. In the Actions column for a specific version of a user-defined package, perform one of the following actions: To edit a user-defined package, click Edit to open the Add Package page, update the package details, and then click SAVE . You cannot edit a user-defined package that is released. To save a copy of the user-defined package to your computer, click Download . To delete the user-defined package, click Delete and then click YES in the confirmation message box. Creating new package versions \u00b6 You can create new versions of both user-defined and managed packages. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To create a new version of a package, in the Actions column for that package, click Create New Version . In the Create New Version From base version window, update details of the new package version. By default, details of the base package version are shown. Note: You must specify a unique version number for this package. Also, the version that you specify must be greater than the base version that you have selected. Click CREATE . (Optional) On the Add Package page, update additional details about the new package version based on your requirements. Click SAVE . A new version of the package appears on the Package List page. Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 \u00b6 In Deploy Accelerator 3.0.2, the Terraform version is upgraded to 0.12.0. Due to this, the blueprints that are created in Deploy Accelerator 2.28.0 and previous releases, require some manual change to be used in 3.0.2. Mandatory attributes for syntax validation \u00b6 Terraform v0.11 and earlier allowed adding attribute/sub-attribute JSON as a Input Variable for the resources, which can be added as an interpolation in the resource attribute. Terraform v0.12 no longer allows such interpolation for JSON and JSON Array for its attributes or sub-attributes. If there is an attribute of the type JSON or JSON Array then it should follow a proper structure for its attributes and sub-attributes. Hence, the Input/Local variables of type JSON cannot be used in interpolation to the Resource or Datasource attributes. Example Resource: aws_route53_record , Attribute: Alias Input Variable for the resource: { \"var.alias_json\" : { \"type\": \"map\", \"default\": { \"name\": \"dns_record_alias\" } } } and value for the alias attribute in the resource for Terraform v0.11 and previous release: alias : [\"${var.alias_json}\"] This syntax is above is not valid. The valid syntax for the alias attribute in the aws_route53_record resource for Terraform v0.12 is as follows: [ { \"evaluate_target_health\": \"\", \"name\": \"${var.alias_name}\", \"zone_id\": \"\" } ] Using Floor function \u00b6 From Terraform 0.12, the Terraform language does not distinguish between integer and float types. Instead, the language just has a single \"number\" type that can represent high-precision point numbers. The Terraform documentation mentions that this new type can represent any value that could be represented before, plus many new values due to the expanded precision. The Terraform documentation also mentions that in most cases this change should not cause any significant behavior change, but please note that in particular the behavior of the division operator is now different: it always performs floating point division, whereas before it would sometimes perform integer division by attempting to infer intent from the argument types. If you are relying on integer division behavior in your configuration, use the floor function to obtain the previous result. A common place this would arise is in index operations, where the index is computed by division. For example: ${floor(length(var.availability_zones)/var.subnet_count) Unfortunately the automatic upgrade tool cannot apply a fix for this case because it does not have enough information to know if floating point or integer division was intended by the configuration author, so this change must be made manually where needed. Using Escape Sequences \u00b6 In JSON attributes, if \\n , \\r , \\t , symbols are not escaped, then Terraform v0.12 does not validate it as a correct JSON. JSON format for Terraform v0.11 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\n \\\")}\" } JSON format for Terraform v0.12 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\\\n \\\")}\" }","title":"Deploy and manage environments"},{"location":"deploy/using/#deploy-and-manage-environments","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to create, deploy, and manage environments in Deploy Accelerator.","title":"Deploy and manage environments"},{"location":"deploy/using/#contents","text":"Overview * Overview of environments * Overview of layered environments * Overview of blueprints * End-to-end process for multiple deployments of an environment Create environments * Configuring providers * Configuring connections * Creating environments * Creating layered environments * Creating new environments from blueprints * Copying environments Deploy environments * Starting new deployments * Viewing deployments * Viewing the plan for deployments * Redeploying existing deployments * Sharing deployments * Destroying deployments Release environments Releasing environment versions Creating new environment versions Manage environments Upgrading the provider version of environments Viewing dependency between environments Configuring custom tags Comparing differences between environments Searching environments Renaming environments Sharing environments Deleting environments Restoring deleted environments Accessing environments and deployments of other users Create and manage blueprints Exporting environments as blueprints Best practices for creating blueprints Adding blueprints in the Blueprint Gallery Manage resources Deploy Accelerator resources Copying resources Renaming resources Manage packages Adding user-defined packages Managing user-defined packages Creating new package versions Perform post-upgrade tasks Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 Note: To access Deploy Accelerator, you must sign in to Hitachi Cloud Accelerator Platform by using your Cloud Accelerator Platform account. You can access Deploy Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions. For information about creating a Cloud Accelerator Platform account, see Create & access account .","title":"Contents"},{"location":"deploy/using/#overview-of-environments","text":"Environments in Deploy Accelerator help you to design the infrastructure that you need to deploy different applications in the cloud. Environments are visual representations of your infrastructure and can be easily created by dragging resources and packages to a canvas. You can also add dependencies between the various resources in an environment. The following image shows an example of an environment in Deploy Accelerator: Each environment contains the following elements: Resources: Entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. Packages: Entities that use open-source infrastructure automation tools to configure compute resources in an environment. Deploy Accelerator provides multiple out-of-the-box packages. If required, you can also create your own user-defined packages by using Chef (Chef Solo or Chef Server) or Puppet . Links: Visual representation of dependencies between various resources in an environment. Scripts: Set of specific actions that must be performed before or after deploying and destroying an environment.","title":"Overview of environments"},{"location":"deploy/using/#environment-versions-and-multiple-deployments","text":"You can release and version environments in Deploy Accelerator. Releasing an environment enables you to freeze any updates to the environment. Typically, when an environment is ready to be deployed in production, you can release that environment. To add or update resources in a released environment, you must create a new environment version. Environment versions enable you to keep a track of changes to an environment over time. After you have designed the infrastructure in an environment version, you can start a deployment to actually create the infrastructure. Based on your requirements, you can also start multiple deployments of the same environment version. Multiple deployments help you to create an environment once and then reuse it to create new infrastructures. For example, you can start QA and Production deployments of an environment version or you can start two Production deployments of that environment version. For each new deployment of an environment version, you can define different values for the input variables (or parameters) that are defined in the environment. For example, you can select different instance types (such as t2. micro or t2.large) for the AWS EC2 instances in your QA and Production deployments. For more information, see end-to-end process for multiple deployments of an environment . You can also redeploy an existing deployment with the same environment version or with another environment version. Deployments that you no longer need can easily be destroyed. Environments that you no longer need can also be deleted. If an environment has multiple versions, each version must be separately deleted. However, before you can delete an environment version, you must destroy all deployments for that version.","title":"Environment versions and multiple deployments"},{"location":"deploy/using/#overview-of-layered-environments","text":"A layered environment is basically a collection of multiple interdependent environments . For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between them, as shown in the following image. While deploying a child environment in a layered environment, you have to select a specific deployment of the parent environment. If parent environments have multiple deployments, you have to select a specific deployment while deploying the child environment. If you do not specify any deployment, the parent deployment with the name default is used to deploy the child environment. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other dependent environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications.","title":"Overview of layered environments"},{"location":"deploy/using/#overview-of-blueprints","text":"Blueprints in Deploy Accelerator are templates of commonly used IT infrastructures. If you have designed modular solutions (layered environments) or simple solutions (standalone environments) that need to be replicated multiple times, you can export the environments as blueprints . If only you plan to create environments from your blueprints, you can maintain the blueprints on your computer. However, if you want other users to create environments from your blueprints, you can add the blueprints in the Blueprint Gallery . Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . For example, you can create an environment for a high-availability deployment of Chef Server and export that environment as a blueprint. If another team needs to deploy its own high-availability Chef Server, a user in that team can import your blueprint, customize the deployment parameters, and start a new deployment. The following image describes the end-to-end lifecycle of a blueprint, from creating and exporting an environment as a blueprint to creating new environments from that blueprint.","title":"Overview of blueprints"},{"location":"deploy/using/#end-to-end-process-for-multiple-deployments-of-an-environment","text":"The following procedure describes the end-to-end process that you can follow to implement multiple deployments of a standalone environment. Create a new environment and design your infrastructure. For example, create an environment with the version 01.00.00. Start a new deployment of the environment that you have created. For example, create a Staging deployment to test your design. (Optional) Based on your requirements, add or update resources in this environment and redeploy the existing deployment . Release the environment version . No changes can be made to a released environment version. However, you can start new deployments or redeploy existing deployments with this environment version. Start another new deployment of the environment. For example, create a Production deployment to deploy your applications. Note: For each new deployment, you can change the input variables based on your requirements. For example, you can use different VPC IDs for your Staging and Productions deployments. (Optional) View all deployments for this environment. To make any updates to the environment, create a new environment version and then add or update resources in this environment version. For example, create an environment version 02.00.00 that is based on version 01.00.00. Based on your requirements, perform one of the following options: Start a new deployment of the environment version that you have created. Plan and redeploy an existing deployment with this environment version. For example, you can redeploy the Staging deployment of environment version 01.00.00 with this new environment version 02.00.00. In this case, only the delta between the two versions is deployed. Resources that have not been changed between the two versions are not impacted. Note: When you redeploy an existing deployment with a different environment version, the deployment gets associated with that environment version. Release the environment version . Note: In the case of layered environments, you must follow these steps for each environment within the layered environment. However, in addition to using the Depends On resource to create a dependency between the environments, you must also create a dependency between specific deployments of the environments. For example, the Staging deployment of a child environment must be dependent on the Staging deployment of the parent environment.","title":"End-to-end process for multiple deployments of an environment"},{"location":"deploy/using/#configuring-providers","text":"Providers in Deploy Accelerator allow you to deploy and manage different types of infrastructure resources. They also contain the authentication details required to deploy and manage the resources. For example, an AWS provider contains credentials for accessing the Amazon Web Services (AWS) account in which you want to deploy and manage resources, such as EC2 instances, subnets, and S3 buckets. Deploy Accelerator supports many different types of providers, such as AWS, Microsoft Azure, and Kubernetes. When you create a new environment in Deploy Accelerator, you have to select a provider. The resources that are available for the environment are based on the selected provider. By default, only you can use the providers that you create. However, you can choose to share your own providers with one or more groups.","title":"Configuring providers"},{"location":"deploy/using/#supported-providers","text":"The following table lists the providers that are supported in the latest version of Deploy Accelerator. It also lists supported versions of the Terraform providers that Deploy Accelerator uses to create and manage resources. Provider Supported Terraform Provider Version ACME 1.6.3 Artifactory 2.2.4 AWS 3.23.0 Azure 2.41.0 Azure Active Directory 1.1.1 Azure DevOps 0.1.0 Azure Stack (Beta) 0.9.0 Consul 2.10.1 Databricks 0.2.9 Datadog 2.18.1 DNS 3.0.0 Docker 2.8.0 Google Cloud Platform 3.51.0 HEC-CP None HEC-VM None Helm2 0.10.6 Helm3 1.3.2 Kibana 0.7.1 Kubernetes 1.13.3 Oracle 1.4.0 Oracle Cloud Infrastructure 4.7.0 vSphere 1.24.2 vSphere NSX-T (Beta) 3.1.0","title":"Supported providers"},{"location":"deploy/using/#create-a-provider","text":"On the Home page, click the More options icon ( ) in the top-right corner. Click Providers . On the Provider page, click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). For a complete list of provider types that are available in Deploy Accelerator, see Supported providers . In Provider Details , enter the authentication details that Deploy Accelerator must use to deploy and manage resources. The provider details that you have to specify differ based on the selected provider type. For more information, see Provider details . You can also click the Terraform Provider Link below the Provider Details box to view the Terraform documentation on the selected provider. (Only for AWS and Kubernetes provider types) To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click SAVE . A new provider appears in the Providers List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details. (Optional) Share the provider with one or more groups. Important: If you later edit this provider, you must re-enter all values in the Provider Details section before you click UPDATE . Otherwise, an error message might be shown for deployments that use this provider.","title":"Create a provider"},{"location":"deploy/using/#delete-a-provider","text":"On the Home page, click the More options icon ( ) in the top-right corner, and then click Providers . Under the Provider List , click Delete ( ), and in the confirmation dialog box, click Yes . Once a provider is deleted the provider credentials are also removed from the database. Before deleting a provider, make sure that no environment is using the provider for deployments. If an environment uses a provider, the provider cannot be deleted.","title":"Delete a provider"},{"location":"deploy/using/#provider-details","text":"Deploy Accelerator supports multiple providers, such as AWS, Azure, and Kubernetes. While creating a provider, you have to specify authentication details that Deploy Accelerator can use to deploy and manage the resources. The following topics provide information about the required details for each provider type: ACME provider details Artifactory provider details AWS provider details Azure provider details Azure DevOps provider details Consul provider details Databricks provider details Datadog provider details Google provider details Helm provider details Kubernetes provider details Kibana provider details Oracle Cloud Infrastructure provider details","title":"Provider details"},{"location":"deploy/using/#acme-provider-details","text":"The Automated Certificate Management Environment (ACME) is an evolving standard for the automation of a domain-validated certificate authority. The ACME provider allows you to acquire a valid SSL certificate from Let's Encrypt. The ACME provider supports a wide list of DNS challenge types, for example gcloud, azure, digitalocean, etc. For the complete list, see the Terraform documentation . Following are the parameters in an ACME provider: Parameter Details server_url The URL to the ACME endpoint's directory. This is a mandatory parameter. config The list of key-value pair required according to the provider. You can enter a key parameter value here to override the values from provider_reference_id. This is an optional parameter. provider_reference_id The reference ID of the reference provider. This is an optional parameter. If you enter this parameter, you will not have to specify the credentials of the cloud provider used as dns_challenge . Provider Details JSON Example { \"server_url\": \"\", \"config\": { AWS_ACCESS_KEY_ID = \"XXXXXX\" AWS_SECRET_ACCESS_KEY = \"XXXXXXX\" AWS_DEFAULT_REGION = \"us-east-1\" }, \"provider_reference_id\": \"1\" }","title":"ACME provider details"},{"location":"deploy/using/#artifactory-provider-details","text":"The Artifactory provider is to manage the resources for Artifactory. Following are the parameters in Artifactory provider: Parameter Details url The URL for customer artifactory. This is a mandatory parameter. username The username for accessing customer artifactory. password The password for logging in to customer artifactory. access_token The access token for accessing the artifactory. api_key The API key for accessing the artifactory. Note: You require only one type of parameter for authentication. Enter a username/password, or an access token, or an API key. Provider Details JSON Example { \"url\": \"http://www.example.com\", \"username\": \"XXXXX\", \"password\": \"XXXXX\", \"access_token\": \"XXXXX\", \"api_key\": \"XXXXXXXXXXX\" }","title":"Artifactory provider details"},{"location":"deploy/using/#aws-provider-details","text":"For the AWS provider type, you must select either Basic Credentials or Instance Profile to specify authentication details of the AWS account that Deploy Accelerator must use to deploy an environment. To use a more secure way of cross-account access, you must select Assume Role . Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can attach a role to the instance on which Deploy Accelerator is deployed. Deploy Accelerator can then assume a role in the other accounts and deploy the resources. For more information about assuming roles, see AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Deploy Accelerator is deployed. For the other accounts, you must specify a role that Deploy Accelerator can assume to deploy the resources. The role that you specify must have permissions to deploy the required resources. It must also define the account in which Deploy Accelerator is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Deploy Accelerator must deploy resources. However, Deploy Accelerator can use this method only if a role is attached to the instance on which Deploy Accelerator is deployed. This role must have the permissions to deploy the required resources. To use the Instance Profile method, you must specify only the region in which the resources must be deployed, as shown in the following example: { \"region\" : \"xx-xxxx-x\" } Static Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can specify long-term access credentials for only the parent account. Deploy Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Deploy Accelerator can assume to deploy resources in that account. The role that you specify must have access to deploy the required resources. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Deploy Accelerator must deploy resources, as shown in the following example: { \"access_key\" : \"ACCESS-KEY\" , \"secret_key\" : \"SECRET-KEY\" , \"region\" : \"xx-xxxx-x\" } The IAM user whose credentials you specify must have access to deploy the required resources.","title":"AWS provider details"},{"location":"deploy/using/#azure-provider-details","text":"The Azure provider is used to interact with the resources supported by Azure. Following are the parameters in an Azure provider: Parameter Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated.","title":"Azure provider details"},{"location":"deploy/using/#azure-devops-provider-details","text":"The Azure DevOps provider is used to interact with the resources supported by Azure DevOps. Following are the parameters in an Azure DevOps provider: Parameter Details org_service_url (Required) This is the Azure DevOps organization URL. personal_access_token (Required) This is the Azure DevOps organization personal access token. The account corresponding to the token will need \"owner\" privileges for this organization.","title":"Azure DevOps provider details"},{"location":"deploy/using/#consul-provider-details","text":"The Consul provider is used to interact with the resources supported by Consul. Following are the parameters in an Consul provider: Parameter Details address Public IP address of the instance on which Consul server is installed. datacenter The datacenter that is configured for the Consul server you have specified. The following is an example of the provider details section for a Consul provider. { \"address\" : \"123.0.57.189\" , \"datacenter\" : \"dc2\" }","title":"Consul provider details"},{"location":"deploy/using/#databricks-provider-details","text":"The Databricks provider is used to interact with the resources supported by Databricks. Following are the parameters in an Databricks provider: Parameters Details host (optional) This is the host of the Databricks workspace. It is a URL that you use to login to your workspace. token (optional) This is the API token to authenticate into the workspace. username (optional) This is the username of the user that can log into the workspace. password (optional) This is the user's password that can log into the workspace. profile (optional) Connection profile specified within ~/.databrickscfg . To learn more about connect profiles, see Databricks documentation.","title":"Databricks provider details"},{"location":"deploy/using/#datadog-provider-details","text":"The Datadog provider is used to interact with the resources supported by Datadog. Following are the parameters in an Datadog provider: Parameters Details api_key Datadog API key. This is a mandatory parameter if the validate parameter is set to true . app_key Datadog APP key. This is a mandatory parameter if the validate parameter is set to true . api_url (optional) The Datagog API URL. validate (optional) Enables validation of the provided API and APP keys during provider initialization. Default is true. When false, api_key and app_key is not checked.","title":"Datadog provider details"},{"location":"deploy/using/#google-provider-details","text":"The Google provider is used to connect to Google Cloud Platform infrastructure products. You can also use Google beta resources in Google environments. Following are the parameters in the google beta provider: Parameters Details Credentials The credentials field is for entering the service account key in JSON format. You can generate a service account key using various methods, for more information, see Google Cloud Platform documentation . Project The project field is your personal project id. The project indicates the default GCP project all of your resources will be created in. Most Terraform resources will have a project field. Region The region is used to choose the default location for regional resources. Regional resources are spread across several zones. Sample { \"credentials\" :{ \"type\" : \"service_account\" , \"project_id\" : \"project-id\" , \"private_key_id\" : \"key-id\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"service-account-email\" , \"client_id\" : \"client-id\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\" }, \"project\" : \"my-project-id\" , \"region\" : \"us-central1\" }","title":"Google provider details"},{"location":"deploy/using/#helm-provider-details","text":"Helm2 and Helm3 providers allow you to specify details of the Kubernetes cluster in which you want to deploy Helm charts. The difference between the Helm2 and Helm3 provider types is the Terraform Helm provider version that is supported. Helm2 provider supports version 0.10.4 Helm3 provider supports version 1.2.1 or later Based on your requirements, you can choose the appropriate Helm provider type. The provider details that you have to specify for both provider types is the same. Note: Your administrator might have set the properties for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm2 or Helm3 provider type. In this case, you do not have to add the Helm Repository data source in your environment if your helm chart is available in this repository. Pre-requisites Before configuring a Helm2 or Helm3 provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. (Only for Helm 2 provider) Ensure that Tiller is running in the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Helm2 or Helm3 provider, you must populate the config_path_Content sub-attribute under the kubernetes attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the kubernetes attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 . Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file entered below the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } } ] } Example 3 The following is an example of the provider details section for a Helm2 or Helm3 provider for AWS EKS. { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"aws\" : [ { \"access_key\" : \"\" , \"secret_key\" : \"\" , \"region\" : \"\" } ], \"provider_reference_id\" : \"88\" } Example 4 The following is an example of the provider details section for a Helm2 or Helm3 provider for GKE. { \"google\": [ { \"zone\": \"XXXXX\", \"region\": \"XXXXXX\", \"project\": \"XXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXXXX\" } ] } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter.","title":"Helm provider details"},{"location":"deploy/using/#kibana-provider-details","text":"The Kibana provider is used to interact with the resources supported by Kibana. Following are the parameters in an Kibana provider: Parameters Details elastic_search_path The path at which elastic search is hosted. kibana_uri The URI at which Kibana is hosted. kibana_version (Optional) The version of Kibana configuration. For the list of supported versions, see Kibana Terraform Provider documentation . In case of empty field the default value is 6.0.0 . kibana_type (Optional) The type of Kibana in the back-end. The default value is KibanaTypeVanilla , which supports the standard open-source kibana distribution. For more information, see Kibana Terraform Provider documentation . To configure logz.io kibana, use KibanaTypeLogzio . kibana_username (Optional) Username for Kibana API authentication. kibana_password (Optional) Password for Kibana API authentication. logzio_client_id (Optional) The client ID used for authentication with logzio. For more information, see Kibana Terraform Provider documentation . logzio_account_id (Optional)The logz.io account id. logzio_mfa_secret (Optional) MFA shared secret, create when signing up user account with MFA.","title":"Kibana provider details"},{"location":"deploy/using/#kubernetes-provider-details","text":"Kubernetes provider allows you to specify details of the Kubernetes cluster in which you want to deploy various resources. Pre-requisites Before configuring a Kubernetes provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Kubernetes provider, you must populate the config_path_Content attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the config_path_Content attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 and Example 5 Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Kubernetes provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"config_path_Content\" : \"\" , \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Kubernetes provider (JSON file entered below the config_path_Content attribute). { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } \"provider_reference_id\" : \"1\" } Example 3 The following is an example of the provider details section for a Kubernetes provider for AWS EKS. { \"config_path_Content\" : \"XXXXXXXXXXXXXXXXXXXXXXXXX\" , \"aws\" : [ { \"access_key\" : \"XXXX\" , \"secret_key\" : \"XXX\" , \"region\" : \"XXXX\" } ] } Example 4 The following is an example of the provider details section for a Kubernetes provider for GKE. { \"google\": [ { \"zone\": \"XXXXXXXX\", \"region\": \"XXXXXXX\", \"project\": \"XXXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXX\" } ] } Example 5 The following is an example of the provider details section for a Kubernetes provider for GKE using a reference provider. { \"google\": [ { \"zone\": \"\", \"region\": \"\", \"project\": \"\", \"service_key_Content\": \"\", \"cluster\": \"\" } ], \"provider_reference_id\": \"1\" } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter.","title":"Kubernetes provider details"},{"location":"deploy/using/#oracle-cloud-infrastructure-provider-details","text":"The Oracle Cloud Infrastructure is for managing the resources of the Oracle Cloud Infrastructure. Following are the parameters in the Oracle Cloud Infrastructure provider: Parameter Details tenancy_ocid OCID of your tenancy. For more information, see Oracle Cloud Infrastructure documentation . user_ocid OCID of the user calling the API. For more information, see Oracle Cloud Infrastructure documentation . fingerprint Fingerprint for the key pair being used. For more information, see Oracle Cloud Infrastructure documentation . private_key The path (including filename) of the private key stored on your computer, required if private_key is not defined. For information on how to create and configure keys, see Oracle Cloud Infrastructure documentation . region An Oracle Cloud Infrastructure region. For more information, see Oracle Cloud Infrastructure documentation . Provider Details JSON Example { \"tenancy_ocid\": \"XXXXXXXXXXX\", \"user_ocid\": \"XXXXXXXXXX\", \"fingerprint\": \"XXXXXXXXX\", \"private_key\": \"XXXXXXXXX\", \"region\": \"XXXXXXXXXXX\" }","title":"Oracle Cloud Infrastructure provider details"},{"location":"deploy/using/#share-a-provider","text":"Deploy Accelerator enables you to share your provider with other users. The ability to share providers is useful when multiple users in a group need to use the same provider to deploy environments. These users can view, edit, share, or delete providers that are shared with them based on the permissions assigned to their group. On the Home page, click the icon in the top-right corner. Click Providers . On the Providers List page, click the icon for the provider that you want to share with other users. You can identify the providers that are shared with you based on the color of the provider name. You can share a shared provider only if your group has been assigned the Share permission for that provider. In the Share Provider : ProviderName window, click the icon for the Group with which you want to share the provider. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group. Select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the provider, select the Warning check box and click SUBMIT . If you no longer want to share the provider, click CANCEL .","title":"Share a provider"},{"location":"deploy/using/#configuring-connections","text":"Amazon Elastic Compute Cloud (EC2) uses public-key cryptography to encrypt and decrypt login information. When Deploy Accelerator launches an instance as part of deploying an environment, it specifies the name of a key pair (set of public and private keys) that must be used to connect to the instance. Each connection that you create contains the private key that can be used to connect to one or more instances in your environments. While creating an environment, you must specify a connection that is applied to all instances by default. You can also share your own connections with one or more groups. Note: If you do not want to use the default connection for all instances in an environment, you can create multiple connections. You can also choose to dynamically generate a key pair and associate it with one or more instances while the environment is being deployed. For an example of the step-by-step procedure to dynamically generate and associate a key pair with an instance, see Configure the dynamic generation of a key pair .","title":"Configuring connections"},{"location":"deploy/using/#create-a-connection","text":"Create the required key pair by using either Amazon EC2 or a third-party tool. If you use a third-party tool, you must also import the public keys to Amazon EC2. For more information, see the Amazon Elastic Compute Cloud User Guide for Linux Instances or the Amazon Elastic Compute Cloud User Guide for Windows Instances . Important: To create a key pair by using Amazon EC2, you must have your own AWS cloud account. On the Home page, click the More options icon ( ) in the top-right corner and then click Connections . On the Add/Edit Connection page, click NEW . Enter the connection name and connection type. If you have selected the WinRM connection type, perform the following actions: In SSH/WinRM User , enter administrator . In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. (Optional) To set up a secure connection, select Https and enter a CA certificate in Cacert Note: If you want Deploy Accelerator to skip validation of the CA certificate, you can select Insecure . (Optional) To use NTLM authentication for remote connection, select NTLM . Important: When you select a WinRM connection for an instance, you must also specify a PowerShell script in the user_data attribute of that EC2 Instance resource. The user and password that you specify in the PowerShell script and the selected WinRM connection must match. Otherwise, you cannot deploy packages on that instance. If you have selected the SSH connection type, enter the user and perform one of the following actions: In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. In SSH Key , enter the SSH key instead of a password. (Optional) To specify details of a Bastion Host in the connection, select Bastion Connection , enter the following details, and then click SET PARAMETER . User Password (enter either the password or the key) Host (IP address of the Bastion host, which must be available in the network) Port (By default, the port number is selected based on the connection type.) Key You can use a Bastion connection based on how your network is configured. If the connection that you are creating is for instances in a private network and a Bastion Host is configured to connect to those instances, you must also specify details for connecting to the Bastion host. Note: The Bastion Connection link is enabled only for the SSH connection type. Click SAVE . A new connection appears under the Connection List section. (Optional) Share the connection with one or more groups.","title":"Create a connection"},{"location":"deploy/using/#share-a-connection","text":"Deploy Accelerator enables you to share your connection with other users. The ability to share connections is useful when multiple users in a group need to use the same connection for the instances that they are deploying. These users can view, edit, or delete connections that are shared with them based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top-right corner. Click Connections . On the Connection List page, click the Share icon ( ) for the connection that you want to share with other users. Note: You can identify the connections that are shared with you based on the color of the connection name. In the Share Connection: ConnectionName window, click the icon for the Group with which you want to share the connection. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the connection, select the Warning check box and click SUBMIT . If you no longer want to share the connection, click CANCEL .","title":"Share a connection"},{"location":"deploy/using/#creating-environments","text":"The process to create an environment includes multiple steps, as shown in the following image. Create an environment Add resources Add packages Configure the environment To create a layered environment , you must perform these steps for each environment that is a part of the layered environment. For more information, see Creating layered environments .","title":"Creating environments"},{"location":"deploy/using/#before-you-begin","text":"Before you create an environment, ensure that you have performed the following actions: Configured the appropriate cloud account (for example, AWS account) in which you want to deploy an environment. For more information, see Configure providers . Created the connections that Deploy Accelerator can use to connect to the instances in your deployed environment. For more information, see Configure connections .","title":"Before you begin"},{"location":"deploy/using/#create-an-environment","text":"Create an environment > Add resources > Add packages > Configure the environment On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter a unique name and description for the environment that you want to create. Enter a version for the environment. By default, 01.00.00 is specified as the version for a new environment. If required, you can specify a different version. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Environment Package Type list, select the package type that you want to use for this environment. Under the Packages tab, the Private Packages list is populated depending on the Environment Package Type selected for creating the environment. Note: Deploy Accelerator supports the Chef Solo, Chef Server, Ansible Solo, and Puppet package types. However, the options that are shown in the Environment Package Type list are based on the configuration by your administrator . From the Connection list, select the default connection for all instances in this environment. You can identify shared connections in the list based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the appropriate account for accessing the cloud provider that you want to use to deploy the environment. You can identify shared providers in the list based on the color of the provider name. (Optional) From the AWS Region list, select the appropriate AWS region where you want to deploy the environment. Note: If you select an AWS Region while creating an environment, any predefined region in the Provider is overwritten with the new AWS Region. (Only if Chef Server is selected as the environment package type) To associate a Chef Server with the environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share the environment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the same groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections .","title":"Create an environment"},{"location":"deploy/using/#add-resources-to-the-environment","text":"Create an environment > Add resources > Add packages > Configure the environment Resources are an important element of environments in Deploy Accelerator. They represent entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. You can also add Terraform data sources, Terraform modules , and Terraform Random Provider resources in your environment. You can add resources and data sources to the environment by using either the resource attributes form or the HCL editor . Notes: After you start a deployment of the environment , you can view various attributes of a resource for that deployment, such as its public IP address, on the Resource Details tab in the right panel. You can also add resources to an environment version after it has been deployed. These new resources get created when you start a new deployment or redeploy an existing deployment. However, no changes can be made to a released environment version. If you rearrange a resource on the canvas, you must save the environment to retain the changed position of that resource.","title":"Add resources to the environment"},{"location":"deploy/using/#add-resources-and-data-sources-to-the-environment","text":"In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name and click CREATE . Click the resource or data source that you have created. In the right panel that opens, on the Resource tab, enter the required details. Consider the following points while configuring the resource attributes. Goal Action Configure the connection and key pair that Deploy Accelerator must use to connect to a non-Windows instance. In the Connection attribute, select the appropriate SSH connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the key_name attribute, specify the corresponding key pair that Deploy Accelerator must associate with the instance when it is launched. Based on your requirements, you can also specify multiple key pairs. Note: To use a key pair that is dynamically generated while the environment is being deployed, you can select Use Custom Connection . The procedure to dynamically generate and associate a key pair with an instance includes multiple steps. For more information, see Configure the dynamic generation of a key pair . Configure the connection and user data that Deploy Accelerator must use to connect to a Windows instance In the Connection attribute, select the appropriate WinRM connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the user_data attribute, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. (See sample PowerShell script ). Important: You must ensure that the security group associated with the instance allows inbound traffic (ingress) from the port that you specify in the PowerShell script. Create multiple resources based on the same configuration In the Count attribute, specify the number of identical resources that you want to create while deploying the environment. Create a dependency on another resource in this environment In the Depends On attribute, specify the name of the resource on which you want to create a dependency. Use a variable that is defined in an Input Variable resource in this environment In any attribute, use the appropriate interpolation syntax based on the type of input variable that you are referring. For more information, see Formats for using input variables in resources . Use a local value that is defined in a locals resource in this environment. In any attribute, use the following interpolation syntax to refer to a local variable that is defined in the locals resource. ${ localResourceName . variableName } A local value can be a simple constant or an expression that can be defined once and used multiple times in different resources within the same environment. For more information, see the Terraform documentation . Reference attributes of another resource in the same environment For information about the supported attributes, select the resource whose attributes you want to reference and then click the resource type link at the top of the Resource tab in the right panel. Use the following interpolation syntax: ${ ResourceName . Attribute } Reference attributes of a data source in the same environment Use the following interpolation syntax: ${ DataSourceName . Attribute } Set array for attributes of the Text type While entering array values in attributes, specify each value on a separate line. The following image shows an example of array values for the vpc_security_groups_ids attribute of an AWS Instance : Apply Flatten flag for string array in JSON, or JSON array of the resources. To apply a Flatten flag on string arrays or nested string arrays in a JSON, or JSON array, select the Flatten toggle switch. The Flatten function is for replacing a nested string array list with a single flattened array. The array with the Flatten flag, returns a flattened sequence while migrating or importing the blueprint. Example -- JSON and JSON array -- String array Add a timeout for creating, reading, updating, or deleting resources. Use the Timeouts attribute for adding a timeout for creating, reading, updating, or deleting resources. You can define timeout for the following actions: - Create : To add a timeout while creating resources. - Update : To add a timeout while updating resources. - Read : To add a timeout while reading resources. - Delete : To add a timeout while deleting resources. The timeout is added in the following JSON format: To check the actions supported by a resource, click the Timeouts attribute in resource attributes form to open its relevant Hashicorp resource documentation. Sample PowerShell script for a Windows instance While creating a Windows instance, you must select a WinRM connection. In the user_data attribute of the instance, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. The following is an example of the PowerShell script that can be specified in the user_data attribute. < powershell > if ( [Environment] :: OSVersion . Version -ge ( new-object 'Version' 6 , 1 )) { New-NetFirewallRule -DisplayName \"Allow WinRM\" -Direction Inbound -Action Allow -Protocol TCP -EdgeTraversalPolicy Allow -LocalPort 5985 } else { netsh advfirewall firewall add rule name = \"Allow WinRM\" dir = in protocol = TCP localport = 5985 action = allow remoteip = any localip = any profile = any } winrm set winrm / config / service / auth '@{Basic=\"true\"}' winrm set winrm / config / service '@{AllowUnencrypted=\"true\"}' $admin = [adsi] ( \"WinNT://./administrator, user\" ) $admin . psbase . invoke ( \"SetPassword\" , \"password@123\" ) </ powershell > To add other resources or data sources, repeat steps 2 to 5. (Optional) Copy an existing resource from either the same environment or a different environment. (Optional) To remove an extra resource or data source from the canvas, click the x icon on that resource. (Optional) To view the links that are created when a resource uses the Depends On attribute or the interpolation syntax to reference another resource, click the Links icon ( ). Click the Save icon ( ). (Optional) To switch from resource attributes form to HCL code, select Switch to Hashicorp Configuration Language . The switch confirmation dialog box appears. Switching from attributes form to HCL code will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the HCL code, see Add resources and data sources using Hashicorp Configuration Language .","title":"Add resources and data sources to the environment"},{"location":"deploy/using/#add-resources-and-data-sources-using-hashicorp-configuration-language","text":"In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name, turn on the Switch to Hashicorp Configuration Language toggle switch, and click CREATE . Click the resource or data source that you have created. In the right panel that opens, you will see an HCL Editor on the Resource tab. In the HCL Editor, insert the HCL code for your resource. Click the Save icon ( ). ( Optional ) To switch the resource definition from HCL code to the default resource attributes form, turn off the Switch to Hashicorp Configuration Language toggle. The switch confirmation dialog box appears. Switching from HCL code to attributes form will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the default resource attributes form, see Add resources and data sources to the environment .","title":"Add resources and data sources using Hashicorp Configuration Language"},{"location":"deploy/using/#add-terraform-modules-to-the-environment","text":"Deploy Accelerator allows you to use an existing Terraform module in the environment. Terraform defines a module as a container for multiple resources that are used together . For more information about creating modules, see the Terraform documentation . To use an existing Terraform module, you can add the Terraform Module resource to the environment and specify the source from where the module can be downloaded. To add a Terraform module to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the Terraform Module resource to the canvas. In the Resource Name window, enter a unique name and click CREATE . Click the Terraform Module resource that you have created. In the right panel that opens, on the Resource tab, enter the following details: source: Enter the source from where to download the Terraform Module. Deploy Accelerator supports the following sources for Terraform Modules. Source type Supported format Terraform public registry <NAMESPACE>/<NAME>/<PROVIDER><br> Example: terraform-aws-modules/vpc/aws GitHub repository (public) github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION><br> Example: github.com/terraform-aws-modules/terraform-aws-vpc.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. GitHub repository (private) git::https://<USERNAME>:<PASSWORD>@github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION> Example: git::https://sampleuser:samplepassword@github.com/sample-demo/terraform-module.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. Artifactory ( configured in Deploy Accelerator ) artifactory::<OBJECT-RELATIVE-PATH> Example: artifactory::local-demo-sample/module/aws_vpc.zip Note: The Artifactory URL that is configured in the dnow.properties file is prefixed to the path that you specify in the source attribute. input: Input parameters for the Terraform Module. You can use the interpolation syntax to reference the output of other resources in the same environment or a parent environment as input to the Terraform module. tags: Tags that you want to assign to all resources in the Terraform Module. However, ensure that the Terraform Module supports tags. version: Version of the Terraform Module to download. Use this attribute when you specify a Terraform public registry in the source attribute. Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module from the Terraform public registry. Click the Save icon ( ). Note: You can use the interpolation syntax to reference the output of this Terraform module in other resources within the same environment or in child environments.","title":"Add Terraform modules to the environment"},{"location":"deploy/using/#add-terraform-random-provider-resources-to-the-environment","text":"Deploy Accelerator allows you to use Terraform Random Provider resources in the environment. These resources are always available on the Resources tab irrespective of the provider type that is selected for the environment. Terraform defines the Random Provider as a logical provider that allows the use of randomness within Terraform configurations...it provides resources that generate random values during their creation and then hold those values steady until the inputs are changed. For more information about the Random Provider, see the Terraform documentation . To add a Random Provider resource to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the appropriate Random Provider resource to the canvas. A few examples of the Random Provider resources are Random Id , Random Password , and Random String . In the Resource Name window, enter a unique name and click CREATE . Click the Random resource that you have created. In the right panel that opens, on the Resource tab, enter the required details. For more information about the resource attributes, click the resource type link at the top of the Resource tab, as shown in the following image. Click the Save icon ( ).","title":"Add Terraform Random Provider resources to the environment"},{"location":"deploy/using/#add-packages-to-the-resources","text":"Create an environment > Add resources > Add packages > Configure the environment Packages in Deploy Accelerator are entities that use supported infrastructure automation tools to configure compute resources in an environment. Chef Solo and Chef Server are configured by default in Deploy Accelerator. However, your administrator might have also configured the use of Ansible and Puppet packages. Each package references a Chef cookbook, Ansible playbook, or Puppet manifest file or module. You can add user-defined packages to install your own custom applications on compute resources. Important: Deploy Accelerator cannot deploy packages on resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a new deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. To add packages to the resources, perform the following actions: In the new environment that you have created, from the left panel, click the Packages tab, and drag the appropriate package to a compute resource (for example, an EC2 Instance resource for AWS) on the canvas. The Packages tab displays packages of only the environment package type (Chef Solo, Chef Server, Ansible Solo or Puppet) that you have selected. If you have selected Chef Server as the environment package type, the Packages tab also displays roles from the selected Chef Server. You can add both Chef Server packages and roles to a compute resource. The following utility packages are always shown irrespective of the environment package type that is selected. Utility package Details file Uploads a file on the resource to which it is added. execute-script Runs a script on the resource to which it is added. You can specify whether the script must be run after the resource is created or before it is destroyed. local-exec Runs a command on the instance on which Deploy Accelerator is deployed. You can specify whether the command must be run after the environment is deployed or before it is destroyed. The local-exec package can be added to any resource in the environment. chef_configuration Restricts Deploy Accelerator from installing the ChefDK client on instances before deploying packages. You must set the skip_install attribute of this package to true . Note: This package is available only in environments for which Chef Server is selected as the environment package type. Click the package or role. In the right panel, on the PACKAGES tab, select the package or role that you have added to the resource. (Optional) From the Package Version list, select another version of the package or role. By default, the latest released version of the package or role is selected. Based on your requirements, you can select another version of the package or role. Ideally, you should add an unreleased version of a package or role to an environment only if you want to test that version. (Chef Solo and Chef Server packages only) From the Recipes list, select one or more recipes that you want to run on the resource. All recipes that the selected cookbook (or package) contains appear in the list. You must select the recipes in the order in which they must be run on the resource. If you do not select any recipe, Deploy Accelerator runs the default recipe that is defined in the cookbook. Enter other details about the package or role. Note: The details that you must specify for each package are different. For example, in case of the execute-script package, you can specify whether the script must be run after the resource is created or before it is destroyed by selecting the appropriate option from the when list. (Optional) To drag additional packages or roles to the resource, repeat steps 1 to 6. If you add multiple packages and roles to a resource, the packages and roles are arranged from top to bottom by default. However, you can drag and rearrange the packages and roles in the order in which you want to run them on the resource. Note: The utility packages are always run on the resource before the other selected packages and roles. However, if you add multiple utility packages to a resource, you must arrange these packages in the order in which you want to run them on the resource. To add packages or roles to other resources, repeat steps 3 to 6. (Optional) To remove a package or role from a resource, click the x icon on that package or role. Click the Save icon ( ).","title":"Add packages to the resources"},{"location":"deploy/using/#configure-the-environment","text":"Create an environment > Add resources > Add packages > Configure the environment Based on your requirements, you can configure additional settings for the environment. This step is optional in the end-to-end process of creating environments. In the new environment that you have created, click the Configure icon ( ). Based on your requirements, perform the following actions in the Configuration window: On the Environment tab, edit the following details for the selected environment version: Edit the description of the environment. Edit the connection, provider, or AWS Region for the environment. For information about configuring connections and providers, see Configure connection and Configure provider . To assign a custom tag to all resources in the environment, select that custom tag from the Custom Tag list. To view the key-pair values in the selected custom tag, move your cursor on the icon. For information about creating or editing a custom tag, see <a href=\"\" ui-sref=\"rean-platform-docs.accelerator({viewAccelerator: 'rean-deploy', viewPage: 'deploy-and-manage-environments', viewSection: 'tags'})\" style=\"text-decoration:none\">Configuring custom tags</a>. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that is selected for the environment. On the Deployment tab, configure the following details that are applied by default to all deployments of this environment version: Under Deploy , specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. By default, Deploy Accelerator does not destroy any deployment. On the Notification tab, specify the email notification details that are applied to all deployments of this environment version: Email: By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts or destroys a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. Deploy Template: Deploy Accelerator uses the default template for sending an email when a deployment is started. However, you can specify your own custom template. Destroy Template: Deploy Accelerator uses the default template for sending an email when a deployment is destroyed. However, you can specify your own custom template. Deployment Initiation: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users as soon as they start new deployments or destroy existing deployments of this environment. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Deployment Complete: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users after new deployments of this environment either succeed or fail, and existing deployments are successfully destroyed. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Note: Contact your system administrator for a list of custom email templates that might be available. Click SAVE .","title":"Configure the environment"},{"location":"deploy/using/#where-to-go-next","text":"When you are ready to create the infrastructure that you have designed in your environment, you can start a new deployment .","title":"Where to go next"},{"location":"deploy/using/#creating-a-layered-environment","text":"Layered environments in Deploy Accelerator are a collection of environments in which some environments (child) are dependent on other environments (parent). You can create layered environments for complex IT infrastructures. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. To create layered environments, you require the following two resources: Output: You must add this resource in the parent environment. This resource enables you to expose some resource attribute values from the parent environment in the form of variables. These variables can then be referenced in other dependent (child) environments. Depends On: You must add this resource in the child environment. This resource enables you to create a dependency on another environment. Based on your requirements, you can add multiple Depends On resources in a child environment. To create a layered environment, perform the following actions: To create the parent environment, perform the following actions: Create an environment . Add the required resources in the environment . To reference attributes of the parent environment in one or more child environments, add the Output resource. In the Output resource, create a JSON file and specify variables that can be used in child environments. The value of these output variables can use the interpolation syntax to reference attributes of other resources in the parent environment. If you have added the Input Variable resource, the output variables can also reference the input variables. Save the environment. To create the child environment, perform the following actions: Create an environment . Add the Depends On resource in the environment. The name of the Depends On resource should ideally indicate the environment on which there is a dependency (for example: dependson_network ). In the reference_type attribute of the Depends On resource, select Environment Name . In the Depends_On attribute, select the appropriate parent environment version. Note: You can also select S3 in the reference_type attribute of the Depends On resource. For more information, see Deploy Accelerator resources . Add other required resources in the child environment . To reference attribute values of resources in the parent environment, use the following interpolation syntax: Syntax: ${ DependsOnResourceName.OutputVariableName } Example: ${ dependson_network.subnet } Save the environment. To confirm that the dependency on the parent environment, click the Environment Dependency View icon ( ).","title":"Creating a layered environment"},{"location":"deploy/using/#creating-new-environments-from-blueprints","text":"Blueprints are templates of commonly used IT infrastructures. Deploy Accelerator users can export their environments as blueprints and add them in the Blueprint Gallery . You can leverage blueprints to quickly create infrastructure for solutions. Deploy Accelerator allows you to create new environments from blueprints by using any of the following methods: Import blueprints from the Blueprint Gallery Import blueprints from the Artifactory by using an API Import blueprints from your computer Important: If you create a layered environment from a blueprint, you must deploy the environments in a particular order. You must first deploy the environment in the top layer, then the environment in the next layer, and so on until the environment in the lowest layer. You must follow this order because child environments can be deployed only if their parent environments are already deployed.","title":"Creating new environments from blueprints"},{"location":"deploy/using/#before-you-begin_1","text":"Before you create an environment from a blueprint, ensure that you have configured the appropriate providers and connections that can be used for each environment in the blueprint.","title":"Before you begin"},{"location":"deploy/using/#import-blueprints-from-the-blueprint-gallery","text":"The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable and configure the Blueprint Gallery . Also, blueprints are not available out of the box and have to be added in the Blueprint Gallery . The following image shows an example of the Blueprint Gallery. To import blueprints from the Blueprint Gallery, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . (Optional) To manually refresh the Blueprint Gallery to display all latest updates, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint data from the Artifactory. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. On the Blueprint Gallery page, browse or search for a blueprint that meets your requirements. The search results include blueprints that contain the search keyword in the blueprint name, description, owner name, or owner email. Searches are not case-sensitive and use partial keyword matching. For example, if you specify layer as the search keyword, the search results include blueprints with the name NetworkLayer and MultilayerEnvironment . Note: You can also sort the blueprints based on the blueprint name or owner name. By default, blueprints are sorted based on the blueprint names in the descending order. From the version list of the blueprint that you want to import, select the appropriate version. For details about the selected blueprint version, hover over Description . To access the ReadMe file of the selected blueprint version, click README . To view the email address of the blueprint owner, hover over the owner name. However, the email address is not shown if the blueprint owner is admin . Also, the owner name and email address is shown only if these values are available in the blueprint metadata. To import the selected blueprint version, click IMPORT . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as a part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. (Optional) Update the description for the environment. By default, the description from the blueprint is shown. From the Connection list, select the appropriate connection. You can identify shared connections based on the color of the connection name. From the Provider list, select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. When you import a blueprint from the Blueprint Gallery, all environments in that blueprint are in the Released state. If input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments .","title":"Import blueprints from the Blueprint Gallery"},{"location":"deploy/using/#import-blueprints-from-the-artifactory-by-using-an-api","text":"The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable the Blueprint Gallery and specify a repository in the Artifactory for storing the blueprints. Based on your requirements, you can either import blueprints from the Blueprint Gallery or import blueprints from the Artifactory by using an API. The importBlueprintFromArtifactory API allows you to import blueprints from the specified repository in the Artifactory. You can specify the following parameters for importing a blueprint: blueprintName blueprintVersion environmentNamePrefix This parameter is optional. The value that you specify for this parameter is added as a prefix to the names of all environments that are imported as a part of the blueprint. groupName This parameter is optional. The group that you specify for this parameter is assigned the View and Deploy share permission for all the imported environments. For more information about the API, see the Deploy Accelerator API documentation: https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html#/Environment/importBlueprintFromArtifactory","title":"Import blueprints from the Artifactory by using an API"},{"location":"deploy/using/#import-blueprints-from-your-computer","text":"On the canvas, click the More icon ( ) and then click Import . In the Import Environment/Blueprint window, perform the following actions: Click Choose File and then select the blueprint (JSON file) that you want to import. Click Upload . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Enter the description for the environment that you are importing. From Connection , select the appropriate connection. You can identify shared connections based on the color of the connection name. From Provider , select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From Chef Server , select the Chef Server from which you want to display packages. From Chef Environment , select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. When you import a blueprint from your computer, the state of environments in that blueprint are dependent on their state when they were exported. For example, if parent and child environments in the Released state were exported as a blueprint, they are in the Released State when the blueprint is imported. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. If the imported environments are in a Released state and if input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments .","title":"Import blueprints from your computer"},{"location":"deploy/using/#copying-environments","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment that you want to copy. On the canvas, click the More icon ( ) and then click Copy . The environment version that is currently selected is copied. In the Create Environment window, enter a unique name, description, and version for the environment that you are creating. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Connection list, select the default connection for all instances in the environment. You can identify shared connections in the drop-down based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the cloud provider that you want to use to deploy the environment. You can identify shared providers in the drop-down based on the color of the provider name. (Optional) After selecting the provider, select the appropriate AWS region where you want to deploy an environment. Note: If you select an AWS region while creating an environment, any predefined region in the provider is overwritten with the new AWS region. If Chef Server is selected as the environment package type in the environment that you are copying, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, the share permissions of the environment that you have copied are automatically applied to this environment. (Optional) Modify the share permissions for this environment by selecting new or clearing existing permission check boxes for the appropriate groups. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the selected groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections .","title":"Copying environments"},{"location":"deploy/using/#starting-new-deployments","text":"Each environment can have multiple versions. When you are ready to create the infrastructure that you have designed in an environment version, you can start a new deployment. If required, each environment version can also have multiple deployments, such as Staging and Production. Important: You can deploy child environments only if their parent environments are already deployed. Therefore, in the case of a layered environment , you must deploy the environments it contains in a particular order. You must first deploy the parent environment in the top layer, then the child environment in the next layer, and so on until the child environment in the lowest layer. To start a new deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to deploy. To start a new deployment, click the Deploy icon ( ). The Review and Deploy: EnvironmentName window displays the default values that are configured for the environment. For more information, see Configure the environment . (Optional) To start a quick deployment with default values, click QUICK DEPLOY . (Optional) On the Environment tab, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for this deployment. Note: If you select an AWS region while starting a deployment, any predefined region in the provider is overwritten with the new AWS region. From the Custom Tag list, select the custom tag that you want to assign to all resources that are deployed. To view the key-pair values in the selected custom tag, hover over the icon. Note: If you have also specified key-pair values in the tags attribute of a resource in the environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for this deployment. Following additional information is displayed: Provider version: The version of the provider used for deploying the environment. Package Type: The package types used for deploying the environment. Created By: The user who created the environment. On the Deployment tab, perform the following actions based on your requirements: Under Deploy , enter the following details: Enter the deployment name and description. Note: The deployment name must be unique across all versions of the selected environment. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while deploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Parent deployments of the same name as the child deployment name are selected by default. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. (Optional) On the Notification tab, specify the email and template details for this deployment. By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. If required, you can specify custom templates for sending an email when a deployment is started or destroyed. Contact your system administrator for a list of custom email templates that might be available. Note: You cannot customize the Deployment Initiation and Deployment Complete settings for a deployment. These settings are configured at the environment level . To start a new deployment, click START NEW DEPLOYMENT . The icon appears on resources that are successfully deployed. The icon appears on resources that could not be successfully deployed. (Optional) To view the resource logs, click the Logs icon ( ). (Optional) To stop a deployment that is in progress, perform the following actions: Click the Stop icon next to the Deploying status. In the confirmation box, click YES STOP IT . The status of the deployment changes to Stopping . Deploy Accelerator deploys all resources in progress and then stops the deployment. The status of the deployment changes to Stopped . Note: If Deploy Accelerator is unable to successfully deploy any of the resources in progress, the status changes to Failed instead of Stopped .","title":"Starting new deployments"},{"location":"deploy/using/#viewing-deployments","text":"Each environment version can have multiple deployments, such as Staging and Production. You can view the list of your deployments for an environment across all versions. You can also view the deployments that other users have shared with you. The deployment list also displays the status of each deployment -- deployed ( ) or failed ( ). On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of your own and shared deployments across all versions of the environment, click the deployment list on the canvas. Note: You can identify the deployments that are shared with you based on the color of the deployment name. In the list of deployments, click the deployment name that you want to open on the canvas. To view additional information related to the deployment after it is complete, select Deployment Details on the Canvas. The Deployment Details displays information about the following parameters: Deployment ID: The ID for deploying the environment. Deployment run ID: The run ID for querying the database for troubleshooting. Deployment Owner: User who had run the environment deployment. Deployment Input Variables: Additional deployment variables.","title":"Viewing deployments"},{"location":"deploy/using/#viewing-the-plan-for-deployments","text":"Before you start a new deployment or redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list, select the appropriate environment version. To view the plan for a deployment, perform one of the following actions: Goal Action To view the plan for a new deployment of the environment. Click the Plan icon ( ) and select Plan Environment . To view the plan for an existing deployment of the environment. Click the Plan icon ( ) and select Plan Selected Deployment . Note: You can view the plan for an existing deployment that is shared with you only if your group has been given the Redeploy permission. (Optional) To view the plan with default values, click QUICK PLAN . (Only to view the plan for a new deployment) In the Review and Plan: EnvironmentName window, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for the plan. The connection, provider, and AWS region that is configured for the environment version is shown by default. Note: If you select an AWS region for the plan, any predefined region in the provider is overwritten with the new AWS region. To plan the deployment with a different environment version, from the Environment Version list, select that version. In Input Variables , add input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while planning is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for the plan. (Only to view the plan for an existing deployment) In the Review and Plan: EnvironmentName window, update the environment version, input variables, and parent deployments mapping based on your requirements. Click PLAN . Note: In the case of a layered environment , the plan for child deployments is available only if the parent deployments are available.","title":"Viewing the plan for deployments"},{"location":"deploy/using/#redeploying-existing-deployments","text":"You can redeploy an existing deployment with the same environment version. If required, you can also redeploy an existing deployment with a different environment version. In both cases, only the changes made in the selected environment version are implemented. Deployed resources that have not been updated in the selected environment version are not impacted. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployments list on the canvas. From the deployments list, click the deployment name that you want to redeploy. Click the Re-Deploy icon ( ) to open the Review and Deploy window. Note: The Re-Deploy icon is available only if the environment version has been previously deployed. (Optional) On the Environment tab, perform the following actions based on your requirements: From the Connection list, select the connection that you want to use to connect to all resources in the environment. From the Custom Tag list, select the custom tag that you want to assign to all resources in the environment. To view the key-pair values in the selected custom tag, hover over the icon. (Optional) On the Deployment tab, perform the following actions based on your requirements: To redeploy with a different environment version, from the Environment Version list, select that version. By default, the environment version that was originally deployed is selected. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while redeploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Select the Confirmation check box. Click Upgrade . If you have redeployed with a different environment version, the deployment is now associated with that version.","title":"Redeploying existing deployments"},{"location":"deploy/using/#sharing-deployments","text":"Deploy Accelerator enables you to collaborate with other users on your deployments of an environment. The ability to share deployments is useful when multiple users need to view or manage the same deployment. All these users can view, redeploy, destroy, or stop the deployment based on the permissions assigned to their group. Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for the deployments. To share a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment whose deployment you want to share. Important: Before you share deployments of an environment version with selected groups, ensure that the environment version is also shared with those groups. Otherwise, users cannot view the shared deployments. From the deployments list, click the deployment name that you want to share. Click the Share icon ( ) and select Share Selected Deployment . You can share a deployment when it is in the Deploying , Deployed , Stopping , or Failed state. In the Share Deployment window, perform the following actions: Click the Share icon ( ) for the Group with which you want to share the deployment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the deployment, select the Warning check box and click SUBMIT . The deployment is shared with all users who are members of the selected groups If you no longer want to share the deployment, click CANCEL . (Optional) To enable users to redeploy the shared deployment by using the provider that is configured in that deployment, share the provider with the same groups. (Optional) To enable users to redeploy the shared deployment by using the connections that are configured in that deployment, share the connections with the same groups. Note: In case of layered environments, ensure that you share both parent and child deployments with the same groups. You must assign at least the View permission for the parent deployments. Otherwise, users cannot redeploy the child deployment.","title":"Sharing deployments"},{"location":"deploy/using/#destroying-deployments","text":"Each environment version can have multiple associated deployments, such as Staging and Production. You can destroy a deployment that is no longer required. This action ensures that you are not paying for resources that are not being used. Destroying a deployment deletes all the deployed resources that it contains. However, the environment version and its other deployments continue to be available. Typically, you destroy test, development, or other non-production deployments. Note: You can destroy parent deployments only if their child deployments are already destroyed. Therefore, in the case of a layered environment , you must destroy the deployments in a particular order. You must first destroy the child deployment in the lowest layer, then the deployment in the next layer, and so on until the parent deployment in the top layer. To destroy a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployment list on the canvas. From the deployments list, click the deployment name that you want to destroy. Click the Destroy icon ( ). Note: The Destroy icon is available only if the environment version has been previously deployed. In the confirmation message box, type Yes and then click SUBMIT . After the deployment is destroyed, an email is sent to your registered email address and to the additional email address that might be configured for the deployment. (Optional) To view the resource logs, click the Logs icon ( ). The Logs icon continues to be available until the deployment is successfully destroyed.","title":"Destroying deployments"},{"location":"deploy/using/#releasing-environment-versions","text":"After an environment version is finalized for production, you can choose to release that version. No changes can be made to a released version. On the Home page, click the More options icon ( ) in the top-right corner, and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to release. From the version list, select Release Version . In the confirmation message box, enter Yes and then click SUBMIT . You can no longer make any changes to the released version of an environment. However, you can start new deployments or redeploy existing deployments with this released version.","title":"Releasing environment versions"},{"location":"deploy/using/#creating-new-environment-versions","text":"Each environment can have multiple versions. After an environment version is released, no changes can be made to that version. To add or update resources to the environment, you must create a new environment version. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the version on which you want to base the new environment version. From the version list, select Create New Version . In the Create New Version From base version window, enter a unique version up to a maximum of 40 characters. The new version must be greater than the base version that you have selected. Alternatively, you can retain the base version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). (Optional) To not apply share permissions from the base environment to the new environment version, clear the Use share permission from base environment check box. By default, share permissions configured for the base environment are applied to the new environment version. Click CREATE . A new version of the environment appears on the canvas. This version also appears on the Environment List page and in the Search Environments box. (Optional) In the new environment version, add or modify resources and packages based on your requirements. (Optional) Update the description and configure additional settings for the environment version based on your requirements. When you are ready to create the infrastructure that you have designed in the new environment version, you can start a new deployment or redeploy an existing deployment with this version.","title":"Creating new environment versions"},{"location":"deploy/using/#upgrading-the-provider-version-of-environments","text":"Deploy Accelerator uses Terraform providers to create, manage, and update resources in AWS, Azure, Google Cloud Platform, and other supported providers. When you create a new environment or a new version of an existing environment, resources are created based on the most recent provider version that is supported in Deploy Accelerator. When you open an environment whose provider version has been automatically upgraded, the Provider Version Upgraded Changes window appears. For each resource in the environment, it lists the attributes that are deprecated or cannot be upgraded. The resources that require user action are marked in red. The resources for which Deploy Accelerator has automatically performed the changes are marked in green. To acknowledge the upgrade changes, click the Confirmation checkbox, and click CONFIRM. However, the list of provider upgrade changes will not be shown after you submit your acknowledgement. To retain the list of provider upgrade changes for some time, you can click CLOSE. To reopen the window, click Provider upgrade changes on the canvas. Important : If the upgrade changes are not acknowledged, the Provider Version Upgraded Changes window appears while deploying the environment. If the upgrade changes are acknowledged but the manual changes are not made to the resources, you will see an error in the deployment log file.","title":"Upgrading the provider version of environments"},{"location":"deploy/using/#viewing-dependency-between-environments","text":"The Environment Dependency View enables you to view the dependency of an environment on other environments (parent environments). You can also view other environments (child environments) that are dependent on an environment. If the environment has deployments, this view displays the parent or child deployments of the selected deployment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. If the selected environment has deployments, the deployment list on the canvas displays the latest deployment of the selected environment version. Click the Environment Dependency View icon ( ). The Environment Dependency View window appears, as shown in the following image: If the environment has a deployment, the Environment Dependency window displays the connection between the selected deployment and its parent or child deployments. The color of the deployment represents its current state. For example, black indicates Not Started, orange indicates Running, green indicates Deployed, and red indicates Failed. Click the toggle switch to see the dependent parent and child environment(s). If the toggle switch is towards Child, the Environment Dependency window displays the connection between the current environment and its child environment(s). If the toggle switch is towards Parent, the Environment Dependency window displays the connection between the current environment and its parent environment(s). (Optional) To switch to another environment or deployment shown in the Environment Dependency View window, select that environment or deployment name.","title":"Viewing dependency between environments"},{"location":"deploy/using/#configuring-custom-tags","text":"You can define tags for individual resources in an environment but this process is repetitive and time consuming. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can create a custom tag, which contains a set of AWS key-pair tags, and attach the custom tag to an environment. Deploy Accelerator attaches this custom tag to all resources in the environment. For example, to control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. To configure a custom tag, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Custom Tags . Under Custom Tag , click NEW . Enter the custom tag name and description. Under Tags , define the tags that you want to assign to all resources in the environment. The following image shows an example of how you must specify tags for your resources. Note: Tag values for all the resources that have tag support in your environment are set to the default values from the tags defined in the selected custom tag. Click SAVE . A new custom tag appears under the Custom Tag List section. Note: You can also add tags to a resource by setting the tags attribute in the RESOURCE panel. If users select a custom tag for an environment and specify values in the tags attribute of a resource in that environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value.","title":"Configuring custom tags"},{"location":"deploy/using/#comparing-differences-between-environments","text":"Deploy Accelerator enables you to compare differences between two environments or two versions of the same environment. Comparing two versions of the same environment helps you to track changes to an environment over time. Comparing environments is also useful when you have two similar environments and you want to retain only one of them. You can compare the differences between these environments, ensure that one of the environments has all the required resources and packages, and then delete the other environment. The comparison detects the following differences between the base and target environments. Comparison Details Environment configuration-level comparison This comparison displays a list of configuration details that have changed in the target environment version: -- Connection, provider, region, and environment package type -- Chef server and Chef environment -- Environment owner -- Custom tag -- Released status -- Notification email address and email templates -- Deploy and Destroy pre-scripts, post-scripts, and destroy after interval Resource-level comparison This comparison displays a list of resources that are added, edited, or removed in the target environment version. For each edited resource, you can see a list of attributes and packages that are added, edited, or removed. If a package has been edited, you can also see the differences in the package attributes. To compare differences between environments, perform the following actions: On the canvas, click the More icon ( ) and then click Compare . The environment version that is currently selected is considered as the Base Environment for comparison. (Optional) In the Compare Resources and Packages window, from the Base Environment and Version lists, select a different environment and environment version. You can identify environment versions that are shared with you based on the color of the environment version. From the Target Environment and Version lists, select the environment and environment version with which you want compare the base environment. The RESOURCES tab is selected by default. In the Resources panel, you can see a list of resources that are added, edited, or removed in the target environment. To help you identify the changes, the resources are highlighted in different colors, as shown in the following table. Highlighted color Meaning Green Added resource Red Deleted resource Blue Edited resource To see the resource-level comparison, click an edited resource in the Resources panel. On the RESOURCE ATTRIBUTES and PACKAGES tabs, you can see the differences in attribute values and packages. Green background color indicates added content while red background color indicates deleted content. To see the environment configuration-level comparison, click the CONFIGURATION tab. On the ATTRIBUTES tab, you can see a list of configuration details that have changed in the target environment. Green background color indicates added content while red background color indicates deleted content. Click CLOSE .","title":"Comparing differences between environments"},{"location":"deploy/using/#searching-environments","text":"On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, in the Search Environment tab, enter a search string to find an environment. The search string filters the environments and displays results with the string. The search string filters environments across all columns on the Environment List page.","title":"Searching environments"},{"location":"deploy/using/#renaming-environments","text":"On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to rename. On the canvas, click the More icon ( ) and then click Rename . In the Rename Environment window, enter a unique name for the environment and click SUBMIT . All versions of the environment are renamed. You can see the updated environment name in the Search Environment box, Environment List page, and all other instances. The environment name is also updated in any child environments that reference this environment. Note: You can rename an environment that has been shared with you only if the root environment version is shared with the Edit permission. Otherwise an error message is shown when you click SUBMIT .","title":"Renaming environments"},{"location":"deploy/using/#sharing-environments","text":"Deploy Accelerator enables you to collaborate with other users on an environment. The ability to share environments is useful when multiple users need permissions to perform different tasks such as edit, deploy, destroy, delete, view, and export on the same environment. All these users can work and collaborate on a shared environment at the same time. Users can perform one or more tasks based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to share with other users. Click the Share icon ( ) and select Share Environment . In the Share Environment window, perform the following actions: (Optional) To share all existing versions of the environment with other users, select Share All Versions . By default, only the currently selected environment version is shared with other users. Note: If you later create a new version of this environment , you must separately share that environment version with users. While creating the new version, you can choose to apply the share permissions from the base environment. Click the icon for the Group with which you want to share the environment version. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign share permissions to users, select the appropriate permission check boxes and click DONE . You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the environment, select the Warning check box and click SUBMIT . The environment version is shared with all users who are members of the selected groups. When you share an environment version, its deployments are not automatically shared with the specified groups. You must separately share each deployment with the appropriate groups and assign the required permissions. If you no longer want to share the environment, click CANCEL . Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for these deployments. (Optional) To enable users to start a deployment of the shared environment version by using the provider that is configured in that version, share the provider with the same groups. (Optional) To enable users to successfully use the connections that are specified in the shared environment version, share the connections with the same groups.","title":"Sharing environments"},{"location":"deploy/using/#deleting-environments","text":"You can choose to delete environments that you no longer need. If an environment has multiple versions, each version must be separately deleted. Deleting an environment version also permanently deletes all configurations related to that version. On the Home page, in the Search Environment box, enter the name of the environment that you want to delete. If the environment has multiple versions, each version is listed separately in the Search Environment box. You can identify environments that have been shared with you based on the color of the environment name. Next to the environment version that you want to delete, click the Delete icon ( ). In the confirmation message box, click YES . The environment version is no longer shown in the list of environments. Note: You cannot delete environment versions that have deployments. Similarly, you cannot delete an environment version that is being used as a parent in one or more child environments.","title":"Deleting environments"},{"location":"deploy/using/#restoring-deleted-environments","text":"You can restore a deleted environment which was deleted by you, or the environments for which you have the Delete permission. On the Home page, click the More options ( ) icon in the top right corner and then click Environments . In the Environment List , select Show Deleted Environments . The deleted environments appear at the end of the list. Under the Actions column, click Restore to restore the environment. In the confirmation message box, click YES . The environment is restored in the list of environments.","title":"Restoring deleted environments"},{"location":"deploy/using/#accessing-environments-and-deployments-of-other-users","text":"By default, Deploy Accelerator allows you to access only the following environments and deployments: Environments and deployments that you own. For more information, see Creating environments and Starting new deployments . Environments and deployments that other users have shared with you. For more information, see Sharing environments and Sharing deployments . However, sometimes you might need to access the environments and deployments of other users even though they are not shared with you. Your administrator can grant you this ability by adding you to the VIEW_ALL_USER'S_ENTITIES group. This group allows you to view all environments and deployments of all users in Deploy Accelerator. Note: To perform any actions (such as delete environments and destroy deployments) on the entities of other users, you must also be a member of the ADMIN group. To access environments and deployments of other users, perform the following actions: To access an environment or a deployment of another user, ensure that you have the ID of the environment version. Note: When an environment version is opened on the canvas, its ID is shown in the URL. Open a browser and enter the Deploy Accelerator URL in the following format: https:// YOUR-PLATFORM-BASE-URL /hcapdeploy/#/home/dnow/ ENVIRONMENT-VERSION-ID To view the list of all deployments across all versions of the environment, click the deployment tab on the canvas. Select the deployment that you want to view.","title":"Accessing environments and deployments of other users"},{"location":"deploy/using/#exporting-environments-as-blueprints","text":"If you create a standalone or layered environment that needs to be deployed multiple times, you can export that environment as a blueprint. Ensure that you have followed the best practices for creating blueprints . If you are exporting a layered environment, also ensure that you export the child environment that is in the lowest layer. This action ensures that parent environments in all other layers are also included in the blueprint. You can also export multiple environments. To export multiple environments, you can select a child environment and all the parent environments in the hierarchy are automatically included. Example of exporting a layered environment Consider a scenario in which you create a Jenkins environment and an APP-VPC environment that is dependent on the Jenkins environment. You then create environments for different applications - Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator), and Hitachi Cloud Accelerator - Repair. Each of these environments has a dependency on the APP-VPC environment, as shown in the following image: When you export one of the application environments as a blueprint, the Jenkins and APP-VPC environments are also included in the blueprint and the dependency between them is retained. To replicate the application environment that you have exported, you can create new environments from the blueprint , make customizations if required, and then start a deployment of the environments. To export an environment as a blueprint, perform the following actions: On the canvas, click the More Options icon ( ) and then click Export . In the Environment Export window, from the list select the environment(s) to export. The environment which is open on the canvas is selected by default. The environments created by you are listed in black color. The environments that other users have shared with you are listed in a different color. Important: If you select a child environment, all its parent environments are also included in the exported file. To filter a long list of environments, enter the search keywords in the Search Environments box. Enter a name for the blueprint. Select the warning checkbox to acknowledge the warning message, and click EXPORT . A JSON file is saved as ***fileName*.blueprint.reandeploy** to your local computer. In this JSON file, the environments, the resources within each environment, and the packages within each resource are alphabetically sorted. The packages that are listed after the environments are also alphabetically sorted.","title":"Exporting environments as blueprints"},{"location":"deploy/using/#best-practices-for-creating-blueprints","text":"Consider the following best practices while creating environments that you want to export as blueprints: Blueprints must ideally use a layered environment. You should create an environment with network resources, such as VPCs and CIDRs. In all other environments within the blueprint, you should get the VPC IDs, CIDRs, and other infrastructure resources from this environment by using the Depends On resource. The name of the Depends On resource in each environment must ideally indicate the environment on which there is a dependency. For example, depends_on_app_vpc . Blueprints must enable users to change resource attribute values by using input variables. Blueprints must output important information, such as Server URLs and IDs of important resources, to outputs. Blueprints must use name attributes that are descriptive and unique. For example, you must use ${var.environment_name}-myserver instead of myserver . Blueprints must not hard-code critical things, such as AWS regions, account IDs, VPCs, owner information, product information, environment information, buckets, user names, and passwords. For things that should be automatically detected, blueprints must use Terraform data sources instead of input variables. The following table lists a few examples of data sources that you can use in your environment. To know all the data sources that you can add to your environment, see the Data Sources section on the Resources tab in the left panel on the Deploy Accelerator Home page. Goal Data source Usage Get the current AWS Account ID aws_caller_identity ${ DataSourceName .current.account_id} Get the name of the current region aws_region ${ DataSourceName .current.name} Get the current partition and use it to build an Amazon Resource Name (ARN) aws_partition ${ DataSourceName .current.partition} Get a list of availability zones aws_availability_zones ${ DataSourceName .available.names} Get the current Elastic Load Balancer (ELB) service account ID and use it in the bucket policy for ELB logs aws_elb_service_account ${ DataSourceName .current}","title":"Best practices for creating blueprints"},{"location":"deploy/using/#adding-blueprints-in-the-blueprint-gallery","text":"If you have created an environment that needs to be replicated multiple times by different users, you can export that environment as a blueprint and then add the blueprint in the Blueprint Gallery. Users can then create a new environment by importing that blueprint from the Blueprint Gallery . Before you begin Ensure that you can access the Artifactory that is configured for Deploy Accelerator. In the Artifactory, ensure that you can access the repository that the Administrator has configured for storing the blueprints that are displayed in the Blueprint Gallery. Export your environment as a blueprint . To add a blueprint in the Blueprint Gallery, perform the following actions: Create additional files that are required for the blueprint. Create a metadata.yml file for the blueprint. This file must contain the attributes that are listed in the following table. Attribute Details name The blueprint name specified in this file is shown in the Blueprint Gallery. Ensure that the blueprint name in this file matches the actual file name of the blueprint (which you provided while exporting the environment as a blueprint from Deploy Accelerator). For example, if you exported an environment from Deploy Accelerator as Chef-Server-HA , the blueprint file name is Chef-Server-HA.blueprint.reandeploy and the name that you must specify in the metadata.yml file must be Chef-Server-HA . description The blueprint description specified in this file is shown in the Blueprint Gallery. version The blueprint version specified in this file is shown in the Blueprint Gallery. image To display an image for the blueprint version in the Blueprint Gallery, specify that image in this file and add the image file in the images folder that you will create in a later step. You must specify the image path relative to the BlueprintName and Version folder in the Artifactory. For example: images/chef.png The image must be in the PNG format. Also, it is recommended that the image dimensions are 130 x 40 pixels. Note: If you later want to update the image for a blueprint version, it is recommended that you use a different image file name and update the metadata.yml file. If you just replace the image file with the same name in the images folder, the updated image is not shown in the Blueprint Gallery. ownerName The owner name specified in this file is shown in the Blueprint Gallery. If you add this attribute in the file but do not specify a value, admin is shown as the owner of this blueprint. ownerName is an optional attribute. ownerEmail The owner email specified in this file is shown in the Blueprint Gallery. ownerEmail is an optional attribute. The following image shows an example of a metadata.yml file. (Optional) Create a README.md file for the blueprint. This file (Markdown format) should ideally include information such as deployment architecture, details about input and output variables used in the environment, and any dependencies. To add an image in the README.md file, you must add the image file in the images folder that you will create in the next step. Also, you must use the IMAGE_BASE_PATH prefix for the image path in the README.md file, as shown in the example below: IMAGE_BASE_PATH/images/chef.png (Optional) Create an images folder and add the image that you want to show for the blueprint version in the Blueprint Gallery. Also, add any images that are used in the README.md file in this folder. Add the blueprint in the Blueprint Gallery. Sign in to the Artifactory that is configured for Deploy Accelerator. Navigate to the repository in which the blueprints must be stored. Create a folder with the same name as the blueprint file name (for example: Chef-Server-HA). Important: The blueprint is displayed in the Blueprint Gallery only if the folder name, blueprint file name, and the blueprint name defined in the metadata.yml file are the same. In the BlueprintName folder, create a folder with the blueprint version (for example: 01.00.00). Ensure that the folder name matches the blueprint version that is defined in the metadata.yml file. In the version folder, add the following files and folder: metadata.yml file ***BlueprintName*.blueprint.reandeploy** file (Optional) README.md file (Optional) images folder Verify that the blueprint is available in the Blueprint Gallery and can be successfully imported and deployed. Sign in to Cloud Accelerator Platform . On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . On the Blueprint Gallery page, search for the blueprint that you have just added in the Artifactory. (Optional) If you cannot see your blueprint in the Blueprint Gallery, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint metadata from the Artifactory. This metadata includes the blueprint name, description, version, and image. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. Confirm that the name, description, version, and image of your blueprint are displayed correctly. Import the blueprint and start a new deployment .","title":"Adding blueprints in the Blueprint Gallery"},{"location":"deploy/using/#deploy-accelerator-resources","text":"The Resources tab displays many resources that you can add to your environment. The following table lists the additional resources that Deploy Accelerator provides to address specific scenarios : Resource name Details Depends On Use this resource to create a dependency on another environment. The name of the Depends On resource must ideally indicate the environment on which there is a dependency. In the reference_type attribute, you can select either Environment Name or S3 . -- If you select Environment Name , click the Depends On box and select the parent environment version on which the environment must be dependent. The selected parent environment appears in the environmentName:environmentVersion format (for example: networkEnv:01.00.00). While exporting the child environment, Deploy Accelerator exports this version of the parent environment. -- If you select S3 , enter the S3 bucket name and the remote state file name of the environment that is referred by the Depends On resource. For example: s3://bucket_name/tfstatefile_name , in which bucket_name is the name of the S3 bucket and tfstatefile_name is the Terraform remote state file name of the environment that is to be referred. If required, you can define variables for the S3 bucket and Terraform state file names in the Input Variables resource. To reference these variables in the Depends On resource, you must use the following interpolation syntax: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } For example: If you have defined the S3bucket and tfstatefile variables in the Input Variables resource, specify the following URL in the Depends On resource. s3:// \\({var.s3bucket}/\\) Ensure that the provider that you use to deploy the environment has the appropriate access to the S3 bucket that you have specified. Note: For information about how you can create layered environments by using the Depends On resource, see Creating layered environments . Input Variable Use this resource to create a JSON file in which you can specify multiple variables and set their values. You can later use these variables in other resources within the same environment. For information about the different ways in which you can define and use input variables, see Formats for defining input variables and Formats for using input variables in resources . Locals Use this resource to create a JSON file in which you can specify multiple local values. You can later use these local values in other resources within the same environment. A local value can be a simple constant or a complex expression that transforms or combines values from other resources in the environment, as shown in the following example. For more information, see the Terraform documentation . Output Use this resource to create a JSON file in which you can specify multiple variables that can be used in other dependent environments. The value of these variables can reference either variables defined in the Input Variable resource or attributes of other resources in the environment by using the interpolation syntax. Note: If you do not want to display an output value in the resource logs of an environment, set the sensitive parameter for that output to true . Existing VM Use this resource to add an existing instance to the environment.","title":"Deploy Accelerator resources"},{"location":"deploy/using/#formats-for-defining-input-variables","text":"In the Input Variable resource, you can use the following formats to define input variables based on your requirements: String: Use the following syntax to define input variables by using the String format: \"string_input\": { \"type\": \"string\", \"default\": \"sample_value\", \"description\": \"this is sample string value for hcap deploy\" } Example: \"string_input\": { \"type\": \"string\", \"first_name\": \"John\", \"description\": \"First name of the user\" } List: Use the following syntax to define input variables by using the List format: \"list_input\": { \"type\": \"list\", \"default\": [ \"sample_value1\", \"sample_value2\" ], \"description\": \"this is sample list value for hcap deploy\" } Example: \"list_input\": { \"type\": \"list\", \"Region\": [ \"us-west-2\", \"us-east-1\" ], \"description\": \"List of regions to be used\" } Map: Use the following syntax to define input variables by using the Map format: \"map_input\": { \"type\": \"map\", \"default\": { \"key\": \"sample_value\" }, \"description\": \"this is sample map value for hcap deploy\" } Example: \"map_input\": { \"type\": \"map\", \"ec2details\": { \"app\": { \"ami\": \"ami-718c6909\", \"type\": \"t2.micro\" }, }, \"description\": \"this is sample map value for hcap deploy\" } The following image shows an example of using these different formats in the Input Variable resource.","title":"Formats for defining input variables"},{"location":"deploy/using/#formats-for-using-input-variables-in-resources","text":"In any environment, instead of hard coding the attribute value of a resource, you can use the interpolation syntax to refer to the input variables that you have defined. The interpolation syntax is based on the type of input variable that you are referring. String: Use the following interpolation syntax to refer to an input variable of the String type: ${var.variableName} Example: ${var.string_input} List: Use the following interpolation syntax to refer to an input variable of the List type: To refer to all the values in a list, use the following syntax: ${var.variableName} To refer to a specific value in a list, use the following syntax: ${var.variableName[index]} Examples: ${var.list_input} ${var.list_input[0]} Map: Use one of the following interpolation syntax to refer to an input variable of the Map type: ${var.variableName} ${var.variableName[\"Key\"]} ``` ${lookup(var.variableName[\"Key\"], \"InnerKey\")} ``` Examples: ``` ${var.map_input} ``` ``` ${var.map_input[\"ec2details\"]} ``` ``` ${lookup(var.map_input[\"ec2details\"], \"app\"} ```","title":"Formats for using input variables in resources"},{"location":"deploy/using/#copying-resources","text":"You can copy one or more existing resources from either the same environment or a different environment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment from which you want to copy a resource. On the canvas, right-click the resource that you want to copy, and click Copy . To copy multiple resources, repeat this step for each resource. Based on your requirements, perform one of the following actions: To create a copy of the resource in the same environment, right-click the canvas and then select the resource from the Paste Resource list. To create a copy of the resource in another environment, open that environment in a new tab, right-click the canvas and then select the resource from the Paste Resource list. All the resources that you have copied appear in the Paste Resource list. You can add a copied resource to any environment only once. ![](/images/rean-deploy/rd_resourcepaste.PNG) From the Paste Resource list, click the appropriate resource and then perform the following actions: In the Resource Name window, enter a name for the resource and click CREATE . In the right panel that opens, on the Resource tab, you can see that the attribute values are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The Packages tab also displays the same packages, along with their attributes, as the original resource. On the Resource tab, add or update the attribute values based on your requirements. Important: If you have copied a resource from another environment, you must update attribute values that use the interpolation syntax to reference input variables or other resources. You must also update the Depends On attribute if it references another resource in the original environment. Otherwise, the new resource might not be successfully deployed. (Optional) On the Packages tab, add or remove packages based on your requirements. Note: After you select a copied resource from the Paste Resource list, it no longer appears in the list. (Optional) Repeat step 5 for the other resources in the Paste Resource list. (Optional) To clear all resources from the Paste Resource list, click clearAll . To save the environment, click the Save icon ( ).","title":"Copying resources"},{"location":"deploy/using/#renaming-resources","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment version. On the canvas, select the resource that you want to rename. In the right panel that opens, on the Resource tab, click the resource name link at the top. In the Resource Name window, update the resource name and click SAVE . The resource name is automatically updated in other resources within the environment that reference it by using either the interpolation syntax or the Depends On attribute. To save the environment, click the Save icon ( ). Important: If you rename a deployed resource, that resource is destroyed and a new resource is created with the updated name when you redeploy existing deployments. Also, until you redeploy the deployments, the deployed icon is not shown on the resource and the deployed values are not shown on the Resource Details tab.","title":"Renaming resources"},{"location":"deploy/using/#adding-user-defined-packages","text":"Packages in Deploy Accelerator enable you to leverage your existing infrastructure automation tool (Chef Solo, Chef Server, Ansible Solo, or Puppet) to configure compute resources in your environments. Each package definition includes the infrastructure automation tool and the Chef cookbook or Puppet module or manifest file that Deploy Accelerator must use to configure the compute resource. Managed packages are available by default after Deploy Accelerator is successfully deployed. You can leverage these managed packages if they meet your requirements. Otherwise, you can add your own custom (or user-defined) packages. In the case of user-defined packages, you must define a versioning scheme for the packages and ensure that each combination of package type, name, and version is unique. The following diagram describes the high-level process for creating and releasing a user-defined package: After a user-defined package is released, you can neither edit nor delete that package. To include additional updates, you must create a new package version.","title":"Adding user-defined packages"},{"location":"deploy/using/#add-a-user-defined-chef-ansible-or-puppet-package","text":"To prepare the user-defined package that you want to add in Deploy Accelerator, perform the following actions: Based on your infrastructure automation tool, perform one of the following actions: Create a Chef cookbook and test it well using the chef-client. For more information, see the Chef documentation . Create an Ansible playbook and test it well using the Ansible-client. For more information, see the Ansible documentation . Create a Puppet module or manifest file and test it well using the puppet-agent. For more information, see the Puppet documentation . Create a ZIP file of the Chef cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the tar.gz format. Commit the ZIP file (tar.gz format) in GitHub, GitLab, Artifactory, or any other repository. You can use a public repository for open source software. Important: If Deploy Accelerator is deployed in the offline mode, you must commit the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the Artifactory. Decide the attributes that you want to define for this package. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . On the Package List page, the Package Type column enables you to identify the package type. In case of Chef Server packages, the Chef Server column enables you to identify the Chef Server from which the packages are shown. To add a user-defined package, click ADD . On the Add Package page, perform the following actions: On the Basic tab, enter the following details: Field Details Package Name Enter the name of the package. This name appears on the Packages tab on the Home page. The name that you enter must be unique across all packages of that type. However, if you are adding a new version of an existing package, ensure that the package name is the same as the existing package name. Note: You cannot enter a name that matches a managed package. Package Type Select chef or ansible as the package type. Depending on the option selected here the package is listed, under the Packages tab, in the Private Packages list. Package Version Enter the version of the package based on your own versioning scheme. The value of this field must be unique across all existing versions of the package that you are adding. Note: Each combination of package type, name, and version must be unique. Description Enter details about the package. Image Url Enter the URL from which Deploy Accelerator must download the image file that is shown on the Packages tab on the Home page. On the Repository tab, enter the following details for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located: Fields Details Download URL Enter the URL from which Deploy Accelerator must download the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Note: If Deploy Accelerator is deployed in the offline mode, you must specify the Artifactory URL from where the Chef Solo cookbook, Ansible playbook or Puppet module or manifest file can be downloaded. Repository Type Select the repository type where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have stored the files in a different repository or location (such as Amazon S3), select the plain option. Access Token Enter the access token for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have selected Artifactory as the repository type, you must enter the API key. Ensure that the access token or API key that you enter is authorized to download the ZIP file from the repository. Note: If you do not enter the access token or API key, Deploy Accelerator takes the default access token or API key that your your administrator has configured . ZIP File Name Enter the name of the ZIP file that contains the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Unzipped Name Enter the name of the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file that is extracted from the ZIP file. Dependent packages Select the other packages that are a prerequisite for installing this package on a resource. On the Attributes tab, specify the attribute values as the array of JSON. The attributes help to parameterize the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file with some dynamic values. All the values that you enter here are shown on the Deploy Accelerator UI to ask the user to provide dynamic values before deployment. A few default attributes are shown for your reference. You can choose to use these attributes or replace them with other attributes that are relevant to your package. Important: The package will be visible to other users, so ideally you should avoid specifying default values for sensitive attributes. Click SAVE . Confirm that this package version appears on the Package List page. The state of this package version is unreleased. Note: In the case of Chef Solo cookbooks, all recipes that the cookbook contains also become available in Deploy Accelerator. While adding a Chef Solo package (or cookbook) to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook Test the user-defined package . Release the user-defined package .","title":"Add a user-defined Chef, Ansible, or Puppet package"},{"location":"deploy/using/#test-a-user-defined-package","text":"Create a test environment of the appropriate environment package type. After the environment is created, confirm that the user-defined package that you want to test appears on the Packages tab in the left panel. From the Resources tab in the left panel, drag a compute resource (For AWS an EC2 instance, or a null VM) to the canvas. From the Packages tab in the left panel, drag the user-defined package to this resource. In the right panel that opens, on the Packages tab, select the unreleased version that you want to test, and enter other package details as required. In case of a Chef Solo package (or cookbook), confirm that all recipes that the cookbook contains are available for selection in the Recipes box. Note: While adding a Chef Solo package to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Save the environment. Start a new deployment of the environment . (Optional) To resolve any issues that are detected during the testing, edit the user-defined package and then redeploy the existing deployment . After all issues with the user-defined package are successfully resolved, release the user-defined package .","title":"Test a user-defined package"},{"location":"deploy/using/#release-a-user-defined-package","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To release a package, in the Actions column for that package, click Edit . On the Attributes tab, click RELEASE . Important: You can neither edit nor delete a released package. In case you need to make any changes, you must create a new package version.","title":"Release a user-defined package"},{"location":"deploy/using/#managing-user-defined-packages","text":"You can edit, download, or delete user-defined packages only if they are not yet released. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. In the Actions column for a specific version of a user-defined package, perform one of the following actions: To edit a user-defined package, click Edit to open the Add Package page, update the package details, and then click SAVE . You cannot edit a user-defined package that is released. To save a copy of the user-defined package to your computer, click Download . To delete the user-defined package, click Delete and then click YES in the confirmation message box.","title":"Managing user-defined packages"},{"location":"deploy/using/#creating-new-package-versions","text":"You can create new versions of both user-defined and managed packages. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To create a new version of a package, in the Actions column for that package, click Create New Version . In the Create New Version From base version window, update details of the new package version. By default, details of the base package version are shown. Note: You must specify a unique version number for this package. Also, the version that you specify must be greater than the base version that you have selected. Click CREATE . (Optional) On the Add Package page, update additional details about the new package version based on your requirements. Click SAVE . A new version of the package appears on the Package List page.","title":"Creating new package versions"},{"location":"deploy/using/#manual-changes-required-for-migration-of-blueprints-from-2280-to-302","text":"In Deploy Accelerator 3.0.2, the Terraform version is upgraded to 0.12.0. Due to this, the blueprints that are created in Deploy Accelerator 2.28.0 and previous releases, require some manual change to be used in 3.0.2.","title":"Manual changes required for migration of blueprints from 2.28.0 to 3.0.2"},{"location":"deploy/using/#mandatory-attributes-for-syntax-validation","text":"Terraform v0.11 and earlier allowed adding attribute/sub-attribute JSON as a Input Variable for the resources, which can be added as an interpolation in the resource attribute. Terraform v0.12 no longer allows such interpolation for JSON and JSON Array for its attributes or sub-attributes. If there is an attribute of the type JSON or JSON Array then it should follow a proper structure for its attributes and sub-attributes. Hence, the Input/Local variables of type JSON cannot be used in interpolation to the Resource or Datasource attributes. Example Resource: aws_route53_record , Attribute: Alias Input Variable for the resource: { \"var.alias_json\" : { \"type\": \"map\", \"default\": { \"name\": \"dns_record_alias\" } } } and value for the alias attribute in the resource for Terraform v0.11 and previous release: alias : [\"${var.alias_json}\"] This syntax is above is not valid. The valid syntax for the alias attribute in the aws_route53_record resource for Terraform v0.12 is as follows: [ { \"evaluate_target_health\": \"\", \"name\": \"${var.alias_name}\", \"zone_id\": \"\" } ]","title":"Mandatory attributes for syntax validation"},{"location":"deploy/using/#using-floor-function","text":"From Terraform 0.12, the Terraform language does not distinguish between integer and float types. Instead, the language just has a single \"number\" type that can represent high-precision point numbers. The Terraform documentation mentions that this new type can represent any value that could be represented before, plus many new values due to the expanded precision. The Terraform documentation also mentions that in most cases this change should not cause any significant behavior change, but please note that in particular the behavior of the division operator is now different: it always performs floating point division, whereas before it would sometimes perform integer division by attempting to infer intent from the argument types. If you are relying on integer division behavior in your configuration, use the floor function to obtain the previous result. A common place this would arise is in index operations, where the index is computed by division. For example: ${floor(length(var.availability_zones)/var.subnet_count) Unfortunately the automatic upgrade tool cannot apply a fix for this case because it does not have enough information to know if floating point or integer division was intended by the configuration author, so this change must be made manually where needed.","title":"Using Floor function"},{"location":"deploy/using/#using-escape-sequences","text":"In JSON attributes, if \\n , \\r , \\t , symbols are not escaped, then Terraform v0.12 does not validate it as a correct JSON. JSON format for Terraform v0.11 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\n \\\")}\" } JSON format for Terraform v0.12 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\\\n \\\")}\" }","title":"Using Escape Sequences"},{"location":"deploy/using_copy/","text":"Deploy and manage environments \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to create, deploy, and manage environments in Deploy Accelerator. Contents \u00b6 Overview * Overview of environments * Overview of layered environments * Overview of blueprints * End-to-end process for multiple deployments of an environment Create environments * Configuring providers * Configuring connections * Creating environments * Creating layered environments * Creating new environments from blueprints * Copying environments Deploy environments * Starting new deployments * Viewing deployments * Viewing the plan for deployments * Redeploying existing deployments * Sharing deployments * Destroying deployments Release environments Releasing environment versions Creating new environment versions Manage environments Upgrading the provider version of environments Viewing dependency between environments Configuring custom tags Comparing differences between environments Searching environments Renaming environments Sharing environments Deleting environments Restoring deleted environments Accessing environments and deployments of other users Create and manage blueprints Exporting environments as blueprints Best practices for creating blueprints Adding blueprints in the Blueprint Gallery Manage resources Deploy Accelerator resources Copying resources Renaming resources Manage packages Adding user-defined packages Managing user-defined packages Creating new package versions Perform post-upgrade tasks Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 Note: To access Deploy Accelerator, you must sign in to Hitachi Cloud Accelerator Platform by using your Cloud Accelerator Platform account. You can access Deploy Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions. For information about creating a Cloud Accelerator Platform account, see Create & access account . Overview of environments \u00b6 Environments in Deploy Accelerator help you to design the infrastructure that you need to deploy different applications in the cloud. Environments are visual representations of your infrastructure and can be easily created by dragging resources and packages to a canvas. You can also add dependencies between the various resources in an environment. The following image shows an example of an environment in Deploy Accelerator: Each environment contains the following elements: Resources: Entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. Packages: Entities that use open-source infrastructure automation tools to configure compute resources in an environment. Deploy Accelerator provides multiple out-of-the-box packages. If required, you can also create your own user-defined packages by using Chef (Chef Solo or Chef Server) or Puppet . Links: Visual representation of dependencies between various resources in an environment. Scripts: Set of specific actions that must be performed before or after deploying and destroying an environment. Environment versions and multiple deployments \u00b6 You can release and version environments in Deploy Accelerator. Releasing an environment enables you to freeze any updates to the environment. Typically, when an environment is ready to be deployed in production, you can release that environment. To add or update resources in a released environment, you must create a new environment version. Environment versions enable you to keep a track of changes to an environment over time. After you have designed the infrastructure in an environment version, you can start a deployment to actually create the infrastructure. Based on your requirements, you can also start multiple deployments of the same environment version. Multiple deployments help you to create an environment once and then reuse it to create new infrastructures. For example, you can start QA and Production deployments of an environment version or you can start two Production deployments of that environment version. For each new deployment of an environment version, you can define different values for the input variables (or parameters) that are defined in the environment. For example, you can select different instance types (such as t2. micro or t2.large) for the AWS EC2 instances in your QA and Production deployments. For more information, see end-to-end process for multiple deployments of an environment . You can also redeploy an existing deployment with the same environment version or with another environment version. Deployments that you no longer need can easily be destroyed. Environments that you no longer need can also be deleted. If an environment has multiple versions, each version must be separately deleted. However, before you can delete an environment version, you must destroy all deployments for that version. Overview of layered environments \u00b6 A layered environment is basically a collection of multiple interdependent environments . For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between them, as shown in the following image. While deploying a child environment in a layered environment, you have to select a specific deployment of the parent environment. If parent environments have multiple deployments, you have to select a specific deployment while deploying the child environment. If you do not specify any deployment, the parent deployment with the name default is used to deploy the child environment. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other dependent environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. Overview of blueprints \u00b6 Blueprints in Deploy Accelerator are templates of commonly used IT infrastructures. If you have designed modular solutions (layered environments) or simple solutions (standalone environments) that need to be replicated multiple times, you can export the environments as blueprints . If only you plan to create environments from your blueprints, you can maintain the blueprints on your computer. However, if you want other users to create environments from your blueprints, you can add the blueprints in the Blueprint Gallery . Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . For example, you can create an environment for a high-availability deployment of Chef Server and export that environment as a blueprint. If another team needs to deploy its own high-availability Chef Server, a user in that team can import your blueprint, customize the deployment parameters, and start a new deployment. The following image describes the end-to-end lifecycle of a blueprint, from creating and exporting an environment as a blueprint to creating new environments from that blueprint. End-to-end process for multiple deployments of an environment \u00b6 The following procedure describes the end-to-end process that you can follow to implement multiple deployments of a standalone environment. Create a new environment and design your infrastructure. For example, create an environment with the version 01.00.00. Start a new deployment of the environment that you have created. For example, create a Staging deployment to test your design. (Optional) Based on your requirements, add or update resources in this environment and redeploy the existing deployment . Release the environment version . No changes can be made to a released environment version. However, you can start new deployments or redeploy existing deployments with this environment version. Start another new deployment of the environment. For example, create a Production deployment to deploy your applications. Note: For each new deployment, you can change the input variables based on your requirements. For example, you can use different VPC IDs for your Staging and Productions deployments. (Optional) View all deployments for this environment. To make any updates to the environment, create a new environment version and then add or update resources in this environment version. For example, create an environment version 02.00.00 that is based on version 01.00.00. Based on your requirements, perform one of the following options: Start a new deployment of the environment version that you have created. Plan and redeploy an existing deployment with this environment version. For example, you can redeploy the Staging deployment of environment version 01.00.00 with this new environment version 02.00.00. In this case, only the delta between the two versions is deployed. Resources that have not been changed between the two versions are not impacted. Note: When you redeploy an existing deployment with a different environment version, the deployment gets associated with that environment version. Release the environment version . Note: In the case of layered environments, you must follow these steps for each environment within the layered environment. However, in addition to using the Depends On resource to create a dependency between the environments, you must also create a dependency between specific deployments of the environments. For example, the Staging deployment of a child environment must be dependent on the Staging deployment of the parent environment. Configuring providers \u00b6 Providers in Deploy Accelerator allow you to deploy and manage different types of infrastructure resources. They also contain the authentication details required to deploy and manage the resources. For example, an AWS provider contains credentials for accessing the Amazon Web Services (AWS) account in which you want to deploy and manage resources, such as EC2 instances, subnets, and S3 buckets. Deploy Accelerator supports many different types of providers, such as AWS, Microsoft Azure, and Kubernetes. When you create a new environment in Deploy Accelerator, you have to select a provider. The resources that are available for the environment are based on the selected provider. By default, only you can use the providers that you create. However, you can choose to share your own providers with one or more groups. Supported providers \u00b6 The following table lists the providers that are supported in the latest version of Deploy Accelerator. It also lists supported versions of the Terraform providers that Deploy Accelerator uses to create and manage resources. Provider Supported Terraform Provider Version ACME 1.6.3 Artifactory 2.2.4 AWS 3.23.0 Azure 2.41.0 Azure Active Directory 1.1.1 Azure DevOps 0.1.0 Azure Stack (Beta) 0.9.0 Consul 2.10.1 Databricks 0.2.9 Datadog 2.18.1 DNS 3.0.0 Docker 2.8.0 Google Cloud Platform 3.51.0 HEC-CP None HEC-VM None Helm2 0.10.6 Helm3 1.3.2 Kibana 0.7.1 Kubernetes 1.13.3 Oracle 1.4.0 Oracle Cloud Infrastructure 4.7.0 vSphere 1.24.2 vSphere NSX-T (Beta) 3.1.0 Create a provider \u00b6 On the Home page, click the More options icon ( ) in the top-right corner. Click Providers . On the Provider page, click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). For a complete list of provider types that are available in Deploy Accelerator, see Supported providers . In Provider Details , enter the authentication details that Deploy Accelerator must use to deploy and manage resources. The provider details that you have to specify differ based on the selected provider type. For more information, see Provider details . You can also click the Terraform Provider Link below the Provider Details box to view the Terraform documentation on the selected provider. (Only for AWS and Kubernetes provider types) To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click SAVE . A new provider appears in the Providers List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details. (Optional) Share the provider with one or more groups. Important: If you later edit this provider, you must re-enter all values in the Provider Details section before you click UPDATE . Otherwise, an error message might be shown for deployments that use this provider. Delete a provider \u00b6 On the Home page, click the More options icon ( ) in the top-right corner, and then click Providers . Under the Provider List , click Delete ( ), and in the confirmation dialog box, click Yes . Once a provider is deleted the provider credentials are also removed from the database. Before deleting a provider, make sure that no environment is using the provider for deployments. If an environment uses a provider, the provider cannot be deleted. Provider details \u00b6 Deploy Accelerator supports multiple providers, such as AWS, Azure, and Kubernetes. While creating a provider, you have to specify authentication details that Deploy Accelerator can use to deploy and manage the resources. The following topics provide information about the required details for each provider type: ACME provider details Artifactory provider details AWS provider details Azure provider details Azure DevOps provider details Consul provider details Databricks provider details Datadog provider details Google provider details Helm provider details Kubernetes provider details Kibana provider details Oracle Cloud Infrastructure provider details ACME provider details \u00b6 The Automated Certificate Management Environment (ACME) is an evolving standard for the automation of a domain-validated certificate authority. The ACME provider allows you to acquire a valid SSL certificate from Let's Encrypt. The ACME provider supports a wide list of DNS challenge types, for example gcloud, azure, digitalocean, etc. For the complete list, see the Terraform documentation . Following are the parameters in an ACME provider: Parameter Details server_url The URL to the ACME endpoint's directory. This is a mandatory parameter. config The list of key-value pair required according to the provider. You can enter a key parameter value here to override the values from provider_reference_id. This is an optional parameter. provider_reference_id The reference ID of the reference provider. This is an optional parameter. If you enter this parameter, you will not have to specify the credentials of the cloud provider used as dns_challenge . Provider Details JSON Example { \"server_url\": \"\", \"config\": { AWS_ACCESS_KEY_ID = \"XXXXXX\" AWS_SECRET_ACCESS_KEY = \"XXXXXXX\" AWS_DEFAULT_REGION = \"us-east-1\" }, \"provider_reference_id\": \"1\" } Artifactory provider details \u00b6 The Artifactory provider is to manage the resources for Artifactory. Following are the parameters in Artifactory provider: Parameter Details url The URL for customer artifactory. This is a mandatory parameter. username The username for accessing customer artifactory. password The password for logging in to customer artifactory. access_token The access token for accessing the artifactory. api_key The API key for accessing the artifactory. Note: You require only one type of parameter for authentication. Enter a username/password, or an access token, or an API key. Provider Details JSON Example { \"url\": \"http://www.example.com\", \"username\": \"XXXXX\", \"password\": \"XXXXX\", \"access_token\": \"XXXXX\", \"api_key\": \"XXXXXXXXXXX\" } AWS provider details \u00b6 For the AWS provider type, you must select either Basic Credentials or Instance Profile to specify authentication details of the AWS account that Deploy Accelerator must use to deploy an environment. To use a more secure way of cross-account access, you must select Assume Role . Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can attach a role to the instance on which Deploy Accelerator is deployed. Deploy Accelerator can then assume a role in the other accounts and deploy the resources. For more information about assuming roles, see AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Deploy Accelerator is deployed. For the other accounts, you must specify a role that Deploy Accelerator can assume to deploy the resources. The role that you specify must have permissions to deploy the required resources. It must also define the account in which Deploy Accelerator is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Deploy Accelerator must deploy resources. However, Deploy Accelerator can use this method only if a role is attached to the instance on which Deploy Accelerator is deployed. This role must have the permissions to deploy the required resources. To use the Instance Profile method, you must specify only the region in which the resources must be deployed, as shown in the following example: { \"region\" : \"xx-xxxx-x\" } Static Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can specify long-term access credentials for only the parent account. Deploy Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Deploy Accelerator can assume to deploy resources in that account. The role that you specify must have access to deploy the required resources. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Deploy Accelerator must deploy resources, as shown in the following example: { \"access_key\" : \"ACCESS-KEY\" , \"secret_key\" : \"SECRET-KEY\" , \"region\" : \"xx-xxxx-x\" } The IAM user whose credentials you specify must have access to deploy the required resources. Azure provider details \u00b6 The Azure provider is used to interact with the resources supported by Azure. Following are the parameters in an Azure provider: Parameter Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated. Azure DevOps provider details \u00b6 The Azure DevOps provider is used to interact with the resources supported by Azure DevOps. Following are the parameters in an Azure DevOps provider: Parameter Details org_service_url (Required) This is the Azure DevOps organization URL. personal_access_token (Required) This is the Azure DevOps organization personal access token. The account corresponding to the token will need \"owner\" privileges for this organization. Consul provider details \u00b6 The Consul provider is used to interact with the resources supported by Consul. Following are the parameters in an Consul provider: Parameter Details address Public IP address of the instance on which Consul server is installed. datacenter The datacenter that is configured for the Consul server you have specified. The following is an example of the provider details section for a Consul provider. { \"address\" : \"123.0.57.189\" , \"datacenter\" : \"dc2\" } Databricks provider details \u00b6 The Databricks provider is used to interact with the resources supported by Databricks. Following are the parameters in an Databricks provider: Parameters Details host (optional) This is the host of the Databricks workspace. It is a URL that you use to login to your workspace. token (optional) This is the API token to authenticate into the workspace. username (optional) This is the username of the user that can log into the workspace. password (optional) This is the user's password that can log into the workspace. profile (optional) Connection profile specified within ~/.databrickscfg . To learn more about connect profiles, see Databricks documentation. Datadog provider details \u00b6 The Datadog provider is used to interact with the resources supported by Datadog. Following are the parameters in an Datadog provider: Parameters Details api_key Datadog API key. This is a mandatory parameter if the validate parameter is set to true . app_key Datadog APP key. This is a mandatory parameter if the validate parameter is set to true . api_url (optional) The Datagog API URL. validate (optional) Enables validation of the provided API and APP keys during provider initialization. Default is true. When false, api_key and app_key is not checked. Google provider details \u00b6 The Google provider is used to connect to Google Cloud Platform infrastructure products. You can also use Google beta resources in Google environments. Following are the parameters in the google beta provider: Parameters Details Credentials The credentials field is for entering the service account key in JSON format. You can generate a service account key using various methods, for more information, see Google Cloud Platform documentation . Project The project field is your personal project id. The project indicates the default GCP project all of your resources will be created in. Most Terraform resources will have a project field. Region The region is used to choose the default location for regional resources. Regional resources are spread across several zones. Sample { \"credentials\" :{ \"type\" : \"service_account\" , \"project_id\" : \"project-id\" , \"private_key_id\" : \"key-id\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"service-account-email\" , \"client_id\" : \"client-id\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\" }, \"project\" : \"my-project-id\" , \"region\" : \"us-central1\" } Helm provider details \u00b6 Helm2 and Helm3 providers allow you to specify details of the Kubernetes cluster in which you want to deploy Helm charts. The difference between the Helm2 and Helm3 provider types is the Terraform Helm provider version that is supported. Helm2 provider supports version 0.10.4 Helm3 provider supports version 1.2.1 or later Based on your requirements, you can choose the appropriate Helm provider type. The provider details that you have to specify for both provider types is the same. Note: Your administrator might have set the properties for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm2 or Helm3 provider type. In this case, you do not have to add the Helm Repository data source in your environment if your helm chart is available in this repository. Pre-requisites Before configuring a Helm2 or Helm3 provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. (Only for Helm 2 provider) Ensure that Tiller is running in the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Helm2 or Helm3 provider, you must populate the config_path_Content sub-attribute under the kubernetes attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the kubernetes attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 . Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file entered below the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } } ] } Example 3 The following is an example of the provider details section for a Helm2 or Helm3 provider for AWS EKS. { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"aws\" : [ { \"access_key\" : \"\" , \"secret_key\" : \"\" , \"region\" : \"\" } ], \"provider_reference_id\" : \"88\" } Example 4 The following is an example of the provider details section for a Helm2 or Helm3 provider for GKE. { \"google\": [ { \"zone\": \"XXXXX\", \"region\": \"XXXXXX\", \"project\": \"XXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXXXX\" } ] } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter. Kibana provider details \u00b6 The Kibana provider is used to interact with the resources supported by Kibana. Following are the parameters in an Kibana provider: Parameters Details elastic_search_path The path at which elastic search is hosted. kibana_uri The URI at which Kibana is hosted. kibana_version (Optional) The version of Kibana configuration. For the list of supported versions, see Kibana Terraform Provider documentation . In case of empty field the default value is 6.0.0 . kibana_type (Optional) The type of Kibana in the back-end. The default value is KibanaTypeVanilla , which supports the standard open-source kibana distribution. For more information, see Kibana Terraform Provider documentation . To configure logz.io kibana, use KibanaTypeLogzio . kibana_username (Optional) Username for Kibana API authentication. kibana_password (Optional) Password for Kibana API authentication. logzio_client_id (Optional) The client ID used for authentication with logzio. For more information, see Kibana Terraform Provider documentation . logzio_account_id (Optional)The logz.io account id. logzio_mfa_secret (Optional) MFA shared secret, create when signing up user account with MFA. Kubernetes provider details \u00b6 Kubernetes provider allows you to specify details of the Kubernetes cluster in which you want to deploy various resources. Pre-requisites Before configuring a Kubernetes provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Kubernetes provider, you must populate the config_path_Content attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the config_path_Content attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 and Example 5 Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Kubernetes provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"config_path_Content\" : \"\" , \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Kubernetes provider (JSON file entered below the config_path_Content attribute). { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } \"provider_reference_id\" : \"1\" } Example 3 The following is an example of the provider details section for a Kubernetes provider for AWS EKS. { \"config_path_Content\" : \"XXXXXXXXXXXXXXXXXXXXXXXXX\" , \"aws\" : [ { \"access_key\" : \"XXXX\" , \"secret_key\" : \"XXX\" , \"region\" : \"XXXX\" } ] } Example 4 The following is an example of the provider details section for a Kubernetes provider for GKE. { \"google\": [ { \"zone\": \"XXXXXXXX\", \"region\": \"XXXXXXX\", \"project\": \"XXXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXX\" } ] } Example 5 The following is an example of the provider details section for a Kubernetes provider for GKE using a reference provider. { \"google\": [ { \"zone\": \"\", \"region\": \"\", \"project\": \"\", \"service_key_Content\": \"\", \"cluster\": \"\" } ], \"provider_reference_id\": \"1\" } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter. Oracle Cloud Infrastructure provider details \u00b6 The Oracle Cloud Infrastructure is for managing the resources of the Oracle Cloud Infrastructure. Following are the parameters in the Oracle Cloud Infrastructure provider: Parameter Details tenancy_ocid OCID of your tenancy. For more information, see Oracle Cloud Infrastructure documentation . user_ocid OCID of the user calling the API. For more information, see Oracle Cloud Infrastructure documentation . fingerprint Fingerprint for the key pair being used. For more information, see Oracle Cloud Infrastructure documentation . private_key The path (including filename) of the private key stored on your computer, required if private_key is not defined. For information on how to create and configure keys, see Oracle Cloud Infrastructure documentation . region An Oracle Cloud Infrastructure region. For more information, see Oracle Cloud Infrastructure documentation . Provider Details JSON Example { \"tenancy_ocid\": \"XXXXXXXXXXX\", \"user_ocid\": \"XXXXXXXXXX\", \"fingerprint\": \"XXXXXXXXX\", \"private_key\": \"XXXXXXXXX\", \"region\": \"XXXXXXXXXXX\" } Share a provider \u00b6 Deploy Accelerator enables you to share your provider with other users. The ability to share providers is useful when multiple users in a group need to use the same provider to deploy environments. These users can view, edit, share, or delete providers that are shared with them based on the permissions assigned to their group. On the Home page, click the icon in the top-right corner. Click Providers . On the Providers List page, click the icon for the provider that you want to share with other users. You can identify the providers that are shared with you based on the color of the provider name. You can share a shared provider only if your group has been assigned the Share permission for that provider. In the Share Provider : ProviderName window, click the icon for the Group with which you want to share the provider. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group. Select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the provider, select the Warning check box and click SUBMIT . If you no longer want to share the provider, click CANCEL . Configuring connections \u00b6 Amazon Elastic Compute Cloud (EC2) uses public-key cryptography to encrypt and decrypt login information. When Deploy Accelerator launches an instance as part of deploying an environment, it specifies the name of a key pair (set of public and private keys) that must be used to connect to the instance. Each connection that you create contains the private key that can be used to connect to one or more instances in your environments. While creating an environment, you must specify a connection that is applied to all instances by default. You can also share your own connections with one or more groups. Note: If you do not want to use the default connection for all instances in an environment, you can create multiple connections. You can also choose to dynamically generate a key pair and associate it with one or more instances while the environment is being deployed. For an example of the step-by-step procedure to dynamically generate and associate a key pair with an instance, see Configure the dynamic generation of a key pair . Create a connection \u00b6 Create the required key pair by using either Amazon EC2 or a third-party tool. If you use a third-party tool, you must also import the public keys to Amazon EC2. For more information, see the Amazon Elastic Compute Cloud User Guide for Linux Instances or the Amazon Elastic Compute Cloud User Guide for Windows Instances . Important: To create a key pair by using Amazon EC2, you must have your own AWS cloud account. On the Home page, click the More options icon ( ) in the top-right corner and then click Connections . On the Add/Edit Connection page, click NEW . Enter the connection name and connection type. If you have selected the WinRM connection type, perform the following actions: In SSH/WinRM User , enter administrator . In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. (Optional) To set up a secure connection, select Https and enter a CA certificate in Cacert Note: If you want Deploy Accelerator to skip validation of the CA certificate, you can select Insecure . (Optional) To use NTLM authentication for remote connection, select NTLM . Important: When you select a WinRM connection for an instance, you must also specify a PowerShell script in the user_data attribute of that EC2 Instance resource. The user and password that you specify in the PowerShell script and the selected WinRM connection must match. Otherwise, you cannot deploy packages on that instance. If you have selected the SSH connection type, enter the user and perform one of the following actions: In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. In SSH Key , enter the SSH key instead of a password. (Optional) To specify details of a Bastion Host in the connection, select Bastion Connection , enter the following details, and then click SET PARAMETER . User Password (enter either the password or the key) Host (IP address of the Bastion host, which must be available in the network) Port (By default, the port number is selected based on the connection type.) Key You can use a Bastion connection based on how your network is configured. If the connection that you are creating is for instances in a private network and a Bastion Host is configured to connect to those instances, you must also specify details for connecting to the Bastion host. Note: The Bastion Connection link is enabled only for the SSH connection type. Click SAVE . A new connection appears under the Connection List section. (Optional) Share the connection with one or more groups. Share a connection \u00b6 Deploy Accelerator enables you to share your connection with other users. The ability to share connections is useful when multiple users in a group need to use the same connection for the instances that they are deploying. These users can view, edit, or delete connections that are shared with them based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top-right corner. Click Connections . On the Connection List page, click the Share icon ( ) for the connection that you want to share with other users. Note: You can identify the connections that are shared with you based on the color of the connection name. In the Share Connection: ConnectionName window, click the icon for the Group with which you want to share the connection. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the connection, select the Warning check box and click SUBMIT . If you no longer want to share the connection, click CANCEL . Creating environments \u00b6 The process to create an environment includes multiple steps, as shown in the following image. Create an environment Add resources Add packages Configure the environment To create a layered environment , you must perform these steps for each environment that is a part of the layered environment. For more information, see Creating layered environments . Before you begin \u00b6 Before you create an environment, ensure that you have performed the following actions: Configured the appropriate cloud account (for example, AWS account) in which you want to deploy an environment. For more information, see Configure providers . Created the connections that Deploy Accelerator can use to connect to the instances in your deployed environment. For more information, see Configure connections . Create an environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter a unique name and description for the environment that you want to create. Enter a version for the environment. By default, 01.00.00 is specified as the version for a new environment. If required, you can specify a different version. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Environment Package Type list, select the package type that you want to use for this environment. Under the Packages tab, the Private Packages list is populated depending on the Environment Package Type selected for creating the environment. Note: Deploy Accelerator supports the Chef Solo, Chef Server, Ansible Solo, and Puppet package types. However, the options that are shown in the Environment Package Type list are based on the configuration by your administrator . From the Connection list, select the default connection for all instances in this environment. You can identify shared connections in the list based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the appropriate account for accessing the cloud provider that you want to use to deploy the environment. You can identify shared providers in the list based on the color of the provider name. (Optional) From the AWS Region list, select the appropriate AWS region where you want to deploy the environment. Note: If you select an AWS Region while creating an environment, any predefined region in the Provider is overwritten with the new AWS Region. (Only if Chef Server is selected as the environment package type) To associate a Chef Server with the environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share the environment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the same groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections . Add resources to the environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment Resources are an important element of environments in Deploy Accelerator. They represent entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. You can also add Terraform data sources, Terraform modules , and Terraform Random Provider resources in your environment. You can add resources and data sources to the environment by using either the resource attributes form or the HCL editor . Notes: After you start a deployment of the environment , you can view various attributes of a resource for that deployment, such as its public IP address, on the Resource Details tab in the right panel. You can also add resources to an environment version after it has been deployed. These new resources get created when you start a new deployment or redeploy an existing deployment. However, no changes can be made to a released environment version. If you rearrange a resource on the canvas, you must save the environment to retain the changed position of that resource. Add resources and data sources to the environment \u00b6 In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name and click CREATE . Click the resource or data source that you have created. In the right panel that opens, on the Resource tab, enter the required details. Consider the following points while configuring the resource attributes. Goal Action Configure the connection and key pair that Deploy Accelerator must use to connect to a non-Windows instance. In the Connection attribute, select the appropriate SSH connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the key_name attribute, specify the corresponding key pair that Deploy Accelerator must associate with the instance when it is launched. Based on your requirements, you can also specify multiple key pairs. Note: To use a key pair that is dynamically generated while the environment is being deployed, you can select Use Custom Connection . The procedure to dynamically generate and associate a key pair with an instance includes multiple steps. For more information, see Configure the dynamic generation of a key pair . Configure the connection and user data that Deploy Accelerator must use to connect to a Windows instance In the Connection attribute, select the appropriate WinRM connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the user_data attribute, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. (See sample PowerShell script ). Important: You must ensure that the security group associated with the instance allows inbound traffic (ingress) from the port that you specify in the PowerShell script. Create multiple resources based on the same configuration In the Count attribute, specify the number of identical resources that you want to create while deploying the environment. Create a dependency on another resource in this environment In the Depends On attribute, specify the name of the resource on which you want to create a dependency. Use a variable that is defined in an Input Variable resource in this environment In any attribute, use the appropriate interpolation syntax based on the type of input variable that you are referring. For more information, see Formats for using input variables in resources . Use a local value that is defined in a locals resource in this environment. In any attribute, use the following interpolation syntax to refer to a local variable that is defined in the locals resource. ${ localResourceName . variableName } A local value can be a simple constant or an expression that can be defined once and used multiple times in different resources within the same environment. For more information, see the Terraform documentation . Reference attributes of another resource in the same environment For information about the supported attributes, select the resource whose attributes you want to reference and then click the resource type link at the top of the Resource tab in the right panel. Use the following interpolation syntax: ${ ResourceName . Attribute } Reference attributes of a data source in the same environment Use the following interpolation syntax: ${ DataSourceName . Attribute } Set array for attributes of the Text type While entering array values in attributes, specify each value on a separate line. The following image shows an example of array values for the vpc_security_groups_ids attribute of an AWS Instance : Apply Flatten flag for string array in JSON, or JSON array of the resources. To apply a Flatten flag on string arrays or nested string arrays in a JSON, or JSON array, select the Flatten toggle switch. The Flatten function is for replacing a nested string array list with a single flattened array. The array with the Flatten flag, returns a flattened sequence while migrating or importing the blueprint. Example -- JSON and JSON array -- String array Add a timeout for creating, reading, updating, or deleting resources. Use the Timeouts attribute for adding a timeout for creating, reading, updating, or deleting resources. You can define timeout for the following actions: - Create : To add a timeout while creating resources. - Update : To add a timeout while updating resources. - Read : To add a timeout while reading resources. - Delete : To add a timeout while deleting resources. The timeout is added in the following JSON format: To check the actions supported by a resource, click the Timeouts attribute in resource attributes form to open its relevant Hashicorp resource documentation. Sample PowerShell script for a Windows instance While creating a Windows instance, you must select a WinRM connection. In the user_data attribute of the instance, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. The following is an example of the PowerShell script that can be specified in the user_data attribute. < powershell > if ( [Environment] :: OSVersion . Version -ge ( new-object 'Version' 6 , 1 )) { New-NetFirewallRule -DisplayName \"Allow WinRM\" -Direction Inbound -Action Allow -Protocol TCP -EdgeTraversalPolicy Allow -LocalPort 5985 } else { netsh advfirewall firewall add rule name = \"Allow WinRM\" dir = in protocol = TCP localport = 5985 action = allow remoteip = any localip = any profile = any } winrm set winrm / config / service / auth '@{Basic=\"true\"}' winrm set winrm / config / service '@{AllowUnencrypted=\"true\"}' $admin = [adsi] ( \"WinNT://./administrator, user\" ) $admin . psbase . invoke ( \"SetPassword\" , \"password@123\" ) </ powershell > To add other resources or data sources, repeat steps 2 to 5. (Optional) Copy an existing resource from either the same environment or a different environment. (Optional) To remove an extra resource or data source from the canvas, click the x icon on that resource. (Optional) To view the links that are created when a resource uses the Depends On attribute or the interpolation syntax to reference another resource, click the Links icon ( ). Click the Save icon ( ). (Optional) To switch from resource attributes form to HCL code, select Switch to Hashicorp Configuration Language . The switch confirmation dialog box appears. Switching from attributes form to HCL code will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the HCL code, see Add resources and data sources using Hashicorp Configuration Language . Add resources and data sources using Hashicorp Configuration Language \u00b6 In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name, turn on the Switch to Hashicorp Configuration Language toggle switch, and click CREATE . Click the resource or data source that you have created. In the right panel that opens, you will see an HCL Editor on the Resource tab. In the HCL Editor, insert the HCL code for your resource. Click the Save icon ( ). ( Optional ) To switch the resource definition from HCL code to the default resource attributes form, turn off the Switch to Hashicorp Configuration Language toggle. The switch confirmation dialog box appears. Switching from HCL code to attributes form will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the default resource attributes form, see Add resources and data sources to the environment . Add Terraform modules to the environment \u00b6 Deploy Accelerator allows you to use an existing Terraform module in the environment. Terraform defines a module as a container for multiple resources that are used together . For more information about creating modules, see the Terraform documentation . To use an existing Terraform module, you can add the Terraform Module resource to the environment and specify the source from where the module can be downloaded. To add a Terraform module to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the Terraform Module resource to the canvas. In the Resource Name window, enter a unique name and click CREATE . Click the Terraform Module resource that you have created. In the right panel that opens, on the Resource tab, enter the following details: source: Enter the source from where to download the Terraform Module. Deploy Accelerator supports the following sources for Terraform Modules. Source type Supported format Terraform public registry <NAMESPACE>/<NAME>/<PROVIDER><br> Example: terraform-aws-modules/vpc/aws GitHub repository (public) github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION><br> Example: github.com/terraform-aws-modules/terraform-aws-vpc.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. GitHub repository (private) git::https://<USERNAME>:<PASSWORD>@github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION> Example: git::https://sampleuser:samplepassword@github.com/sample-demo/terraform-module.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. Artifactory ( configured in Deploy Accelerator ) artifactory::<OBJECT-RELATIVE-PATH> Example: artifactory::local-demo-sample/module/aws_vpc.zip Note: The Artifactory URL that is configured in the dnow.properties file is prefixed to the path that you specify in the source attribute. input: Input parameters for the Terraform Module. You can use the interpolation syntax to reference the output of other resources in the same environment or a parent environment as input to the Terraform module. tags: Tags that you want to assign to all resources in the Terraform Module. However, ensure that the Terraform Module supports tags. version: Version of the Terraform Module to download. Use this attribute when you specify a Terraform public registry in the source attribute. Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module from the Terraform public registry. Click the Save icon ( ). Note: You can use the interpolation syntax to reference the output of this Terraform module in other resources within the same environment or in child environments. Add Terraform Random Provider resources to the environment \u00b6 Deploy Accelerator allows you to use Terraform Random Provider resources in the environment. These resources are always available on the Resources tab irrespective of the provider type that is selected for the environment. Terraform defines the Random Provider as a logical provider that allows the use of randomness within Terraform configurations...it provides resources that generate random values during their creation and then hold those values steady until the inputs are changed. For more information about the Random Provider, see the Terraform documentation . To add a Random Provider resource to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the appropriate Random Provider resource to the canvas. A few examples of the Random Provider resources are Random Id , Random Password , and Random String . In the Resource Name window, enter a unique name and click CREATE . Click the Random resource that you have created. In the right panel that opens, on the Resource tab, enter the required details. For more information about the resource attributes, click the resource type link at the top of the Resource tab, as shown in the following image. Click the Save icon ( ). Add packages to the resources \u00b6 Create an environment > Add resources > Add packages > Configure the environment Packages in Deploy Accelerator are entities that use supported infrastructure automation tools to configure compute resources in an environment. Chef Solo and Chef Server are configured by default in Deploy Accelerator. However, your administrator might have also configured the use of Ansible and Puppet packages. Each package references a Chef cookbook, Ansible playbook, or Puppet manifest file or module. You can add user-defined packages to install your own custom applications on compute resources. Important: Deploy Accelerator cannot deploy packages on resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a new deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. To add packages to the resources, perform the following actions: In the new environment that you have created, from the left panel, click the Packages tab, and drag the appropriate package to a compute resource (for example, an EC2 Instance resource for AWS) on the canvas. The Packages tab displays packages of only the environment package type (Chef Solo, Chef Server, Ansible Solo or Puppet) that you have selected. If you have selected Chef Server as the environment package type, the Packages tab also displays roles from the selected Chef Server. You can add both Chef Server packages and roles to a compute resource. The following utility packages are always shown irrespective of the environment package type that is selected. Utility package Details file Uploads a file on the resource to which it is added. execute-script Runs a script on the resource to which it is added. You can specify whether the script must be run after the resource is created or before it is destroyed. local-exec Runs a command on the instance on which Deploy Accelerator is deployed. You can specify whether the command must be run after the environment is deployed or before it is destroyed. The local-exec package can be added to any resource in the environment. chef_configuration Restricts Deploy Accelerator from installing the ChefDK client on instances before deploying packages. You must set the skip_install attribute of this package to true . Note: This package is available only in environments for which Chef Server is selected as the environment package type. Click the package or role. In the right panel, on the PACKAGES tab, select the package or role that you have added to the resource. (Optional) From the Package Version list, select another version of the package or role. By default, the latest released version of the package or role is selected. Based on your requirements, you can select another version of the package or role. Ideally, you should add an unreleased version of a package or role to an environment only if you want to test that version. (Chef Solo and Chef Server packages only) From the Recipes list, select one or more recipes that you want to run on the resource. All recipes that the selected cookbook (or package) contains appear in the list. You must select the recipes in the order in which they must be run on the resource. If you do not select any recipe, Deploy Accelerator runs the default recipe that is defined in the cookbook. Enter other details about the package or role. Note: The details that you must specify for each package are different. For example, in case of the execute-script package, you can specify whether the script must be run after the resource is created or before it is destroyed by selecting the appropriate option from the when list. (Optional) To drag additional packages or roles to the resource, repeat steps 1 to 6. If you add multiple packages and roles to a resource, the packages and roles are arranged from top to bottom by default. However, you can drag and rearrange the packages and roles in the order in which you want to run them on the resource. Note: The utility packages are always run on the resource before the other selected packages and roles. However, if you add multiple utility packages to a resource, you must arrange these packages in the order in which you want to run them on the resource. To add packages or roles to other resources, repeat steps 3 to 6. (Optional) To remove a package or role from a resource, click the x icon on that package or role. Click the Save icon ( ). Configure the environment \u00b6 Create an environment > Add resources > Add packages > Configure the environment Based on your requirements, you can configure additional settings for the environment. This step is optional in the end-to-end process of creating environments. In the new environment that you have created, click the Configure icon ( ). Based on your requirements, perform the following actions in the Configuration window: On the Environment tab, edit the following details for the selected environment version: Edit the description of the environment. Edit the connection, provider, or AWS Region for the environment. For information about configuring connections and providers, see Configure connection and Configure provider . To assign a custom tag to all resources in the environment, select that custom tag from the Custom Tag list. To view the key-pair values in the selected custom tag, move your cursor on the icon. For information about creating or editing a custom tag, see <a href=\"\" ui-sref=\"rean-platform-docs.accelerator({viewAccelerator: 'rean-deploy', viewPage: 'deploy-and-manage-environments', viewSection: 'tags'})\" style=\"text-decoration:none\">Configuring custom tags</a>. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that is selected for the environment. On the Deployment tab, configure the following details that are applied by default to all deployments of this environment version: Under Deploy , specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. By default, Deploy Accelerator does not destroy any deployment. On the Notification tab, specify the email notification details that are applied to all deployments of this environment version: Email: By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts or destroys a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. Deploy Template: Deploy Accelerator uses the default template for sending an email when a deployment is started. However, you can specify your own custom template. Destroy Template: Deploy Accelerator uses the default template for sending an email when a deployment is destroyed. However, you can specify your own custom template. Deployment Initiation: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users as soon as they start new deployments or destroy existing deployments of this environment. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Deployment Complete: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users after new deployments of this environment either succeed or fail, and existing deployments are successfully destroyed. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Note: Contact your system administrator for a list of custom email templates that might be available. Click SAVE . Where to go next \u00b6 When you are ready to create the infrastructure that you have designed in your environment, you can start a new deployment . Creating a layered environment \u00b6 Layered environments in Deploy Accelerator are a collection of environments in which some environments (child) are dependent on other environments (parent). You can create layered environments for complex IT infrastructures. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. To create layered environments, you require the following two resources: Output: You must add this resource in the parent environment. This resource enables you to expose some resource attribute values from the parent environment in the form of variables. These variables can then be referenced in other dependent (child) environments. Depends On: You must add this resource in the child environment. This resource enables you to create a dependency on another environment. Based on your requirements, you can add multiple Depends On resources in a child environment. To create a layered environment, perform the following actions: To create the parent environment, perform the following actions: Create an environment . Add the required resources in the environment . To reference attributes of the parent environment in one or more child environments, add the Output resource. In the Output resource, create a JSON file and specify variables that can be used in child environments. The value of these output variables can use the interpolation syntax to reference attributes of other resources in the parent environment. If you have added the Input Variable resource, the output variables can also reference the input variables. Save the environment. To create the child environment, perform the following actions: Create an environment . Add the Depends On resource in the environment. The name of the Depends On resource should ideally indicate the environment on which there is a dependency (for example: dependson_network ). In the reference_type attribute of the Depends On resource, select Environment Name . In the Depends_On attribute, select the appropriate parent environment version. Note: You can also select S3 in the reference_type attribute of the Depends On resource. For more information, see Deploy Accelerator resources . Add other required resources in the child environment . To reference attribute values of resources in the parent environment, use the following interpolation syntax: Syntax: ${ DependsOnResourceName.OutputVariableName } Example: ${ dependson_network.subnet } Save the environment. To confirm that the dependency on the parent environment, click the Environment Dependency View icon ( ). Creating new environments from blueprints \u00b6 Blueprints are templates of commonly used IT infrastructures. Deploy Accelerator users can export their environments as blueprints and add them in the Blueprint Gallery . You can leverage blueprints to quickly create infrastructure for solutions. Deploy Accelerator allows you to create new environments from blueprints by using any of the following methods: Import blueprints from the Blueprint Gallery Import blueprints from the Artifactory by using an API Import blueprints from your computer Important: If you create a layered environment from a blueprint, you must deploy the environments in a particular order. You must first deploy the environment in the top layer, then the environment in the next layer, and so on until the environment in the lowest layer. You must follow this order because child environments can be deployed only if their parent environments are already deployed. Before you begin \u00b6 Before you create an environment from a blueprint, ensure that you have configured the appropriate providers and connections that can be used for each environment in the blueprint. Import blueprints from the Blueprint Gallery \u00b6 The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable and configure the Blueprint Gallery . Also, blueprints are not available out of the box and have to be added in the Blueprint Gallery . The following image shows an example of the Blueprint Gallery. To import blueprints from the Blueprint Gallery, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . (Optional) To manually refresh the Blueprint Gallery to display all latest updates, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint data from the Artifactory. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. On the Blueprint Gallery page, browse or search for a blueprint that meets your requirements. The search results include blueprints that contain the search keyword in the blueprint name, description, owner name, or owner email. Searches are not case-sensitive and use partial keyword matching. For example, if you specify layer as the search keyword, the search results include blueprints with the name NetworkLayer and MultilayerEnvironment . Note: You can also sort the blueprints based on the blueprint name or owner name. By default, blueprints are sorted based on the blueprint names in the descending order. From the version list of the blueprint that you want to import, select the appropriate version. For details about the selected blueprint version, hover over Description . To access the ReadMe file of the selected blueprint version, click README . To view the email address of the blueprint owner, hover over the owner name. However, the email address is not shown if the blueprint owner is admin . Also, the owner name and email address is shown only if these values are available in the blueprint metadata. To import the selected blueprint version, click IMPORT . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as a part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. (Optional) Update the description for the environment. By default, the description from the blueprint is shown. From the Connection list, select the appropriate connection. You can identify shared connections based on the color of the connection name. From the Provider list, select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. When you import a blueprint from the Blueprint Gallery, all environments in that blueprint are in the Released state. If input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments . Import blueprints from the Artifactory by using an API \u00b6 The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable the Blueprint Gallery and specify a repository in the Artifactory for storing the blueprints. Based on your requirements, you can either import blueprints from the Blueprint Gallery or import blueprints from the Artifactory by using an API. The importBlueprintFromArtifactory API allows you to import blueprints from the specified repository in the Artifactory. You can specify the following parameters for importing a blueprint: blueprintName blueprintVersion environmentNamePrefix This parameter is optional. The value that you specify for this parameter is added as a prefix to the names of all environments that are imported as a part of the blueprint. groupName This parameter is optional. The group that you specify for this parameter is assigned the View and Deploy share permission for all the imported environments. For more information about the API, see the Deploy Accelerator API documentation: https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html#/Environment/importBlueprintFromArtifactory Import blueprints from your computer \u00b6 On the canvas, click the More icon ( ) and then click Import . In the Import Environment/Blueprint window, perform the following actions: Click Choose File and then select the blueprint (JSON file) that you want to import. Click Upload . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Enter the description for the environment that you are importing. From Connection , select the appropriate connection. You can identify shared connections based on the color of the connection name. From Provider , select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From Chef Server , select the Chef Server from which you want to display packages. From Chef Environment , select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. When you import a blueprint from your computer, the state of environments in that blueprint are dependent on their state when they were exported. For example, if parent and child environments in the Released state were exported as a blueprint, they are in the Released State when the blueprint is imported. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. If the imported environments are in a Released state and if input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments . Copying environments \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment that you want to copy. On the canvas, click the More icon ( ) and then click Copy . The environment version that is currently selected is copied. In the Create Environment window, enter a unique name, description, and version for the environment that you are creating. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Connection list, select the default connection for all instances in the environment. You can identify shared connections in the drop-down based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the cloud provider that you want to use to deploy the environment. You can identify shared providers in the drop-down based on the color of the provider name. (Optional) After selecting the provider, select the appropriate AWS region where you want to deploy an environment. Note: If you select an AWS region while creating an environment, any predefined region in the provider is overwritten with the new AWS region. If Chef Server is selected as the environment package type in the environment that you are copying, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, the share permissions of the environment that you have copied are automatically applied to this environment. (Optional) Modify the share permissions for this environment by selecting new or clearing existing permission check boxes for the appropriate groups. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the selected groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections . Starting new deployments \u00b6 Each environment can have multiple versions. When you are ready to create the infrastructure that you have designed in an environment version, you can start a new deployment. If required, each environment version can also have multiple deployments, such as Staging and Production. Important: You can deploy child environments only if their parent environments are already deployed. Therefore, in the case of a layered environment , you must deploy the environments it contains in a particular order. You must first deploy the parent environment in the top layer, then the child environment in the next layer, and so on until the child environment in the lowest layer. To start a new deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to deploy. To start a new deployment, click the Deploy icon ( ). The Review and Deploy: EnvironmentName window displays the default values that are configured for the environment. For more information, see Configure the environment . (Optional) To start a quick deployment with default values, click QUICK DEPLOY . (Optional) On the Environment tab, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for this deployment. Note: If you select an AWS region while starting a deployment, any predefined region in the provider is overwritten with the new AWS region. From the Custom Tag list, select the custom tag that you want to assign to all resources that are deployed. To view the key-pair values in the selected custom tag, hover over the icon. Note: If you have also specified key-pair values in the tags attribute of a resource in the environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for this deployment. Following additional information is displayed: Provider version: The version of the provider used for deploying the environment. Package Type: The package types used for deploying the environment. Created By: The user who created the environment. On the Deployment tab, perform the following actions based on your requirements: Under Deploy , enter the following details: Enter the deployment name and description. Note: The deployment name must be unique across all versions of the selected environment. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while deploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Parent deployments of the same name as the child deployment name are selected by default. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. (Optional) On the Notification tab, specify the email and template details for this deployment. By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. If required, you can specify custom templates for sending an email when a deployment is started or destroyed. Contact your system administrator for a list of custom email templates that might be available. Note: You cannot customize the Deployment Initiation and Deployment Complete settings for a deployment. These settings are configured at the environment level . To start a new deployment, click START NEW DEPLOYMENT . The icon appears on resources that are successfully deployed. The icon appears on resources that could not be successfully deployed. (Optional) To view the resource logs, click the Logs icon ( ). (Optional) To stop a deployment that is in progress, perform the following actions: Click the Stop icon next to the Deploying status. In the confirmation box, click YES STOP IT . The status of the deployment changes to Stopping . Deploy Accelerator deploys all resources in progress and then stops the deployment. The status of the deployment changes to Stopped . Note: If Deploy Accelerator is unable to successfully deploy any of the resources in progress, the status changes to Failed instead of Stopped . Viewing deployments \u00b6 Each environment version can have multiple deployments, such as Staging and Production. You can view the list of your deployments for an environment across all versions. You can also view the deployments that other users have shared with you. The deployment list also displays the status of each deployment -- deployed ( ) or failed ( ). On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of your own and shared deployments across all versions of the environment, click the deployment list on the canvas. Note: You can identify the deployments that are shared with you based on the color of the deployment name. In the list of deployments, click the deployment name that you want to open on the canvas. To view additional information related to the deployment after it is complete, select Deployment Details on the Canvas. The Deployment Details displays information about the following parameters: Deployment ID: The ID for deploying the environment. Deployment run ID: The run ID for querying the database for troubleshooting. Deployment Owner: User who had run the environment deployment. Deployment Input Variables: Additional deployment variables. Viewing the plan for deployments \u00b6 Before you start a new deployment or redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list, select the appropriate environment version. To view the plan for a deployment, perform one of the following actions: Goal Action To view the plan for a new deployment of the environment. Click the Plan icon ( ) and select Plan Environment . To view the plan for an existing deployment of the environment. Click the Plan icon ( ) and select Plan Selected Deployment . Note: You can view the plan for an existing deployment that is shared with you only if your group has been given the Redeploy permission. (Optional) To view the plan with default values, click QUICK PLAN . (Only to view the plan for a new deployment) In the Review and Plan: EnvironmentName window, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for the plan. The connection, provider, and AWS region that is configured for the environment version is shown by default. Note: If you select an AWS region for the plan, any predefined region in the provider is overwritten with the new AWS region. To plan the deployment with a different environment version, from the Environment Version list, select that version. In Input Variables , add input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while planning is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for the plan. (Only to view the plan for an existing deployment) In the Review and Plan: EnvironmentName window, update the environment version, input variables, and parent deployments mapping based on your requirements. Click PLAN . Note: In the case of a layered environment , the plan for child deployments is available only if the parent deployments are available. Redeploying existing deployments \u00b6 You can redeploy an existing deployment with the same environment version. If required, you can also redeploy an existing deployment with a different environment version. In both cases, only the changes made in the selected environment version are implemented. Deployed resources that have not been updated in the selected environment version are not impacted. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployments list on the canvas. From the deployments list, click the deployment name that you want to redeploy. Click the Re-Deploy icon ( ) to open the Review and Deploy window. Note: The Re-Deploy icon is available only if the environment version has been previously deployed. (Optional) On the Environment tab, perform the following actions based on your requirements: From the Connection list, select the connection that you want to use to connect to all resources in the environment. From the Custom Tag list, select the custom tag that you want to assign to all resources in the environment. To view the key-pair values in the selected custom tag, hover over the icon. (Optional) On the Deployment tab, perform the following actions based on your requirements: To redeploy with a different environment version, from the Environment Version list, select that version. By default, the environment version that was originally deployed is selected. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while redeploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Select the Confirmation check box. Click Upgrade . If you have redeployed with a different environment version, the deployment is now associated with that version. Sharing deployments \u00b6 Deploy Accelerator enables you to collaborate with other users on your deployments of an environment. The ability to share deployments is useful when multiple users need to view or manage the same deployment. All these users can view, redeploy, destroy, or stop the deployment based on the permissions assigned to their group. Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for the deployments. To share a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment whose deployment you want to share. Important: Before you share deployments of an environment version with selected groups, ensure that the environment version is also shared with those groups. Otherwise, users cannot view the shared deployments. From the deployments list, click the deployment name that you want to share. Click the Share icon ( ) and select Share Selected Deployment . You can share a deployment when it is in the Deploying , Deployed , Stopping , or Failed state. In the Share Deployment window, perform the following actions: Click the Share icon ( ) for the Group with which you want to share the deployment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the deployment, select the Warning check box and click SUBMIT . The deployment is shared with all users who are members of the selected groups If you no longer want to share the deployment, click CANCEL . (Optional) To enable users to redeploy the shared deployment by using the provider that is configured in that deployment, share the provider with the same groups. (Optional) To enable users to redeploy the shared deployment by using the connections that are configured in that deployment, share the connections with the same groups. Note: In case of layered environments, ensure that you share both parent and child deployments with the same groups. You must assign at least the View permission for the parent deployments. Otherwise, users cannot redeploy the child deployment. Destroying deployments \u00b6 Each environment version can have multiple associated deployments, such as Staging and Production. You can destroy a deployment that is no longer required. This action ensures that you are not paying for resources that are not being used. Destroying a deployment deletes all the deployed resources that it contains. However, the environment version and its other deployments continue to be available. Typically, you destroy test, development, or other non-production deployments. Note: You can destroy parent deployments only if their child deployments are already destroyed. Therefore, in the case of a layered environment , you must destroy the deployments in a particular order. You must first destroy the child deployment in the lowest layer, then the deployment in the next layer, and so on until the parent deployment in the top layer. To destroy a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployment list on the canvas. From the deployments list, click the deployment name that you want to destroy. Click the Destroy icon ( ). Note: The Destroy icon is available only if the environment version has been previously deployed. In the confirmation message box, type Yes and then click SUBMIT . After the deployment is destroyed, an email is sent to your registered email address and to the additional email address that might be configured for the deployment. (Optional) To view the resource logs, click the Logs icon ( ). The Logs icon continues to be available until the deployment is successfully destroyed. Releasing environment versions \u00b6 After an environment version is finalized for production, you can choose to release that version. No changes can be made to a released version. On the Home page, click the More options icon ( ) in the top-right corner, and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to release. From the version list, select Release Version . In the confirmation message box, enter Yes and then click SUBMIT . You can no longer make any changes to the released version of an environment. However, you can start new deployments or redeploy existing deployments with this released version. Creating new environment versions \u00b6 Each environment can have multiple versions. After an environment version is released, no changes can be made to that version. To add or update resources to the environment, you must create a new environment version. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the version on which you want to base the new environment version. From the version list, select Create New Version . In the Create New Version From base version window, enter a unique version up to a maximum of 40 characters. The new version must be greater than the base version that you have selected. Alternatively, you can retain the base version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). (Optional) To not apply share permissions from the base environment to the new environment version, clear the Use share permission from base environment check box. By default, share permissions configured for the base environment are applied to the new environment version. Click CREATE . A new version of the environment appears on the canvas. This version also appears on the Environment List page and in the Search Environments box. (Optional) In the new environment version, add or modify resources and packages based on your requirements. (Optional) Update the description and configure additional settings for the environment version based on your requirements. When you are ready to create the infrastructure that you have designed in the new environment version, you can start a new deployment or redeploy an existing deployment with this version. Upgrading the provider version of environments \u00b6 Deploy Accelerator uses Terraform providers to create, manage, and update resources in AWS, Azure, Google Cloud Platform, and other supported providers. When you create a new environment or a new version of an existing environment, resources are created based on the most recent provider version that is supported in Deploy Accelerator. When you open an environment whose provider version has been automatically upgraded, the Provider Version Upgraded Changes window appears. For each resource in the environment, it lists the attributes that are deprecated or cannot be upgraded. The resources that require user action are marked in red. The resources for which Deploy Accelerator has automatically performed the changes are marked in green. To acknowledge the upgrade changes, click the Confirmation checkbox, and click CONFIRM. However, the list of provider upgrade changes will not be shown after you submit your acknowledgement. To retain the list of provider upgrade changes for some time, you can click CLOSE. To reopen the window, click Provider upgrade changes on the canvas. Important : If the upgrade changes are not acknowledged, the Provider Version Upgraded Changes window appears while deploying the environment. If the upgrade changes are acknowledged but the manual changes are not made to the resources, you will see an error in the deployment log file. Viewing dependency between environments \u00b6 The Environment Dependency View enables you to view the dependency of an environment on other environments (parent environments). You can also view other environments (child environments) that are dependent on an environment. If the environment has deployments, this view displays the parent or child deployments of the selected deployment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. If the selected environment has deployments, the deployment list on the canvas displays the latest deployment of the selected environment version. Click the Environment Dependency View icon ( ). The Environment Dependency View window appears, as shown in the following image: If the environment has a deployment, the Environment Dependency window displays the connection between the selected deployment and its parent or child deployments. The color of the deployment represents its current state. For example, black indicates Not Started, orange indicates Running, green indicates Deployed, and red indicates Failed. Click the toggle switch to see the dependent parent and child environment(s). If the toggle switch is towards Child, the Environment Dependency window displays the connection between the current environment and its child environment(s). If the toggle switch is towards Parent, the Environment Dependency window displays the connection between the current environment and its parent environment(s). (Optional) To switch to another environment or deployment shown in the Environment Dependency View window, select that environment or deployment name. Configuring custom tags \u00b6 You can define tags for individual resources in an environment but this process is repetitive and time consuming. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can create a custom tag, which contains a set of AWS key-pair tags, and attach the custom tag to an environment. Deploy Accelerator attaches this custom tag to all resources in the environment. For example, to control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. To configure a custom tag, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Custom Tags . Under Custom Tag , click NEW . Enter the custom tag name and description. Under Tags , define the tags that you want to assign to all resources in the environment. The following image shows an example of how you must specify tags for your resources. Note: Tag values for all the resources that have tag support in your environment are set to the default values from the tags defined in the selected custom tag. Click SAVE . A new custom tag appears under the Custom Tag List section. Note: You can also add tags to a resource by setting the tags attribute in the RESOURCE panel. If users select a custom tag for an environment and specify values in the tags attribute of a resource in that environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. Comparing differences between environments \u00b6 Deploy Accelerator enables you to compare differences between two environments or two versions of the same environment. Comparing two versions of the same environment helps you to track changes to an environment over time. Comparing environments is also useful when you have two similar environments and you want to retain only one of them. You can compare the differences between these environments, ensure that one of the environments has all the required resources and packages, and then delete the other environment. The comparison detects the following differences between the base and target environments. Comparison Details Environment configuration-level comparison This comparison displays a list of configuration details that have changed in the target environment version: -- Connection, provider, region, and environment package type -- Chef server and Chef environment -- Environment owner -- Custom tag -- Released status -- Notification email address and email templates -- Deploy and Destroy pre-scripts, post-scripts, and destroy after interval Resource-level comparison This comparison displays a list of resources that are added, edited, or removed in the target environment version. For each edited resource, you can see a list of attributes and packages that are added, edited, or removed. If a package has been edited, you can also see the differences in the package attributes. To compare differences between environments, perform the following actions: On the canvas, click the More icon ( ) and then click Compare . The environment version that is currently selected is considered as the Base Environment for comparison. (Optional) In the Compare Resources and Packages window, from the Base Environment and Version lists, select a different environment and environment version. You can identify environment versions that are shared with you based on the color of the environment version. From the Target Environment and Version lists, select the environment and environment version with which you want compare the base environment. The RESOURCES tab is selected by default. In the Resources panel, you can see a list of resources that are added, edited, or removed in the target environment. To help you identify the changes, the resources are highlighted in different colors, as shown in the following table. Highlighted color Meaning Green Added resource Red Deleted resource Blue Edited resource To see the resource-level comparison, click an edited resource in the Resources panel. On the RESOURCE ATTRIBUTES and PACKAGES tabs, you can see the differences in attribute values and packages. Green background color indicates added content while red background color indicates deleted content. To see the environment configuration-level comparison, click the CONFIGURATION tab. On the ATTRIBUTES tab, you can see a list of configuration details that have changed in the target environment. Green background color indicates added content while red background color indicates deleted content. Click CLOSE . Searching environments \u00b6 On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, in the Search Environment tab, enter a search string to find an environment. The search string filters the environments and displays results with the string. The search string filters environments across all columns on the Environment List page. Renaming environments \u00b6 On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to rename. On the canvas, click the More icon ( ) and then click Rename . In the Rename Environment window, enter a unique name for the environment and click SUBMIT . All versions of the environment are renamed. You can see the updated environment name in the Search Environment box, Environment List page, and all other instances. The environment name is also updated in any child environments that reference this environment. Note: You can rename an environment that has been shared with you only if the root environment version is shared with the Edit permission. Otherwise an error message is shown when you click SUBMIT . Sharing environments \u00b6 Deploy Accelerator enables you to collaborate with other users on an environment. The ability to share environments is useful when multiple users need permissions to perform different tasks such as edit, deploy, destroy, delete, view, and export on the same environment. All these users can work and collaborate on a shared environment at the same time. Users can perform one or more tasks based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to share with other users. Click the Share icon ( ) and select Share Environment . In the Share Environment window, perform the following actions: (Optional) To share all existing versions of the environment with other users, select Share All Versions . By default, only the currently selected environment version is shared with other users. Note: If you later create a new version of this environment , you must separately share that environment version with users. While creating the new version, you can choose to apply the share permissions from the base environment. Click the icon for the Group with which you want to share the environment version. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign share permissions to users, select the appropriate permission check boxes and click DONE . You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the environment, select the Warning check box and click SUBMIT . The environment version is shared with all users who are members of the selected groups. When you share an environment version, its deployments are not automatically shared with the specified groups. You must separately share each deployment with the appropriate groups and assign the required permissions. If you no longer want to share the environment, click CANCEL . Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for these deployments. (Optional) To enable users to start a deployment of the shared environment version by using the provider that is configured in that version, share the provider with the same groups. (Optional) To enable users to successfully use the connections that are specified in the shared environment version, share the connections with the same groups. Deleting environments \u00b6 You can choose to delete environments that you no longer need. If an environment has multiple versions, each version must be separately deleted. Deleting an environment version also permanently deletes all configurations related to that version. On the Home page, in the Search Environment box, enter the name of the environment that you want to delete. If the environment has multiple versions, each version is listed separately in the Search Environment box. You can identify environments that have been shared with you based on the color of the environment name. Next to the environment version that you want to delete, click the Delete icon ( ). In the confirmation message box, click YES . The environment version is no longer shown in the list of environments. Note: You cannot delete environment versions that have deployments. Similarly, you cannot delete an environment version that is being used as a parent in one or more child environments. Restoring deleted environments \u00b6 You can restore a deleted environment which was deleted by you, or the environments for which you have the Delete permission. On the Home page, click the More options ( ) icon in the top right corner and then click Environments . In the Environment List , select Show Deleted Environments . The deleted environments appear at the end of the list. Under the Actions column, click Restore to restore the environment. In the confirmation message box, click YES . The environment is restored in the list of environments. Accessing environments and deployments of other users \u00b6 By default, Deploy Accelerator allows you to access only the following environments and deployments: Environments and deployments that you own. For more information, see Creating environments and Starting new deployments . Environments and deployments that other users have shared with you. For more information, see Sharing environments and Sharing deployments . However, sometimes you might need to access the environments and deployments of other users even though they are not shared with you. Your administrator can grant you this ability by adding you to the VIEW_ALL_USER'S_ENTITIES group. This group allows you to view all environments and deployments of all users in Deploy Accelerator. Note: To perform any actions (such as delete environments and destroy deployments) on the entities of other users, you must also be a member of the ADMIN group. To access environments and deployments of other users, perform the following actions: To access an environment or a deployment of another user, ensure that you have the ID of the environment version. Note: When an environment version is opened on the canvas, its ID is shown in the URL. Open a browser and enter the Deploy Accelerator URL in the following format: https:// YOUR-PLATFORM-BASE-URL /hcapdeploy/#/home/dnow/ ENVIRONMENT-VERSION-ID To view the list of all deployments across all versions of the environment, click the deployment tab on the canvas. Select the deployment that you want to view. Exporting environments as blueprints \u00b6 If you create a standalone or layered environment that needs to be deployed multiple times, you can export that environment as a blueprint. Ensure that you have followed the best practices for creating blueprints . If you are exporting a layered environment, also ensure that you export the child environment that is in the lowest layer. This action ensures that parent environments in all other layers are also included in the blueprint. You can also export multiple environments. To export multiple environments, you can select a child environment and all the parent environments in the hierarchy are automatically included. Example of exporting a layered environment Consider a scenario in which you create a Jenkins environment and an APP-VPC environment that is dependent on the Jenkins environment. You then create environments for different applications - Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator), and Hitachi Cloud Accelerator - Repair. Each of these environments has a dependency on the APP-VPC environment, as shown in the following image: When you export one of the application environments as a blueprint, the Jenkins and APP-VPC environments are also included in the blueprint and the dependency between them is retained. To replicate the application environment that you have exported, you can create new environments from the blueprint , make customizations if required, and then start a deployment of the environments. To export an environment as a blueprint, perform the following actions: On the canvas, click the More Options icon ( ) and then click Export . In the Environment Export window, from the list select the environment(s) to export. The environment which is open on the canvas is selected by default. The environments created by you are listed in black color. The environments that other users have shared with you are listed in a different color. Important: If you select a child environment, all its parent environments are also included in the exported file. To filter a long list of environments, enter the search keywords in the Search Environments box. Enter a name for the blueprint. Select the warning checkbox to acknowledge the warning message, and click EXPORT . A JSON file is saved as ***fileName*.blueprint.reandeploy** to your local computer. In this JSON file, the environments, the resources within each environment, and the packages within each resource are alphabetically sorted. The packages that are listed after the environments are also alphabetically sorted. Best practices for creating blueprints \u00b6 Consider the following best practices while creating environments that you want to export as blueprints: Blueprints must ideally use a layered environment. You should create an environment with network resources, such as VPCs and CIDRs. In all other environments within the blueprint, you should get the VPC IDs, CIDRs, and other infrastructure resources from this environment by using the Depends On resource. The name of the Depends On resource in each environment must ideally indicate the environment on which there is a dependency. For example, depends_on_app_vpc . Blueprints must enable users to change resource attribute values by using input variables. Blueprints must output important information, such as Server URLs and IDs of important resources, to outputs. Blueprints must use name attributes that are descriptive and unique. For example, you must use ${var.environment_name}-myserver instead of myserver . Blueprints must not hard-code critical things, such as AWS regions, account IDs, VPCs, owner information, product information, environment information, buckets, user names, and passwords. For things that should be automatically detected, blueprints must use Terraform data sources instead of input variables. The following table lists a few examples of data sources that you can use in your environment. To know all the data sources that you can add to your environment, see the Data Sources section on the Resources tab in the left panel on the Deploy Accelerator Home page. Goal Data source Usage Get the current AWS Account ID aws_caller_identity ${ DataSourceName .current.account_id} Get the name of the current region aws_region ${ DataSourceName .current.name} Get the current partition and use it to build an Amazon Resource Name (ARN) aws_partition ${ DataSourceName .current.partition} Get a list of availability zones aws_availability_zones ${ DataSourceName .available.names} Get the current Elastic Load Balancer (ELB) service account ID and use it in the bucket policy for ELB logs aws_elb_service_account ${ DataSourceName .current} Adding blueprints in the Blueprint Gallery \u00b6 If you have created an environment that needs to be replicated multiple times by different users, you can export that environment as a blueprint and then add the blueprint in the Blueprint Gallery. Users can then create a new environment by importing that blueprint from the Blueprint Gallery . Before you begin Ensure that you can access the Artifactory that is configured for Deploy Accelerator. In the Artifactory, ensure that you can access the repository that the Administrator has configured for storing the blueprints that are displayed in the Blueprint Gallery. Export your environment as a blueprint . To add a blueprint in the Blueprint Gallery, perform the following actions: Create additional files that are required for the blueprint. Create a metadata.yml file for the blueprint. This file must contain the attributes that are listed in the following table. Attribute Details name The blueprint name specified in this file is shown in the Blueprint Gallery. Ensure that the blueprint name in this file matches the actual file name of the blueprint (which you provided while exporting the environment as a blueprint from Deploy Accelerator). For example, if you exported an environment from Deploy Accelerator as Chef-Server-HA , the blueprint file name is Chef-Server-HA.blueprint.reandeploy and the name that you must specify in the metadata.yml file must be Chef-Server-HA . description The blueprint description specified in this file is shown in the Blueprint Gallery. version The blueprint version specified in this file is shown in the Blueprint Gallery. image To display an image for the blueprint version in the Blueprint Gallery, specify that image in this file and add the image file in the images folder that you will create in a later step. You must specify the image path relative to the BlueprintName and Version folder in the Artifactory. For example: images/chef.png The image must be in the PNG format. Also, it is recommended that the image dimensions are 130 x 40 pixels. Note: If you later want to update the image for a blueprint version, it is recommended that you use a different image file name and update the metadata.yml file. If you just replace the image file with the same name in the images folder, the updated image is not shown in the Blueprint Gallery. ownerName The owner name specified in this file is shown in the Blueprint Gallery. If you add this attribute in the file but do not specify a value, admin is shown as the owner of this blueprint. ownerName is an optional attribute. ownerEmail The owner email specified in this file is shown in the Blueprint Gallery. ownerEmail is an optional attribute. The following image shows an example of a metadata.yml file. (Optional) Create a README.md file for the blueprint. This file (Markdown format) should ideally include information such as deployment architecture, details about input and output variables used in the environment, and any dependencies. To add an image in the README.md file, you must add the image file in the images folder that you will create in the next step. Also, you must use the IMAGE_BASE_PATH prefix for the image path in the README.md file, as shown in the example below: IMAGE_BASE_PATH/images/chef.png (Optional) Create an images folder and add the image that you want to show for the blueprint version in the Blueprint Gallery. Also, add any images that are used in the README.md file in this folder. Add the blueprint in the Blueprint Gallery. Sign in to the Artifactory that is configured for Deploy Accelerator. Navigate to the repository in which the blueprints must be stored. Create a folder with the same name as the blueprint file name (for example: Chef-Server-HA). Important: The blueprint is displayed in the Blueprint Gallery only if the folder name, blueprint file name, and the blueprint name defined in the metadata.yml file are the same. In the BlueprintName folder, create a folder with the blueprint version (for example: 01.00.00). Ensure that the folder name matches the blueprint version that is defined in the metadata.yml file. In the version folder, add the following files and folder: metadata.yml file ***BlueprintName*.blueprint.reandeploy** file (Optional) README.md file (Optional) images folder Verify that the blueprint is available in the Blueprint Gallery and can be successfully imported and deployed. Sign in to Cloud Accelerator Platform . On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . On the Blueprint Gallery page, search for the blueprint that you have just added in the Artifactory. (Optional) If you cannot see your blueprint in the Blueprint Gallery, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint metadata from the Artifactory. This metadata includes the blueprint name, description, version, and image. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. Confirm that the name, description, version, and image of your blueprint are displayed correctly. Import the blueprint and start a new deployment . Deploy Accelerator resources \u00b6 The Resources tab displays many resources that you can add to your environment. The following table lists the additional resources that Deploy Accelerator provides to address specific scenarios : Resource name Details Depends On Use this resource to create a dependency on another environment. The name of the Depends On resource must ideally indicate the environment on which there is a dependency. In the reference_type attribute, you can select either Environment Name or S3 . -- If you select Environment Name , click the Depends On box and select the parent environment version on which the environment must be dependent. The selected parent environment appears in the environmentName:environmentVersion format (for example: networkEnv:01.00.00). While exporting the child environment, Deploy Accelerator exports this version of the parent environment. -- If you select S3 , enter the S3 bucket name and the remote state file name of the environment that is referred by the Depends On resource. For example: s3://bucket_name/tfstatefile_name , in which bucket_name is the name of the S3 bucket and tfstatefile_name is the Terraform remote state file name of the environment that is to be referred. If required, you can define variables for the S3 bucket and Terraform state file names in the Input Variables resource. To reference these variables in the Depends On resource, you must use the following interpolation syntax: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } For example: If you have defined the S3bucket and tfstatefile variables in the Input Variables resource, specify the following URL in the Depends On resource. s3:// \\({var.s3bucket}/\\) Ensure that the provider that you use to deploy the environment has the appropriate access to the S3 bucket that you have specified. Note: For information about how you can create layered environments by using the Depends On resource, see Creating layered environments . Input Variable Use this resource to create a JSON file in which you can specify multiple variables and set their values. You can later use these variables in other resources within the same environment. For information about the different ways in which you can define and use input variables, see Formats for defining input variables and Formats for using input variables in resources . Locals Use this resource to create a JSON file in which you can specify multiple local values. You can later use these local values in other resources within the same environment. A local value can be a simple constant or a complex expression that transforms or combines values from other resources in the environment, as shown in the following example. For more information, see the Terraform documentation . Output Use this resource to create a JSON file in which you can specify multiple variables that can be used in other dependent environments. The value of these variables can reference either variables defined in the Input Variable resource or attributes of other resources in the environment by using the interpolation syntax. Note: If you do not want to display an output value in the resource logs of an environment, set the sensitive parameter for that output to true . Existing VM Use this resource to add an existing instance to the environment. Formats for defining input variables \u00b6 In the Input Variable resource, you can use the following formats to define input variables based on your requirements: String: Use the following syntax to define input variables by using the String format: \"string_input\": { \"type\": \"string\", \"default\": \"sample_value\", \"description\": \"this is sample string value for hcap deploy\" } Example: \"string_input\": { \"type\": \"string\", \"first_name\": \"John\", \"description\": \"First name of the user\" } List: Use the following syntax to define input variables by using the List format: \"list_input\": { \"type\": \"list\", \"default\": [ \"sample_value1\", \"sample_value2\" ], \"description\": \"this is sample list value for hcap deploy\" } Example: \"list_input\": { \"type\": \"list\", \"Region\": [ \"us-west-2\", \"us-east-1\" ], \"description\": \"List of regions to be used\" } Map: Use the following syntax to define input variables by using the Map format: \"map_input\": { \"type\": \"map\", \"default\": { \"key\": \"sample_value\" }, \"description\": \"this is sample map value for hcap deploy\" } Example: \"map_input\": { \"type\": \"map\", \"ec2details\": { \"app\": { \"ami\": \"ami-718c6909\", \"type\": \"t2.micro\" }, }, \"description\": \"this is sample map value for hcap deploy\" } The following image shows an example of using these different formats in the Input Variable resource. Formats for using input variables in resources \u00b6 In any environment, instead of hard coding the attribute value of a resource, you can use the interpolation syntax to refer to the input variables that you have defined. The interpolation syntax is based on the type of input variable that you are referring. String: Use the following interpolation syntax to refer to an input variable of the String type: ${var.variableName} Example: ${var.string_input} List: Use the following interpolation syntax to refer to an input variable of the List type: To refer to all the values in a list, use the following syntax: ${var.variableName} To refer to a specific value in a list, use the following syntax: ${var.variableName[index]} Examples: ${var.list_input} ${var.list_input[0]} Map: Use one of the following interpolation syntax to refer to an input variable of the Map type: ${var.variableName} ${var.variableName[\"Key\"]} ``` ${lookup(var.variableName[\"Key\"], \"InnerKey\")} ``` Examples: ``` ${var.map_input} ``` ``` ${var.map_input[\"ec2details\"]} ``` ``` ${lookup(var.map_input[\"ec2details\"], \"app\"} ``` Copying resources \u00b6 You can copy one or more existing resources from either the same environment or a different environment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment from which you want to copy a resource. On the canvas, right-click the resource that you want to copy, and click Copy . To copy multiple resources, repeat this step for each resource. Based on your requirements, perform one of the following actions: To create a copy of the resource in the same environment, right-click the canvas and then select the resource from the Paste Resource list. To create a copy of the resource in another environment, open that environment in a new tab, right-click the canvas and then select the resource from the Paste Resource list. All the resources that you have copied appear in the Paste Resource list. You can add a copied resource to any environment only once. ![](/images/rean-deploy/rd_resourcepaste.PNG) From the Paste Resource list, click the appropriate resource and then perform the following actions: In the Resource Name window, enter a name for the resource and click CREATE . In the right panel that opens, on the Resource tab, you can see that the attribute values are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The Packages tab also displays the same packages, along with their attributes, as the original resource. On the Resource tab, add or update the attribute values based on your requirements. Important: If you have copied a resource from another environment, you must update attribute values that use the interpolation syntax to reference input variables or other resources. You must also update the Depends On attribute if it references another resource in the original environment. Otherwise, the new resource might not be successfully deployed. (Optional) On the Packages tab, add or remove packages based on your requirements. Note: After you select a copied resource from the Paste Resource list, it no longer appears in the list. (Optional) Repeat step 5 for the other resources in the Paste Resource list. (Optional) To clear all resources from the Paste Resource list, click clearAll . To save the environment, click the Save icon ( ). Renaming resources \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment version. On the canvas, select the resource that you want to rename. In the right panel that opens, on the Resource tab, click the resource name link at the top. In the Resource Name window, update the resource name and click SAVE . The resource name is automatically updated in other resources within the environment that reference it by using either the interpolation syntax or the Depends On attribute. To save the environment, click the Save icon ( ). Important: If you rename a deployed resource, that resource is destroyed and a new resource is created with the updated name when you redeploy existing deployments. Also, until you redeploy the deployments, the deployed icon is not shown on the resource and the deployed values are not shown on the Resource Details tab. Adding user-defined packages \u00b6 Packages in Deploy Accelerator enable you to leverage your existing infrastructure automation tool (Chef Solo, Chef Server, Ansible Solo, or Puppet) to configure compute resources in your environments. Each package definition includes the infrastructure automation tool and the Chef cookbook or Puppet module or manifest file that Deploy Accelerator must use to configure the compute resource. Managed packages are available by default after Deploy Accelerator is successfully deployed. You can leverage these managed packages if they meet your requirements. Otherwise, you can add your own custom (or user-defined) packages. In the case of user-defined packages, you must define a versioning scheme for the packages and ensure that each combination of package type, name, and version is unique. The following diagram describes the high-level process for creating and releasing a user-defined package: After a user-defined package is released, you can neither edit nor delete that package. To include additional updates, you must create a new package version. Add a user-defined Chef, Ansible, or Puppet package \u00b6 To prepare the user-defined package that you want to add in Deploy Accelerator, perform the following actions: Based on your infrastructure automation tool, perform one of the following actions: Create a Chef cookbook and test it well using the chef-client. For more information, see the Chef documentation . Create an Ansible playbook and test it well using the Ansible-client. For more information, see the Ansible documentation . Create a Puppet module or manifest file and test it well using the puppet-agent. For more information, see the Puppet documentation . Create a ZIP file of the Chef cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the tar.gz format. Commit the ZIP file (tar.gz format) in GitHub, GitLab, Artifactory, or any other repository. You can use a public repository for open source software. Important: If Deploy Accelerator is deployed in the offline mode, you must commit the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the Artifactory. Decide the attributes that you want to define for this package. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . On the Package List page, the Package Type column enables you to identify the package type. In case of Chef Server packages, the Chef Server column enables you to identify the Chef Server from which the packages are shown. To add a user-defined package, click ADD . On the Add Package page, perform the following actions: On the Basic tab, enter the following details: Field Details Package Name Enter the name of the package. This name appears on the Packages tab on the Home page. The name that you enter must be unique across all packages of that type. However, if you are adding a new version of an existing package, ensure that the package name is the same as the existing package name. Note: You cannot enter a name that matches a managed package. Package Type Select chef or ansible as the package type. Depending on the option selected here the package is listed, under the Packages tab, in the Private Packages list. Package Version Enter the version of the package based on your own versioning scheme. The value of this field must be unique across all existing versions of the package that you are adding. Note: Each combination of package type, name, and version must be unique. Description Enter details about the package. Image Url Enter the URL from which Deploy Accelerator must download the image file that is shown on the Packages tab on the Home page. On the Repository tab, enter the following details for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located: Fields Details Download URL Enter the URL from which Deploy Accelerator must download the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Note: If Deploy Accelerator is deployed in the offline mode, you must specify the Artifactory URL from where the Chef Solo cookbook, Ansible playbook or Puppet module or manifest file can be downloaded. Repository Type Select the repository type where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have stored the files in a different repository or location (such as Amazon S3), select the plain option. Access Token Enter the access token for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have selected Artifactory as the repository type, you must enter the API key. Ensure that the access token or API key that you enter is authorized to download the ZIP file from the repository. Note: If you do not enter the access token or API key, Deploy Accelerator takes the default access token or API key that your your administrator has configured . ZIP File Name Enter the name of the ZIP file that contains the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Unzipped Name Enter the name of the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file that is extracted from the ZIP file. Dependent packages Select the other packages that are a prerequisite for installing this package on a resource. On the Attributes tab, specify the attribute values as the array of JSON. The attributes help to parameterize the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file with some dynamic values. All the values that you enter here are shown on the Deploy Accelerator UI to ask the user to provide dynamic values before deployment. A few default attributes are shown for your reference. You can choose to use these attributes or replace them with other attributes that are relevant to your package. Important: The package will be visible to other users, so ideally you should avoid specifying default values for sensitive attributes. Click SAVE . Confirm that this package version appears on the Package List page. The state of this package version is unreleased. Note: In the case of Chef Solo cookbooks, all recipes that the cookbook contains also become available in Deploy Accelerator. While adding a Chef Solo package (or cookbook) to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook Test the user-defined package . Release the user-defined package . Test a user-defined package \u00b6 Create a test environment of the appropriate environment package type. After the environment is created, confirm that the user-defined package that you want to test appears on the Packages tab in the left panel. From the Resources tab in the left panel, drag a compute resource (For AWS an EC2 instance, or a null VM) to the canvas. From the Packages tab in the left panel, drag the user-defined package to this resource. In the right panel that opens, on the Packages tab, select the unreleased version that you want to test, and enter other package details as required. In case of a Chef Solo package (or cookbook), confirm that all recipes that the cookbook contains are available for selection in the Recipes box. Note: While adding a Chef Solo package to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Save the environment. Start a new deployment of the environment . (Optional) To resolve any issues that are detected during the testing, edit the user-defined package and then redeploy the existing deployment . After all issues with the user-defined package are successfully resolved, release the user-defined package . Release a user-defined package \u00b6 On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To release a package, in the Actions column for that package, click Edit . On the Attributes tab, click RELEASE . Important: You can neither edit nor delete a released package. In case you need to make any changes, you must create a new package version. Managing user-defined packages \u00b6 You can edit, download, or delete user-defined packages only if they are not yet released. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. In the Actions column for a specific version of a user-defined package, perform one of the following actions: To edit a user-defined package, click Edit to open the Add Package page, update the package details, and then click SAVE . You cannot edit a user-defined package that is released. To save a copy of the user-defined package to your computer, click Download . To delete the user-defined package, click Delete and then click YES in the confirmation message box. Creating new package versions \u00b6 You can create new versions of both user-defined and managed packages. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To create a new version of a package, in the Actions column for that package, click Create New Version . In the Create New Version From base version window, update details of the new package version. By default, details of the base package version are shown. Note: You must specify a unique version number for this package. Also, the version that you specify must be greater than the base version that you have selected. Click CREATE . (Optional) On the Add Package page, update additional details about the new package version based on your requirements. Click SAVE . A new version of the package appears on the Package List page. Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 \u00b6 In Deploy Accelerator 3.0.2, the Terraform version is upgraded to 0.12.0. Due to this, the blueprints that are created in Deploy Accelerator 2.28.0 and previous releases, require some manual change to be used in 3.0.2. Mandatory attributes for syntax validation \u00b6 Terraform v0.11 and earlier allowed adding attribute/sub-attribute JSON as a Input Variable for the resources, which can be added as an interpolation in the resource attribute. Terraform v0.12 no longer allows such interpolation for JSON and JSON Array for its attributes or sub-attributes. If there is an attribute of the type JSON or JSON Array then it should follow a proper structure for its attributes and sub-attributes. Hence, the Input/Local variables of type JSON cannot be used in interpolation to the Resource or Datasource attributes. Example Resource: aws_route53_record , Attribute: Alias Input Variable for the resource: { \"var.alias_json\" : { \"type\": \"map\", \"default\": { \"name\": \"dns_record_alias\" } } } and value for the alias attribute in the resource for Terraform v0.11 and previous release: alias : [\"${var.alias_json}\"] This syntax is above is not valid. The valid syntax for the alias attribute in the aws_route53_record resource for Terraform v0.12 is as follows: [ { \"evaluate_target_health\": \"\", \"name\": \"${var.alias_name}\", \"zone_id\": \"\" } ] Using Floor function \u00b6 From Terraform 0.12, the Terraform language does not distinguish between integer and float types. Instead, the language just has a single \"number\" type that can represent high-precision point numbers. The Terraform documentation mentions that this new type can represent any value that could be represented before, plus many new values due to the expanded precision. The Terraform documentation also mentions that in most cases this change should not cause any significant behavior change, but please note that in particular the behavior of the division operator is now different: it always performs floating point division, whereas before it would sometimes perform integer division by attempting to infer intent from the argument types. If you are relying on integer division behavior in your configuration, use the floor function to obtain the previous result. A common place this would arise is in index operations, where the index is computed by division. For example: ${floor(length(var.availability_zones)/var.subnet_count) Unfortunately the automatic upgrade tool cannot apply a fix for this case because it does not have enough information to know if floating point or integer division was intended by the configuration author, so this change must be made manually where needed. Using Escape Sequences \u00b6 In JSON attributes, if \\n , \\r , \\t , symbols are not escaped, then Terraform v0.12 does not validate it as a correct JSON. JSON format for Terraform v0.11 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\n \\\")}\" } JSON format for Terraform v0.12 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\\\n \\\")}\" }","title":"Deploy"},{"location":"deploy/using_copy/#deploy-and-manage-environments","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. This topic describes how to create, deploy, and manage environments in Deploy Accelerator.","title":"Deploy and manage environments"},{"location":"deploy/using_copy/#contents","text":"Overview * Overview of environments * Overview of layered environments * Overview of blueprints * End-to-end process for multiple deployments of an environment Create environments * Configuring providers * Configuring connections * Creating environments * Creating layered environments * Creating new environments from blueprints * Copying environments Deploy environments * Starting new deployments * Viewing deployments * Viewing the plan for deployments * Redeploying existing deployments * Sharing deployments * Destroying deployments Release environments Releasing environment versions Creating new environment versions Manage environments Upgrading the provider version of environments Viewing dependency between environments Configuring custom tags Comparing differences between environments Searching environments Renaming environments Sharing environments Deleting environments Restoring deleted environments Accessing environments and deployments of other users Create and manage blueprints Exporting environments as blueprints Best practices for creating blueprints Adding blueprints in the Blueprint Gallery Manage resources Deploy Accelerator resources Copying resources Renaming resources Manage packages Adding user-defined packages Managing user-defined packages Creating new package versions Perform post-upgrade tasks Manual changes required for migration of blueprints from 2.28.0 to 3.0.2 Note: To access Deploy Accelerator, you must sign in to Hitachi Cloud Accelerator Platform by using your Cloud Accelerator Platform account. You can access Deploy Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions. For information about creating a Cloud Accelerator Platform account, see Create & access account .","title":"Contents"},{"location":"deploy/using_copy/#overview-of-environments","text":"Environments in Deploy Accelerator help you to design the infrastructure that you need to deploy different applications in the cloud. Environments are visual representations of your infrastructure and can be easily created by dragging resources and packages to a canvas. You can also add dependencies between the various resources in an environment. The following image shows an example of an environment in Deploy Accelerator: Each environment contains the following elements: Resources: Entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. Packages: Entities that use open-source infrastructure automation tools to configure compute resources in an environment. Deploy Accelerator provides multiple out-of-the-box packages. If required, you can also create your own user-defined packages by using Chef (Chef Solo or Chef Server) or Puppet . Links: Visual representation of dependencies between various resources in an environment. Scripts: Set of specific actions that must be performed before or after deploying and destroying an environment.","title":"Overview of environments"},{"location":"deploy/using_copy/#environment-versions-and-multiple-deployments","text":"You can release and version environments in Deploy Accelerator. Releasing an environment enables you to freeze any updates to the environment. Typically, when an environment is ready to be deployed in production, you can release that environment. To add or update resources in a released environment, you must create a new environment version. Environment versions enable you to keep a track of changes to an environment over time. After you have designed the infrastructure in an environment version, you can start a deployment to actually create the infrastructure. Based on your requirements, you can also start multiple deployments of the same environment version. Multiple deployments help you to create an environment once and then reuse it to create new infrastructures. For example, you can start QA and Production deployments of an environment version or you can start two Production deployments of that environment version. For each new deployment of an environment version, you can define different values for the input variables (or parameters) that are defined in the environment. For example, you can select different instance types (such as t2. micro or t2.large) for the AWS EC2 instances in your QA and Production deployments. For more information, see end-to-end process for multiple deployments of an environment . You can also redeploy an existing deployment with the same environment version or with another environment version. Deployments that you no longer need can easily be destroyed. Environments that you no longer need can also be deleted. If an environment has multiple versions, each version must be separately deleted. However, before you can delete an environment version, you must destroy all deployments for that version.","title":"Environment versions and multiple deployments"},{"location":"deploy/using_copy/#overview-of-layered-environments","text":"A layered environment is basically a collection of multiple interdependent environments . For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between them, as shown in the following image. While deploying a child environment in a layered environment, you have to select a specific deployment of the parent environment. If parent environments have multiple deployments, you have to select a specific deployment while deploying the child environment. If you do not specify any deployment, the parent deployment with the name default is used to deploy the child environment. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other dependent environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications.","title":"Overview of layered environments"},{"location":"deploy/using_copy/#overview-of-blueprints","text":"Blueprints in Deploy Accelerator are templates of commonly used IT infrastructures. If you have designed modular solutions (layered environments) or simple solutions (standalone environments) that need to be replicated multiple times, you can export the environments as blueprints . If only you plan to create environments from your blueprints, you can maintain the blueprints on your computer. However, if you want other users to create environments from your blueprints, you can add the blueprints in the Blueprint Gallery . Users can leverage blueprints to quickly create infrastructure for solutions. For more information, see Creating new environments from blueprints . For example, you can create an environment for a high-availability deployment of Chef Server and export that environment as a blueprint. If another team needs to deploy its own high-availability Chef Server, a user in that team can import your blueprint, customize the deployment parameters, and start a new deployment. The following image describes the end-to-end lifecycle of a blueprint, from creating and exporting an environment as a blueprint to creating new environments from that blueprint.","title":"Overview of blueprints"},{"location":"deploy/using_copy/#end-to-end-process-for-multiple-deployments-of-an-environment","text":"The following procedure describes the end-to-end process that you can follow to implement multiple deployments of a standalone environment. Create a new environment and design your infrastructure. For example, create an environment with the version 01.00.00. Start a new deployment of the environment that you have created. For example, create a Staging deployment to test your design. (Optional) Based on your requirements, add or update resources in this environment and redeploy the existing deployment . Release the environment version . No changes can be made to a released environment version. However, you can start new deployments or redeploy existing deployments with this environment version. Start another new deployment of the environment. For example, create a Production deployment to deploy your applications. Note: For each new deployment, you can change the input variables based on your requirements. For example, you can use different VPC IDs for your Staging and Productions deployments. (Optional) View all deployments for this environment. To make any updates to the environment, create a new environment version and then add or update resources in this environment version. For example, create an environment version 02.00.00 that is based on version 01.00.00. Based on your requirements, perform one of the following options: Start a new deployment of the environment version that you have created. Plan and redeploy an existing deployment with this environment version. For example, you can redeploy the Staging deployment of environment version 01.00.00 with this new environment version 02.00.00. In this case, only the delta between the two versions is deployed. Resources that have not been changed between the two versions are not impacted. Note: When you redeploy an existing deployment with a different environment version, the deployment gets associated with that environment version. Release the environment version . Note: In the case of layered environments, you must follow these steps for each environment within the layered environment. However, in addition to using the Depends On resource to create a dependency between the environments, you must also create a dependency between specific deployments of the environments. For example, the Staging deployment of a child environment must be dependent on the Staging deployment of the parent environment.","title":"End-to-end process for multiple deployments of an environment"},{"location":"deploy/using_copy/#configuring-providers","text":"Providers in Deploy Accelerator allow you to deploy and manage different types of infrastructure resources. They also contain the authentication details required to deploy and manage the resources. For example, an AWS provider contains credentials for accessing the Amazon Web Services (AWS) account in which you want to deploy and manage resources, such as EC2 instances, subnets, and S3 buckets. Deploy Accelerator supports many different types of providers, such as AWS, Microsoft Azure, and Kubernetes. When you create a new environment in Deploy Accelerator, you have to select a provider. The resources that are available for the environment are based on the selected provider. By default, only you can use the providers that you create. However, you can choose to share your own providers with one or more groups.","title":"Configuring providers"},{"location":"deploy/using_copy/#supported-providers","text":"The following table lists the providers that are supported in the latest version of Deploy Accelerator. It also lists supported versions of the Terraform providers that Deploy Accelerator uses to create and manage resources. Provider Supported Terraform Provider Version ACME 1.6.3 Artifactory 2.2.4 AWS 3.23.0 Azure 2.41.0 Azure Active Directory 1.1.1 Azure DevOps 0.1.0 Azure Stack (Beta) 0.9.0 Consul 2.10.1 Databricks 0.2.9 Datadog 2.18.1 DNS 3.0.0 Docker 2.8.0 Google Cloud Platform 3.51.0 HEC-CP None HEC-VM None Helm2 0.10.6 Helm3 1.3.2 Kibana 0.7.1 Kubernetes 1.13.3 Oracle 1.4.0 Oracle Cloud Infrastructure 4.7.0 vSphere 1.24.2 vSphere NSX-T (Beta) 3.1.0","title":"Supported providers"},{"location":"deploy/using_copy/#create-a-provider","text":"On the Home page, click the More options icon ( ) in the top-right corner. Click Providers . On the Provider page, click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). For a complete list of provider types that are available in Deploy Accelerator, see Supported providers . In Provider Details , enter the authentication details that Deploy Accelerator must use to deploy and manage resources. The provider details that you have to specify differ based on the selected provider type. For more information, see Provider details . You can also click the Terraform Provider Link below the Provider Details box to view the Terraform documentation on the selected provider. (Only for AWS and Kubernetes provider types) To verify whether the JSON syntax and the authentication details that you have specified are correct, click VALIDATE . To also save the provider after successfully validating the provider details, click VALIDATE & SAVE . Note: The VALIDATE and VALIDATE & SAVE buttons are enabled only if you have specified a valid JSON syntax in the provider details. Click SAVE . A new provider appears in the Providers List section. Note: The SAVE button is enabled only if you have specified a valid JSON syntax in the provider details. (Optional) Share the provider with one or more groups. Important: If you later edit this provider, you must re-enter all values in the Provider Details section before you click UPDATE . Otherwise, an error message might be shown for deployments that use this provider.","title":"Create a provider"},{"location":"deploy/using_copy/#delete-a-provider","text":"On the Home page, click the More options icon ( ) in the top-right corner, and then click Providers . Under the Provider List , click Delete ( ), and in the confirmation dialog box, click Yes . Once a provider is deleted the provider credentials are also removed from the database. Before deleting a provider, make sure that no environment is using the provider for deployments. If an environment uses a provider, the provider cannot be deleted.","title":"Delete a provider"},{"location":"deploy/using_copy/#provider-details","text":"Deploy Accelerator supports multiple providers, such as AWS, Azure, and Kubernetes. While creating a provider, you have to specify authentication details that Deploy Accelerator can use to deploy and manage the resources. The following topics provide information about the required details for each provider type: ACME provider details Artifactory provider details AWS provider details Azure provider details Azure DevOps provider details Consul provider details Databricks provider details Datadog provider details Google provider details Helm provider details Kubernetes provider details Kibana provider details Oracle Cloud Infrastructure provider details","title":"Provider details"},{"location":"deploy/using_copy/#acme-provider-details","text":"The Automated Certificate Management Environment (ACME) is an evolving standard for the automation of a domain-validated certificate authority. The ACME provider allows you to acquire a valid SSL certificate from Let's Encrypt. The ACME provider supports a wide list of DNS challenge types, for example gcloud, azure, digitalocean, etc. For the complete list, see the Terraform documentation . Following are the parameters in an ACME provider: Parameter Details server_url The URL to the ACME endpoint's directory. This is a mandatory parameter. config The list of key-value pair required according to the provider. You can enter a key parameter value here to override the values from provider_reference_id. This is an optional parameter. provider_reference_id The reference ID of the reference provider. This is an optional parameter. If you enter this parameter, you will not have to specify the credentials of the cloud provider used as dns_challenge . Provider Details JSON Example { \"server_url\": \"\", \"config\": { AWS_ACCESS_KEY_ID = \"XXXXXX\" AWS_SECRET_ACCESS_KEY = \"XXXXXXX\" AWS_DEFAULT_REGION = \"us-east-1\" }, \"provider_reference_id\": \"1\" }","title":"ACME provider details"},{"location":"deploy/using_copy/#artifactory-provider-details","text":"The Artifactory provider is to manage the resources for Artifactory. Following are the parameters in Artifactory provider: Parameter Details url The URL for customer artifactory. This is a mandatory parameter. username The username for accessing customer artifactory. password The password for logging in to customer artifactory. access_token The access token for accessing the artifactory. api_key The API key for accessing the artifactory. Note: You require only one type of parameter for authentication. Enter a username/password, or an access token, or an API key. Provider Details JSON Example { \"url\": \"http://www.example.com\", \"username\": \"XXXXX\", \"password\": \"XXXXX\", \"access_token\": \"XXXXX\", \"api_key\": \"XXXXXXXXXXX\" }","title":"Artifactory provider details"},{"location":"deploy/using_copy/#aws-provider-details","text":"For the AWS provider type, you must select either Basic Credentials or Instance Profile to specify authentication details of the AWS account that Deploy Accelerator must use to deploy an environment. To use a more secure way of cross-account access, you must select Assume Role . Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a more secure way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can attach a role to the instance on which Deploy Accelerator is deployed. Deploy Accelerator can then assume a role in the other accounts and deploy the resources. For more information about assuming roles, see AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Deploy Accelerator is deployed. For the other accounts, you must specify a role that Deploy Accelerator can assume to deploy the resources. The role that you specify must have permissions to deploy the required resources. It must also define the account in which Deploy Accelerator is deployed as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Deploy Accelerator must deploy resources. However, Deploy Accelerator can use this method only if a role is attached to the instance on which Deploy Accelerator is deployed. This role must have the permissions to deploy the required resources. To use the Instance Profile method, you must specify only the region in which the resources must be deployed, as shown in the following example: { \"region\" : \"xx-xxxx-x\" } Static Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Deploy Accelerator is deployed in an AWS account that is different from the accounts in which Deploy Accelerator must deploy the resources. Instead of storing the access credentials for all these accounts in Deploy Accelerator, you can specify long-term access credentials for only the parent account. Deploy Accelerator can then use temporary credentials to access all other child accounts by assuming roles in those accounts. For more information about assuming roles, see the AWS documentation . To use the Static Credentials with Assume Role method, you must specify the credentials for only the parent account. For each child account, you must specify a role that Deploy Accelerator can assume to deploy resources in that account. The role that you specify must have access to deploy the required resources. It must also define the parent account as a trusted entity. In the Provider Details section, you must specify the details, as shown in the following example: { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Static Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Deploy Accelerator must deploy resources, as shown in the following example: { \"access_key\" : \"ACCESS-KEY\" , \"secret_key\" : \"SECRET-KEY\" , \"region\" : \"xx-xxxx-x\" } The IAM user whose credentials you specify must have access to deploy the required resources.","title":"AWS provider details"},{"location":"deploy/using_copy/#azure-provider-details","text":"The Azure provider is used to interact with the resources supported by Azure. Following are the parameters in an Azure provider: Parameter Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated.","title":"Azure provider details"},{"location":"deploy/using_copy/#azure-devops-provider-details","text":"The Azure DevOps provider is used to interact with the resources supported by Azure DevOps. Following are the parameters in an Azure DevOps provider: Parameter Details org_service_url (Required) This is the Azure DevOps organization URL. personal_access_token (Required) This is the Azure DevOps organization personal access token. The account corresponding to the token will need \"owner\" privileges for this organization.","title":"Azure DevOps provider details"},{"location":"deploy/using_copy/#consul-provider-details","text":"The Consul provider is used to interact with the resources supported by Consul. Following are the parameters in an Consul provider: Parameter Details address Public IP address of the instance on which Consul server is installed. datacenter The datacenter that is configured for the Consul server you have specified. The following is an example of the provider details section for a Consul provider. { \"address\" : \"123.0.57.189\" , \"datacenter\" : \"dc2\" }","title":"Consul provider details"},{"location":"deploy/using_copy/#databricks-provider-details","text":"The Databricks provider is used to interact with the resources supported by Databricks. Following are the parameters in an Databricks provider: Parameters Details host (optional) This is the host of the Databricks workspace. It is a URL that you use to login to your workspace. token (optional) This is the API token to authenticate into the workspace. username (optional) This is the username of the user that can log into the workspace. password (optional) This is the user's password that can log into the workspace. profile (optional) Connection profile specified within ~/.databrickscfg . To learn more about connect profiles, see Databricks documentation.","title":"Databricks provider details"},{"location":"deploy/using_copy/#datadog-provider-details","text":"The Datadog provider is used to interact with the resources supported by Datadog. Following are the parameters in an Datadog provider: Parameters Details api_key Datadog API key. This is a mandatory parameter if the validate parameter is set to true . app_key Datadog APP key. This is a mandatory parameter if the validate parameter is set to true . api_url (optional) The Datagog API URL. validate (optional) Enables validation of the provided API and APP keys during provider initialization. Default is true. When false, api_key and app_key is not checked.","title":"Datadog provider details"},{"location":"deploy/using_copy/#google-provider-details","text":"The Google provider is used to connect to Google Cloud Platform infrastructure products. You can also use Google beta resources in Google environments. Following are the parameters in the google beta provider: Parameters Details Credentials The credentials field is for entering the service account key in JSON format. You can generate a service account key using various methods, for more information, see Google Cloud Platform documentation . Project The project field is your personal project id. The project indicates the default GCP project all of your resources will be created in. Most Terraform resources will have a project field. Region The region is used to choose the default location for regional resources. Regional resources are spread across several zones. Sample { \"credentials\" :{ \"type\" : \"service_account\" , \"project_id\" : \"project-id\" , \"private_key_id\" : \"key-id\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"service-account-email\" , \"client_id\" : \"client-id\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\" }, \"project\" : \"my-project-id\" , \"region\" : \"us-central1\" }","title":"Google provider details"},{"location":"deploy/using_copy/#helm-provider-details","text":"Helm2 and Helm3 providers allow you to specify details of the Kubernetes cluster in which you want to deploy Helm charts. The difference between the Helm2 and Helm3 provider types is the Terraform Helm provider version that is supported. Helm2 provider supports version 0.10.4 Helm3 provider supports version 1.2.1 or later Based on your requirements, you can choose the appropriate Helm provider type. The provider details that you have to specify for both provider types is the same. Note: Your administrator might have set the properties for the default helm repository from which Deploy Accelerator downloads helm charts while deploying environments with the Helm2 or Helm3 provider type. In this case, you do not have to add the Helm Repository data source in your environment if your helm chart is available in this repository. Pre-requisites Before configuring a Helm2 or Helm3 provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. (Only for Helm 2 provider) Ensure that Tiller is running in the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Helm2 or Helm3 provider, you must populate the config_path_Content sub-attribute under the kubernetes attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the kubernetes attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 . Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Helm2 or Helm3 provider (JSON file entered below the config_path_Content attribute). { \"kubernetes\" : [ { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } } ] } Example 3 The following is an example of the provider details section for a Helm2 or Helm3 provider for AWS EKS. { \"kubernetes\" : [ { \"config_path_Content\" : \"\" } ], \"aws\" : [ { \"access_key\" : \"\" , \"secret_key\" : \"\" , \"region\" : \"\" } ], \"provider_reference_id\" : \"88\" } Example 4 The following is an example of the provider details section for a Helm2 or Helm3 provider for GKE. { \"google\": [ { \"zone\": \"XXXXX\", \"region\": \"XXXXXX\", \"project\": \"XXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXXXX\" } ] } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter.","title":"Helm provider details"},{"location":"deploy/using_copy/#kibana-provider-details","text":"The Kibana provider is used to interact with the resources supported by Kibana. Following are the parameters in an Kibana provider: Parameters Details elastic_search_path The path at which elastic search is hosted. kibana_uri The URI at which Kibana is hosted. kibana_version (Optional) The version of Kibana configuration. For the list of supported versions, see Kibana Terraform Provider documentation . In case of empty field the default value is 6.0.0 . kibana_type (Optional) The type of Kibana in the back-end. The default value is KibanaTypeVanilla , which supports the standard open-source kibana distribution. For more information, see Kibana Terraform Provider documentation . To configure logz.io kibana, use KibanaTypeLogzio . kibana_username (Optional) Username for Kibana API authentication. kibana_password (Optional) Password for Kibana API authentication. logzio_client_id (Optional) The client ID used for authentication with logzio. For more information, see Kibana Terraform Provider documentation . logzio_account_id (Optional)The logz.io account id. logzio_mfa_secret (Optional) MFA shared secret, create when signing up user account with MFA.","title":"Kibana provider details"},{"location":"deploy/using_copy/#kubernetes-provider-details","text":"Kubernetes provider allows you to specify details of the Kubernetes cluster in which you want to deploy various resources. Pre-requisites Before configuring a Kubernetes provider, you must perform the following actions: Ensure that the instance on which Deploy Accelerator is installed can access the Kubernetes cluster. Modify the kubeconfig file (YAML format) that is used to configure access to your Kubernetes clusters. A kubeconfig file enables you to organize your clusters, users, and namespaces, and to define multiple contexts. In the kubeconfig file, locate the certificate-authority , client-certificate , and client-key attributes and perform the following actions: Replace the file paths specified in these attributes with the content of those files. Append the attributes with -data , as shown below: certificate-authority-data , client-certificate-data , client-key-data Save the kubeconfig file. (Optional) If required, convert the kubeconfig file content to the base64 encoded format. Convert the kubeconfig file content from the YAML format to the JSON format. Provider details While creating a Kubernetes provider, you must populate the config_path_Content attribute. In this attribute, enter the content (in JSON format) of the kubeconfig file that is used to configure access to your Kubernetes clusters. Ensure that you have completed all the pre-requisites for the kubeconfig file. You can enter the JSON content in one of the following ways: Paste the JSON file (with base64 encoded content) as a STRING value of the attribute, as shown in Example 1 . Paste the JSON file content directly below the attribute, as shown in Example 2 . If your cluster is deployed on AWS Elastic Kubernetes Service (EKS), you must also add the aws attribute below the config_path_Content attribute. In the aws attribute, enter your authentication details for the AWS account that was used to create the cluster, as shown in Example 3 . You can use all the supported methods of adding AWS authentication details. These methods include instance profile with and without assume role, and static credentials with and without assume role. For more information, see AWS provider details . If your cluster is deployed on Google Kuberneters Engine (GKE), you must enter the authentication details for the Google account, as shown in Example 4 and Example 5 Note : In all examples of provider details, you can add a reference provider to avoid specifying access key and the secret key in case of AWS, and service key content in case of Google cloud in the providers JSON. Note: If your kubeconfig file has multiple contexts, you must create a separate provider for each context that you want to use in Deploy Accelerator. To specify a context in a provider, you must also add the config_context attribute. If you do not specify the context while creating a provider, Deploy Accelerator considers the current context that is defined in the kubeconfig file. Example 1 The following is an example of the provider details section for a Kubernetes provider (JSON file with base64 encoded content entered as a STRING value of the config_path_Content attribute). { \"config_path_Content\" : \"\" , \"provider_reference_id\" : \"1\" } Example 2 The following is an example of the provider details section for a Kubernetes provider (JSON file entered below the config_path_Content attribute). { \"config_path_Content\" : { \"apiVersion\" : \"v1\" , \"clusters\" : [ { \"cluster\" : null } ], \"certificate-authority-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"server\" : \"https://XXX.XX.XX.XX:8443\" , \"name\" : \"KubernetesCluster\" , \"contexts\" : [ { \"context\" : null } ], \"cluster\" : \"KubernetesCluster\" , \"user\" : \"kubernetes\" , \"current-context\" : \"kubernetes\" , \"kind\" : \"Config\" , \"preferences\" : {}, \"users\" : [ { \"name\" : \"kubernetes\" } ], \"client-certificate-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"client-key-data\" : \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } \"provider_reference_id\" : \"1\" } Example 3 The following is an example of the provider details section for a Kubernetes provider for AWS EKS. { \"config_path_Content\" : \"XXXXXXXXXXXXXXXXXXXXXXXXX\" , \"aws\" : [ { \"access_key\" : \"XXXX\" , \"secret_key\" : \"XXX\" , \"region\" : \"XXXX\" } ] } Example 4 The following is an example of the provider details section for a Kubernetes provider for GKE. { \"google\": [ { \"zone\": \"XXXXXXXX\", \"region\": \"XXXXXXX\", \"project\": \"XXXXXXX\", \"service_key_Content\": \"\", \"cluster\": \"XXXXXXXXX\" } ] } Example 5 The following is an example of the provider details section for a Kubernetes provider for GKE using a reference provider. { \"google\": [ { \"zone\": \"\", \"region\": \"\", \"project\": \"\", \"service_key_Content\": \"\", \"cluster\": \"\" } ], \"provider_reference_id\": \"1\" } To create a cluster in single or multiple zones, use the zone parameter. In this case, do not enter the value for the region parameter. To create a regional cluster, use the region parameter. In this case, do not enter the value for the zone parameter.","title":"Kubernetes provider details"},{"location":"deploy/using_copy/#oracle-cloud-infrastructure-provider-details","text":"The Oracle Cloud Infrastructure is for managing the resources of the Oracle Cloud Infrastructure. Following are the parameters in the Oracle Cloud Infrastructure provider: Parameter Details tenancy_ocid OCID of your tenancy. For more information, see Oracle Cloud Infrastructure documentation . user_ocid OCID of the user calling the API. For more information, see Oracle Cloud Infrastructure documentation . fingerprint Fingerprint for the key pair being used. For more information, see Oracle Cloud Infrastructure documentation . private_key The path (including filename) of the private key stored on your computer, required if private_key is not defined. For information on how to create and configure keys, see Oracle Cloud Infrastructure documentation . region An Oracle Cloud Infrastructure region. For more information, see Oracle Cloud Infrastructure documentation . Provider Details JSON Example { \"tenancy_ocid\": \"XXXXXXXXXXX\", \"user_ocid\": \"XXXXXXXXXX\", \"fingerprint\": \"XXXXXXXXX\", \"private_key\": \"XXXXXXXXX\", \"region\": \"XXXXXXXXXXX\" }","title":"Oracle Cloud Infrastructure provider details"},{"location":"deploy/using_copy/#share-a-provider","text":"Deploy Accelerator enables you to share your provider with other users. The ability to share providers is useful when multiple users in a group need to use the same provider to deploy environments. These users can view, edit, share, or delete providers that are shared with them based on the permissions assigned to their group. On the Home page, click the icon in the top-right corner. Click Providers . On the Providers List page, click the icon for the provider that you want to share with other users. You can identify the providers that are shared with you based on the color of the provider name. You can share a shared provider only if your group has been assigned the Share permission for that provider. In the Share Provider : ProviderName window, click the icon for the Group with which you want to share the provider. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group. Select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the provider, select the Warning check box and click SUBMIT . If you no longer want to share the provider, click CANCEL .","title":"Share a provider"},{"location":"deploy/using_copy/#configuring-connections","text":"Amazon Elastic Compute Cloud (EC2) uses public-key cryptography to encrypt and decrypt login information. When Deploy Accelerator launches an instance as part of deploying an environment, it specifies the name of a key pair (set of public and private keys) that must be used to connect to the instance. Each connection that you create contains the private key that can be used to connect to one or more instances in your environments. While creating an environment, you must specify a connection that is applied to all instances by default. You can also share your own connections with one or more groups. Note: If you do not want to use the default connection for all instances in an environment, you can create multiple connections. You can also choose to dynamically generate a key pair and associate it with one or more instances while the environment is being deployed. For an example of the step-by-step procedure to dynamically generate and associate a key pair with an instance, see Configure the dynamic generation of a key pair .","title":"Configuring connections"},{"location":"deploy/using_copy/#create-a-connection","text":"Create the required key pair by using either Amazon EC2 or a third-party tool. If you use a third-party tool, you must also import the public keys to Amazon EC2. For more information, see the Amazon Elastic Compute Cloud User Guide for Linux Instances or the Amazon Elastic Compute Cloud User Guide for Windows Instances . Important: To create a key pair by using Amazon EC2, you must have your own AWS cloud account. On the Home page, click the More options icon ( ) in the top-right corner and then click Connections . On the Add/Edit Connection page, click NEW . Enter the connection name and connection type. If you have selected the WinRM connection type, perform the following actions: In SSH/WinRM User , enter administrator . In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. (Optional) To set up a secure connection, select Https and enter a CA certificate in Cacert Note: If you want Deploy Accelerator to skip validation of the CA certificate, you can select Insecure . (Optional) To use NTLM authentication for remote connection, select NTLM . Important: When you select a WinRM connection for an instance, you must also specify a PowerShell script in the user_data attribute of that EC2 Instance resource. The user and password that you specify in the PowerShell script and the selected WinRM connection must match. Otherwise, you cannot deploy packages on that instance. If you have selected the SSH connection type, enter the user and perform one of the following actions: In SSH/WinRM Password , enter the password that Deploy Accelerator must use for the connection. In SSH Key , enter the SSH key instead of a password. (Optional) To specify details of a Bastion Host in the connection, select Bastion Connection , enter the following details, and then click SET PARAMETER . User Password (enter either the password or the key) Host (IP address of the Bastion host, which must be available in the network) Port (By default, the port number is selected based on the connection type.) Key You can use a Bastion connection based on how your network is configured. If the connection that you are creating is for instances in a private network and a Bastion Host is configured to connect to those instances, you must also specify details for connecting to the Bastion host. Note: The Bastion Connection link is enabled only for the SSH connection type. Click SAVE . A new connection appears under the Connection List section. (Optional) Share the connection with one or more groups.","title":"Create a connection"},{"location":"deploy/using_copy/#share-a-connection","text":"Deploy Accelerator enables you to share your connection with other users. The ability to share connections is useful when multiple users in a group need to use the same connection for the instances that they are deploying. These users can view, edit, or delete connections that are shared with them based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top-right corner. Click Connections . On the Connection List page, click the Share icon ( ) for the connection that you want to share with other users. Note: You can identify the connections that are shared with you based on the color of the connection name. In the Share Connection: ConnectionName window, click the icon for the Group with which you want to share the connection. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the connection, select the Warning check box and click SUBMIT . If you no longer want to share the connection, click CANCEL .","title":"Share a connection"},{"location":"deploy/using_copy/#creating-environments","text":"The process to create an environment includes multiple steps, as shown in the following image. Create an environment Add resources Add packages Configure the environment To create a layered environment , you must perform these steps for each environment that is a part of the layered environment. For more information, see Creating layered environments .","title":"Creating environments"},{"location":"deploy/using_copy/#before-you-begin","text":"Before you create an environment, ensure that you have performed the following actions: Configured the appropriate cloud account (for example, AWS account) in which you want to deploy an environment. For more information, see Configure providers . Created the connections that Deploy Accelerator can use to connect to the instances in your deployed environment. For more information, see Configure connections .","title":"Before you begin"},{"location":"deploy/using_copy/#create-an-environment","text":"Create an environment > Add resources > Add packages > Configure the environment On the canvas, click the Create Environment icon ( ). In the Create Environment window, enter a unique name and description for the environment that you want to create. Enter a version for the environment. By default, 01.00.00 is specified as the version for a new environment. If required, you can specify a different version. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Environment Package Type list, select the package type that you want to use for this environment. Under the Packages tab, the Private Packages list is populated depending on the Environment Package Type selected for creating the environment. Note: Deploy Accelerator supports the Chef Solo, Chef Server, Ansible Solo, and Puppet package types. However, the options that are shown in the Environment Package Type list are based on the configuration by your administrator . From the Connection list, select the default connection for all instances in this environment. You can identify shared connections in the list based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the appropriate account for accessing the cloud provider that you want to use to deploy the environment. You can identify shared providers in the list based on the color of the provider name. (Optional) From the AWS Region list, select the appropriate AWS region where you want to deploy the environment. Note: If you select an AWS Region while creating an environment, any predefined region in the Provider is overwritten with the new AWS Region. (Only if Chef Server is selected as the environment package type) To associate a Chef Server with the environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share the environment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the same groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections .","title":"Create an environment"},{"location":"deploy/using_copy/#add-resources-to-the-environment","text":"Create an environment > Add resources > Add packages > Configure the environment Resources are an important element of environments in Deploy Accelerator. They represent entities that you require to set up the infrastructure, such as an instance, Internet gateway, VPN connection, and Security group. You can also add Terraform data sources, Terraform modules , and Terraform Random Provider resources in your environment. You can add resources and data sources to the environment by using either the resource attributes form or the HCL editor . Notes: After you start a deployment of the environment , you can view various attributes of a resource for that deployment, such as its public IP address, on the Resource Details tab in the right panel. You can also add resources to an environment version after it has been deployed. These new resources get created when you start a new deployment or redeploy an existing deployment. However, no changes can be made to a released environment version. If you rearrange a resource on the canvas, you must save the environment to retain the changed position of that resource.","title":"Add resources to the environment"},{"location":"deploy/using_copy/#add-resources-and-data-sources-to-the-environment","text":"In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name and click CREATE . Click the resource or data source that you have created. In the right panel that opens, on the Resource tab, enter the required details. Consider the following points while configuring the resource attributes. Goal Action Configure the connection and key pair that Deploy Accelerator must use to connect to a non-Windows instance. In the Connection attribute, select the appropriate SSH connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the key_name attribute, specify the corresponding key pair that Deploy Accelerator must associate with the instance when it is launched. Based on your requirements, you can also specify multiple key pairs. Note: To use a key pair that is dynamically generated while the environment is being deployed, you can select Use Custom Connection . The procedure to dynamically generate and associate a key pair with an instance includes multiple steps. For more information, see Configure the dynamic generation of a key pair . Configure the connection and user data that Deploy Accelerator must use to connect to a Windows instance In the Connection attribute, select the appropriate WinRM connection to connect to an instance. By default, the connection that is specified while creating this environment is associated with the instance. In the user_data attribute, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. (See sample PowerShell script ). Important: You must ensure that the security group associated with the instance allows inbound traffic (ingress) from the port that you specify in the PowerShell script. Create multiple resources based on the same configuration In the Count attribute, specify the number of identical resources that you want to create while deploying the environment. Create a dependency on another resource in this environment In the Depends On attribute, specify the name of the resource on which you want to create a dependency. Use a variable that is defined in an Input Variable resource in this environment In any attribute, use the appropriate interpolation syntax based on the type of input variable that you are referring. For more information, see Formats for using input variables in resources . Use a local value that is defined in a locals resource in this environment. In any attribute, use the following interpolation syntax to refer to a local variable that is defined in the locals resource. ${ localResourceName . variableName } A local value can be a simple constant or an expression that can be defined once and used multiple times in different resources within the same environment. For more information, see the Terraform documentation . Reference attributes of another resource in the same environment For information about the supported attributes, select the resource whose attributes you want to reference and then click the resource type link at the top of the Resource tab in the right panel. Use the following interpolation syntax: ${ ResourceName . Attribute } Reference attributes of a data source in the same environment Use the following interpolation syntax: ${ DataSourceName . Attribute } Set array for attributes of the Text type While entering array values in attributes, specify each value on a separate line. The following image shows an example of array values for the vpc_security_groups_ids attribute of an AWS Instance : Apply Flatten flag for string array in JSON, or JSON array of the resources. To apply a Flatten flag on string arrays or nested string arrays in a JSON, or JSON array, select the Flatten toggle switch. The Flatten function is for replacing a nested string array list with a single flattened array. The array with the Flatten flag, returns a flattened sequence while migrating or importing the blueprint. Example -- JSON and JSON array -- String array Add a timeout for creating, reading, updating, or deleting resources. Use the Timeouts attribute for adding a timeout for creating, reading, updating, or deleting resources. You can define timeout for the following actions: - Create : To add a timeout while creating resources. - Update : To add a timeout while updating resources. - Read : To add a timeout while reading resources. - Delete : To add a timeout while deleting resources. The timeout is added in the following JSON format: To check the actions supported by a resource, click the Timeouts attribute in resource attributes form to open its relevant Hashicorp resource documentation. Sample PowerShell script for a Windows instance While creating a Windows instance, you must select a WinRM connection. In the user_data attribute of the instance, you must specify a PowerShell script to enable Windows Remote Management (WinRM) on the instance and set the user and password. The following is an example of the PowerShell script that can be specified in the user_data attribute. < powershell > if ( [Environment] :: OSVersion . Version -ge ( new-object 'Version' 6 , 1 )) { New-NetFirewallRule -DisplayName \"Allow WinRM\" -Direction Inbound -Action Allow -Protocol TCP -EdgeTraversalPolicy Allow -LocalPort 5985 } else { netsh advfirewall firewall add rule name = \"Allow WinRM\" dir = in protocol = TCP localport = 5985 action = allow remoteip = any localip = any profile = any } winrm set winrm / config / service / auth '@{Basic=\"true\"}' winrm set winrm / config / service '@{AllowUnencrypted=\"true\"}' $admin = [adsi] ( \"WinNT://./administrator, user\" ) $admin . psbase . invoke ( \"SetPassword\" , \"password@123\" ) </ powershell > To add other resources or data sources, repeat steps 2 to 5. (Optional) Copy an existing resource from either the same environment or a different environment. (Optional) To remove an extra resource or data source from the canvas, click the x icon on that resource. (Optional) To view the links that are created when a resource uses the Depends On attribute or the interpolation syntax to reference another resource, click the Links icon ( ). Click the Save icon ( ). (Optional) To switch from resource attributes form to HCL code, select Switch to Hashicorp Configuration Language . The switch confirmation dialog box appears. Switching from attributes form to HCL code will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the HCL code, see Add resources and data sources using Hashicorp Configuration Language .","title":"Add resources and data sources to the environment"},{"location":"deploy/using_copy/#add-resources-and-data-sources-using-hashicorp-configuration-language","text":"In the new environment that you have created, from the left panel, click the Resources tab. The Resources tab contains the following sections: The Resources section displays many resources that you can add to your environment. You can also use the additional Deploy Accelerator resources to address specific scenarios . The Data Sources section displays many Terraform data sources that you can add to your environment. For more information about data sources, see the HashiCorp Terraform documentation . From the Resources tab, drag a resource or data source to the canvas. You can identify a data source in your environment based on the color of the data source name, as shown in the following example: In the Resource Name window, enter a unique name, turn on the Switch to Hashicorp Configuration Language toggle switch, and click CREATE . Click the resource or data source that you have created. In the right panel that opens, you will see an HCL Editor on the Resource tab. In the HCL Editor, insert the HCL code for your resource. Click the Save icon ( ). ( Optional ) To switch the resource definition from HCL code to the default resource attributes form, turn off the Switch to Hashicorp Configuration Language toggle. The switch confirmation dialog box appears. Switching from HCL code to attributes form will delete previous configurations defined for the resources. To continue with the switch, select the checkbox, and click SWITCH . For information about using the default resource attributes form, see Add resources and data sources to the environment .","title":"Add resources and data sources using Hashicorp Configuration Language"},{"location":"deploy/using_copy/#add-terraform-modules-to-the-environment","text":"Deploy Accelerator allows you to use an existing Terraform module in the environment. Terraform defines a module as a container for multiple resources that are used together . For more information about creating modules, see the Terraform documentation . To use an existing Terraform module, you can add the Terraform Module resource to the environment and specify the source from where the module can be downloaded. To add a Terraform module to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the Terraform Module resource to the canvas. In the Resource Name window, enter a unique name and click CREATE . Click the Terraform Module resource that you have created. In the right panel that opens, on the Resource tab, enter the following details: source: Enter the source from where to download the Terraform Module. Deploy Accelerator supports the following sources for Terraform Modules. Source type Supported format Terraform public registry <NAMESPACE>/<NAME>/<PROVIDER><br> Example: terraform-aws-modules/vpc/aws GitHub repository (public) github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION><br> Example: github.com/terraform-aws-modules/terraform-aws-vpc.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. GitHub repository (private) git::https://<USERNAME>:<PASSWORD>@github.com/<REPOSITORY-CLONE-URL>?ref=<VERSION> Example: git::https://sampleuser:samplepassword@github.com/sample-demo/terraform-module.git?ref=v1.66.0 Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module. Artifactory ( configured in Deploy Accelerator ) artifactory::<OBJECT-RELATIVE-PATH> Example: artifactory::local-demo-sample/module/aws_vpc.zip Note: The Artifactory URL that is configured in the dnow.properties file is prefixed to the path that you specify in the source attribute. input: Input parameters for the Terraform Module. You can use the interpolation syntax to reference the output of other resources in the same environment or a parent environment as input to the Terraform module. tags: Tags that you want to assign to all resources in the Terraform Module. However, ensure that the Terraform Module supports tags. version: Version of the Terraform Module to download. Use this attribute when you specify a Terraform public registry in the source attribute. Note: If you do not specify the version, Deploy Accelerator downloads the latest version of the Terraform Module from the Terraform public registry. Click the Save icon ( ). Note: You can use the interpolation syntax to reference the output of this Terraform module in other resources within the same environment or in child environments.","title":"Add Terraform modules to the environment"},{"location":"deploy/using_copy/#add-terraform-random-provider-resources-to-the-environment","text":"Deploy Accelerator allows you to use Terraform Random Provider resources in the environment. These resources are always available on the Resources tab irrespective of the provider type that is selected for the environment. Terraform defines the Random Provider as a logical provider that allows the use of randomness within Terraform configurations...it provides resources that generate random values during their creation and then hold those values steady until the inputs are changed. For more information about the Random Provider, see the Terraform documentation . To add a Random Provider resource to the environment, perform the following actions: In the new environment that you have created, from the left panel, click the Resources tab. From the Resources tab, drag the appropriate Random Provider resource to the canvas. A few examples of the Random Provider resources are Random Id , Random Password , and Random String . In the Resource Name window, enter a unique name and click CREATE . Click the Random resource that you have created. In the right panel that opens, on the Resource tab, enter the required details. For more information about the resource attributes, click the resource type link at the top of the Resource tab, as shown in the following image. Click the Save icon ( ).","title":"Add Terraform Random Provider resources to the environment"},{"location":"deploy/using_copy/#add-packages-to-the-resources","text":"Create an environment > Add resources > Add packages > Configure the environment Packages in Deploy Accelerator are entities that use supported infrastructure automation tools to configure compute resources in an environment. Chef Solo and Chef Server are configured by default in Deploy Accelerator. However, your administrator might have also configured the use of Ansible and Puppet packages. Each package references a Chef cookbook, Ansible playbook, or Puppet manifest file or module. You can add user-defined packages to install your own custom applications on compute resources. Important: Deploy Accelerator cannot deploy packages on resources that are already deployed. Therefore, you must add the appropriate packages to a resource before you start a new deployment of the environment. Best practice: If you need to frequently deploy new versions of a package on a resource, you can create a layered environment . The parent environment can contain all your infrastructure resources, while the child environment can contain the Virtual VM resource to which you can add the appropriate package version. Each time you update the package version on the Virtual VM resource, you can destroy and redeploy the child environment--without having to destroy the parent environment that contains the infrastructure. To add packages to the resources, perform the following actions: In the new environment that you have created, from the left panel, click the Packages tab, and drag the appropriate package to a compute resource (for example, an EC2 Instance resource for AWS) on the canvas. The Packages tab displays packages of only the environment package type (Chef Solo, Chef Server, Ansible Solo or Puppet) that you have selected. If you have selected Chef Server as the environment package type, the Packages tab also displays roles from the selected Chef Server. You can add both Chef Server packages and roles to a compute resource. The following utility packages are always shown irrespective of the environment package type that is selected. Utility package Details file Uploads a file on the resource to which it is added. execute-script Runs a script on the resource to which it is added. You can specify whether the script must be run after the resource is created or before it is destroyed. local-exec Runs a command on the instance on which Deploy Accelerator is deployed. You can specify whether the command must be run after the environment is deployed or before it is destroyed. The local-exec package can be added to any resource in the environment. chef_configuration Restricts Deploy Accelerator from installing the ChefDK client on instances before deploying packages. You must set the skip_install attribute of this package to true . Note: This package is available only in environments for which Chef Server is selected as the environment package type. Click the package or role. In the right panel, on the PACKAGES tab, select the package or role that you have added to the resource. (Optional) From the Package Version list, select another version of the package or role. By default, the latest released version of the package or role is selected. Based on your requirements, you can select another version of the package or role. Ideally, you should add an unreleased version of a package or role to an environment only if you want to test that version. (Chef Solo and Chef Server packages only) From the Recipes list, select one or more recipes that you want to run on the resource. All recipes that the selected cookbook (or package) contains appear in the list. You must select the recipes in the order in which they must be run on the resource. If you do not select any recipe, Deploy Accelerator runs the default recipe that is defined in the cookbook. Enter other details about the package or role. Note: The details that you must specify for each package are different. For example, in case of the execute-script package, you can specify whether the script must be run after the resource is created or before it is destroyed by selecting the appropriate option from the when list. (Optional) To drag additional packages or roles to the resource, repeat steps 1 to 6. If you add multiple packages and roles to a resource, the packages and roles are arranged from top to bottom by default. However, you can drag and rearrange the packages and roles in the order in which you want to run them on the resource. Note: The utility packages are always run on the resource before the other selected packages and roles. However, if you add multiple utility packages to a resource, you must arrange these packages in the order in which you want to run them on the resource. To add packages or roles to other resources, repeat steps 3 to 6. (Optional) To remove a package or role from a resource, click the x icon on that package or role. Click the Save icon ( ).","title":"Add packages to the resources"},{"location":"deploy/using_copy/#configure-the-environment","text":"Create an environment > Add resources > Add packages > Configure the environment Based on your requirements, you can configure additional settings for the environment. This step is optional in the end-to-end process of creating environments. In the new environment that you have created, click the Configure icon ( ). Based on your requirements, perform the following actions in the Configuration window: On the Environment tab, edit the following details for the selected environment version: Edit the description of the environment. Edit the connection, provider, or AWS Region for the environment. For information about configuring connections and providers, see Configure connection and Configure provider . To assign a custom tag to all resources in the environment, select that custom tag from the Custom Tag list. To view the key-pair values in the selected custom tag, move your cursor on the icon. For information about creating or editing a custom tag, see <a href=\"\" ui-sref=\"rean-platform-docs.accelerator({viewAccelerator: 'rean-deploy', viewPage: 'deploy-and-manage-environments', viewSection: 'tags'})\" style=\"text-decoration:none\">Configuring custom tags</a>. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that is selected for the environment. On the Deployment tab, configure the following details that are applied by default to all deployments of this environment version: Under Deploy , specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. By default, Deploy Accelerator does not destroy any deployment. On the Notification tab, specify the email notification details that are applied to all deployments of this environment version: Email: By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts or destroys a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. Deploy Template: Deploy Accelerator uses the default template for sending an email when a deployment is started. However, you can specify your own custom template. Destroy Template: Deploy Accelerator uses the default template for sending an email when a deployment is destroyed. However, you can specify your own custom template. Deployment Initiation: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users as soon as they start new deployments or destroy existing deployments of this environment. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Deployment Complete: Deploy Accelerator uses this setting to determine whether or not email notifications must be sent to users after new deployments of this environment either succeed or fail, and existing deployments are successfully destroyed. To set environment-level preferences, select YES or NO . If you select DEFAULT , Deploy Accelerator considers the user-level preferences of the users. If none are set, Deploy Accelerator considers the application-level preferences . Note: Contact your system administrator for a list of custom email templates that might be available. Click SAVE .","title":"Configure the environment"},{"location":"deploy/using_copy/#where-to-go-next","text":"When you are ready to create the infrastructure that you have designed in your environment, you can start a new deployment .","title":"Where to go next"},{"location":"deploy/using_copy/#creating-a-layered-environment","text":"Layered environments in Deploy Accelerator are a collection of environments in which some environments (child) are dependent on other environments (parent). You can create layered environments for complex IT infrastructures. The implementation of layered environments allows you to easily update, deploy, and destroy a set of resources (or a layer) without affecting resources in other environments (or layers). Therefore, it is recommended that you create layered environments, instead of standalone environments, to deploy applications. To create layered environments, you require the following two resources: Output: You must add this resource in the parent environment. This resource enables you to expose some resource attribute values from the parent environment in the form of variables. These variables can then be referenced in other dependent (child) environments. Depends On: You must add this resource in the child environment. This resource enables you to create a dependency on another environment. Based on your requirements, you can add multiple Depends On resources in a child environment. To create a layered environment, perform the following actions: To create the parent environment, perform the following actions: Create an environment . Add the required resources in the environment . To reference attributes of the parent environment in one or more child environments, add the Output resource. In the Output resource, create a JSON file and specify variables that can be used in child environments. The value of these output variables can use the interpolation syntax to reference attributes of other resources in the parent environment. If you have added the Input Variable resource, the output variables can also reference the input variables. Save the environment. To create the child environment, perform the following actions: Create an environment . Add the Depends On resource in the environment. The name of the Depends On resource should ideally indicate the environment on which there is a dependency (for example: dependson_network ). In the reference_type attribute of the Depends On resource, select Environment Name . In the Depends_On attribute, select the appropriate parent environment version. Note: You can also select S3 in the reference_type attribute of the Depends On resource. For more information, see Deploy Accelerator resources . Add other required resources in the child environment . To reference attribute values of resources in the parent environment, use the following interpolation syntax: Syntax: ${ DependsOnResourceName.OutputVariableName } Example: ${ dependson_network.subnet } Save the environment. To confirm that the dependency on the parent environment, click the Environment Dependency View icon ( ).","title":"Creating a layered environment"},{"location":"deploy/using_copy/#creating-new-environments-from-blueprints","text":"Blueprints are templates of commonly used IT infrastructures. Deploy Accelerator users can export their environments as blueprints and add them in the Blueprint Gallery . You can leverage blueprints to quickly create infrastructure for solutions. Deploy Accelerator allows you to create new environments from blueprints by using any of the following methods: Import blueprints from the Blueprint Gallery Import blueprints from the Artifactory by using an API Import blueprints from your computer Important: If you create a layered environment from a blueprint, you must deploy the environments in a particular order. You must first deploy the environment in the top layer, then the environment in the next layer, and so on until the environment in the lowest layer. You must follow this order because child environments can be deployed only if their parent environments are already deployed.","title":"Creating new environments from blueprints"},{"location":"deploy/using_copy/#before-you-begin_1","text":"Before you create an environment from a blueprint, ensure that you have configured the appropriate providers and connections that can be used for each environment in the blueprint.","title":"Before you begin"},{"location":"deploy/using_copy/#import-blueprints-from-the-blueprint-gallery","text":"The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable and configure the Blueprint Gallery . Also, blueprints are not available out of the box and have to be added in the Blueprint Gallery . The following image shows an example of the Blueprint Gallery. To import blueprints from the Blueprint Gallery, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . (Optional) To manually refresh the Blueprint Gallery to display all latest updates, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint data from the Artifactory. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. On the Blueprint Gallery page, browse or search for a blueprint that meets your requirements. The search results include blueprints that contain the search keyword in the blueprint name, description, owner name, or owner email. Searches are not case-sensitive and use partial keyword matching. For example, if you specify layer as the search keyword, the search results include blueprints with the name NetworkLayer and MultilayerEnvironment . Note: You can also sort the blueprints based on the blueprint name or owner name. By default, blueprints are sorted based on the blueprint names in the descending order. From the version list of the blueprint that you want to import, select the appropriate version. For details about the selected blueprint version, hover over Description . To access the ReadMe file of the selected blueprint version, click README . To view the email address of the blueprint owner, hover over the owner name. However, the email address is not shown if the blueprint owner is admin . Also, the owner name and email address is shown only if these values are available in the blueprint metadata. To import the selected blueprint version, click IMPORT . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as a part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. (Optional) Update the description for the environment. By default, the description from the blueprint is shown. From the Connection list, select the appropriate connection. You can identify shared connections based on the color of the connection name. From the Provider list, select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. When you import a blueprint from the Blueprint Gallery, all environments in that blueprint are in the Released state. If input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments .","title":"Import blueprints from the Blueprint Gallery"},{"location":"deploy/using_copy/#import-blueprints-from-the-artifactory-by-using-an-api","text":"The Blueprint Gallery in Deploy Accelerator is a repository of blueprints that can be templates of industry-standard cloud or IT infrastructure architectures or custom solutions. Your administrator has to enable the Blueprint Gallery and specify a repository in the Artifactory for storing the blueprints. Based on your requirements, you can either import blueprints from the Blueprint Gallery or import blueprints from the Artifactory by using an API. The importBlueprintFromArtifactory API allows you to import blueprints from the specified repository in the Artifactory. You can specify the following parameters for importing a blueprint: blueprintName blueprintVersion environmentNamePrefix This parameter is optional. The value that you specify for this parameter is added as a prefix to the names of all environments that are imported as a part of the blueprint. groupName This parameter is optional. The group that you specify for this parameter is assigned the View and Deploy share permission for all the imported environments. For more information about the API, see the Deploy Accelerator API documentation: https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcapdeploy/api-docs/index.html#/Environment/importBlueprintFromArtifactory","title":"Import blueprints from the Artifactory by using an API"},{"location":"deploy/using_copy/#import-blueprints-from-your-computer","text":"On the canvas, click the More icon ( ) and then click Import . In the Import Environment/Blueprint window, perform the following actions: Click Choose File and then select the blueprint (JSON file) that you want to import. Click Upload . (Only if the blueprint contains multiple environments) In the Select Environments window, select the check boxes for the environments that you want to import and click START IMPORT . You can choose not to select an environment that already exists in Deploy Accelerator. This environment might have been previously imported as part of another blueprint. Note: While importing a blueprint, if you did not select a parent environment because it already exists in Deploy Accelerator, the dependency between the existing parent environment and the other selected environments in the blueprint is automatically retained. However, the name and version of the parent environment in Deploy Accelerator and the blueprint must match. In the Import Environments window, for each environment, perform the following actions: Enter a unique combination of name and version for the environment that you are importing. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). Note: If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Enter the description for the environment that you are importing. From Connection , select the appropriate connection. You can identify shared connections based on the color of the connection name. From Provider , select the provider that you want to use to deploy the environment. You can identify shared providers based on the color of the provider name. Note: The list displays providers of only the provider type that was used to create an environment in the blueprint. For example, if an environment in the blueprint was created by using an AWS provider, you can view only available AWS providers in the list. If Chef Server is set as the environment package type in the selected environment, perform the following actions: From Chef Server , select the Chef Server from which you want to display packages. From Chef Environment , select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To import all environments, click IMPORT ALL . To import all environments and share with other users, perform the following actions: Click IMPORT ALL & SHARE . In the Share Environment window, select the appropriate permission check boxes for the groups with which you want to share all environments. You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Import Environments window and modify environment details, click the Edit icon ( ). Click IMPORT ALL & SHARE . The imported environments are shared with all users who are members of the selected groups. Note: The providers and connections configured in these imported environments are not automatically shared with the same groups. You must separately share the providers and connections with the appropriate groups. After all the environments in the blueprint are successfully imported, the environment in the lowest layer of the blueprint is opened by default. All other selected environments that are a part of the blueprint appear in the list of environments. When you import a blueprint from your computer, the state of environments in that blueprint are dependent on their state when they were exported. For example, if parent and child environments in the Released state were exported as a blueprint, they are in the Released State when the blueprint is imported. Note: If there is an error in importing even one of the environments in a blueprint, the other environments in the blueprint are also not imported. If the imported environments are in a Released state and if input variables have been used for any resource attributes, you can update the values of these input variables while starting a new deployment . Alternatively, you can create a new environment version and update the values of any resource attributes. (Optional) Ensure that the environment in the lowest layer of the blueprint is opened on the canvas and click the Environment Dependency View icon ( ). This view helps you to verify that the dependency between the environments in the imported blueprint has been maintained, For more information, see Viewing dependency between environments .","title":"Import blueprints from your computer"},{"location":"deploy/using_copy/#copying-environments","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment that you want to copy. On the canvas, click the More icon ( ) and then click Copy . The environment version that is currently selected is copied. In the Create Environment window, enter a unique name, description, and version for the environment that you are creating. The environment version that you specify must not exceed 40 characters. Also, the initial part of the version must be in the xx.xx.xx format (for example, 02.10.18 ). You can choose to append this value with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated) . Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). From the Connection list, select the default connection for all instances in the environment. You can identify shared connections in the drop-down based on the color of the connection name. Note: You can choose to associate a different connection when an instance is added to the environment. The connection associated with an instance takes precedence over the default connection that is specified while creating the environment. From the Provider list, select the cloud provider that you want to use to deploy the environment. You can identify shared providers in the drop-down based on the color of the provider name. (Optional) After selecting the provider, select the appropriate AWS region where you want to deploy an environment. Note: If you select an AWS region while creating an environment, any predefined region in the provider is overwritten with the new AWS region. If Chef Server is selected as the environment package type in the environment that you are copying, perform the following actions: From the Chef Server list, select the Chef Server from which you want to display packages. From the Chef Environment list, select one of the environments that are available on the selected Chef Server. For this environment, the Packages tab in the left panel displays packages that are available in the selected Chef Server and environment. Based on your requirements, perform one of the following actions: To create the environment, click CREATE . A new environment is created and added to the list of environments. If required, you can later share this environment with other users. To create the environment and share it with other users, perform the following actions: Click CREATE & SHARE . In the Share Environment window, the share permissions of the environment that you have copied are automatically applied to this environment. (Optional) Modify the share permissions for this environment by selecting new or clearing existing permission check boxes for the appropriate groups. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. (Optional) To go back to the Create Environment window and modify environment details, click the Edit icon ( ). To create and share the environment, click CREATE & SHARE . The environment is shared with all users who are members of the selected groups. Note: The following entities related to this environment are not automatically shared with the selected groups: New versions of the environment Deployments of the environment Providers and connections configured in the environment You must separately share these entities with the appropriate groups. For more information, see sharing deployments , sharing environments , sharing providers , and sharing connections .","title":"Copying environments"},{"location":"deploy/using_copy/#starting-new-deployments","text":"Each environment can have multiple versions. When you are ready to create the infrastructure that you have designed in an environment version, you can start a new deployment. If required, each environment version can also have multiple deployments, such as Staging and Production. Important: You can deploy child environments only if their parent environments are already deployed. Therefore, in the case of a layered environment , you must deploy the environments it contains in a particular order. You must first deploy the parent environment in the top layer, then the child environment in the next layer, and so on until the child environment in the lowest layer. To start a new deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to deploy. To start a new deployment, click the Deploy icon ( ). The Review and Deploy: EnvironmentName window displays the default values that are configured for the environment. For more information, see Configure the environment . (Optional) To start a quick deployment with default values, click QUICK DEPLOY . (Optional) On the Environment tab, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for this deployment. Note: If you select an AWS region while starting a deployment, any predefined region in the provider is overwritten with the new AWS region. From the Custom Tag list, select the custom tag that you want to assign to all resources that are deployed. To view the key-pair values in the selected custom tag, hover over the icon. Note: If you have also specified key-pair values in the tags attribute of a resource in the environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for this deployment. Following additional information is displayed: Provider version: The version of the provider used for deploying the environment. Package Type: The package types used for deploying the environment. Created By: The user who created the environment. On the Deployment tab, perform the following actions based on your requirements: Under Deploy , enter the following details: Enter the deployment name and description. Note: The deployment name must be unique across all versions of the selected environment. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while deploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Parent deployments of the same name as the child deployment name are selected by default. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Specify the scripts that must be run before or after starting a deploying. Under Destroy , specify the scripts that must be run before or after destroying a deployment. Select Destroy After and then specify the duration after which Deploy Accelerator must destroy a deployment. (Optional) On the Notification tab, specify the email and template details for this deployment. By default, Deploy Accelerator sends email notifications to the registered email address of the user who starts a deployment. However, you can specify an additional email address to which all emails notifications must also be sent. If required, you can specify custom templates for sending an email when a deployment is started or destroyed. Contact your system administrator for a list of custom email templates that might be available. Note: You cannot customize the Deployment Initiation and Deployment Complete settings for a deployment. These settings are configured at the environment level . To start a new deployment, click START NEW DEPLOYMENT . The icon appears on resources that are successfully deployed. The icon appears on resources that could not be successfully deployed. (Optional) To view the resource logs, click the Logs icon ( ). (Optional) To stop a deployment that is in progress, perform the following actions: Click the Stop icon next to the Deploying status. In the confirmation box, click YES STOP IT . The status of the deployment changes to Stopping . Deploy Accelerator deploys all resources in progress and then stops the deployment. The status of the deployment changes to Stopped . Note: If Deploy Accelerator is unable to successfully deploy any of the resources in progress, the status changes to Failed instead of Stopped .","title":"Starting new deployments"},{"location":"deploy/using_copy/#viewing-deployments","text":"Each environment version can have multiple deployments, such as Staging and Production. You can view the list of your deployments for an environment across all versions. You can also view the deployments that other users have shared with you. The deployment list also displays the status of each deployment -- deployed ( ) or failed ( ). On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of your own and shared deployments across all versions of the environment, click the deployment list on the canvas. Note: You can identify the deployments that are shared with you based on the color of the deployment name. In the list of deployments, click the deployment name that you want to open on the canvas. To view additional information related to the deployment after it is complete, select Deployment Details on the Canvas. The Deployment Details displays information about the following parameters: Deployment ID: The ID for deploying the environment. Deployment run ID: The run ID for querying the database for troubleshooting. Deployment Owner: User who had run the environment deployment. Deployment Input Variables: Additional deployment variables.","title":"Viewing deployments"},{"location":"deploy/using_copy/#viewing-the-plan-for-deployments","text":"Before you start a new deployment or redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list, select the appropriate environment version. To view the plan for a deployment, perform one of the following actions: Goal Action To view the plan for a new deployment of the environment. Click the Plan icon ( ) and select Plan Environment . To view the plan for an existing deployment of the environment. Click the Plan icon ( ) and select Plan Selected Deployment . Note: You can view the plan for an existing deployment that is shared with you only if your group has been given the Redeploy permission. (Optional) To view the plan with default values, click QUICK PLAN . (Only to view the plan for a new deployment) In the Review and Plan: EnvironmentName window, perform the following actions based on your requirements: Edit the connection , provider , and AWS region that must be used for the plan. The connection, provider, and AWS region that is configured for the environment version is shown by default. Note: If you select an AWS region for the plan, any predefined region in the provider is overwritten with the new AWS region. To plan the deployment with a different environment version, from the Environment Version list, select that version. In Input Variables , add input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while planning is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. (Only if Chef Server is selected as the environment package type) Edit the Chef Server or Chef Environment that must be used for the plan. (Only to view the plan for an existing deployment) In the Review and Plan: EnvironmentName window, update the environment version, input variables, and parent deployments mapping based on your requirements. Click PLAN . Note: In the case of a layered environment , the plan for child deployments is available only if the parent deployments are available.","title":"Viewing the plan for deployments"},{"location":"deploy/using_copy/#redeploying-existing-deployments","text":"You can redeploy an existing deployment with the same environment version. If required, you can also redeploy an existing deployment with a different environment version. In both cases, only the changes made in the selected environment version are implemented. Deployed resources that have not been updated in the selected environment version are not impacted. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployments list on the canvas. From the deployments list, click the deployment name that you want to redeploy. Click the Re-Deploy icon ( ) to open the Review and Deploy window. Note: The Re-Deploy icon is available only if the environment version has been previously deployed. (Optional) On the Environment tab, perform the following actions based on your requirements: From the Connection list, select the connection that you want to use to connect to all resources in the environment. From the Custom Tag list, select the custom tag that you want to assign to all resources in the environment. To view the key-pair values in the selected custom tag, hover over the icon. (Optional) On the Deployment tab, perform the following actions based on your requirements: To redeploy with a different environment version, from the Environment Version list, select that version. By default, the environment version that was originally deployed is selected. In Input Variables , add or update the input variables based on your requirements. These input variables are additional to the input variables that are already defined in the Input Variables resource in the selected environment version. If the same input variable is defined in both locations, the value specified while redeploying is given precedence. For information about the different ways in which you can define input variables, see Formats for defining input variables . Under Parent Deployments Mapping , select a parent deployment for each Depends On resource from the corresponding list. Each list displays deployments of only the parent environment version that is selected in the Depends On resource. Note: The Parent Deployment Mapping section is visible only if the environment has at least one Depends On resource. Select the Confirmation check box. Click Upgrade . If you have redeployed with a different environment version, the deployment is now associated with that version.","title":"Redeploying existing deployments"},{"location":"deploy/using_copy/#sharing-deployments","text":"Deploy Accelerator enables you to collaborate with other users on your deployments of an environment. The ability to share deployments is useful when multiple users need to view or manage the same deployment. All these users can view, redeploy, destroy, or stop the deployment based on the permissions assigned to their group. Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for the deployments. To share a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment whose deployment you want to share. Important: Before you share deployments of an environment version with selected groups, ensure that the environment version is also shared with those groups. Otherwise, users cannot view the shared deployments. From the deployments list, click the deployment name that you want to share. Click the Share icon ( ) and select Share Selected Deployment . You can share a deployment when it is in the Deploying , Deployed , Stopping , or Failed state. In the Share Deployment window, perform the following actions: Click the Share icon ( ) for the Group with which you want to share the deployment. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign permissions to users of the selected group, select the appropriate permission check boxes and click DONE . Note: You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the deployment, select the Warning check box and click SUBMIT . The deployment is shared with all users who are members of the selected groups If you no longer want to share the deployment, click CANCEL . (Optional) To enable users to redeploy the shared deployment by using the provider that is configured in that deployment, share the provider with the same groups. (Optional) To enable users to redeploy the shared deployment by using the connections that are configured in that deployment, share the connections with the same groups. Note: In case of layered environments, ensure that you share both parent and child deployments with the same groups. You must assign at least the View permission for the parent deployments. Otherwise, users cannot redeploy the child deployment.","title":"Sharing deployments"},{"location":"deploy/using_copy/#destroying-deployments","text":"Each environment version can have multiple associated deployments, such as Staging and Production. You can destroy a deployment that is no longer required. This action ensures that you are not paying for resources that are not being used. Destroying a deployment deletes all the deployed resources that it contains. However, the environment version and its other deployments continue to be available. Typically, you destroy test, development, or other non-production deployments. Note: You can destroy parent deployments only if their child deployments are already destroyed. Therefore, in the case of a layered environment , you must destroy the deployments in a particular order. You must first destroy the child deployment in the lowest layer, then the deployment in the next layer, and so on until the parent deployment in the top layer. To destroy a deployment, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. The deployment name tab on the canvas displays the latest deployment of the selected environment version. To view the list of deployments across all environment versions, click the deployment list on the canvas. From the deployments list, click the deployment name that you want to destroy. Click the Destroy icon ( ). Note: The Destroy icon is available only if the environment version has been previously deployed. In the confirmation message box, type Yes and then click SUBMIT . After the deployment is destroyed, an email is sent to your registered email address and to the additional email address that might be configured for the deployment. (Optional) To view the resource logs, click the Logs icon ( ). The Logs icon continues to be available until the deployment is successfully destroyed.","title":"Destroying deployments"},{"location":"deploy/using_copy/#releasing-environment-versions","text":"After an environment version is finalized for production, you can choose to release that version. No changes can be made to a released version. On the Home page, click the More options icon ( ) in the top-right corner, and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the environment version that you want to release. From the version list, select Release Version . In the confirmation message box, enter Yes and then click SUBMIT . You can no longer make any changes to the released version of an environment. However, you can start new deployments or redeploy existing deployments with this released version.","title":"Releasing environment versions"},{"location":"deploy/using_copy/#creating-new-environment-versions","text":"Each environment can have multiple versions. After an environment version is released, no changes can be made to that version. To add or update resources to the environment, you must create a new environment version. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. From the version list on the canvas, select the version on which you want to base the new environment version. From the version list, select Create New Version . In the Create New Version From base version window, enter a unique version up to a maximum of 40 characters. The new version must be greater than the base version that you have selected. Alternatively, you can retain the base version and append it with a dash (-) followed by a custom alphanumeric value (for example, 02.10.18-updated ). Ensure that there is no space before the dash (-) and the custom value that you specify does not contain a colon (:). (Optional) To not apply share permissions from the base environment to the new environment version, clear the Use share permission from base environment check box. By default, share permissions configured for the base environment are applied to the new environment version. Click CREATE . A new version of the environment appears on the canvas. This version also appears on the Environment List page and in the Search Environments box. (Optional) In the new environment version, add or modify resources and packages based on your requirements. (Optional) Update the description and configure additional settings for the environment version based on your requirements. When you are ready to create the infrastructure that you have designed in the new environment version, you can start a new deployment or redeploy an existing deployment with this version.","title":"Creating new environment versions"},{"location":"deploy/using_copy/#upgrading-the-provider-version-of-environments","text":"Deploy Accelerator uses Terraform providers to create, manage, and update resources in AWS, Azure, Google Cloud Platform, and other supported providers. When you create a new environment or a new version of an existing environment, resources are created based on the most recent provider version that is supported in Deploy Accelerator. When you open an environment whose provider version has been automatically upgraded, the Provider Version Upgraded Changes window appears. For each resource in the environment, it lists the attributes that are deprecated or cannot be upgraded. The resources that require user action are marked in red. The resources for which Deploy Accelerator has automatically performed the changes are marked in green. To acknowledge the upgrade changes, click the Confirmation checkbox, and click CONFIRM. However, the list of provider upgrade changes will not be shown after you submit your acknowledgement. To retain the list of provider upgrade changes for some time, you can click CLOSE. To reopen the window, click Provider upgrade changes on the canvas. Important : If the upgrade changes are not acknowledged, the Provider Version Upgraded Changes window appears while deploying the environment. If the upgrade changes are acknowledged but the manual changes are not made to the resources, you will see an error in the deployment log file.","title":"Upgrading the provider version of environments"},{"location":"deploy/using_copy/#viewing-dependency-between-environments","text":"The Environment Dependency View enables you to view the dependency of an environment on other environments (parent environments). You can also view other environments (child environments) that are dependent on an environment. If the environment has deployments, this view displays the parent or child deployments of the selected deployment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment. If the selected environment has deployments, the deployment list on the canvas displays the latest deployment of the selected environment version. Click the Environment Dependency View icon ( ). The Environment Dependency View window appears, as shown in the following image: If the environment has a deployment, the Environment Dependency window displays the connection between the selected deployment and its parent or child deployments. The color of the deployment represents its current state. For example, black indicates Not Started, orange indicates Running, green indicates Deployed, and red indicates Failed. Click the toggle switch to see the dependent parent and child environment(s). If the toggle switch is towards Child, the Environment Dependency window displays the connection between the current environment and its child environment(s). If the toggle switch is towards Parent, the Environment Dependency window displays the connection between the current environment and its parent environment(s). (Optional) To switch to another environment or deployment shown in the Environment Dependency View window, select that environment or deployment name.","title":"Viewing dependency between environments"},{"location":"deploy/using_copy/#configuring-custom-tags","text":"You can define tags for individual resources in an environment but this process is repetitive and time consuming. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can create a custom tag, which contains a set of AWS key-pair tags, and attach the custom tag to an environment. Deploy Accelerator attaches this custom tag to all resources in the environment. For example, to control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. To configure a custom tag, perform the following actions: On the Home page, click the More options icon ( ) in the top-right corner and then click Custom Tags . Under Custom Tag , click NEW . Enter the custom tag name and description. Under Tags , define the tags that you want to assign to all resources in the environment. The following image shows an example of how you must specify tags for your resources. Note: Tag values for all the resources that have tag support in your environment are set to the default values from the tags defined in the selected custom tag. Click SAVE . A new custom tag appears under the Custom Tag List section. Note: You can also add tags to a resource by setting the tags attribute in the RESOURCE panel. If users select a custom tag for an environment and specify values in the tags attribute of a resource in that environment, Deploy Accelerator attaches both the custom tag and resource-level tag values to the resource. However, if the same tag is defined in both locations, precedence is given to the resource-level tag value.","title":"Configuring custom tags"},{"location":"deploy/using_copy/#comparing-differences-between-environments","text":"Deploy Accelerator enables you to compare differences between two environments or two versions of the same environment. Comparing two versions of the same environment helps you to track changes to an environment over time. Comparing environments is also useful when you have two similar environments and you want to retain only one of them. You can compare the differences between these environments, ensure that one of the environments has all the required resources and packages, and then delete the other environment. The comparison detects the following differences between the base and target environments. Comparison Details Environment configuration-level comparison This comparison displays a list of configuration details that have changed in the target environment version: -- Connection, provider, region, and environment package type -- Chef server and Chef environment -- Environment owner -- Custom tag -- Released status -- Notification email address and email templates -- Deploy and Destroy pre-scripts, post-scripts, and destroy after interval Resource-level comparison This comparison displays a list of resources that are added, edited, or removed in the target environment version. For each edited resource, you can see a list of attributes and packages that are added, edited, or removed. If a package has been edited, you can also see the differences in the package attributes. To compare differences between environments, perform the following actions: On the canvas, click the More icon ( ) and then click Compare . The environment version that is currently selected is considered as the Base Environment for comparison. (Optional) In the Compare Resources and Packages window, from the Base Environment and Version lists, select a different environment and environment version. You can identify environment versions that are shared with you based on the color of the environment version. From the Target Environment and Version lists, select the environment and environment version with which you want compare the base environment. The RESOURCES tab is selected by default. In the Resources panel, you can see a list of resources that are added, edited, or removed in the target environment. To help you identify the changes, the resources are highlighted in different colors, as shown in the following table. Highlighted color Meaning Green Added resource Red Deleted resource Blue Edited resource To see the resource-level comparison, click an edited resource in the Resources panel. On the RESOURCE ATTRIBUTES and PACKAGES tabs, you can see the differences in attribute values and packages. Green background color indicates added content while red background color indicates deleted content. To see the environment configuration-level comparison, click the CONFIGURATION tab. On the ATTRIBUTES tab, you can see a list of configuration details that have changed in the target environment. Green background color indicates added content while red background color indicates deleted content. Click CLOSE .","title":"Comparing differences between environments"},{"location":"deploy/using_copy/#searching-environments","text":"On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, in the Search Environment tab, enter a search string to find an environment. The search string filters the environments and displays results with the string. The search string filters environments across all columns on the Environment List page.","title":"Searching environments"},{"location":"deploy/using_copy/#renaming-environments","text":"On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to rename. On the canvas, click the More icon ( ) and then click Rename . In the Rename Environment window, enter a unique name for the environment and click SUBMIT . All versions of the environment are renamed. You can see the updated environment name in the Search Environment box, Environment List page, and all other instances. The environment name is also updated in any child environments that reference this environment. Note: You can rename an environment that has been shared with you only if the root environment version is shared with the Edit permission. Otherwise an error message is shown when you click SUBMIT .","title":"Renaming environments"},{"location":"deploy/using_copy/#sharing-environments","text":"Deploy Accelerator enables you to collaborate with other users on an environment. The ability to share environments is useful when multiple users need permissions to perform different tasks such as edit, deploy, destroy, delete, view, and export on the same environment. All these users can work and collaborate on a shared environment at the same time. Users can perform one or more tasks based on the permissions assigned to their group. On the Home page, click the More options icon ( ) in the top right corner and then click Environments . On the Environment List page, click the environment that you want to share with other users. Click the Share icon ( ) and select Share Environment . In the Share Environment window, perform the following actions: (Optional) To share all existing versions of the environment with other users, select Share All Versions . By default, only the currently selected environment version is shared with other users. Note: If you later create a new version of this environment , you must separately share that environment version with users. While creating the new version, you can choose to apply the share permissions from the base environment. Click the icon for the Group with which you want to share the environment version. Note: You can view only the groups to which you belong. Only Cloud Accelerator Platform administrators can view all groups. To assign share permissions to users, select the appropriate permission check boxes and click DONE . You can select one or more permission check boxes for each group of users. Read the warning message and perform one of the following actions: If you want to proceed with sharing the environment, select the Warning check box and click SUBMIT . The environment version is shared with all users who are members of the selected groups. When you share an environment version, its deployments are not automatically shared with the specified groups. You must separately share each deployment with the appropriate groups and assign the required permissions. If you no longer want to share the environment, click CANCEL . Important: If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the specified groups. The permissions assigned for these deployments are based on the assigned permissions for the environment version. Based on your requirements, you can manually update the sharing permissions for these deployments. (Optional) To enable users to start a deployment of the shared environment version by using the provider that is configured in that version, share the provider with the same groups. (Optional) To enable users to successfully use the connections that are specified in the shared environment version, share the connections with the same groups.","title":"Sharing environments"},{"location":"deploy/using_copy/#deleting-environments","text":"You can choose to delete environments that you no longer need. If an environment has multiple versions, each version must be separately deleted. Deleting an environment version also permanently deletes all configurations related to that version. On the Home page, in the Search Environment box, enter the name of the environment that you want to delete. If the environment has multiple versions, each version is listed separately in the Search Environment box. You can identify environments that have been shared with you based on the color of the environment name. Next to the environment version that you want to delete, click the Delete icon ( ). In the confirmation message box, click YES . The environment version is no longer shown in the list of environments. Note: You cannot delete environment versions that have deployments. Similarly, you cannot delete an environment version that is being used as a parent in one or more child environments.","title":"Deleting environments"},{"location":"deploy/using_copy/#restoring-deleted-environments","text":"You can restore a deleted environment which was deleted by you, or the environments for which you have the Delete permission. On the Home page, click the More options ( ) icon in the top right corner and then click Environments . In the Environment List , select Show Deleted Environments . The deleted environments appear at the end of the list. Under the Actions column, click Restore to restore the environment. In the confirmation message box, click YES . The environment is restored in the list of environments.","title":"Restoring deleted environments"},{"location":"deploy/using_copy/#accessing-environments-and-deployments-of-other-users","text":"By default, Deploy Accelerator allows you to access only the following environments and deployments: Environments and deployments that you own. For more information, see Creating environments and Starting new deployments . Environments and deployments that other users have shared with you. For more information, see Sharing environments and Sharing deployments . However, sometimes you might need to access the environments and deployments of other users even though they are not shared with you. Your administrator can grant you this ability by adding you to the VIEW_ALL_USER'S_ENTITIES group. This group allows you to view all environments and deployments of all users in Deploy Accelerator. Note: To perform any actions (such as delete environments and destroy deployments) on the entities of other users, you must also be a member of the ADMIN group. To access environments and deployments of other users, perform the following actions: To access an environment or a deployment of another user, ensure that you have the ID of the environment version. Note: When an environment version is opened on the canvas, its ID is shown in the URL. Open a browser and enter the Deploy Accelerator URL in the following format: https:// YOUR-PLATFORM-BASE-URL /hcapdeploy/#/home/dnow/ ENVIRONMENT-VERSION-ID To view the list of all deployments across all versions of the environment, click the deployment tab on the canvas. Select the deployment that you want to view.","title":"Accessing environments and deployments of other users"},{"location":"deploy/using_copy/#exporting-environments-as-blueprints","text":"If you create a standalone or layered environment that needs to be deployed multiple times, you can export that environment as a blueprint. Ensure that you have followed the best practices for creating blueprints . If you are exporting a layered environment, also ensure that you export the child environment that is in the lowest layer. This action ensures that parent environments in all other layers are also included in the blueprint. You can also export multiple environments. To export multiple environments, you can select a child environment and all the parent environments in the hierarchy are automatically included. Example of exporting a layered environment Consider a scenario in which you create a Jenkins environment and an APP-VPC environment that is dependent on the Jenkins environment. You then create environments for different applications - Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator), and Hitachi Cloud Accelerator - Repair. Each of these environments has a dependency on the APP-VPC environment, as shown in the following image: When you export one of the application environments as a blueprint, the Jenkins and APP-VPC environments are also included in the blueprint and the dependency between them is retained. To replicate the application environment that you have exported, you can create new environments from the blueprint , make customizations if required, and then start a deployment of the environments. To export an environment as a blueprint, perform the following actions: On the canvas, click the More Options icon ( ) and then click Export . In the Environment Export window, from the list select the environment(s) to export. The environment which is open on the canvas is selected by default. The environments created by you are listed in black color. The environments that other users have shared with you are listed in a different color. Important: If you select a child environment, all its parent environments are also included in the exported file. To filter a long list of environments, enter the search keywords in the Search Environments box. Enter a name for the blueprint. Select the warning checkbox to acknowledge the warning message, and click EXPORT . A JSON file is saved as ***fileName*.blueprint.reandeploy** to your local computer. In this JSON file, the environments, the resources within each environment, and the packages within each resource are alphabetically sorted. The packages that are listed after the environments are also alphabetically sorted.","title":"Exporting environments as blueprints"},{"location":"deploy/using_copy/#best-practices-for-creating-blueprints","text":"Consider the following best practices while creating environments that you want to export as blueprints: Blueprints must ideally use a layered environment. You should create an environment with network resources, such as VPCs and CIDRs. In all other environments within the blueprint, you should get the VPC IDs, CIDRs, and other infrastructure resources from this environment by using the Depends On resource. The name of the Depends On resource in each environment must ideally indicate the environment on which there is a dependency. For example, depends_on_app_vpc . Blueprints must enable users to change resource attribute values by using input variables. Blueprints must output important information, such as Server URLs and IDs of important resources, to outputs. Blueprints must use name attributes that are descriptive and unique. For example, you must use ${var.environment_name}-myserver instead of myserver . Blueprints must not hard-code critical things, such as AWS regions, account IDs, VPCs, owner information, product information, environment information, buckets, user names, and passwords. For things that should be automatically detected, blueprints must use Terraform data sources instead of input variables. The following table lists a few examples of data sources that you can use in your environment. To know all the data sources that you can add to your environment, see the Data Sources section on the Resources tab in the left panel on the Deploy Accelerator Home page. Goal Data source Usage Get the current AWS Account ID aws_caller_identity ${ DataSourceName .current.account_id} Get the name of the current region aws_region ${ DataSourceName .current.name} Get the current partition and use it to build an Amazon Resource Name (ARN) aws_partition ${ DataSourceName .current.partition} Get a list of availability zones aws_availability_zones ${ DataSourceName .available.names} Get the current Elastic Load Balancer (ELB) service account ID and use it in the bucket policy for ELB logs aws_elb_service_account ${ DataSourceName .current}","title":"Best practices for creating blueprints"},{"location":"deploy/using_copy/#adding-blueprints-in-the-blueprint-gallery","text":"If you have created an environment that needs to be replicated multiple times by different users, you can export that environment as a blueprint and then add the blueprint in the Blueprint Gallery. Users can then create a new environment by importing that blueprint from the Blueprint Gallery . Before you begin Ensure that you can access the Artifactory that is configured for Deploy Accelerator. In the Artifactory, ensure that you can access the repository that the Administrator has configured for storing the blueprints that are displayed in the Blueprint Gallery. Export your environment as a blueprint . To add a blueprint in the Blueprint Gallery, perform the following actions: Create additional files that are required for the blueprint. Create a metadata.yml file for the blueprint. This file must contain the attributes that are listed in the following table. Attribute Details name The blueprint name specified in this file is shown in the Blueprint Gallery. Ensure that the blueprint name in this file matches the actual file name of the blueprint (which you provided while exporting the environment as a blueprint from Deploy Accelerator). For example, if you exported an environment from Deploy Accelerator as Chef-Server-HA , the blueprint file name is Chef-Server-HA.blueprint.reandeploy and the name that you must specify in the metadata.yml file must be Chef-Server-HA . description The blueprint description specified in this file is shown in the Blueprint Gallery. version The blueprint version specified in this file is shown in the Blueprint Gallery. image To display an image for the blueprint version in the Blueprint Gallery, specify that image in this file and add the image file in the images folder that you will create in a later step. You must specify the image path relative to the BlueprintName and Version folder in the Artifactory. For example: images/chef.png The image must be in the PNG format. Also, it is recommended that the image dimensions are 130 x 40 pixels. Note: If you later want to update the image for a blueprint version, it is recommended that you use a different image file name and update the metadata.yml file. If you just replace the image file with the same name in the images folder, the updated image is not shown in the Blueprint Gallery. ownerName The owner name specified in this file is shown in the Blueprint Gallery. If you add this attribute in the file but do not specify a value, admin is shown as the owner of this blueprint. ownerName is an optional attribute. ownerEmail The owner email specified in this file is shown in the Blueprint Gallery. ownerEmail is an optional attribute. The following image shows an example of a metadata.yml file. (Optional) Create a README.md file for the blueprint. This file (Markdown format) should ideally include information such as deployment architecture, details about input and output variables used in the environment, and any dependencies. To add an image in the README.md file, you must add the image file in the images folder that you will create in the next step. Also, you must use the IMAGE_BASE_PATH prefix for the image path in the README.md file, as shown in the example below: IMAGE_BASE_PATH/images/chef.png (Optional) Create an images folder and add the image that you want to show for the blueprint version in the Blueprint Gallery. Also, add any images that are used in the README.md file in this folder. Add the blueprint in the Blueprint Gallery. Sign in to the Artifactory that is configured for Deploy Accelerator. Navigate to the repository in which the blueprints must be stored. Create a folder with the same name as the blueprint file name (for example: Chef-Server-HA). Important: The blueprint is displayed in the Blueprint Gallery only if the folder name, blueprint file name, and the blueprint name defined in the metadata.yml file are the same. In the BlueprintName folder, create a folder with the blueprint version (for example: 01.00.00). Ensure that the folder name matches the blueprint version that is defined in the metadata.yml file. In the version folder, add the following files and folder: metadata.yml file ***BlueprintName*.blueprint.reandeploy** file (Optional) README.md file (Optional) images folder Verify that the blueprint is available in the Blueprint Gallery and can be successfully imported and deployed. Sign in to Cloud Accelerator Platform . On the Home page, click the More options icon ( ) in the top-right corner and then click Blueprint Gallery . On the Blueprint Gallery page, search for the blueprint that you have just added in the Artifactory. (Optional) If you cannot see your blueprint in the Blueprint Gallery, perform the following actions: Click Sync Blueprint . In the confirmation message box, click YES . It might take a few minutes to sync the blueprint metadata from the Artifactory. This metadata includes the blueprint name, description, version, and image. Refresh the browser after a few minutes. Note: By default, the Blueprint Gallery is automatically refreshed every 30 minutes. However, your administrator might have configured a different sync interval based on requirements. Confirm that the name, description, version, and image of your blueprint are displayed correctly. Import the blueprint and start a new deployment .","title":"Adding blueprints in the Blueprint Gallery"},{"location":"deploy/using_copy/#deploy-accelerator-resources","text":"The Resources tab displays many resources that you can add to your environment. The following table lists the additional resources that Deploy Accelerator provides to address specific scenarios : Resource name Details Depends On Use this resource to create a dependency on another environment. The name of the Depends On resource must ideally indicate the environment on which there is a dependency. In the reference_type attribute, you can select either Environment Name or S3 . -- If you select Environment Name , click the Depends On box and select the parent environment version on which the environment must be dependent. The selected parent environment appears in the environmentName:environmentVersion format (for example: networkEnv:01.00.00). While exporting the child environment, Deploy Accelerator exports this version of the parent environment. -- If you select S3 , enter the S3 bucket name and the remote state file name of the environment that is referred by the Depends On resource. For example: s3://bucket_name/tfstatefile_name , in which bucket_name is the name of the S3 bucket and tfstatefile_name is the Terraform remote state file name of the environment that is to be referred. If required, you can define variables for the S3 bucket and Terraform state file names in the Input Variables resource. To reference these variables in the Depends On resource, you must use the following interpolation syntax: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } For example: If you have defined the S3bucket and tfstatefile variables in the Input Variables resource, specify the following URL in the Depends On resource. s3:// \\({var.s3bucket}/\\) Ensure that the provider that you use to deploy the environment has the appropriate access to the S3 bucket that you have specified. Note: For information about how you can create layered environments by using the Depends On resource, see Creating layered environments . Input Variable Use this resource to create a JSON file in which you can specify multiple variables and set their values. You can later use these variables in other resources within the same environment. For information about the different ways in which you can define and use input variables, see Formats for defining input variables and Formats for using input variables in resources . Locals Use this resource to create a JSON file in which you can specify multiple local values. You can later use these local values in other resources within the same environment. A local value can be a simple constant or a complex expression that transforms or combines values from other resources in the environment, as shown in the following example. For more information, see the Terraform documentation . Output Use this resource to create a JSON file in which you can specify multiple variables that can be used in other dependent environments. The value of these variables can reference either variables defined in the Input Variable resource or attributes of other resources in the environment by using the interpolation syntax. Note: If you do not want to display an output value in the resource logs of an environment, set the sensitive parameter for that output to true . Existing VM Use this resource to add an existing instance to the environment.","title":"Deploy Accelerator resources"},{"location":"deploy/using_copy/#formats-for-defining-input-variables","text":"In the Input Variable resource, you can use the following formats to define input variables based on your requirements: String: Use the following syntax to define input variables by using the String format: \"string_input\": { \"type\": \"string\", \"default\": \"sample_value\", \"description\": \"this is sample string value for hcap deploy\" } Example: \"string_input\": { \"type\": \"string\", \"first_name\": \"John\", \"description\": \"First name of the user\" } List: Use the following syntax to define input variables by using the List format: \"list_input\": { \"type\": \"list\", \"default\": [ \"sample_value1\", \"sample_value2\" ], \"description\": \"this is sample list value for hcap deploy\" } Example: \"list_input\": { \"type\": \"list\", \"Region\": [ \"us-west-2\", \"us-east-1\" ], \"description\": \"List of regions to be used\" } Map: Use the following syntax to define input variables by using the Map format: \"map_input\": { \"type\": \"map\", \"default\": { \"key\": \"sample_value\" }, \"description\": \"this is sample map value for hcap deploy\" } Example: \"map_input\": { \"type\": \"map\", \"ec2details\": { \"app\": { \"ami\": \"ami-718c6909\", \"type\": \"t2.micro\" }, }, \"description\": \"this is sample map value for hcap deploy\" } The following image shows an example of using these different formats in the Input Variable resource.","title":"Formats for defining input variables"},{"location":"deploy/using_copy/#formats-for-using-input-variables-in-resources","text":"In any environment, instead of hard coding the attribute value of a resource, you can use the interpolation syntax to refer to the input variables that you have defined. The interpolation syntax is based on the type of input variable that you are referring. String: Use the following interpolation syntax to refer to an input variable of the String type: ${var.variableName} Example: ${var.string_input} List: Use the following interpolation syntax to refer to an input variable of the List type: To refer to all the values in a list, use the following syntax: ${var.variableName} To refer to a specific value in a list, use the following syntax: ${var.variableName[index]} Examples: ${var.list_input} ${var.list_input[0]} Map: Use one of the following interpolation syntax to refer to an input variable of the Map type: ${var.variableName} ${var.variableName[\"Key\"]} ``` ${lookup(var.variableName[\"Key\"], \"InnerKey\")} ``` Examples: ``` ${var.map_input} ``` ``` ${var.map_input[\"ec2details\"]} ``` ``` ${lookup(var.map_input[\"ec2details\"], \"app\"} ```","title":"Formats for using input variables in resources"},{"location":"deploy/using_copy/#copying-resources","text":"You can copy one or more existing resources from either the same environment or a different environment. On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the environment from which you want to copy a resource. On the canvas, right-click the resource that you want to copy, and click Copy . To copy multiple resources, repeat this step for each resource. Based on your requirements, perform one of the following actions: To create a copy of the resource in the same environment, right-click the canvas and then select the resource from the Paste Resource list. To create a copy of the resource in another environment, open that environment in a new tab, right-click the canvas and then select the resource from the Paste Resource list. All the resources that you have copied appear in the Paste Resource list. You can add a copied resource to any environment only once. ![](/images/rean-deploy/rd_resourcepaste.PNG) From the Paste Resource list, click the appropriate resource and then perform the following actions: In the Resource Name window, enter a name for the resource and click CREATE . In the right panel that opens, on the Resource tab, you can see that the attribute values are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The Packages tab also displays the same packages, along with their attributes, as the original resource. On the Resource tab, add or update the attribute values based on your requirements. Important: If you have copied a resource from another environment, you must update attribute values that use the interpolation syntax to reference input variables or other resources. You must also update the Depends On attribute if it references another resource in the original environment. Otherwise, the new resource might not be successfully deployed. (Optional) On the Packages tab, add or remove packages based on your requirements. Note: After you select a copied resource from the Paste Resource list, it no longer appears in the list. (Optional) Repeat step 5 for the other resources in the Paste Resource list. (Optional) To clear all resources from the Paste Resource list, click clearAll . To save the environment, click the Save icon ( ).","title":"Copying resources"},{"location":"deploy/using_copy/#renaming-resources","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Environments . On the Environment List page, click the appropriate environment version. On the canvas, select the resource that you want to rename. In the right panel that opens, on the Resource tab, click the resource name link at the top. In the Resource Name window, update the resource name and click SAVE . The resource name is automatically updated in other resources within the environment that reference it by using either the interpolation syntax or the Depends On attribute. To save the environment, click the Save icon ( ). Important: If you rename a deployed resource, that resource is destroyed and a new resource is created with the updated name when you redeploy existing deployments. Also, until you redeploy the deployments, the deployed icon is not shown on the resource and the deployed values are not shown on the Resource Details tab.","title":"Renaming resources"},{"location":"deploy/using_copy/#adding-user-defined-packages","text":"Packages in Deploy Accelerator enable you to leverage your existing infrastructure automation tool (Chef Solo, Chef Server, Ansible Solo, or Puppet) to configure compute resources in your environments. Each package definition includes the infrastructure automation tool and the Chef cookbook or Puppet module or manifest file that Deploy Accelerator must use to configure the compute resource. Managed packages are available by default after Deploy Accelerator is successfully deployed. You can leverage these managed packages if they meet your requirements. Otherwise, you can add your own custom (or user-defined) packages. In the case of user-defined packages, you must define a versioning scheme for the packages and ensure that each combination of package type, name, and version is unique. The following diagram describes the high-level process for creating and releasing a user-defined package: After a user-defined package is released, you can neither edit nor delete that package. To include additional updates, you must create a new package version.","title":"Adding user-defined packages"},{"location":"deploy/using_copy/#add-a-user-defined-chef-ansible-or-puppet-package","text":"To prepare the user-defined package that you want to add in Deploy Accelerator, perform the following actions: Based on your infrastructure automation tool, perform one of the following actions: Create a Chef cookbook and test it well using the chef-client. For more information, see the Chef documentation . Create an Ansible playbook and test it well using the Ansible-client. For more information, see the Ansible documentation . Create a Puppet module or manifest file and test it well using the puppet-agent. For more information, see the Puppet documentation . Create a ZIP file of the Chef cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the tar.gz format. Commit the ZIP file (tar.gz format) in GitHub, GitLab, Artifactory, or any other repository. You can use a public repository for open source software. Important: If Deploy Accelerator is deployed in the offline mode, you must commit the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file in the Artifactory. Decide the attributes that you want to define for this package. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . On the Package List page, the Package Type column enables you to identify the package type. In case of Chef Server packages, the Chef Server column enables you to identify the Chef Server from which the packages are shown. To add a user-defined package, click ADD . On the Add Package page, perform the following actions: On the Basic tab, enter the following details: Field Details Package Name Enter the name of the package. This name appears on the Packages tab on the Home page. The name that you enter must be unique across all packages of that type. However, if you are adding a new version of an existing package, ensure that the package name is the same as the existing package name. Note: You cannot enter a name that matches a managed package. Package Type Select chef or ansible as the package type. Depending on the option selected here the package is listed, under the Packages tab, in the Private Packages list. Package Version Enter the version of the package based on your own versioning scheme. The value of this field must be unique across all existing versions of the package that you are adding. Note: Each combination of package type, name, and version must be unique. Description Enter details about the package. Image Url Enter the URL from which Deploy Accelerator must download the image file that is shown on the Packages tab on the Home page. On the Repository tab, enter the following details for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located: Fields Details Download URL Enter the URL from which Deploy Accelerator must download the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Note: If Deploy Accelerator is deployed in the offline mode, you must specify the Artifactory URL from where the Chef Solo cookbook, Ansible playbook or Puppet module or manifest file can be downloaded. Repository Type Select the repository type where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have stored the files in a different repository or location (such as Amazon S3), select the plain option. Access Token Enter the access token for the repository where the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file is located. If you have selected Artifactory as the repository type, you must enter the API key. Ensure that the access token or API key that you enter is authorized to download the ZIP file from the repository. Note: If you do not enter the access token or API key, Deploy Accelerator takes the default access token or API key that your your administrator has configured . ZIP File Name Enter the name of the ZIP file that contains the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file. Unzipped Name Enter the name of the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file that is extracted from the ZIP file. Dependent packages Select the other packages that are a prerequisite for installing this package on a resource. On the Attributes tab, specify the attribute values as the array of JSON. The attributes help to parameterize the Chef Solo cookbook, Ansible playbook, Puppet module, or Puppet manifest file with some dynamic values. All the values that you enter here are shown on the Deploy Accelerator UI to ask the user to provide dynamic values before deployment. A few default attributes are shown for your reference. You can choose to use these attributes or replace them with other attributes that are relevant to your package. Important: The package will be visible to other users, so ideally you should avoid specifying default values for sensitive attributes. Click SAVE . Confirm that this package version appears on the Package List page. The state of this package version is unreleased. Note: In the case of Chef Solo cookbooks, all recipes that the cookbook contains also become available in Deploy Accelerator. While adding a Chef Solo package (or cookbook) to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook Test the user-defined package . Release the user-defined package .","title":"Add a user-defined Chef, Ansible, or Puppet package"},{"location":"deploy/using_copy/#test-a-user-defined-package","text":"Create a test environment of the appropriate environment package type. After the environment is created, confirm that the user-defined package that you want to test appears on the Packages tab in the left panel. From the Resources tab in the left panel, drag a compute resource (For AWS an EC2 instance, or a null VM) to the canvas. From the Packages tab in the left panel, drag the user-defined package to this resource. In the right panel that opens, on the Packages tab, select the unreleased version that you want to test, and enter other package details as required. In case of a Chef Solo package (or cookbook), confirm that all recipes that the cookbook contains are available for selection in the Recipes box. Note: While adding a Chef Solo package to a compute resource in an environment, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. Save the environment. Start a new deployment of the environment . (Optional) To resolve any issues that are detected during the testing, edit the user-defined package and then redeploy the existing deployment . After all issues with the user-defined package are successfully resolved, release the user-defined package .","title":"Test a user-defined package"},{"location":"deploy/using_copy/#release-a-user-defined-package","text":"On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To release a package, in the Actions column for that package, click Edit . On the Attributes tab, click RELEASE . Important: You can neither edit nor delete a released package. In case you need to make any changes, you must create a new package version.","title":"Release a user-defined package"},{"location":"deploy/using_copy/#managing-user-defined-packages","text":"You can edit, download, or delete user-defined packages only if they are not yet released. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. In the Actions column for a specific version of a user-defined package, perform one of the following actions: To edit a user-defined package, click Edit to open the Add Package page, update the package details, and then click SAVE . You cannot edit a user-defined package that is released. To save a copy of the user-defined package to your computer, click Download . To delete the user-defined package, click Delete and then click YES in the confirmation message box.","title":"Managing user-defined packages"},{"location":"deploy/using_copy/#creating-new-package-versions","text":"You can create new versions of both user-defined and managed packages. On the Home page, click the More options icon ( ) in the top-right corner and then click Packages . Each version of a package is listed separately on the Package List page. To create a new version of a package, in the Actions column for that package, click Create New Version . In the Create New Version From base version window, update details of the new package version. By default, details of the base package version are shown. Note: You must specify a unique version number for this package. Also, the version that you specify must be greater than the base version that you have selected. Click CREATE . (Optional) On the Add Package page, update additional details about the new package version based on your requirements. Click SAVE . A new version of the package appears on the Package List page.","title":"Creating new package versions"},{"location":"deploy/using_copy/#manual-changes-required-for-migration-of-blueprints-from-2280-to-302","text":"In Deploy Accelerator 3.0.2, the Terraform version is upgraded to 0.12.0. Due to this, the blueprints that are created in Deploy Accelerator 2.28.0 and previous releases, require some manual change to be used in 3.0.2.","title":"Manual changes required for migration of blueprints from 2.28.0 to 3.0.2"},{"location":"deploy/using_copy/#mandatory-attributes-for-syntax-validation","text":"Terraform v0.11 and earlier allowed adding attribute/sub-attribute JSON as a Input Variable for the resources, which can be added as an interpolation in the resource attribute. Terraform v0.12 no longer allows such interpolation for JSON and JSON Array for its attributes or sub-attributes. If there is an attribute of the type JSON or JSON Array then it should follow a proper structure for its attributes and sub-attributes. Hence, the Input/Local variables of type JSON cannot be used in interpolation to the Resource or Datasource attributes. Example Resource: aws_route53_record , Attribute: Alias Input Variable for the resource: { \"var.alias_json\" : { \"type\": \"map\", \"default\": { \"name\": \"dns_record_alias\" } } } and value for the alias attribute in the resource for Terraform v0.11 and previous release: alias : [\"${var.alias_json}\"] This syntax is above is not valid. The valid syntax for the alias attribute in the aws_route53_record resource for Terraform v0.12 is as follows: [ { \"evaluate_target_health\": \"\", \"name\": \"${var.alias_name}\", \"zone_id\": \"\" } ]","title":"Mandatory attributes for syntax validation"},{"location":"deploy/using_copy/#using-floor-function","text":"From Terraform 0.12, the Terraform language does not distinguish between integer and float types. Instead, the language just has a single \"number\" type that can represent high-precision point numbers. The Terraform documentation mentions that this new type can represent any value that could be represented before, plus many new values due to the expanded precision. The Terraform documentation also mentions that in most cases this change should not cause any significant behavior change, but please note that in particular the behavior of the division operator is now different: it always performs floating point division, whereas before it would sometimes perform integer division by attempting to infer intent from the argument types. If you are relying on integer division behavior in your configuration, use the floor function to obtain the previous result. A common place this would arise is in index operations, where the index is computed by division. For example: ${floor(length(var.availability_zones)/var.subnet_count) Unfortunately the automatic upgrade tool cannot apply a fix for this case because it does not have enough information to know if floating point or integer division was intended by the configuration author, so this change must be made manually where needed.","title":"Using Floor function"},{"location":"deploy/using_copy/#using-escape-sequences","text":"In JSON attributes, if \\n , \\r , \\t , symbols are not escaped, then Terraform v0.12 does not validate it as a correct JSON. JSON format for Terraform v0.11 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\n \\\")}\" } JSON format for Terraform v0.12 : { \"aws_load_balancer_ssl_ports\" : \"${var.cloud_provider == \\\"aws\\\" && var.ingress_service_type == \\\"LoadBalancer\\\" ? \\\"service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\\\\n service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\\\" : \\\"\\\"}\" , \"additional_configs\" : \"${replace(var.additional_configs,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_nodeports_mapping\" : \"${replace(var.tcp_port_nodeports_mapping,\\\",\\\", \\\"\\\\n \\\")}\" , \"tcp_port_service_mapping\" : \"${replace(var.tcp_port_service_mapping,\\\",\\\", \\\"\\\\n \\\")}\" }","title":"Using Escape Sequences"},{"location":"deploy-infobytes/info-btyes-01/","text":"Deploy Accelerator info byte 1: Product overview \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a tool that enables you to easily design the IT infrastructure, generate infrastructure as code (IaC), and deploy and configure the infrastructure in the cloud securely, reliably, and consistently. It integrates with well-known cloud platforms (such as Amazon Web Services, Google Cloud Platform, and Microsoft Azure) and provides out-of-the-box support for the Chef and Puppet infrastructure automation tools. Why Deploy Accelerator \u00b6 Deploy Accelerator offers the following key advantages to speed up the delivery of workloads in the cloud: Automates deployment of new and existing workloads or applications in the cloud Reduces deployment time and has a huge impact on time to value Improves productivity through prepackaged content (deployment blueprints and packages) that is based on industry standards Simplifies cloud adoption and reduces the need for subject-matter experts Key concepts in Deploy Accelerator \u00b6 Before using Deploy Accelerator, understand the key concepts so that you can effectively design, deploy, and configure IT infrastructure in the cloud: Providers enable you to specify the cloud platform, the account in which the infrastructure must be deployed, and credentials for accessing that account. Environments help you to design the infrastructure that you need to deploy different solutions (or workloads) in the cloud. Resources are entities that you require to set up the infrastructure (for example, instances, VPCs, subnets, and security groups). Packages are entities that use the supported infrastructure automation tools to configure compute instances. Users can install, deploy, configure software and applications on compute instances by using packages. Related topic \u00b6 Overview of Deploy Accelerator Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 1- Product overview"},{"location":"deploy-infobytes/info-btyes-01/#deploy-accelerator-info-byte-1-product-overview","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a tool that enables you to easily design the IT infrastructure, generate infrastructure as code (IaC), and deploy and configure the infrastructure in the cloud securely, reliably, and consistently. It integrates with well-known cloud platforms (such as Amazon Web Services, Google Cloud Platform, and Microsoft Azure) and provides out-of-the-box support for the Chef and Puppet infrastructure automation tools.","title":" Deploy Accelerator info byte 1: Product overview"},{"location":"deploy-infobytes/info-btyes-01/#why-deploy-accelerator","text":"Deploy Accelerator offers the following key advantages to speed up the delivery of workloads in the cloud: Automates deployment of new and existing workloads or applications in the cloud Reduces deployment time and has a huge impact on time to value Improves productivity through prepackaged content (deployment blueprints and packages) that is based on industry standards Simplifies cloud adoption and reduces the need for subject-matter experts","title":"Why Deploy Accelerator"},{"location":"deploy-infobytes/info-btyes-01/#key-concepts-in-deploy-accelerator","text":"Before using Deploy Accelerator, understand the key concepts so that you can effectively design, deploy, and configure IT infrastructure in the cloud: Providers enable you to specify the cloud platform, the account in which the infrastructure must be deployed, and credentials for accessing that account. Environments help you to design the infrastructure that you need to deploy different solutions (or workloads) in the cloud. Resources are entities that you require to set up the infrastructure (for example, instances, VPCs, subnets, and security groups). Packages are entities that use the supported infrastructure automation tools to configure compute instances. Users can install, deploy, configure software and applications on compute instances by using packages.","title":"Key concepts in Deploy Accelerator"},{"location":"deploy-infobytes/info-btyes-01/#related-topic","text":"Overview of Deploy Accelerator Deploy Accelerator info bytes | Next info byte >>","title":"Related topic"},{"location":"deploy-infobytes/info-btyes-02/","text":"Deploy Accelerator info byte 2: Layered environments \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) enables you to create two types of environments: Standalone environments , which include all the required resources and do not depend on any other environment. Layered environments , which are a collection of multiple environments in which some environments (child environments) are dependent on other environments (parent environments). In a layered environment, resources in child environments can reference resource values from parent environments. You can create layered environments for complex IT infrastructures. For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between the environments. An Instance resource in the application environment can reference the subnet and security group values from the network environment Why use layered environments \u00b6 The following are a few advantages of using layered environments to create the IT infrastructure: You can easily update, deploy, and destroy a set of resources in the parent or child environments without affecting resources in the other environments. You have to redeploy the other environments only if any resource values that they reference have been updated. You can create a common network environment as the parent environment . And then create child environments for each application that you want to deploy in this network. Resources in each child application environment can reference resource values from the parent network environment. You can configure different ownership for each environment within a layered environment . For example, the network team can own the network environment while different application teams can only view this environment and use it as a parent for their own application environments. Related topics \u00b6 Overview of layered environments Creating a layered environment << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 2- Layered environments"},{"location":"deploy-infobytes/info-btyes-02/#deploy-accelerator-info-byte-2-layered-environments","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) enables you to create two types of environments: Standalone environments , which include all the required resources and do not depend on any other environment. Layered environments , which are a collection of multiple environments in which some environments (child environments) are dependent on other environments (parent environments). In a layered environment, resources in child environments can reference resource values from parent environments. You can create layered environments for complex IT infrastructures. For example, instead of adding network, database, and application resources in the same environment, you can create separate environments for each set of resources and then create a dependency between the environments. An Instance resource in the application environment can reference the subnet and security group values from the network environment","title":"Deploy Accelerator info byte 2: Layered environments"},{"location":"deploy-infobytes/info-btyes-02/#why-use-layered-environments","text":"The following are a few advantages of using layered environments to create the IT infrastructure: You can easily update, deploy, and destroy a set of resources in the parent or child environments without affecting resources in the other environments. You have to redeploy the other environments only if any resource values that they reference have been updated. You can create a common network environment as the parent environment . And then create child environments for each application that you want to deploy in this network. Resources in each child application environment can reference resource values from the parent network environment. You can configure different ownership for each environment within a layered environment . For example, the network team can own the network environment while different application teams can only view this environment and use it as a parent for their own application environments.","title":"Why use layered environments"},{"location":"deploy-infobytes/info-btyes-02/#related-topics","text":"Overview of layered environments Creating a layered environment << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Related topics"},{"location":"deploy-infobytes/info-btyes-03/","text":"Deploy Accelerator info byte 3: Multiple deployments of an environment \u00b6 After you have designed the infrastructure for a solution in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), you have to create the resources in the selected cloud platform. For that, you have to start a deployment of the environment. Ideally, you will want to test the environment before you deploy it in production. Deploy Accelerator enables you to start and maintain multiple deployments of the same environment. For example, you can start a QA deployment, and if it is successful, you can start a new production deployment. Deploy Accelerator also enables you to configure different resource values for each deployment of the same environment. For example, you can define different VPC IDs for the QA and production deployments. Why start multiple deployments of an environment \u00b6 The following are a few advantages of having multiple deployments of the same environment: You can maintain different deployments for QA (testing the IT infrastructure), staging, and production . You can maintain multiple production deployments of the same environment . For example, say two departments have to deploy applications in different networks. In this case, you can create a network environment based on industry-standard IT infrastructure architecture. You can then have two production deployments with different resource values (such as VPC IDs, security groups, and subnets) for the two departments. You can implement the Blue-Green deployment technique by having two simultaneous production deployments. Related topic \u00b6 End-to-end process for multiple deployments of an environment << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 3- Multiple deployment"},{"location":"deploy-infobytes/info-btyes-03/#deploy-accelerator-info-byte-3-multiple-deployments-of-an-environment","text":"After you have designed the infrastructure for a solution in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), you have to create the resources in the selected cloud platform. For that, you have to start a deployment of the environment. Ideally, you will want to test the environment before you deploy it in production. Deploy Accelerator enables you to start and maintain multiple deployments of the same environment. For example, you can start a QA deployment, and if it is successful, you can start a new production deployment. Deploy Accelerator also enables you to configure different resource values for each deployment of the same environment. For example, you can define different VPC IDs for the QA and production deployments.","title":"Deploy Accelerator info byte 3: Multiple deployments of an environment"},{"location":"deploy-infobytes/info-btyes-03/#why-start-multiple-deployments-of-an-environment","text":"The following are a few advantages of having multiple deployments of the same environment: You can maintain different deployments for QA (testing the IT infrastructure), staging, and production . You can maintain multiple production deployments of the same environment . For example, say two departments have to deploy applications in different networks. In this case, you can create a network environment based on industry-standard IT infrastructure architecture. You can then have two production deployments with different resource values (such as VPC IDs, security groups, and subnets) for the two departments. You can implement the Blue-Green deployment technique by having two simultaneous production deployments.","title":"Why start multiple deployments of an environment"},{"location":"deploy-infobytes/info-btyes-03/#related-topic","text":"End-to-end process for multiple deployments of an environment << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Related topic"},{"location":"deploy-infobytes/info-btyes-04/","text":"Deploy Accelerator info byte 4: Environment versions and releases \u00b6 Each environment in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) can have multiple versions. When an environment version is ready for production, you can release that version. Once released, an environment version cannot be updated. To add or update resources in the environment, you have to create a new environment version. Deploy Accelerator also enables you to redeploy existing deployments with new environment versions. Environment versions and releases in Deploy Accelerator ensure that you do not have to maintain multiple environments for the same application (or workload). Each time you need to upgrade an application you can just create a new environment version. Why create environment versions and releases \u00b6 The following are a couple of advantages of environment versions and releases: You can create environment versions to ensure that a record is maintained of the changes that are made to an environment. The Compare feature enables you to easily identify the differences between two environment versions. You can release an environment version to ensure that no changes can be made to the deployed production resources. Related topics \u00b6 Creating new environment versions Releasing environment versions Comparing differences between environments << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 4- Environment versions and releases"},{"location":"deploy-infobytes/info-btyes-04/#deploy-accelerator-info-byte-4-environment-versions-and-releases","text":"Each environment in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) can have multiple versions. When an environment version is ready for production, you can release that version. Once released, an environment version cannot be updated. To add or update resources in the environment, you have to create a new environment version. Deploy Accelerator also enables you to redeploy existing deployments with new environment versions. Environment versions and releases in Deploy Accelerator ensure that you do not have to maintain multiple environments for the same application (or workload). Each time you need to upgrade an application you can just create a new environment version.","title":" Deploy Accelerator info byte 4: Environment versions and releases"},{"location":"deploy-infobytes/info-btyes-04/#why-create-environment-versions-and-releases","text":"The following are a couple of advantages of environment versions and releases: You can create environment versions to ensure that a record is maintained of the changes that are made to an environment. The Compare feature enables you to easily identify the differences between two environment versions. You can release an environment version to ensure that no changes can be made to the deployed production resources.","title":"Why create environment versions and releases"},{"location":"deploy-infobytes/info-btyes-04/#related-topics","text":"Creating new environment versions Releasing environment versions Comparing differences between environments << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Related topics"},{"location":"deploy-infobytes/info-btyes-05/","text":"Deploy Accelerator info byte 5: Blueprints \u00b6 Blueprints in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) are templates of commonly-used IT infrastructures. If you have designed a standalone or layered environment for a solution that needs to be replicated multiple times, you can export that environment as a blueprint. The Blueprint Gallery in Deploy Accelerator also provides a few out-of-the-box blueprints that are templates of industry-standard cloud or IT infrastructure architectures. Deploy Accelerator users can import a blueprint to create a new environment and quickly start a new deployment of that environment. Why use blueprints \u00b6 The following are a few advantages of using blueprints: Users can quickly create IT infrastructure for solutions . For example, users can create a new environment from the Chef Server HA blueprint to quickly set up a high-availability (HA) environment for Chef Server. Non-expert users can leverage blueprints to create complex infrastructure . For example, users who are new to Google Cloud Platform (GCP) can leverage the Google Cloud N-layer Network blueprint to quickly create an industry-standard network layer in GCP. Users can easily and quickly replicate environments multiple times. Related topics \u00b6 Overview of blueprints Exporting environments as blueprints Creating new environments from blueprints << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 5- Blueprints"},{"location":"deploy-infobytes/info-btyes-05/#deploy-accelerator-info-byte-5-blueprints","text":"Blueprints in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) are templates of commonly-used IT infrastructures. If you have designed a standalone or layered environment for a solution that needs to be replicated multiple times, you can export that environment as a blueprint. The Blueprint Gallery in Deploy Accelerator also provides a few out-of-the-box blueprints that are templates of industry-standard cloud or IT infrastructure architectures. Deploy Accelerator users can import a blueprint to create a new environment and quickly start a new deployment of that environment.","title":"Deploy Accelerator info byte 5: Blueprints"},{"location":"deploy-infobytes/info-btyes-05/#why-use-blueprints","text":"The following are a few advantages of using blueprints: Users can quickly create IT infrastructure for solutions . For example, users can create a new environment from the Chef Server HA blueprint to quickly set up a high-availability (HA) environment for Chef Server. Non-expert users can leverage blueprints to create complex infrastructure . For example, users who are new to Google Cloud Platform (GCP) can leverage the Google Cloud N-layer Network blueprint to quickly create an industry-standard network layer in GCP. Users can easily and quickly replicate environments multiple times.","title":"Why use blueprints"},{"location":"deploy-infobytes/info-btyes-05/#related-topics","text":"Overview of blueprints Exporting environments as blueprints Creating new environments from blueprints << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Related topics"},{"location":"deploy-infobytes/info-btyes-06/","text":"Deploy Accelerator info byte 6: Packages \u00b6 Packages in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) can leverage different infrastructure automation tools to configure compute instances in an environment. Deploy Accelerator includes out-of-the-box support for Chef Solo, Chef Server, and Puppet. You can add and manage your own custom Chef Solo or Puppet packages. You can also leverage an existing setup of Chef Servers to configure compute instances in an environment. Deploy Accelerator also provides a few utility packages for specific use cases. For example, the file package enables you to upload a file on a compute instance and the execute-script package enables you to run a command on a compute instance. Why use packages \u00b6 The following are a couple of advantages of using packages: You can automate the configuration of compute instances in your environment. For example, at the end of a deployment, you can get a compute instance that has Tomcat and NGINX installed on it. You can create a package for your own custom application and deploy it on a compute instance in the environment. Related topics \u00b6 Adding user-defined packages Creating new package versions Registering Chef servers in Deploy Accelerator << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Infobyte 6- Packages"},{"location":"deploy-infobytes/info-btyes-06/#deploy-accelerator-info-byte-6-packages","text":"Packages in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) can leverage different infrastructure automation tools to configure compute instances in an environment. Deploy Accelerator includes out-of-the-box support for Chef Solo, Chef Server, and Puppet. You can add and manage your own custom Chef Solo or Puppet packages. You can also leverage an existing setup of Chef Servers to configure compute instances in an environment. Deploy Accelerator also provides a few utility packages for specific use cases. For example, the file package enables you to upload a file on a compute instance and the execute-script package enables you to run a command on a compute instance.","title":"Deploy Accelerator info byte 6: Packages"},{"location":"deploy-infobytes/info-btyes-06/#why-use-packages","text":"The following are a couple of advantages of using packages: You can automate the configuration of compute instances in your environment. For example, at the end of a deployment, you can get a compute instance that has Tomcat and NGINX installed on it. You can create a package for your own custom application and deploy it on a compute instance in the environment.","title":"Why use packages"},{"location":"deploy-infobytes/info-btyes-06/#related-topics","text":"Adding user-defined packages Creating new package versions Registering Chef servers in Deploy Accelerator << Previous info byte | Deploy Accelerator info bytes | Next info byte >>","title":"Related topics"},{"location":"deploy-infobytes/info-btyes-07/","text":"Deploy Accelerator info byte 7: Platform CLI tool \u00b6 The Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI) tool enables you to perform various actions across multiple Accelerators. This tool currently supports Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), and the Admin Console. In case of Deploy Accelerator, you can use the Cloud Platform CLI tool to perform multiple tasks, such as create, deploy, and destroy environments. Why use the Cloud Accelerator Platform CLI tool \u00b6 The following are a couple of advantages of using the Cloud Accelerator Platform CLI tool: You can use one tool to perform actions across multiple Accelerators. You can configure and use the Cloud Accelerator Platform CLI tool in pipelines to integrate different Accelerators. For example, you can create a Jenkins pipeline that uses the Cloud Accelerator Platform CLI tool to deploy an environment in Deploy Accelerator and then trigger an infrastructure test in Test Accelerator for that deployment ID. Related topic \u00b6 Install and use Cloud Accelerator Platform CLI << Previous info byte | Deploy Accelerator info bytes","title":"Infobyte 7- Platform CLI tool"},{"location":"deploy-infobytes/info-btyes-07/#deploy-accelerator-info-byte-7-platform-cli-tool","text":"The Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI) tool enables you to perform various actions across multiple Accelerators. This tool currently supports Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), and the Admin Console. In case of Deploy Accelerator, you can use the Cloud Platform CLI tool to perform multiple tasks, such as create, deploy, and destroy environments.","title":"Deploy Accelerator info byte 7: Platform CLI tool"},{"location":"deploy-infobytes/info-btyes-07/#why-use-the-cloud-accelerator-platform-cli-tool","text":"The following are a couple of advantages of using the Cloud Accelerator Platform CLI tool: You can use one tool to perform actions across multiple Accelerators. You can configure and use the Cloud Accelerator Platform CLI tool in pipelines to integrate different Accelerators. For example, you can create a Jenkins pipeline that uses the Cloud Accelerator Platform CLI tool to deploy an environment in Deploy Accelerator and then trigger an infrastructure test in Test Accelerator for that deployment ID.","title":"Why use the Cloud Accelerator Platform CLI tool"},{"location":"deploy-infobytes/info-btyes-07/#related-topic","text":"Install and use Cloud Accelerator Platform CLI << Previous info byte | Deploy Accelerator info bytes","title":"Related topic"},{"location":"deploy-infobytes/info-btyes/","text":"Deploy Accelerator info bytes \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) info bytes quickly introduce you to the core capabilities of Deploy Accelerator. Each of the following info bytes describes a Deploy Accelerator feature and highlights the key advantages of using that feature. Info byte 1: Product overview Info byte 2: Layered environments Info byte 3: Multiple deployments of an environment Info byte 4: Environment versions and releases Info byte 5: Blueprints Info byte 6: Packages Info byte 7: Platform CLI tool","title":"Deploy"},{"location":"deploy-infobytes/info-btyes/#deploy-accelerator-info-bytes","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) info bytes quickly introduce you to the core capabilities of Deploy Accelerator. Each of the following info bytes describes a Deploy Accelerator feature and highlights the key advantages of using that feature. Info byte 1: Product overview Info byte 2: Layered environments Info byte 3: Multiple deployments of an environment Info byte 4: Environment versions and releases Info byte 5: Blueprints Info byte 6: Packages Info byte 7: Platform CLI tool","title":"Deploy Accelerator info bytes"},{"location":"devops/gettingStarted/","text":"Overview of DevSecOps Services \u00b6 Many people struggle on their cloud adoption journey because of their manual approaches towards the cloud environment. Hitachi Vantara provides an end-to-end automated solution with customizable out-of-the box capabilities to address the customer\u2019s requirements. Customers can leverage Hitachi Cloud Accelerator Platform to deploy, verify and manage along with DevSecOps Services from Hitachi Vantara for their cloud infrastructure. Cloud Accelerator Platform helps enabling the adoption of DevSecOps principles to increase the pace of innovation and improve the reliability. DevSecOps Services primarily focus on the principle of automation. The principle of automation includes testing, deployment, and operations. DevSecOps Services is a framework for orchestrating CI/CD pipelines which are mostly focused on CD pipelines. It has may tools to generate pipeline project that leverage its libraries for reusing CI/CD pipelines and utility code to use within the pipelines. DevSecOps Services has capabilities such as configuring DSL and externalization, to integrate with various tools and products. It is built with a \u201ctool agnostic\u201d philosophy to allow its users to swap-out one tool for another by changing pipeline configuration.","title":"Overview of DevSecOps Services"},{"location":"devops/gettingStarted/#overview-of-devsecops-services","text":"Many people struggle on their cloud adoption journey because of their manual approaches towards the cloud environment. Hitachi Vantara provides an end-to-end automated solution with customizable out-of-the box capabilities to address the customer\u2019s requirements. Customers can leverage Hitachi Cloud Accelerator Platform to deploy, verify and manage along with DevSecOps Services from Hitachi Vantara for their cloud infrastructure. Cloud Accelerator Platform helps enabling the adoption of DevSecOps principles to increase the pace of innovation and improve the reliability. DevSecOps Services primarily focus on the principle of automation. The principle of automation includes testing, deployment, and operations. DevSecOps Services is a framework for orchestrating CI/CD pipelines which are mostly focused on CD pipelines. It has may tools to generate pipeline project that leverage its libraries for reusing CI/CD pipelines and utility code to use within the pipelines. DevSecOps Services has capabilities such as configuring DSL and externalization, to integrate with various tools and products. It is built with a \u201ctool agnostic\u201d philosophy to allow its users to swap-out one tool for another by changing pipeline configuration.","title":"Overview of DevSecOps Services"},{"location":"devsecops/Building-and-Testing-from-Jenkins/","text":"Build and test from Jenkins \u00b6 Contents \u00b6 Jenkins Folder Structure Configuring AMI Pipelines About Seed Jobs Initial Jenkins Configuration for Seed Jobs Running AMI build Jenkins Configuration Jenkins Folder Structure \u00b6 This section explains how to configure the platform Jenkins instance for running AMI jobs. The platform Jenkins is created and configured by the platform installation process. To add these capabilities to other Jenkins instances, see Jenkins Configuration . Jobs Structure \u00b6 . \u2514\u2500\u2500 Seed Jobs - Folder containing all seed jobs \u251c\u2500\u2500 baseAmis - Platform provided AMIs seed job \u2514\u2500\u2500 amis - Standard AMI seed job Configuring AMI pipelines \u00b6 AMI pipelines are used to build, test, and deliver AMIs in an automated way. The pipelines abstracts away the logic and processes required to Build, Test, and Deliver AMIs. About Seed jobs \u00b6 Seed jobs are Jenkins jobs that create one or more Jenkins jobs. Seeds jobs execute Jenkins Job DSL code. There are two types of seed jobs available for image pipelines: Platform Seed Jobs \u00b6 Platform seed jobs are already existing AMI jobs provided by the platform. They include the hardened BaseOS and other AMIs to run the platform. The Platform Seed jobs creates the required pipelines to build, test, and deliver the platform images. Standard Seed Jobs \u00b6 Standard seed jobs are used to create customer image pipelines. Pipeline-generator creates skeleton projects. The standard seed job then creates pipelines for each of the pipeline projects created with the help of the pipeline-generator gem. Initial Jenkins Configuration for Seed Jobs \u00b6 It is necessary to complete the initial Jenkins configuration before creating the Seed jobs. For more information, see Jenkins Configuration . Following is the list of required Jenkins Plugins: Folders Plugin Job DSL Environment Injector Credentials Binding Git Pipeline: Job (workflow-job) Pipeline Installing the required plugins \u00b6 Login to Jenkins. On the Jenkins homepage, click Manage Jenkins . Click Manage Plugins . Click Available and wait for it to finish loading. Type the name of the plugin in the search bar Filter (e.g. environment injector ). Select the plugin in the list of results and click the check mark to the left of the plugin name. Similarly, search for the other plugins and select the check boxes. After installing all required plugins, click on Download now and install after restart . Select the check box for Restart Jenkins when installation is complete and no jobs are running . Wait for all the plugins to install. The Jenkins will restart itself automatically once all plugins are installed. Configure Jenkins environment variables \u00b6 On the Jenkins homepage, click Manage Jenkins . Click Configure System . Under Global Properties , select the Environment variables checkbox and click Add . Add the Name to what is shown in the image below but the value should be your own artifactory values (these values are merely examples). After adding all the values, scroll to the bottom of the page and click Save . Additionally, STIGTOOL_S3_BUCKET should have the bucket created during installation. Creating Platform Seed Jobs \u00b6 On the Jenkins homepage, click New Item . Enter the name as Seed Jobs , select type Folder and click OK . Enter the Display Name , Description and then click Save . On the Jenkins homepage, click New Item . Enter the data as shown in the image below and click OK . Fill in the Description , then select the This project is parameterized checkbox, and from the drop-down select String Parameter . Enter the following entries as shown below: Under Source Code Management , enter the details as shown in the image below. Note: The Git Credentials must be configured as part of Jenkins initial configuration. The Repository URL must be updated to the repo URL of platform-jenkins-customerjobs in your Git Server Under the Build section, click on Add build step , select Process Job DSLs . Select Look on Filesystem , under DSL Scripts , provide the path as shown below: Click Advanced , the view will expand. Fill in src/main/jobs under Additional classpath and then click on Save . Make sure that Script Security is disabled for Job DSL under Jenkins Global Security Running Platform Seed jobs \u00b6 On the Jenkins homepage, click Seed Jobs . Click baseAMIs . Click Build with Parameters . Enter the following details: extraEnvVars - Any extra environmental variables to be added to the created jobs, keep empty if not needed. configRepo - External configuration repository in your Git server. branch - The branch to pull the seed job logic from. Click Build . On the Jenkins UI, there should be a folder named AMIs . Creating standard Seed Job \u00b6 On the Jenkins homepage, click Seed Jobs . Click New Item . Enter an item name, select Freestyle project and click OK . Fill in the job Description and fill in the job parameters as shown in the image below. Scroll down to the Source Code Management section and fill in the details as shown below: Note : The Git Credentials must be configured as part of Jenkins initial configuration. The Repository URL needs to be updated to the repo URL of platform-jenkins-customerjobs from your Git Server. Under the Build section, click Add build step** , select Process Job DSLs . Fill in the config as below and then click Advanced . Add the Additional classpath as src/main/jobs . Scroll to the bottom of the page and click Save . Running standard Seed Job \u00b6 On the Jenkins homepage, click Seed Jobs . Click ami . Click Build with Parameters . Enter the following details that are applicable to the build: application - The name of the application. Can be separated into folders by using / appName - Human readable application name sourceCodeRepo - The Git repository that holds the application source code sourceCodeCredsId - Jenkins credentials id that has the remote git user credentials configRepo - External configuration repository in your Git server. extraEnvVars - extra environmental variables to be added to the created jobs, keep empty if not needed. branch - The branch to pull the seed job logic from. Click Build . On the Jenkins homepage, click AMIs . Click the**Rhel7** folder. Click the ColdFusion folder. The ColdFusion folder must contain three sub-folders AdHoc, Develop, and Release. Running AMI builds \u00b6 AMI jobs folder structure \u00b6 . \u2514\u2500\u2500 amis - Top level folder containing all AMIs jobs. \u2514\u2500\u2500 rhel7 - OS specific images \u2514\u2500\u2500 baseos - A version of the OS specific image pipeline supporting different environments \u251c\u2500\u2500 adhoc - Adhoc AMI jobs \u2502 \u2514\u2500\u2500 Build - Adhoc has only build, this is purely for testing purposes before pull requests are merged to develop \u251c\u2500\u2500 dev - Development AMI jobs (daily CI and development purposes). \u2502 \u251c\u2500\u2500 Build - Builds an image, optionally running tests and compliance scans. \u2502 \u2514\u2500\u2500 Deliver - Creates encrypted copies of the image and copies to workload accounts. \u2514\u2500\u2500 prod - Production AMI jobs (production AMIs releases). \u251c\u2500\u2500 Start - Cuts a release branch from develop \u251c\u2500\u2500 Build - Build the source code of release branch and rollback if anything fails \u251c\u2500\u2500 Finish - Finish the release process by merging the release branch to master and publishing source code and manifests to artifactory \u2514\u2500\u2500 Deliver - Creates encrypted copies of the image in workload accounts Running AMI jobs \u00b6 On the Jenkins homepage, click AMIs . To build RHEL based images, click RHEL 7.x . (The steps for any other images types are similar.) To build a baseOS image, click BaseOS . Select one of the three pipelines: Ad-Hoc : Used for testing changes in image builds without delivering them from an arbitrary SCM branch and from any arbitrary external configuration SCM branch. Develop : Used for building and delivering images from the develop branch of SCM. Release : Used for creating a release branch, building from that branch and releasing a new version and then delivering the image. The folder structure for these folders is explained at Image jobs folder structure . Running Ad-Hoc pipeline \u00b6 Click Ad-Hoc . Click Build . Click Build with Parameters . Enter the following details, and click Build . Value Description branch The SCM branch for the pipeline repository pipelineEnv The environment tag to use for the launched instances pipelineConfigRef The SCM branch for the external config repository Running Develop Pipeline \u00b6 Click Develop . Click Build . Click Build Now . This will build the image and after a successful run it will automatically trigger the Deliver job. Running Release Pipeline \u00b6 Click Release . Click Start . Click Build Now . After a successful run of the Start job, the subsequent jobs will be started automatically. Job Description Start Creates a release branch from the develop branch Build Builds the image from the created release branch Finish Finishes the release and merges the release branch into master and bumps the version in develop branch Deliver Delivers the images to respective accounts Jenkins Configuration \u00b6 The below steps only needs to be performed if your Jenkins is not configured using the Jenkins bootstrap. This section provides instructions how to enhance existing Jenkins with the platform capabilities. Prerequisites : JENKINS_HOME directory is used to provide disk space for builds and archives. JENKINS_HOME is an environment variable. The value of JENKINS_HOME is available in the Jenkins configuration screen ( Manage Jenkins\\Configure System ). Artifactory Artifactory DNS: This is the URL of artifactory where you will download and install packages from. Artifactory UserName: This is the username to authenticate artifactory. Artifactory Password: This is the Password to authenticate artifactory. Git Server Git server DNS Git server credentials Installation and configuration instructions : Configuring REAN Platform Artifactory repo to download packages To resolve .rpm files using the YUM client, edit or create the artifactory.repo file with root privileges: sudo vi /etc/yum.repos.d/artifactory.repo update file with following contents: [ Artifactory ] name = Artifactory baseurl = https://<URL_ENCODED_USERNAME>:<PASSWORD>@<Artifactory DNS>/artifactory/virtual-yum-rhel7 enabled = 1 gpgcheck = 0 Installing necessary yum packages Some yum packages are pre-requisites for jenkins. command to install package wget sudo yum -y install wget command to install package git sudo yum -y install git command to install package jq sudo yum -y install jq command to install package maven sudo yum -y install maven Python If you don't have Python 3, please use instructions below to install it sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python3.6 -V python-pip pip is the standard package manager for Python. curl -O https://bootstrap.pypa.io/get-pip.py # if you are using python2.X python get-pip.py --user # if you are using python 3.X python3 get-pip.py --user export PATH = ~/.local/bin: $PATH source ~/.bash_profile Now you can test to verify that pip is installed correctly. pip3 --version AWS CLI This is required for executing AWS commands. pip3 install awscli --upgrade --user When you use the --user switch, pip installs the AWS CLI to ~/.local/bin . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/1.16.116 Python/3.6.8 Linux/4.14.77-81.59-amzn2.x86_64 botocore/1.12.106 AWS CLI configuration after AWS CLI is installed, configure AWS credentials using the command below. # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region: <Region of AWS> Default output format [ None ] : < we prefer ` json ` > Installing necessary jenkins plugins. Please see below for the list of required Jenkins plugins. You should have Jenkins admin permissions to install the plugins. You have to navigate to Manage Jenkins and then select Manage Plugins . On the menu tab, please select Available . You will see a small filter on the top right below the username (not the filter to the lest of the username). Paste the plugins one by one and then install them. Some plugins need Jenkins restarts (make sure no jobs are running). You may find some plugins are already installed just skip them or update them if you think they are old. ansicolor build-name-setter build-with-parameters cloudbees-folder config-file-provider configuration-as-code configuration-as-code-support copyartifact credentials credentials-binding doclinks envinject git github greenballs htmlpublisher htmlresource job-dsl jobConfigHistory matrix-auth multiple-scms parameterized-trigger plain-credentials role-strategy RubyMetrics rvm ssh-agent ssh-credentials workflow-aggregator workflow-aggregator ws-cleanup Installing Terraform. wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform/0.11.8/terraform_0.11.8_linux_amd64.zip tar -zxvf terraform_0.11.8_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/bin Installing Terraform Providers. # installing terraform providers wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-aws/2.25.0/terraform-provider-aws_v2.25.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-external/1.0.0_x4/terraform-provider-external_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-helm/0.10.2/terraform-provider-helm_v0.10.2_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-kubernetes/1.8.1/terraform-provider-kubernetes_v1.8.1_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-local/1.3.0/terraform-provider-local_v1.3.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-null/1.0.0_x4/terraform-provider-null_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-template/1.0.0_x4/terraform-provider-template_v1.0.0_x4 chmod +x terraform-provider-* mv terraform-provider-* /usr/local Installing Packer wget https://<Artifactory DNS>/artifactory/virtual-misc/packer/1.4.3/packer_1.4.3_linux_amd64.zip tar -zxvf packer_1.4.3_linux_amd64.zip chmod +x packer sudo mv packer /usr/bin Installing RVM wget https://<Artifactory DNS>/artifactory/virtual-misc/rvm/1.29.3/rvm-1.29.3.tar mkdir rvm && cd rvm tar --strip-components = 1 -xzf ../rvm-stable.tar.gz ./install --auto-dotfiles source ~/.rvm/scripts/rvm wget https://<Artifactory DNS>/artifactory/virtual-misc/ruby/2.5.7/ruby-2.5.7.tar.bz2 # Save these packages for offline use by storing them in the rvm archive folder $rvm_path /archives/ # An alternate archive folder can be specified in the .rvmrc file echo rvm_archives_path = /path/to/tarballs/ >> ~/.rvmrc # Disable automatic dependencies (\"requirements\") fetching: rvm autolibs read-fail # Clean default gems: echo \"\" > ~/.rvm/gemsets/default.gems # Clean global gems: echo \"\" > ~/.rvm/gemsets/global.gems rvm install 2 .5.7 # (this may require sudo password for autolibs) # Set default Ruby version: rvm use 2 .5.7 --default Configuring RubyGems ```bash add new gem source as artifactory \u00b6 gem source -a https:// : @ /artifactory/api/gems/virtual-rubygems/ remove community gem server \u00b6 gem sources -r https://rubygems.org Configure jenkins to release gems into artifactory \u00b6 curl -u : https:// /artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials Configure bundle to use artifactory gem repo \u00b6 bundle config mirror. https://rubygems.org https:// : @ /artifactory/api/gems/virtual-rubygems/ ``` Installing ChefDk sudo yum -y install chefdk Configure Knife Knife is a command-line tool providing communications between the local chef-repo and the Chef server. sudo su tomcat - cd $TOMCAT_HOME mkdir .chef touch ./.chef/knife.rb # update following env vars and update it export $ARTIFACTORY_USER = '< update with your artifactory user account >' export $ARTIFACTORY_KEY = '< update with you artifactory password >' export $ARTIFACTORY_DNS = '< update with artifactory dns eg.. artifactory.reancloud.com >' export $SUPERMARKET_REPO = '< virtual supermarket repo name >' echo \"knife[:supermarket_site]=\" https:// $ARTIFACTORY_USER : $ARTIFACTORY_KEY @ $ARTIFACTORY_DNS /api/chef/ $SUPERMARKET_REPO \" > ./.chef/knife.rb echo \" knife [ :cookbook_path ]= [ '.' , '..' , './cookbooks' , '~/cookbooks' ] \" >> ./.chef/knife.rb Configure Git # create a .gitconfig file in the Jenkins Home cd $JENKINS_HOME touch .gitconfig # add content below to that file # This is Git's per-user configuration file. [ user ] name = < Jenkins Git User Name > email = < Jenkins Email Account > Install REAN-platform CLI gem install reanplatform-tools Install Radar publisher wget $ArtifactoryURL /virtual-misc/<PATH to RADAR Publisher>/radar-publisher.hpi cp radar-publisher.hpi $JENKINS_HOME /plugins/radar-publisher.hpi # restart tomcat or jenkins service # restart tomcat service sudo systemctl tomcat restart # If jenkins is installed with embedded tomcat sudo systemctl jenkins restart Installing kubernetes client. ```bash download kubectl CLI tools from Artifactory \u00b6 wget $ARTIFACTORY_URL/virtual-misc/KUBECTL-PATH/kubectl_ sudo cp kubectl_ /usr/local/bin/kubectl ``` Installing HELM Package ```bash download desired version of HELM \u00b6 wget $ArtifactoryURL/virtual-misc/ /helm-v -linux-amd64.tgz tar -zxvf helm-v -linux-amd64.tgz mv linux-amd64/helm /usr/local/bin/helm ``` Post Install configuration credentials setup Git Credentials Setup, add your GIT credentials for your GIT server with the global scope. Setup ENV Vars for Jenkins Global Properties. ARTIFACTORY_ENDPOINT ARTIFACTORY_INSPEC_URL ARTIFACTORY_USERNAME AWS_DEFAULT_REGION AWS_REGION ENABLE_NESSUS PIPELINE_COLORS RUBYGEMS_PUSH_URL STIGTOOL_S3_BUCKET STIGTOOL_S3_BUCKET should have the bucket created during installation. ARTIFACTORY_ENDPOINT , ARTIFACTORY_INSPEC_URL , ARTIFACTORY_USERNAME , RUBYGEMS_PUSH_URL should be configured with values from the customer artifactory. The variable jenkins-global-propertiess should be configured like below. Note: The artifactory values in the image below should be replaced with your artifactory URL . Configuring the Jenkins shared library. You have to upload the Jenkins shared library into your version control system (git server). Then you have to configure Jenkins to use it like below. Navigate to Manage Jenkins \u2192 Configure System . Update the Global Pipeline Libraries, as shown below.","title":"Build and test from Jenkins"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#build-and-test-from-jenkins","text":"","title":"Build and test from Jenkins"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#contents","text":"Jenkins Folder Structure Configuring AMI Pipelines About Seed Jobs Initial Jenkins Configuration for Seed Jobs Running AMI build Jenkins Configuration","title":"Contents"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#jenkins-folder-structure","text":"This section explains how to configure the platform Jenkins instance for running AMI jobs. The platform Jenkins is created and configured by the platform installation process. To add these capabilities to other Jenkins instances, see Jenkins Configuration .","title":"Jenkins Folder Structure"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#jobs-structure","text":". \u2514\u2500\u2500 Seed Jobs - Folder containing all seed jobs \u251c\u2500\u2500 baseAmis - Platform provided AMIs seed job \u2514\u2500\u2500 amis - Standard AMI seed job","title":"Jobs Structure"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#configuring-ami-pipelines","text":"AMI pipelines are used to build, test, and deliver AMIs in an automated way. The pipelines abstracts away the logic and processes required to Build, Test, and Deliver AMIs.","title":"Configuring AMI pipelines"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#about-seed-jobs","text":"Seed jobs are Jenkins jobs that create one or more Jenkins jobs. Seeds jobs execute Jenkins Job DSL code. There are two types of seed jobs available for image pipelines:","title":"About Seed jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#platform-seed-jobs","text":"Platform seed jobs are already existing AMI jobs provided by the platform. They include the hardened BaseOS and other AMIs to run the platform. The Platform Seed jobs creates the required pipelines to build, test, and deliver the platform images.","title":"Platform Seed Jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#standard-seed-jobs","text":"Standard seed jobs are used to create customer image pipelines. Pipeline-generator creates skeleton projects. The standard seed job then creates pipelines for each of the pipeline projects created with the help of the pipeline-generator gem.","title":"Standard Seed Jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#initial-jenkins-configuration-for-seed-jobs","text":"It is necessary to complete the initial Jenkins configuration before creating the Seed jobs. For more information, see Jenkins Configuration . Following is the list of required Jenkins Plugins: Folders Plugin Job DSL Environment Injector Credentials Binding Git Pipeline: Job (workflow-job) Pipeline","title":"Initial Jenkins Configuration for Seed Jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#installing-the-required-plugins","text":"Login to Jenkins. On the Jenkins homepage, click Manage Jenkins . Click Manage Plugins . Click Available and wait for it to finish loading. Type the name of the plugin in the search bar Filter (e.g. environment injector ). Select the plugin in the list of results and click the check mark to the left of the plugin name. Similarly, search for the other plugins and select the check boxes. After installing all required plugins, click on Download now and install after restart . Select the check box for Restart Jenkins when installation is complete and no jobs are running . Wait for all the plugins to install. The Jenkins will restart itself automatically once all plugins are installed.","title":"Installing the required plugins"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#configure-jenkins-environment-variables","text":"On the Jenkins homepage, click Manage Jenkins . Click Configure System . Under Global Properties , select the Environment variables checkbox and click Add . Add the Name to what is shown in the image below but the value should be your own artifactory values (these values are merely examples). After adding all the values, scroll to the bottom of the page and click Save . Additionally, STIGTOOL_S3_BUCKET should have the bucket created during installation.","title":"Configure Jenkins environment variables"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#creating-platform-seed-jobs","text":"On the Jenkins homepage, click New Item . Enter the name as Seed Jobs , select type Folder and click OK . Enter the Display Name , Description and then click Save . On the Jenkins homepage, click New Item . Enter the data as shown in the image below and click OK . Fill in the Description , then select the This project is parameterized checkbox, and from the drop-down select String Parameter . Enter the following entries as shown below: Under Source Code Management , enter the details as shown in the image below. Note: The Git Credentials must be configured as part of Jenkins initial configuration. The Repository URL must be updated to the repo URL of platform-jenkins-customerjobs in your Git Server Under the Build section, click on Add build step , select Process Job DSLs . Select Look on Filesystem , under DSL Scripts , provide the path as shown below: Click Advanced , the view will expand. Fill in src/main/jobs under Additional classpath and then click on Save . Make sure that Script Security is disabled for Job DSL under Jenkins Global Security","title":"Creating Platform Seed Jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-platform-seed-jobs","text":"On the Jenkins homepage, click Seed Jobs . Click baseAMIs . Click Build with Parameters . Enter the following details: extraEnvVars - Any extra environmental variables to be added to the created jobs, keep empty if not needed. configRepo - External configuration repository in your Git server. branch - The branch to pull the seed job logic from. Click Build . On the Jenkins UI, there should be a folder named AMIs .","title":"Running Platform Seed jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#creating-standard-seed-job","text":"On the Jenkins homepage, click Seed Jobs . Click New Item . Enter an item name, select Freestyle project and click OK . Fill in the job Description and fill in the job parameters as shown in the image below. Scroll down to the Source Code Management section and fill in the details as shown below: Note : The Git Credentials must be configured as part of Jenkins initial configuration. The Repository URL needs to be updated to the repo URL of platform-jenkins-customerjobs from your Git Server. Under the Build section, click Add build step** , select Process Job DSLs . Fill in the config as below and then click Advanced . Add the Additional classpath as src/main/jobs . Scroll to the bottom of the page and click Save .","title":"Creating standard Seed Job"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-standard-seed-job","text":"On the Jenkins homepage, click Seed Jobs . Click ami . Click Build with Parameters . Enter the following details that are applicable to the build: application - The name of the application. Can be separated into folders by using / appName - Human readable application name sourceCodeRepo - The Git repository that holds the application source code sourceCodeCredsId - Jenkins credentials id that has the remote git user credentials configRepo - External configuration repository in your Git server. extraEnvVars - extra environmental variables to be added to the created jobs, keep empty if not needed. branch - The branch to pull the seed job logic from. Click Build . On the Jenkins homepage, click AMIs . Click the**Rhel7** folder. Click the ColdFusion folder. The ColdFusion folder must contain three sub-folders AdHoc, Develop, and Release.","title":"Running standard Seed Job"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-ami-builds","text":"","title":"Running AMI builds"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#ami-jobs-folder-structure","text":". \u2514\u2500\u2500 amis - Top level folder containing all AMIs jobs. \u2514\u2500\u2500 rhel7 - OS specific images \u2514\u2500\u2500 baseos - A version of the OS specific image pipeline supporting different environments \u251c\u2500\u2500 adhoc - Adhoc AMI jobs \u2502 \u2514\u2500\u2500 Build - Adhoc has only build, this is purely for testing purposes before pull requests are merged to develop \u251c\u2500\u2500 dev - Development AMI jobs (daily CI and development purposes). \u2502 \u251c\u2500\u2500 Build - Builds an image, optionally running tests and compliance scans. \u2502 \u2514\u2500\u2500 Deliver - Creates encrypted copies of the image and copies to workload accounts. \u2514\u2500\u2500 prod - Production AMI jobs (production AMIs releases). \u251c\u2500\u2500 Start - Cuts a release branch from develop \u251c\u2500\u2500 Build - Build the source code of release branch and rollback if anything fails \u251c\u2500\u2500 Finish - Finish the release process by merging the release branch to master and publishing source code and manifests to artifactory \u2514\u2500\u2500 Deliver - Creates encrypted copies of the image in workload accounts","title":"AMI jobs folder structure"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-ami-jobs","text":"On the Jenkins homepage, click AMIs . To build RHEL based images, click RHEL 7.x . (The steps for any other images types are similar.) To build a baseOS image, click BaseOS . Select one of the three pipelines: Ad-Hoc : Used for testing changes in image builds without delivering them from an arbitrary SCM branch and from any arbitrary external configuration SCM branch. Develop : Used for building and delivering images from the develop branch of SCM. Release : Used for creating a release branch, building from that branch and releasing a new version and then delivering the image. The folder structure for these folders is explained at Image jobs folder structure .","title":"Running AMI jobs"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-ad-hoc-pipeline","text":"Click Ad-Hoc . Click Build . Click Build with Parameters . Enter the following details, and click Build . Value Description branch The SCM branch for the pipeline repository pipelineEnv The environment tag to use for the launched instances pipelineConfigRef The SCM branch for the external config repository","title":"Running Ad-Hoc pipeline"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-develop-pipeline","text":"Click Develop . Click Build . Click Build Now . This will build the image and after a successful run it will automatically trigger the Deliver job.","title":"Running Develop Pipeline"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#running-release-pipeline","text":"Click Release . Click Start . Click Build Now . After a successful run of the Start job, the subsequent jobs will be started automatically. Job Description Start Creates a release branch from the develop branch Build Builds the image from the created release branch Finish Finishes the release and merges the release branch into master and bumps the version in develop branch Deliver Delivers the images to respective accounts","title":"Running Release Pipeline"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#jenkins-configuration","text":"The below steps only needs to be performed if your Jenkins is not configured using the Jenkins bootstrap. This section provides instructions how to enhance existing Jenkins with the platform capabilities. Prerequisites : JENKINS_HOME directory is used to provide disk space for builds and archives. JENKINS_HOME is an environment variable. The value of JENKINS_HOME is available in the Jenkins configuration screen ( Manage Jenkins\\Configure System ). Artifactory Artifactory DNS: This is the URL of artifactory where you will download and install packages from. Artifactory UserName: This is the username to authenticate artifactory. Artifactory Password: This is the Password to authenticate artifactory. Git Server Git server DNS Git server credentials Installation and configuration instructions : Configuring REAN Platform Artifactory repo to download packages To resolve .rpm files using the YUM client, edit or create the artifactory.repo file with root privileges: sudo vi /etc/yum.repos.d/artifactory.repo update file with following contents: [ Artifactory ] name = Artifactory baseurl = https://<URL_ENCODED_USERNAME>:<PASSWORD>@<Artifactory DNS>/artifactory/virtual-yum-rhel7 enabled = 1 gpgcheck = 0 Installing necessary yum packages Some yum packages are pre-requisites for jenkins. command to install package wget sudo yum -y install wget command to install package git sudo yum -y install git command to install package jq sudo yum -y install jq command to install package maven sudo yum -y install maven Python If you don't have Python 3, please use instructions below to install it sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python3.6 -V python-pip pip is the standard package manager for Python. curl -O https://bootstrap.pypa.io/get-pip.py # if you are using python2.X python get-pip.py --user # if you are using python 3.X python3 get-pip.py --user export PATH = ~/.local/bin: $PATH source ~/.bash_profile Now you can test to verify that pip is installed correctly. pip3 --version AWS CLI This is required for executing AWS commands. pip3 install awscli --upgrade --user When you use the --user switch, pip installs the AWS CLI to ~/.local/bin . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/1.16.116 Python/3.6.8 Linux/4.14.77-81.59-amzn2.x86_64 botocore/1.12.106 AWS CLI configuration after AWS CLI is installed, configure AWS credentials using the command below. # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region: <Region of AWS> Default output format [ None ] : < we prefer ` json ` > Installing necessary jenkins plugins. Please see below for the list of required Jenkins plugins. You should have Jenkins admin permissions to install the plugins. You have to navigate to Manage Jenkins and then select Manage Plugins . On the menu tab, please select Available . You will see a small filter on the top right below the username (not the filter to the lest of the username). Paste the plugins one by one and then install them. Some plugins need Jenkins restarts (make sure no jobs are running). You may find some plugins are already installed just skip them or update them if you think they are old. ansicolor build-name-setter build-with-parameters cloudbees-folder config-file-provider configuration-as-code configuration-as-code-support copyartifact credentials credentials-binding doclinks envinject git github greenballs htmlpublisher htmlresource job-dsl jobConfigHistory matrix-auth multiple-scms parameterized-trigger plain-credentials role-strategy RubyMetrics rvm ssh-agent ssh-credentials workflow-aggregator workflow-aggregator ws-cleanup Installing Terraform. wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform/0.11.8/terraform_0.11.8_linux_amd64.zip tar -zxvf terraform_0.11.8_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/bin Installing Terraform Providers. # installing terraform providers wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-aws/2.25.0/terraform-provider-aws_v2.25.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-external/1.0.0_x4/terraform-provider-external_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-helm/0.10.2/terraform-provider-helm_v0.10.2_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-kubernetes/1.8.1/terraform-provider-kubernetes_v1.8.1_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-local/1.3.0/terraform-provider-local_v1.3.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-null/1.0.0_x4/terraform-provider-null_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-template/1.0.0_x4/terraform-provider-template_v1.0.0_x4 chmod +x terraform-provider-* mv terraform-provider-* /usr/local Installing Packer wget https://<Artifactory DNS>/artifactory/virtual-misc/packer/1.4.3/packer_1.4.3_linux_amd64.zip tar -zxvf packer_1.4.3_linux_amd64.zip chmod +x packer sudo mv packer /usr/bin Installing RVM wget https://<Artifactory DNS>/artifactory/virtual-misc/rvm/1.29.3/rvm-1.29.3.tar mkdir rvm && cd rvm tar --strip-components = 1 -xzf ../rvm-stable.tar.gz ./install --auto-dotfiles source ~/.rvm/scripts/rvm wget https://<Artifactory DNS>/artifactory/virtual-misc/ruby/2.5.7/ruby-2.5.7.tar.bz2 # Save these packages for offline use by storing them in the rvm archive folder $rvm_path /archives/ # An alternate archive folder can be specified in the .rvmrc file echo rvm_archives_path = /path/to/tarballs/ >> ~/.rvmrc # Disable automatic dependencies (\"requirements\") fetching: rvm autolibs read-fail # Clean default gems: echo \"\" > ~/.rvm/gemsets/default.gems # Clean global gems: echo \"\" > ~/.rvm/gemsets/global.gems rvm install 2 .5.7 # (this may require sudo password for autolibs) # Set default Ruby version: rvm use 2 .5.7 --default Configuring RubyGems ```bash","title":"Jenkins Configuration"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#add-new-gem-source-as-artifactory","text":"gem source -a https:// : @ /artifactory/api/gems/virtual-rubygems/","title":"add new gem source as artifactory"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#remove-community-gem-server","text":"gem sources -r https://rubygems.org","title":"remove community gem server"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#configure-jenkins-to-release-gems-into-artifactory","text":"curl -u : https:// /artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials","title":"Configure jenkins to release gems into artifactory"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#configure-bundle-to-use-artifactory-gem-repo","text":"bundle config mirror. https://rubygems.org https:// : @ /artifactory/api/gems/virtual-rubygems/ ``` Installing ChefDk sudo yum -y install chefdk Configure Knife Knife is a command-line tool providing communications between the local chef-repo and the Chef server. sudo su tomcat - cd $TOMCAT_HOME mkdir .chef touch ./.chef/knife.rb # update following env vars and update it export $ARTIFACTORY_USER = '< update with your artifactory user account >' export $ARTIFACTORY_KEY = '< update with you artifactory password >' export $ARTIFACTORY_DNS = '< update with artifactory dns eg.. artifactory.reancloud.com >' export $SUPERMARKET_REPO = '< virtual supermarket repo name >' echo \"knife[:supermarket_site]=\" https:// $ARTIFACTORY_USER : $ARTIFACTORY_KEY @ $ARTIFACTORY_DNS /api/chef/ $SUPERMARKET_REPO \" > ./.chef/knife.rb echo \" knife [ :cookbook_path ]= [ '.' , '..' , './cookbooks' , '~/cookbooks' ] \" >> ./.chef/knife.rb Configure Git # create a .gitconfig file in the Jenkins Home cd $JENKINS_HOME touch .gitconfig # add content below to that file # This is Git's per-user configuration file. [ user ] name = < Jenkins Git User Name > email = < Jenkins Email Account > Install REAN-platform CLI gem install reanplatform-tools Install Radar publisher wget $ArtifactoryURL /virtual-misc/<PATH to RADAR Publisher>/radar-publisher.hpi cp radar-publisher.hpi $JENKINS_HOME /plugins/radar-publisher.hpi # restart tomcat or jenkins service # restart tomcat service sudo systemctl tomcat restart # If jenkins is installed with embedded tomcat sudo systemctl jenkins restart Installing kubernetes client. ```bash","title":"Configure bundle to use artifactory gem repo"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#download-kubectl-cli-tools-from-artifactory","text":"wget $ARTIFACTORY_URL/virtual-misc/KUBECTL-PATH/kubectl_ sudo cp kubectl_ /usr/local/bin/kubectl ``` Installing HELM Package ```bash","title":"download kubectl CLI tools from Artifactory"},{"location":"devsecops/Building-and-Testing-from-Jenkins/#download-desired-version-of-helm","text":"wget $ArtifactoryURL/virtual-misc/ /helm-v -linux-amd64.tgz tar -zxvf helm-v -linux-amd64.tgz mv linux-amd64/helm /usr/local/bin/helm ``` Post Install configuration credentials setup Git Credentials Setup, add your GIT credentials for your GIT server with the global scope. Setup ENV Vars for Jenkins Global Properties. ARTIFACTORY_ENDPOINT ARTIFACTORY_INSPEC_URL ARTIFACTORY_USERNAME AWS_DEFAULT_REGION AWS_REGION ENABLE_NESSUS PIPELINE_COLORS RUBYGEMS_PUSH_URL STIGTOOL_S3_BUCKET STIGTOOL_S3_BUCKET should have the bucket created during installation. ARTIFACTORY_ENDPOINT , ARTIFACTORY_INSPEC_URL , ARTIFACTORY_USERNAME , RUBYGEMS_PUSH_URL should be configured with values from the customer artifactory. The variable jenkins-global-propertiess should be configured like below. Note: The artifactory values in the image below should be replaced with your artifactory URL . Configuring the Jenkins shared library. You have to upload the Jenkins shared library into your version control system (git server). Then you have to configure Jenkins to use it like below. Navigate to Manage Jenkins \u2192 Configure System . Update the Global Pipeline Libraries, as shown below.","title":"download desired version of HELM"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/","text":"Build and test from your workstation \u00b6 Preparing to build \u00b6 The process of building an AMI (Amazon Machine Image) uses various tools such as Chef, Packer, Direnv, and Git. If you are unfamiliar with any of these DevOps tools please take time to read the Learn the Concepts . Building an Amazon Machine Image (AMI) using the Projector Generator \u00b6 Overview \u00b6 The image build/test/deliver stage is responsible for building an AMI, testing it against baseline specifications, running compliance scans against them and then sharing it to multiple workload accounts and creating encrypted copies of the AMI. Machine image pipelines are set of rake tasks defined by the framework. For more information, in the Learn the Concepts topic, see Running builds with Rake . These Rake tasks can be used either in local machine or on Jenkins with appropriate settings and configurations. Before starting we need to set up the machine with appropriate tools. For this please refer the topics Configure a DevSecOps Workstation and Learn the Concepts . Here are the rake tasks in the order: bundle exec rake config :pull This task creates a pipeline-config directory under target directory with the content from external configuration repository. bundle exec rake build will build an AMI using the provided packer template. Note this task actually creates an un-encrypted AMI. bundle exec rake test will launch an instance using the AMI built in previous step and run server tests and compliance tests. bundle exec rake deliver will create encrypted copies of the previously created un-encrypted images. NOTE: The Core account is where we will bake the images. We will actually use the AMIs to spin up instances and run the required application related services in Workload accounts. Before starting this tutorial of building an AMI locally; make sure to first read through the Create Projects , Customize projects , and Configure Projects sections. It is critically important to understand the information presented in those sections before proceeding in this tutorial. Setting up the Build \u00b6 The build task spins up an instance using Packer , runs the Packer provisioners as mentioned in the build.json file of Packer. For more information, see Packer docs Overview of the build.json file \u00b6 The build.json file contains much of the variables and configurations for the packer AMI build. The build.json contains mainly these sections: variables - Default packer variables that can be overridden while building builders - contains what type of AMI is being built and the cloud provider type provisioners - Configuration and scripts that needs to be applied on the AMI post-processors - Tasks that needs to be run after the AMI is built For more information on configuring build.json , in the Learn the Concepts topic, Building with Packer . Multiple AWS Settings also need to be configured as Environmental variables and configuring the profile being used to create this image. The required ENV settings are shown below in step 2. Step 1: Create an AMI project with the project generator using the packer style template. \u00b6 Use the project generator gem to create a new AMI pipeline. generate pipeline my-project ../my-ami -s image/packer Here my-project is the name of the application and my-ami is the folder in which the image pipeline project is generated. All steps below assume that the commands/steps are performed in the my-ami project directory created in the project generator script used above. Step 2: Configure Environment (ENV) Variables \u00b6 It is recommended to use direnv . tool to set the environment variables using the .envrc file. If direnv is not installed, the ENV variables can be set using the export VARIABLE_NAME=value in the terminal. To see all the Environment available variables and their values, in the terminal, type printenv . The benefit of using the direnv tool is that it allows you to create a project level file for ENV variables without cluttering up the global ENV variables or having naming conflicts and/or conflicting values. The following ENV (Environment Variables) values (see table below) should be set when building a local AMI from your workstation. As you can see, the only required one is the AWS_PROFILE which is includes the AWS_REGION within the profile needed for this pipeline. Variable Usage Optional AWS_PROFILE Set this to the same profile as the account in which the AMI is being built true AWS_REGION Region in which the image is being built, if it's not set will read from config false VAR_FILE Set this if using a local override JSON file for packer variables true PACKER_LOG Set to 1 to enable packer debug logs true PACKER_LOG_PATH File to save the build logs to true PIPELINE_CONFIG_REF External Configuration's GIT Branch false PIPELINE_CONFIG_URL External configuration URL false SOURCE_AMI BASE AMI using used for AMI baking process false STIGTOOL_S3_BUCKET Required only if you need publishing STIG state to S3 true Note: The external configuration can be disabled by setting :external_config_type to :none. An example .envrc file export PIPELINE_CONFIG_REF = develop export PIPELINE_CONFIG_URL = git@github.com/reancloud/rean-pipeline-config.git export AWS_PROFILE = rean-pldev export VAR_FILE = vars.json export SOURCE_AMI = ami-053f6d5d4cb2dbae7 Environment variables are used to configure values that the framework passes into the packer builder. Step 3: Modifying config.rb \u00b6 This file should be modified with all user specific configurations. The default variables in the table below are always passed to the packer command. Value Description expiry_date Calculated as the current date plus 1 day aws_region Value of AWS_REGION or AWS_DEFAULT_REGION environment variables build_url Value of BUILD_URL environment variable if it is set, defaults to None The framework sets :default_image_build_vars configuration variable which consists of the following values: Value Description environment Value of PIPELINE_ENV environment variable, defaults to dev. version Current project version in the VERSION file name AMI_NAME environment variable if set source_ami Checks for SOURCE_IMAGE environment variable fallback to SOURCE_IMAGE_FILE ami_users Comma separated value of accounts to which to share the image, get's auto-generated from workload_account*.json from external pipeline repository data The source_ami key specifies the source AMI to be used while building an AMI. When building a base AMI this can either come from the external pipeline repository or the local vars.json or the SOURCE_IMAGE environment variable which has the AMI id. When building a dependent AMI based on an already built AMI, either use the SOURCE_IMAGE or SOURCE_IMAGE_FILE environment variable. The SOURCE_IMAGE_FILE environment variable points to a JSON file containing the AMI info in a format as mentioned below: { \"ami_id\" : \"ami-0f18387ab543f07be\" } The framework sets the variables that are passed to packer command as image_build_vars which initially points to the default_image_build_vars # using the default values set_if_empty :image_build_vars , lambda { fetch ( :default_image_build_vars ) } The user can add additional variables as shown below: # using the default values and adding one variable set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars } The following configuration values are used for AMI server and compliance testing. :compliance_tool : This is the tool used for compliance scanning of STIGs. For more information, in the Learn the Concepts topic, see Security Technical Implementation Guides . Defaults to :stigtool . When using :stigtool , the state publisher s3 bucket can be configured using the environment variable STIGTOOL_S3_BUCKET . To disable the compliance checking, set the value to :none in the config.rb. # disable the compliance checking set :compliance_tool , :none :server_test_tool : The tools used for server testing. Defaults to :inspec . Step 4: Modify the build.json file \u00b6 The project generator sets the Packer build default values in the build.json created in the root project directory. Any value of null in the variables section must either be defined via external configuration repository json files, local var.json file or provided in the config.rb . For more information, in the Create Project, see Set Basic Configuration in Config.rb . All values defined in upper case in build.json in the variables section should be updated with user specified values. Values Description MY_AMI A meaningful name for the packer builder MY_OWNER Propagates as Owner tag for the image MY_PRODUCT Propagates as Product tag for the image MY_PROJECT Propagates as Project tag for the image MY_SG_ID Comma separated list of security groups MY_SOURCE_AMI The source image to be used, can be over-ridden by SOURCE_AMI environment variable MY_SSH_USERNAME The SSH username used to connect to the launched machine MY_SUBNET_ID The subnet id to use MY_VPC_ID The VPC id to use MY_INSTANCE_PROFILE It is in the generated build.json MY_INSTANCE_SIZE It is in the generated build.json Any values defined as null in the variables section of build.json are auto-generated by the framework. Value Description ami_users Comma separated value of accounts to which to share the image, get's auto-generated from workload_account*.json from external pipeline repository data aws_region Value of AWS_REGION or AWS_DEFAULT_REGION environment variables build_url Value of BUILD_URL environment variable if set, defaults to None. environment Value of PIPELINE_ENV environment variable, defaults to dev. expiry_date Calculated as the current date plus 1 day version Current project version in the VERSION file The public_ip variable can be set to true if required. Any of the variables defined in build.json under variables key can be overridden by the local vars.json . If we are not using VPC infrastructure or we haven't been bootstrapped by the framework, the variables vpc_id (Virtual Private Cloud Id), sg_ids (Security Group Ids), subnet_id (Network Subnet Id), etc. should be added to the local vars.json For more information, in the Learn the Concepts topic, see Network Connectivity: AWS image pipeline . Running the build \u00b6 This steps generate Packer inputs from multiple sources (config.rb, external configuration, environment variables etc) and then runs packer to build an initial un-encrypted AMI, creates details of built AMI under target/image.json , target/image-\\<region>.json files and then verifies the AMI is available for further use. Run the following command which will download all the gems and dependencies: bundle install Then run config:pull . It actually creates a pipeline-config directory under target directory with the content from external configuration repository and checkouts the reference branch. The directory target/pipeline-config will have configuration information for all pipelines. bundle exec rake config:pull bundle exec rake build This build rake task above will run the actual AMI build using packer and creates the un-encrypted AMI, then creates target/image.json , target/image-us-east-1.json with the built AMI info and verifies the AMI is available. **Below are examples of generated files from the AMI build: ** target/image.json { \"ami_ids\" : { \"us-east-1\" : \"ami-XXXXXXXXXXXXXXXX\" } } target/image-us-east-1.json { \"ami_id\" : \"ami-XXXXXXXXXXXXXXXXXXXX\" } build rake task dependencies \u00b6 If this is your first time using Rake tasks for task management or if you want to learn more about Ruby Rake task management. For more information, in the Learn the Concepts topic, see Running builds with Rake . . \u2514\u2500\u2500 build - Top level build task \u251c\u2500\u2500 vendor_clean - Cleans the existing vendor-cookbooks folder \u251c\u2500\u2500 vendor-cookbooks - Downloads the cookbook dependencies \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 build:build - Task to runs the image build \u251c\u2500\u2500 build:inputs - Generates the packer inputs(`target/image-inputs.json`) from multiple sources \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 build:prereq_json - Backwards compatibility for builds that uses json_updater packer plugin Running tests and compliance scans \u00b6 The test task is responsible for spinning up a temporary EC2 machine instance based on the created AMI, running baseline tests, running compliance scanning tests, and finally destroying the temporary infrastructure. This uses terraform to launch the temporary infrastructure. The configuration for the terraform is standard across all projects and is stored as test.tf . To learn more about terraform as well as InSpec which are used for the testing, refer to Testing with Terraform and Inspec . The compliance scans are run according to STIG guidelines; to learn more about STIGS--what they are and how they are used, see Security Technical Implementation Guides . The high level overview consists of the following: Create a temporary infrastructure (EC2 machine) using terraform from the previously created AMI Wait for the instance to be available (status checks to pass) Fetch the password for WinRM connection in case of Windows machines Wait for the connection to be available (SSH in case of Linux machines and WinRM in case of Windows machines) Run Inspec scans to test the EC2 instance against baseline configuration Run compliance scans using Nessus/Inspec if compliance scanning is enabled Generate/Record the results if compliance scanning is enabled Destroy the temporary infrastructure using terraform, independent of the status of the test/compliance scanning Running Test \u00b6 Run tests on an AMI that was built during the previous steps. This assumes that the target/image.json file is created as part of the AMI build process. Option 1 \u00b6 Runs the whole test process mentioned above from the overview in a single command. bundle exec rake test If any task/stage fails the AMI get de-registered Option 2 \u00b6 Runs each stage of the test process individually This helps to run baseline tests/compliance scans over and over again without having to create/destroy temporary EC2 machine every single time. This method should be used while running scans/tests locally. Test Option 2 > Step 1 bundle exec rake test:pre This task will first generate a temporary SSH keypair and launches the EC2 using terraform Test Option 2 > Step 2 bundle exec rake test:integration This task runs the baseline scans on the AMI as per the test profiles present under the test directory. This task can be re-run after fixing any failing tests to confirm they are working without having to re-create the EC2 instance. Test Option 2 > Step 3 bundle exec rake test:compliance This task runs the compliance scans on the AMI as per the compliance profiles present under the audit directory. This task can be re-run after fixing any failing compliance tests to confirm they are working without having to re-create the EC2 instance. Test Option 2 > Step 4 bundle exec rake test:post This task is used to destroy the temporary EC2 that was spun up to run tests/compliance scans. Make sure to run this task always after running tests/compliance scans locally . Test rake task dependencies listing \u00b6 If any stage/task in the test stage fails the framework will call the expire or destroy task as per the configuration. The default action is to destroy Note: See in the tree below if you call test at the outer level, it will call all the subsequent tasks within this task. However, you can also call test:pre , test:integration , test:compliance , and test:post separately which allows more flexibility and not needing to re-create the EC2 instance when re-running tests. . \u2514\u2500\u2500 test - Top level test task \u251c\u2500\u2500 test:pre - Creates temporary infrastructure for the tests \u2502 \u2514\u2500\u2500 test:infra:deploy:test - Infra task which deploys a server internally named **test** server \u2502 \u251c\u2500\u2500 target - Creates the target folder if missing \u2502 \u251c\u2500\u2500 test:infra:deploy:test:init - Task to run **terraform init** \u2502 \u2502 \u251c\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2502 \u251c\u2500\u2500 build:inputs - Generates the packer inputs from multiple sources \u2502 \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2502 \u2514\u2500\u2500 test:infra:keypair - Generates temporary keypair for standing up the ec2 and for connecting to the launched instance \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u251c\u2500\u2500 test:infra:deploy:test:apply - Task to run **terraform apply** \u2502 \u2502 \u2514\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2514\u2500\u2500 test:infra:deploy:test:outputs - Task to run **terraform output** \u251c\u2500\u2500 test:all - Task to run the integration and compliance scans \u2502 \u251c\u2500\u2500 test:integration - Task to run inspec scans \u2502 \u2502 \u2514\u2500\u2500 test:integration:test - Task to run inspec against the internally named **test** server \u2502 \u2502 \u2514\u2500\u2500 test:integration:test:prepare - Task to check if the ec2 is up and running and is able connect to that \u2502 \u2514\u2500\u2500 test:compliance - Task to run STIG compliance scan \u2502 \u251c\u2500\u2500 test:compliance:test - task to run STIG compliance scan against the internally named **test** server \u2502 \u2514\u2500\u2500 test:integration:test:prepare - Task to check if the ec2 is up and running and is able connect to that \u2514\u2500\u2500 test:post - Task to destroy the temporary infrastructure \u2514\u2500\u2500 test:infra:destroy:test - Infra task to destroy the internally named **test** server \u251c\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 test:infra:deploy:test:init - Task to run **terraform init** \u2502 \u2514\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 test:infra:destroy:test:destroy - Task to run **terraform destroy** \u251c\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 build:inputs - Generates the packer inputs from multiple sources \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 test:infra:keypair - Generates temporary keypair for standing up the ec2 \u2514\u2500\u2500 target - Creates the target folder if missing Manually inspecting a server \u00b6 To manually inspect a server, a server or server_source object needs to be configured / initialized in config.rb, see Configure Projects . To inspect an AMI, you may also choose to manually create the test infrastructure (EC2 instance) and inspect the server brought up by the image. However if testing manually using this method; remember the infrastructure will not automatically be destroyed and you must destroy after inspecting it. Destroying an image \u00b6 When inside the project directory of the AMI image your are working on; you can use a Rake command to destroy the AMI. Again to do this, you need to be inside the AMI project directory as you have been working for all previous sections. If you run the following command as mentioned previously you will see all the rake tasks available: bundle exec rake -T You will see one called destroy and this is the command we will use to target the AMI of the project folder we are in. Run the following command to destroy the current AMI within the directory we are working: bundle exec rake destroy Publishing an image \u00b6 The deliver task optionally shares the already created un-encrypted AMI to the workload accounts--workload accounts are where tasks and builds can happen; however the deployment would take place using the core account--and creates encrypted copies of the AMI in the same account where it was built (core account) and optionally to the external workload accounts. If the Image type is Windows and Sysprep is enabled, an EC2 instance is used for copy using the AWS Instance launch API. If the image type is Windows and Sysprep is disabled and if the image does not have BillingCode EC2 CopyImage API is used. If the image type is Windows and Sysprep is disabled, and if the image have BillingCode packer is used for copy. If the image type is Linux and if the image does not have billing code enabled EC2 CopyImage API is used. If the image type is Linux and if the image have BillingCode packer is used for copy. Fore more information on Sysprep, see Microsoft docs . deliver rake task dependencies \u00b6 The deliver task copies the unencrypted image built at the build stage to various workload accounts. The external configuration should define the workload_accounts. # the workload accounts in `us-gov-west-1` AWS region # the workload account codes are \"rean-sd\", \"pldev\", and \"plwl\" #workload_accounts.us-gov-west-1.json { \"rean-sd\" : { \"subnet_id\" : \"subnet-87e34ce3\" , \"account_id\" : \"XXXXX\" , \"vpc_id\" : \"vpc-95691df0\" , \"sg_id\" : \"sg-492fd32d\" }, \"pldev\" : { \"subnet_id\" : \"subnet-36add540\" , \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-6e4ff30a\" , \"sg_id\" : \"sg-4e6f5329\" }, \"plwl\" : { \"subnet_id\" : \"subnet-61e90d05\" , \"account_id\" : \"XXXXXXXX\" , \"sg_id\" : \"sg-bd29d5d9\" , \"vpc_id\" : \"vpc-4c6b1f29\" } } # the workload accounts in `us-east-1.` AWS region # the workload account codes are \"product\", \"trainee\", and \"reanms\" #workload_accounts.us-east-1.json { \"product\" : { #core account \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-0181557b\" , \"subnet_id\" : \"subnet-29cb4f75\" , \"sg_id\" :\"sg-6f18e222\" , \"owner\" : \"engineer1\" }, \"trainee\" : { \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-07ced0d080a8f9419\" , \"sg_id\" : \"sg-0ab199d93a6328210\" , \"subnet_id\" : \"subnet-04bab323ac37ee117\" , \"owner\" : \"engineer1\" }, \"reanms\" : { \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-0998d5622512e678e\" , \"subnet_id\" : \"subnet-0b7c153ee6773191d\" , \"sg_id\" :\"sg-03429fd7a87beff63\" , \"owner\" : \"engineer1\" } } . \u2514\u2500\u2500 deliver - Top level deliver tasks \u2514\u2500\u2500 deliver:all - Task to call all deliver copies \u2514\u2500\u2500 deliver:copy - Task which calls the copy \u251c\u2500\u2500 deliver:copy:<workload-account-1> - Copy to workload account 1 \u251c\u2500\u2500 deliver:copy:<workload-account-2> - Copy to workload account 2 \u251c\u2500\u2500 deliver:copy:<workload-account-.> - .......................... \u251c\u2500\u2500 deliver:copy:<workload-account-.> - .......................... \u2514\u2500\u2500 deliver:copy:<workload-account-n> - Copy to workload account **n** The workload accounts deliver tasks dynamically generated based on workload accounts data. By default the framework looks for a JSON file at target/pipeline-config/amis/workload_account.\\<region>.json The AWS CLI needs to be configured with all workload account credentials. The framework expects one AWS profile per the workload account. The AWS profile names should match the workload accounts. The framework passes in the profile name when running the workload account copy task, so the AWS CLI needs to be configured for each workload account in the workload_accounts. .json. #example config ~/.aws/config [ pldev ] #AWS_PROFILE as referrenced in the work load accounts region = us-gov-west-1 # Default region aws_access_key_id = AKIAZNNNNNNNNNNNNN aws_secret_access_key = 1234567890 Note : One of work load accounts could actually be a core account in which the AMI is built at the build stage. It is added to workload accounts so that it can be copied as an encrypted AMI. During the build phase, the unencrypted AMI is shared across all workload accounts but due to AWS licensing issues, some AMIs can not be copied using the AWS copy AMI API unless it is in the same account. So the deliver task will have to rebuild the AMI by launching an AMI using unencrypted AMIs and then encrypt the resulting AMI. If our AWS_REGION is us-east-1 and the workload accounts are product , trainee , and reanms , we will have the following deliver tasks. - rake deliver - rake deliver:all - This task uses already shared AMI during build task. - rake deliver:copy - This is a combination of tasks below - rake deliver:copy:product - This task uses AWS AMI copy API as it runs in a core account. This task also validates if the AMI is already copied by using name tags. If it is already copied, then it will not create another AMI (this rule also applies for the other two tasks). - rake deliver:copy:trainee - This task rebuilds AMI using the unencrypted AMI built at the build stage and encrypts it - rake deliver:copy:reanms - This task rebuilds AMI using the unencrypted AMI built at the build stage and encrypts it","title":"Build and test from your workstation"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#build-and-test-from-your-workstation","text":"","title":"Build and test from your workstation"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#preparing-to-build","text":"The process of building an AMI (Amazon Machine Image) uses various tools such as Chef, Packer, Direnv, and Git. If you are unfamiliar with any of these DevOps tools please take time to read the Learn the Concepts .","title":"Preparing to build"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#building-an-amazon-machine-image-ami-using-the-projector-generator","text":"","title":"Building an Amazon Machine Image (AMI) using the Projector Generator"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#overview","text":"The image build/test/deliver stage is responsible for building an AMI, testing it against baseline specifications, running compliance scans against them and then sharing it to multiple workload accounts and creating encrypted copies of the AMI. Machine image pipelines are set of rake tasks defined by the framework. For more information, in the Learn the Concepts topic, see Running builds with Rake . These Rake tasks can be used either in local machine or on Jenkins with appropriate settings and configurations. Before starting we need to set up the machine with appropriate tools. For this please refer the topics Configure a DevSecOps Workstation and Learn the Concepts . Here are the rake tasks in the order: bundle exec rake config :pull This task creates a pipeline-config directory under target directory with the content from external configuration repository. bundle exec rake build will build an AMI using the provided packer template. Note this task actually creates an un-encrypted AMI. bundle exec rake test will launch an instance using the AMI built in previous step and run server tests and compliance tests. bundle exec rake deliver will create encrypted copies of the previously created un-encrypted images. NOTE: The Core account is where we will bake the images. We will actually use the AMIs to spin up instances and run the required application related services in Workload accounts. Before starting this tutorial of building an AMI locally; make sure to first read through the Create Projects , Customize projects , and Configure Projects sections. It is critically important to understand the information presented in those sections before proceeding in this tutorial.","title":"Overview"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#setting-up-the-build","text":"The build task spins up an instance using Packer , runs the Packer provisioners as mentioned in the build.json file of Packer. For more information, see Packer docs","title":"Setting up the Build"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#overview-of-the-buildjson-file","text":"The build.json file contains much of the variables and configurations for the packer AMI build. The build.json contains mainly these sections: variables - Default packer variables that can be overridden while building builders - contains what type of AMI is being built and the cloud provider type provisioners - Configuration and scripts that needs to be applied on the AMI post-processors - Tasks that needs to be run after the AMI is built For more information on configuring build.json , in the Learn the Concepts topic, Building with Packer . Multiple AWS Settings also need to be configured as Environmental variables and configuring the profile being used to create this image. The required ENV settings are shown below in step 2.","title":"Overview of the build.json file"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#step-1-create-an-ami-project-with-the-project-generator-using-the-packer-style-template","text":"Use the project generator gem to create a new AMI pipeline. generate pipeline my-project ../my-ami -s image/packer Here my-project is the name of the application and my-ami is the folder in which the image pipeline project is generated. All steps below assume that the commands/steps are performed in the my-ami project directory created in the project generator script used above.","title":"Step 1: Create an AMI project with the project generator using the packer style template."},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#step-2-configure-environment-env-variables","text":"It is recommended to use direnv . tool to set the environment variables using the .envrc file. If direnv is not installed, the ENV variables can be set using the export VARIABLE_NAME=value in the terminal. To see all the Environment available variables and their values, in the terminal, type printenv . The benefit of using the direnv tool is that it allows you to create a project level file for ENV variables without cluttering up the global ENV variables or having naming conflicts and/or conflicting values. The following ENV (Environment Variables) values (see table below) should be set when building a local AMI from your workstation. As you can see, the only required one is the AWS_PROFILE which is includes the AWS_REGION within the profile needed for this pipeline. Variable Usage Optional AWS_PROFILE Set this to the same profile as the account in which the AMI is being built true AWS_REGION Region in which the image is being built, if it's not set will read from config false VAR_FILE Set this if using a local override JSON file for packer variables true PACKER_LOG Set to 1 to enable packer debug logs true PACKER_LOG_PATH File to save the build logs to true PIPELINE_CONFIG_REF External Configuration's GIT Branch false PIPELINE_CONFIG_URL External configuration URL false SOURCE_AMI BASE AMI using used for AMI baking process false STIGTOOL_S3_BUCKET Required only if you need publishing STIG state to S3 true Note: The external configuration can be disabled by setting :external_config_type to :none. An example .envrc file export PIPELINE_CONFIG_REF = develop export PIPELINE_CONFIG_URL = git@github.com/reancloud/rean-pipeline-config.git export AWS_PROFILE = rean-pldev export VAR_FILE = vars.json export SOURCE_AMI = ami-053f6d5d4cb2dbae7 Environment variables are used to configure values that the framework passes into the packer builder.","title":"Step 2: Configure Environment (ENV) Variables"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#step-3-modifying-configrb","text":"This file should be modified with all user specific configurations. The default variables in the table below are always passed to the packer command. Value Description expiry_date Calculated as the current date plus 1 day aws_region Value of AWS_REGION or AWS_DEFAULT_REGION environment variables build_url Value of BUILD_URL environment variable if it is set, defaults to None The framework sets :default_image_build_vars configuration variable which consists of the following values: Value Description environment Value of PIPELINE_ENV environment variable, defaults to dev. version Current project version in the VERSION file name AMI_NAME environment variable if set source_ami Checks for SOURCE_IMAGE environment variable fallback to SOURCE_IMAGE_FILE ami_users Comma separated value of accounts to which to share the image, get's auto-generated from workload_account*.json from external pipeline repository data The source_ami key specifies the source AMI to be used while building an AMI. When building a base AMI this can either come from the external pipeline repository or the local vars.json or the SOURCE_IMAGE environment variable which has the AMI id. When building a dependent AMI based on an already built AMI, either use the SOURCE_IMAGE or SOURCE_IMAGE_FILE environment variable. The SOURCE_IMAGE_FILE environment variable points to a JSON file containing the AMI info in a format as mentioned below: { \"ami_id\" : \"ami-0f18387ab543f07be\" } The framework sets the variables that are passed to packer command as image_build_vars which initially points to the default_image_build_vars # using the default values set_if_empty :image_build_vars , lambda { fetch ( :default_image_build_vars ) } The user can add additional variables as shown below: # using the default values and adding one variable set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars } The following configuration values are used for AMI server and compliance testing. :compliance_tool : This is the tool used for compliance scanning of STIGs. For more information, in the Learn the Concepts topic, see Security Technical Implementation Guides . Defaults to :stigtool . When using :stigtool , the state publisher s3 bucket can be configured using the environment variable STIGTOOL_S3_BUCKET . To disable the compliance checking, set the value to :none in the config.rb. # disable the compliance checking set :compliance_tool , :none :server_test_tool : The tools used for server testing. Defaults to :inspec .","title":"Step 3: Modifying config.rb"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#step-4-modify-the-buildjson-file","text":"The project generator sets the Packer build default values in the build.json created in the root project directory. Any value of null in the variables section must either be defined via external configuration repository json files, local var.json file or provided in the config.rb . For more information, in the Create Project, see Set Basic Configuration in Config.rb . All values defined in upper case in build.json in the variables section should be updated with user specified values. Values Description MY_AMI A meaningful name for the packer builder MY_OWNER Propagates as Owner tag for the image MY_PRODUCT Propagates as Product tag for the image MY_PROJECT Propagates as Project tag for the image MY_SG_ID Comma separated list of security groups MY_SOURCE_AMI The source image to be used, can be over-ridden by SOURCE_AMI environment variable MY_SSH_USERNAME The SSH username used to connect to the launched machine MY_SUBNET_ID The subnet id to use MY_VPC_ID The VPC id to use MY_INSTANCE_PROFILE It is in the generated build.json MY_INSTANCE_SIZE It is in the generated build.json Any values defined as null in the variables section of build.json are auto-generated by the framework. Value Description ami_users Comma separated value of accounts to which to share the image, get's auto-generated from workload_account*.json from external pipeline repository data aws_region Value of AWS_REGION or AWS_DEFAULT_REGION environment variables build_url Value of BUILD_URL environment variable if set, defaults to None. environment Value of PIPELINE_ENV environment variable, defaults to dev. expiry_date Calculated as the current date plus 1 day version Current project version in the VERSION file The public_ip variable can be set to true if required. Any of the variables defined in build.json under variables key can be overridden by the local vars.json . If we are not using VPC infrastructure or we haven't been bootstrapped by the framework, the variables vpc_id (Virtual Private Cloud Id), sg_ids (Security Group Ids), subnet_id (Network Subnet Id), etc. should be added to the local vars.json For more information, in the Learn the Concepts topic, see Network Connectivity: AWS image pipeline .","title":"Step 4: Modify the build.json file"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#running-the-build","text":"This steps generate Packer inputs from multiple sources (config.rb, external configuration, environment variables etc) and then runs packer to build an initial un-encrypted AMI, creates details of built AMI under target/image.json , target/image-\\<region>.json files and then verifies the AMI is available for further use. Run the following command which will download all the gems and dependencies: bundle install Then run config:pull . It actually creates a pipeline-config directory under target directory with the content from external configuration repository and checkouts the reference branch. The directory target/pipeline-config will have configuration information for all pipelines. bundle exec rake config:pull bundle exec rake build This build rake task above will run the actual AMI build using packer and creates the un-encrypted AMI, then creates target/image.json , target/image-us-east-1.json with the built AMI info and verifies the AMI is available. **Below are examples of generated files from the AMI build: ** target/image.json { \"ami_ids\" : { \"us-east-1\" : \"ami-XXXXXXXXXXXXXXXX\" } } target/image-us-east-1.json { \"ami_id\" : \"ami-XXXXXXXXXXXXXXXXXXXX\" }","title":"Running the build"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#build-rake-task-dependencies","text":"If this is your first time using Rake tasks for task management or if you want to learn more about Ruby Rake task management. For more information, in the Learn the Concepts topic, see Running builds with Rake . . \u2514\u2500\u2500 build - Top level build task \u251c\u2500\u2500 vendor_clean - Cleans the existing vendor-cookbooks folder \u251c\u2500\u2500 vendor-cookbooks - Downloads the cookbook dependencies \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 build:build - Task to runs the image build \u251c\u2500\u2500 build:inputs - Generates the packer inputs(`target/image-inputs.json`) from multiple sources \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 build:prereq_json - Backwards compatibility for builds that uses json_updater packer plugin","title":"build rake task dependencies"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#running-tests-and-compliance-scans","text":"The test task is responsible for spinning up a temporary EC2 machine instance based on the created AMI, running baseline tests, running compliance scanning tests, and finally destroying the temporary infrastructure. This uses terraform to launch the temporary infrastructure. The configuration for the terraform is standard across all projects and is stored as test.tf . To learn more about terraform as well as InSpec which are used for the testing, refer to Testing with Terraform and Inspec . The compliance scans are run according to STIG guidelines; to learn more about STIGS--what they are and how they are used, see Security Technical Implementation Guides . The high level overview consists of the following: Create a temporary infrastructure (EC2 machine) using terraform from the previously created AMI Wait for the instance to be available (status checks to pass) Fetch the password for WinRM connection in case of Windows machines Wait for the connection to be available (SSH in case of Linux machines and WinRM in case of Windows machines) Run Inspec scans to test the EC2 instance against baseline configuration Run compliance scans using Nessus/Inspec if compliance scanning is enabled Generate/Record the results if compliance scanning is enabled Destroy the temporary infrastructure using terraform, independent of the status of the test/compliance scanning","title":"Running tests and compliance scans"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#running-test","text":"Run tests on an AMI that was built during the previous steps. This assumes that the target/image.json file is created as part of the AMI build process.","title":"Running Test"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#option-1","text":"Runs the whole test process mentioned above from the overview in a single command. bundle exec rake test If any task/stage fails the AMI get de-registered","title":"Option 1"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#option-2","text":"Runs each stage of the test process individually This helps to run baseline tests/compliance scans over and over again without having to create/destroy temporary EC2 machine every single time. This method should be used while running scans/tests locally. Test Option 2 > Step 1 bundle exec rake test:pre This task will first generate a temporary SSH keypair and launches the EC2 using terraform Test Option 2 > Step 2 bundle exec rake test:integration This task runs the baseline scans on the AMI as per the test profiles present under the test directory. This task can be re-run after fixing any failing tests to confirm they are working without having to re-create the EC2 instance. Test Option 2 > Step 3 bundle exec rake test:compliance This task runs the compliance scans on the AMI as per the compliance profiles present under the audit directory. This task can be re-run after fixing any failing compliance tests to confirm they are working without having to re-create the EC2 instance. Test Option 2 > Step 4 bundle exec rake test:post This task is used to destroy the temporary EC2 that was spun up to run tests/compliance scans. Make sure to run this task always after running tests/compliance scans locally .","title":"Option 2"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#test-rake-task-dependencies-listing","text":"If any stage/task in the test stage fails the framework will call the expire or destroy task as per the configuration. The default action is to destroy Note: See in the tree below if you call test at the outer level, it will call all the subsequent tasks within this task. However, you can also call test:pre , test:integration , test:compliance , and test:post separately which allows more flexibility and not needing to re-create the EC2 instance when re-running tests. . \u2514\u2500\u2500 test - Top level test task \u251c\u2500\u2500 test:pre - Creates temporary infrastructure for the tests \u2502 \u2514\u2500\u2500 test:infra:deploy:test - Infra task which deploys a server internally named **test** server \u2502 \u251c\u2500\u2500 target - Creates the target folder if missing \u2502 \u251c\u2500\u2500 test:infra:deploy:test:init - Task to run **terraform init** \u2502 \u2502 \u251c\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2502 \u251c\u2500\u2500 build:inputs - Generates the packer inputs from multiple sources \u2502 \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2502 \u2514\u2500\u2500 test:infra:keypair - Generates temporary keypair for standing up the ec2 and for connecting to the launched instance \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u251c\u2500\u2500 test:infra:deploy:test:apply - Task to run **terraform apply** \u2502 \u2502 \u2514\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2502 \u2514\u2500\u2500 test:infra:deploy:test:outputs - Task to run **terraform output** \u251c\u2500\u2500 test:all - Task to run the integration and compliance scans \u2502 \u251c\u2500\u2500 test:integration - Task to run inspec scans \u2502 \u2502 \u2514\u2500\u2500 test:integration:test - Task to run inspec against the internally named **test** server \u2502 \u2502 \u2514\u2500\u2500 test:integration:test:prepare - Task to check if the ec2 is up and running and is able connect to that \u2502 \u2514\u2500\u2500 test:compliance - Task to run STIG compliance scan \u2502 \u251c\u2500\u2500 test:compliance:test - task to run STIG compliance scan against the internally named **test** server \u2502 \u2514\u2500\u2500 test:integration:test:prepare - Task to check if the ec2 is up and running and is able connect to that \u2514\u2500\u2500 test:post - Task to destroy the temporary infrastructure \u2514\u2500\u2500 test:infra:destroy:test - Infra task to destroy the internally named **test** server \u251c\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 test:infra:deploy:test:init - Task to run **terraform init** \u2502 \u2514\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 test:infra:destroy:test:destroy - Task to run **terraform destroy** \u251c\u2500\u2500 test:infra:deploy:test:inputs - Generates terraform inputs \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u251c\u2500\u2500 build:inputs - Generates the packer inputs from multiple sources \u2502 \u2514\u2500\u2500 target - Creates the target folder if missing \u2514\u2500\u2500 test:infra:keypair - Generates temporary keypair for standing up the ec2 \u2514\u2500\u2500 target - Creates the target folder if missing","title":"Test rake task dependencies listing"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#manually-inspecting-a-server","text":"To manually inspect a server, a server or server_source object needs to be configured / initialized in config.rb, see Configure Projects . To inspect an AMI, you may also choose to manually create the test infrastructure (EC2 instance) and inspect the server brought up by the image. However if testing manually using this method; remember the infrastructure will not automatically be destroyed and you must destroy after inspecting it.","title":"Manually inspecting a server"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#destroying-an-image","text":"When inside the project directory of the AMI image your are working on; you can use a Rake command to destroy the AMI. Again to do this, you need to be inside the AMI project directory as you have been working for all previous sections. If you run the following command as mentioned previously you will see all the rake tasks available: bundle exec rake -T You will see one called destroy and this is the command we will use to target the AMI of the project folder we are in. Run the following command to destroy the current AMI within the directory we are working: bundle exec rake destroy","title":"Destroying an image"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#publishing-an-image","text":"The deliver task optionally shares the already created un-encrypted AMI to the workload accounts--workload accounts are where tasks and builds can happen; however the deployment would take place using the core account--and creates encrypted copies of the AMI in the same account where it was built (core account) and optionally to the external workload accounts. If the Image type is Windows and Sysprep is enabled, an EC2 instance is used for copy using the AWS Instance launch API. If the image type is Windows and Sysprep is disabled and if the image does not have BillingCode EC2 CopyImage API is used. If the image type is Windows and Sysprep is disabled, and if the image have BillingCode packer is used for copy. If the image type is Linux and if the image does not have billing code enabled EC2 CopyImage API is used. If the image type is Linux and if the image have BillingCode packer is used for copy. Fore more information on Sysprep, see Microsoft docs .","title":"Publishing an image"},{"location":"devsecops/Building-and-Testing-from-your-Workstation/#deliver-rake-task-dependencies","text":"The deliver task copies the unencrypted image built at the build stage to various workload accounts. The external configuration should define the workload_accounts. # the workload accounts in `us-gov-west-1` AWS region # the workload account codes are \"rean-sd\", \"pldev\", and \"plwl\" #workload_accounts.us-gov-west-1.json { \"rean-sd\" : { \"subnet_id\" : \"subnet-87e34ce3\" , \"account_id\" : \"XXXXX\" , \"vpc_id\" : \"vpc-95691df0\" , \"sg_id\" : \"sg-492fd32d\" }, \"pldev\" : { \"subnet_id\" : \"subnet-36add540\" , \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-6e4ff30a\" , \"sg_id\" : \"sg-4e6f5329\" }, \"plwl\" : { \"subnet_id\" : \"subnet-61e90d05\" , \"account_id\" : \"XXXXXXXX\" , \"sg_id\" : \"sg-bd29d5d9\" , \"vpc_id\" : \"vpc-4c6b1f29\" } } # the workload accounts in `us-east-1.` AWS region # the workload account codes are \"product\", \"trainee\", and \"reanms\" #workload_accounts.us-east-1.json { \"product\" : { #core account \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-0181557b\" , \"subnet_id\" : \"subnet-29cb4f75\" , \"sg_id\" :\"sg-6f18e222\" , \"owner\" : \"engineer1\" }, \"trainee\" : { \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-07ced0d080a8f9419\" , \"sg_id\" : \"sg-0ab199d93a6328210\" , \"subnet_id\" : \"subnet-04bab323ac37ee117\" , \"owner\" : \"engineer1\" }, \"reanms\" : { \"account_id\" : \"XXXXXXXXX\" , \"vpc_id\" : \"vpc-0998d5622512e678e\" , \"subnet_id\" : \"subnet-0b7c153ee6773191d\" , \"sg_id\" :\"sg-03429fd7a87beff63\" , \"owner\" : \"engineer1\" } } . \u2514\u2500\u2500 deliver - Top level deliver tasks \u2514\u2500\u2500 deliver:all - Task to call all deliver copies \u2514\u2500\u2500 deliver:copy - Task which calls the copy \u251c\u2500\u2500 deliver:copy:<workload-account-1> - Copy to workload account 1 \u251c\u2500\u2500 deliver:copy:<workload-account-2> - Copy to workload account 2 \u251c\u2500\u2500 deliver:copy:<workload-account-.> - .......................... \u251c\u2500\u2500 deliver:copy:<workload-account-.> - .......................... \u2514\u2500\u2500 deliver:copy:<workload-account-n> - Copy to workload account **n** The workload accounts deliver tasks dynamically generated based on workload accounts data. By default the framework looks for a JSON file at target/pipeline-config/amis/workload_account.\\<region>.json The AWS CLI needs to be configured with all workload account credentials. The framework expects one AWS profile per the workload account. The AWS profile names should match the workload accounts. The framework passes in the profile name when running the workload account copy task, so the AWS CLI needs to be configured for each workload account in the workload_accounts. .json. #example config ~/.aws/config [ pldev ] #AWS_PROFILE as referrenced in the work load accounts region = us-gov-west-1 # Default region aws_access_key_id = AKIAZNNNNNNNNNNNNN aws_secret_access_key = 1234567890 Note : One of work load accounts could actually be a core account in which the AMI is built at the build stage. It is added to workload accounts so that it can be copied as an encrypted AMI. During the build phase, the unencrypted AMI is shared across all workload accounts but due to AWS licensing issues, some AMIs can not be copied using the AWS copy AMI API unless it is in the same account. So the deliver task will have to rebuild the AMI by launching an AMI using unencrypted AMIs and then encrypt the resulting AMI. If our AWS_REGION is us-east-1 and the workload accounts are product , trainee , and reanms , we will have the following deliver tasks. - rake deliver - rake deliver:all - This task uses already shared AMI during build task. - rake deliver:copy - This is a combination of tasks below - rake deliver:copy:product - This task uses AWS AMI copy API as it runs in a core account. This task also validates if the AMI is already copied by using name tags. If it is already copied, then it will not create another AMI (this rule also applies for the other two tasks). - rake deliver:copy:trainee - This task rebuilds AMI using the unencrypted AMI built at the build stage and encrypts it - rake deliver:copy:reanms - This task rebuilds AMI using the unencrypted AMI built at the build stage and encrypts it","title":"deliver rake task dependencies"},{"location":"devsecops/Configuring-Projects/","text":"Configure Projects \u00b6 Contents \u00b6 Working with config.rb Variables in config.rb Configuring Values for Variables in the config.rb file Purpose of image_build_vars, image_build, and var_files with build.json Example of config.rb for an AMI project InSpec, Terraform and Compliance testing settings Testing with InSpec Testing with Terraform Working with config.rb \u00b6 The configuration of a local image build is done in the config.rb file. This file is available in the projects root directory. In the config.rb file, you can set various mandatory project variables, and also provide custom values relevant for the image build. Variables in config.rb \u00b6 These values in the table below relevant only for AMI (image/packer) projects. Variable Usage Required Default Value :application The application name yes name from project generator cli :application_type The application_type sets what type of the project you are creating yes 'ami' :compliance_tool Tool to be used for compliance scanning yes :stigtool :compliance_options Options that can be passed into the compliance tool no {} :stigtool_options Options that be be passed into the stigtool no {} Configuring Values for Variables in the config.rb file \u00b6 About set method \u00b6 In the config.rb file, variables are configured using the set method, which adds the key-value pair to the hash of globally defined values. The format for using the set method is set :variable_symbol_name, 'value' . For additional information on using the set method, in the Customizing project topic, see the Adding Configuration Settings. Using Lambdas and function blocks with the set method \u00b6 The set method can also use Ruby lambdas and blocks for values. Using this is equivalent to an anonymous function or closure similar to other languages. Similar to any method in Ruby, the set method does not need a declared return statement and the lambda method will return the last line of the function block. Following is an example of using a Lambda: set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars [ :build_url ] = ENV [ 'BUILD_URL' ] if ENV [ 'BUILD_URL' ] vars [ :ca_cert_bundle ] = target ( 'ca-certs.tar' ) vars } In the code example above, the lambda provides a useful method to set the aggregated vars array (last line returned) to the global hash key of :image_build_vars . In Ruby, you can also set a :symbol hash value using the shorter stabby lambda syntax as in the example below: set :image_build_var_files , -> { fetch ( :default_image_build_var_files ) } Purpose of image_build_vars, image_build, and var_files with build.json \u00b6 build.json and vars.json \u00b6 Any of the variables defined in build.json under variables key can be overridden by the local vars.json . Furthermore, any variables set in the config.rb file will override the vars.json values of the same variable name. image_build_vars and image_build_var_files \u00b6 Additionally, :image_build_vars can be set to add additional values in addition to the build.json variables from the config.rb . :image_build_var_files fetches the various image files where the configuration values are stored. Examples of the :image_build_vars and :image_build_var_files are shown in the example config.rb file below. Example of config.rb for an AMI project \u00b6 set :application , 'baseos-rhel7' # Using defaults to set :image_build_var_files, adding :source_ami and :build_url set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) # this default value is set by the framework vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars [ :build_url ] = ENV [ 'BUILD_URL' ] if ENV [ 'BUILD_URL' ] vars } # Using defaults to set :image_build_var_files set :image_build_var_files , lambda { fetch ( :default_image_build_var_files ) # this default value is set by the framework } InSpec, Terraform and Compliance testing settings \u00b6 Testing with Inspec \u00b6 InSpec is a free and open-source framework for testing and auditing applications and infrastructure. To learn more about InSpec, in the Learn the Concepts section, see Testing with Terraform and Inspec . InSpec server testing \u00b6 InSpec uses test and compliance profiles , which organize controls to support dependency management and code reuse. Each profile is a standalone structure with its own distribution and execution flows. InSpec uses the testing profile which are located in the inspec.yml file. A testing profile should be defined under the test directory under the :application name sub-folder (by default). The sub-folder can be overwritten by setting :test_suite (see below). An example inspec.yml file is shown below--also note only the name value is required at a minimum to be set in profiles. name : REAN-baseos-RHEL7 title : InSpec Profile for REAN RHEL7 maintainer : John Doe copyright : REAN Cloud copyright_email : john.doe@hitachivantara.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.3 supports : - os-family : redhat Individual local InSpec tests can then be added under the controls sub-folder that resides under the :application name sub-folder. When InSpec profiles are used for server testing, we can customize the sub-folder by setting :test_suite to the directory with the InSpec profile. If :test_suite is not set, it defaults to :application for the subfolder under test as noted above. # in this example, we should have our InSpec profile in \"test/my-inspec-profile\" set :test_suite , 'my-inspec-profile' The InSpec executions generate HTML, JSON, and XML reports in target/reports/*<test_profile>*/ . The file names are validation.html , validation.json , validatiion.xml , and validatiion.txt . InSpec options \u00b6 InSpec attributes can be overridden by <server_name>_inspec_attributes and <server_name>_inspec_attributes_files . In the example below, :my_server_name is a name of a defined server, this of course would be named as the server name you are testing. To see how to create servers with servers names, please see the documentation on Adding server and website objects #config.rb set :my_server_name_inspec_attributes_files , 'test/my_server_name/inspec_attributes.yml' set :my_server_name_inspec_attributes , -> { { { \"attr1\" : 'attr1Value' , \"attr2\" : 'attr2Value' } } } \u00b6 Testing with Terraform \u00b6 A test.tf is automatically added to the AMI project and this file is used by Terraform to create a temporary infrastructure (servers) for testing. Compliance testing settings \u00b6 Compliance scanning is supported via InSpec profiles in the audit folder. Within this audit folder is a sub-folder based upon the value of the :application key. The InSpec profile verifies the STIG rules. With compliance scanning, the scans are run against STIGS (Security Technical Implementation Guides) which are basically security protocol rules. For more information on STIGS as well as POAM, Skip and Filter exclusions, see the Security and Compliance section in the Learn the Concepts . When running compliance scanning using the stigtool one can write skip.yml , poam.yml and inspec_attributes.yml ; these files are pulled in via external configuration dependent on the project :application name. An example InSpec profile for the audit folder is shown below. name : REAN-baseos-RHEL7 title : Compliance Audit for REAN RHEL7 maintainer : John Doe copyright : REAN Cloud copyright_email : john.doe@hitachivantara.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 inspec_version : - \"> 3.6.6\" - \"< 4.0\" supports : - os-family : redhat depends : - name : RHEL_7_STIG url : <artifactory_url> username : <artifactory_username> password : <artifactory_password> As you can see the depends section references the name of the STIG which to be downloaded from artifactory which is RHEL_7_STIG Directory of the audit folder \u00b6 \u251c\u2500\u2500 audit \u2502 \u2514\u2500\u2500 baseos-rhel7 \u2502 \u251c\u2500\u2500 controls \u2502 \u2502 \u2514\u2500\u2500 RHEL_7_STIG.rb \u2502 \u251c\u2500\u2500 inspec.lock \u2502 \u2514\u2500\u2500 inspec.yml As you can see above, within the controls sub-folder under the application named sub-folder which is under the audit directory, is where the STIG files are to run with the same name(s) present within the depends block from the inspec profile. Hence from the inspec profile example above, there should be a file in the controls folder named RHEL_7_STIG.rb Inside of this RHEL_7_STIG.rb file the code would look as below: include_controls 'RHEL_7_STIG' do end The above include_controls method calls the STIG downloaded from artifactory to execute and run the compliance scans. However, the question might be 'what if we don't want to run certain security protocol rules to be scanned for various reasons?' This is where POAMs and Skips are used. POAM and Skip options \u00b6 POAMs \u00b6 A POAM is a temporary exclusion for an InSpec test. A POAM will have an expiration date and once that date passes; the compliance test will fail. An example of a POAM exclusion in a POAM.yml file is shown below: - benchmark : RHEL_7_STIG cookbook : STIG-RHEL-7 profile : STIG-RHEL-7 rules : RHEL-07-010500 : reason : \"We need a test environment, and LDAP server, and physical smart cards in order to implement and test this feature.\" requested_by : john.doe@hitachivantara.com requested_on : 2019-07-15 16:47:56 approved_by : approved_on : days_allowed : 180 Notice above the RHEL-07-010500 is the specific name of the STIG rule to be excluded. The POAM also references the profile name. Skip \u00b6 InSpec also supports skipping tests entirely with a Skip . Unlike POAMs, Skips have no expiration date for the particular rule to be excluded. An example of a Skip rule in an aws.<region>.skip.yml is shown below: - benchmark : RHEL_7_STIG cookbook : STIG-RHEL-7 profile : STIG-RHEL-7 rules : RHEL-07-010480 : reason : \"AWS EC2 does not allow booting into single-user or maintenance mode, and does not expose the BIOS to the virtual machine.\" status : notapplicable requested_by : john.doe@hitachivantara.com requested_on : 2018-02-19 16:47:56 approved_by : approved_on : Notice this is a similar structure to the POAM with the name of the STIG to be skipped under the rules block; however a major difference is the naming of the skip file. Skip files can include the aws region; meaning skip rule exclusions can be written for specific regions .","title":"Configure Projects"},{"location":"devsecops/Configuring-Projects/#configure-projects","text":"","title":"Configure Projects"},{"location":"devsecops/Configuring-Projects/#contents","text":"Working with config.rb Variables in config.rb Configuring Values for Variables in the config.rb file Purpose of image_build_vars, image_build, and var_files with build.json Example of config.rb for an AMI project InSpec, Terraform and Compliance testing settings Testing with InSpec Testing with Terraform","title":"Contents"},{"location":"devsecops/Configuring-Projects/#working-with-configrb","text":"The configuration of a local image build is done in the config.rb file. This file is available in the projects root directory. In the config.rb file, you can set various mandatory project variables, and also provide custom values relevant for the image build.","title":"Working with config.rb"},{"location":"devsecops/Configuring-Projects/#variables-in-configrb","text":"These values in the table below relevant only for AMI (image/packer) projects. Variable Usage Required Default Value :application The application name yes name from project generator cli :application_type The application_type sets what type of the project you are creating yes 'ami' :compliance_tool Tool to be used for compliance scanning yes :stigtool :compliance_options Options that can be passed into the compliance tool no {} :stigtool_options Options that be be passed into the stigtool no {}","title":"Variables in config.rb"},{"location":"devsecops/Configuring-Projects/#configuring-values-for-variables-in-the-configrb-file","text":"","title":"Configuring Values for Variables in the config.rb file"},{"location":"devsecops/Configuring-Projects/#about-set-method","text":"In the config.rb file, variables are configured using the set method, which adds the key-value pair to the hash of globally defined values. The format for using the set method is set :variable_symbol_name, 'value' . For additional information on using the set method, in the Customizing project topic, see the Adding Configuration Settings.","title":"About set method"},{"location":"devsecops/Configuring-Projects/#using-lambdas-and-function-blocks-with-the-set-method","text":"The set method can also use Ruby lambdas and blocks for values. Using this is equivalent to an anonymous function or closure similar to other languages. Similar to any method in Ruby, the set method does not need a declared return statement and the lambda method will return the last line of the function block. Following is an example of using a Lambda: set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars [ :build_url ] = ENV [ 'BUILD_URL' ] if ENV [ 'BUILD_URL' ] vars [ :ca_cert_bundle ] = target ( 'ca-certs.tar' ) vars } In the code example above, the lambda provides a useful method to set the aggregated vars array (last line returned) to the global hash key of :image_build_vars . In Ruby, you can also set a :symbol hash value using the shorter stabby lambda syntax as in the example below: set :image_build_var_files , -> { fetch ( :default_image_build_var_files ) }","title":"Using Lambdas and function blocks with the set method"},{"location":"devsecops/Configuring-Projects/#purpose-of-image_build_vars-image_build-and-var_files-with-buildjson","text":"","title":"Purpose of image_build_vars, image_build, and var_files with build.json"},{"location":"devsecops/Configuring-Projects/#buildjson-and-varsjson","text":"Any of the variables defined in build.json under variables key can be overridden by the local vars.json . Furthermore, any variables set in the config.rb file will override the vars.json values of the same variable name.","title":"build.json and vars.json"},{"location":"devsecops/Configuring-Projects/#image_build_vars-and-image_build_var_files","text":"Additionally, :image_build_vars can be set to add additional values in addition to the build.json variables from the config.rb . :image_build_var_files fetches the various image files where the configuration values are stored. Examples of the :image_build_vars and :image_build_var_files are shown in the example config.rb file below.","title":"image_build_vars and image_build_var_files"},{"location":"devsecops/Configuring-Projects/#example-of-configrb-for-an-ami-project","text":"set :application , 'baseos-rhel7' # Using defaults to set :image_build_var_files, adding :source_ami and :build_url set :image_build_vars , lambda { vars = fetch ( :default_image_build_vars ) # this default value is set by the framework vars [ :source_ami ] = ENV [ 'SOURCE_AMI' ] if ENV [ 'SOURCE_AMI' ] vars [ :build_url ] = ENV [ 'BUILD_URL' ] if ENV [ 'BUILD_URL' ] vars } # Using defaults to set :image_build_var_files set :image_build_var_files , lambda { fetch ( :default_image_build_var_files ) # this default value is set by the framework }","title":"Example of config.rb for an AMI project"},{"location":"devsecops/Configuring-Projects/#inspec-terraform-and-compliance-testing-settings","text":"","title":"InSpec, Terraform and Compliance testing settings"},{"location":"devsecops/Configuring-Projects/#testing-with-inspec","text":"InSpec is a free and open-source framework for testing and auditing applications and infrastructure. To learn more about InSpec, in the Learn the Concepts section, see Testing with Terraform and Inspec .","title":"Testing with Inspec"},{"location":"devsecops/Configuring-Projects/#inspec-server-testing","text":"InSpec uses test and compliance profiles , which organize controls to support dependency management and code reuse. Each profile is a standalone structure with its own distribution and execution flows. InSpec uses the testing profile which are located in the inspec.yml file. A testing profile should be defined under the test directory under the :application name sub-folder (by default). The sub-folder can be overwritten by setting :test_suite (see below). An example inspec.yml file is shown below--also note only the name value is required at a minimum to be set in profiles. name : REAN-baseos-RHEL7 title : InSpec Profile for REAN RHEL7 maintainer : John Doe copyright : REAN Cloud copyright_email : john.doe@hitachivantara.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.3 supports : - os-family : redhat Individual local InSpec tests can then be added under the controls sub-folder that resides under the :application name sub-folder. When InSpec profiles are used for server testing, we can customize the sub-folder by setting :test_suite to the directory with the InSpec profile. If :test_suite is not set, it defaults to :application for the subfolder under test as noted above. # in this example, we should have our InSpec profile in \"test/my-inspec-profile\" set :test_suite , 'my-inspec-profile' The InSpec executions generate HTML, JSON, and XML reports in target/reports/*<test_profile>*/ . The file names are validation.html , validation.json , validatiion.xml , and validatiion.txt .","title":"InSpec server testing"},{"location":"devsecops/Configuring-Projects/#inspec-options","text":"InSpec attributes can be overridden by <server_name>_inspec_attributes and <server_name>_inspec_attributes_files . In the example below, :my_server_name is a name of a defined server, this of course would be named as the server name you are testing. To see how to create servers with servers names, please see the documentation on Adding server and website objects","title":"InSpec options"},{"location":"devsecops/Configuring-Projects/#configrb-set-my_server_name_inspec_attributes_files-testmy_server_nameinspec_attributesyml-set-my_server_name_inspec_attributes-attr1-attr1value-attr2-attr2value","text":"","title":"    #config.rb\n    set :my_server_name_inspec_attributes_files, &#39;test/my_server_name/inspec_attributes.yml&#39;\n\n    set :my_server_name_inspec_attributes, -&gt; {\n      {\n        {\n          &quot;attr1&quot; : &#39;attr1Value&#39;,\n          &quot;attr2&quot; : &#39;attr2Value&#39;\n        }\n      }\n\n    }\n"},{"location":"devsecops/Configuring-Projects/#testing-with-terraform","text":"A test.tf is automatically added to the AMI project and this file is used by Terraform to create a temporary infrastructure (servers) for testing.","title":"Testing with Terraform"},{"location":"devsecops/Configuring-Projects/#compliance-testing-settings","text":"Compliance scanning is supported via InSpec profiles in the audit folder. Within this audit folder is a sub-folder based upon the value of the :application key. The InSpec profile verifies the STIG rules. With compliance scanning, the scans are run against STIGS (Security Technical Implementation Guides) which are basically security protocol rules. For more information on STIGS as well as POAM, Skip and Filter exclusions, see the Security and Compliance section in the Learn the Concepts . When running compliance scanning using the stigtool one can write skip.yml , poam.yml and inspec_attributes.yml ; these files are pulled in via external configuration dependent on the project :application name. An example InSpec profile for the audit folder is shown below. name : REAN-baseos-RHEL7 title : Compliance Audit for REAN RHEL7 maintainer : John Doe copyright : REAN Cloud copyright_email : john.doe@hitachivantara.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 inspec_version : - \"> 3.6.6\" - \"< 4.0\" supports : - os-family : redhat depends : - name : RHEL_7_STIG url : <artifactory_url> username : <artifactory_username> password : <artifactory_password> As you can see the depends section references the name of the STIG which to be downloaded from artifactory which is RHEL_7_STIG","title":"Compliance testing settings"},{"location":"devsecops/Configuring-Projects/#directory-of-the-audit-folder","text":"\u251c\u2500\u2500 audit \u2502 \u2514\u2500\u2500 baseos-rhel7 \u2502 \u251c\u2500\u2500 controls \u2502 \u2502 \u2514\u2500\u2500 RHEL_7_STIG.rb \u2502 \u251c\u2500\u2500 inspec.lock \u2502 \u2514\u2500\u2500 inspec.yml As you can see above, within the controls sub-folder under the application named sub-folder which is under the audit directory, is where the STIG files are to run with the same name(s) present within the depends block from the inspec profile. Hence from the inspec profile example above, there should be a file in the controls folder named RHEL_7_STIG.rb Inside of this RHEL_7_STIG.rb file the code would look as below: include_controls 'RHEL_7_STIG' do end The above include_controls method calls the STIG downloaded from artifactory to execute and run the compliance scans. However, the question might be 'what if we don't want to run certain security protocol rules to be scanned for various reasons?' This is where POAMs and Skips are used.","title":"Directory of the audit folder"},{"location":"devsecops/Configuring-Projects/#poam-and-skip-options","text":"","title":"POAM and Skip options"},{"location":"devsecops/Configuring-Projects/#poams","text":"A POAM is a temporary exclusion for an InSpec test. A POAM will have an expiration date and once that date passes; the compliance test will fail. An example of a POAM exclusion in a POAM.yml file is shown below: - benchmark : RHEL_7_STIG cookbook : STIG-RHEL-7 profile : STIG-RHEL-7 rules : RHEL-07-010500 : reason : \"We need a test environment, and LDAP server, and physical smart cards in order to implement and test this feature.\" requested_by : john.doe@hitachivantara.com requested_on : 2019-07-15 16:47:56 approved_by : approved_on : days_allowed : 180 Notice above the RHEL-07-010500 is the specific name of the STIG rule to be excluded. The POAM also references the profile name.","title":"POAMs"},{"location":"devsecops/Configuring-Projects/#skip","text":"InSpec also supports skipping tests entirely with a Skip . Unlike POAMs, Skips have no expiration date for the particular rule to be excluded. An example of a Skip rule in an aws.<region>.skip.yml is shown below: - benchmark : RHEL_7_STIG cookbook : STIG-RHEL-7 profile : STIG-RHEL-7 rules : RHEL-07-010480 : reason : \"AWS EC2 does not allow booting into single-user or maintenance mode, and does not expose the BIOS to the virtual machine.\" status : notapplicable requested_by : john.doe@hitachivantara.com requested_on : 2018-02-19 16:47:56 approved_by : approved_on : Notice this is a similar structure to the POAM with the name of the STIG to be skipped under the rules block; however a major difference is the naming of the skip file. Skip files can include the aws region; meaning skip rule exclusions can be written for specific regions .","title":"Skip"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/","text":"Configure a DevSecOps Workstation \u00b6 You must first configure a machine for creating a DevSecOps project, and then building and testing the pipelines created in that project. The configuration of a DevSecOps workstation can be done on a Linux/ Unix, or a Mac machine, and for a Windows machine you need to install a Linux Virtual Machine(VM). Prerequisites \u00b6 Before you start with the configuration of the workstation, following are the prerequisites: You have access to an active AWS Account. You know how to launch an EC2 instance in AWS. You have the following details of your AWS Account: AWS Account ID AWS Access Key AWS Secret Key VPC-ID Subnet-ID Security Groups for SSH and egress SSH key to login Bastion host (if key is deployed on a private subnet) Bastion key You have an Artifactory account created for you. You have the following Artifactory details: Artifactory URL Artifactory user name Artifactory API key Notes You need an Artifactory API Key and Artifactory username for configuring your system to use artifactory. You need an Artifactory DNS for downloading packages from the artifactory and installing them on your workstation. Configure your workstation: Linux or Unix \u00b6 The Linux configuration can be done using in one of the following options: Using the dev_env docker container Configuring workstation from scratch Using the dev_env docker container \u00b6 The dev_env docker container contains all the prerequisites setup required for the DevSecOps workstation. This configuration process has the following steps: Download and install docker engine Setup environment variable for private docker repository Download the dev_env docker image Launch the docker image Update gem sources an`d bundle configuration Download and install docker engine \u00b6 Install the Docker Engine - Community for CentOS. For information on how to install Docker, see Docker Engine Documentation . Set environment variable for private docker repository \u00b6 After downloading and installing the docker engine, set the private artifactory URL to an environment variable. To create environment variables, it is recommended to use direnv , which can provide project specific functionality. To learn more about direnv , in the Learn the Concepts section, see Managing Environment Variables with direnv If you have not installed direnv , then use the export VARIABLE_NAME=value command to get the private docker registry repository. Add the registry location with the repository name. The docker environment variable DOCKER-REPO will be based off the Artifactory DNS. Example of using this format for an artifactory URL is shown below: export DOCKER-REPO=virtual-docker-reanplatform.<Artifactory DNS> Download the dev_env docker image \u00b6 Use your artifactory credentials to download the docker image (dev_env) from the artifactory Login to the artifactory-based docker registry with the artifactory credentials: # login into to docker registry with read-only credentials docker login $DOCKER -REPO Enter your artifactory username and password after running the login command. Pull the docker image # pull the dev_env image docker pull $DOCKER -REPO/reanplatform-builder_image:latest Mount the local directories \u00b6 Before running the docker image you have to mount the directories. We recommend using two locations. All binaries and any other install related files are in one location and all platform repos are in another location. In the example below we are mounting the binaries to /opt/scratch and the platform repos to /opt/media . docker run -it -d -v <local-directory>/opt/media:Z -v <local-directory>/opt/scratch:Z $DOCKER-REPO/reanplatform-builder_image:latest /bin/bash --login Launch the docker image \u00b6 Once you start the container you can login anytime using following command: # to get the list of running container docker ps # get the container-id of the dev_env and then login into it docker exec -it <container-id> /bin/bash All required DevOps tools are already installed in the dev_env docker image. Update your gem sources and bundle config \u00b6 The following steps must be followed inside the docker container: Configure Gem sources to use the Artifactory rubygems repo gem source -a https://<USERNAME>:<API_KEY>@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/ Remove the community gem repo gem sources -r https://rubygems.org Run the command to setup your gem tool curl -u<USERNAME>:<API_KEY> https://<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials Configure bundler bundle config mirror.https://rubygems.org https://<USERNAME>:<API_KEY>@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/ Configuring workstation from scratch \u00b6 The following instructions are applicable to RedHat compatible Linux distributions. To configure the Linux workstation from scratch use the following steps: Update Yum configuration Install Python-pip Install AWS CLI Configure AWS CLI Install ChefDK Install Packer Install Terraform Install Git Configure bundle and bash profile Install Ruby with RVM Configure Gem sources Configure Bundler Configure Python Update Yum Configuration \u00b6 You must use the Artifactory to install the yum packages. To resolve .rpm files using the yum client, edit or create the artifactory.repo file with root privileges: sudo vi /etc/yum.repos.d/artifactory.repo update file with following contents: [ Artifactory ] name = Artifactory baseurl = https://<URL_ENCODED_USERNAME>:<PASSWORD>@<Artifactory DNS>/artifactory/virtual-yum-rhel7 enabled = 1 gpgcheck = 0 Install Python-pip \u00b6 Python-pip is the standard package manager for Python. If you don't have Python 3, please use instructions below to install: Python Installation in CentOS/RHEL: sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python3.6 -V Installation of Python-pip: curl -O https://bootstrap.pypa.io/get-pip.py # if you are using python2.X python get-pip.py --user # if you are using python 3.X python3 get-pip.py --user export PATH = ~/.local/bin: $PATH source ~/.bash_profile Test to verify if pip is installed correctly. pip3 --version Install AWS CLI \u00b6 AWS CLI is required to execute AWS commands. Use the following command to install AWS CLI: pip3 install awscli --upgrade --user When you use the --user switch, pip installs the AWS CLI to ~/.local/bin . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/1.16.116 Python/3.6.8 Linux/4.14.77-81.59-amzn2.x86_64 botocore/1.12.106 Configure AWS CLI \u00b6 After AWS CLI is installed, configure AWS credentials using the command below. # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region: <Region of AWS> Default output format [ None ] : < we prefer ` json ` > Install ChefDK \u00b6 sudo yum -y install chefdk Install Packer \u00b6 To Learn more about Packer, in the Learn the Concepts topic, see Building with Packer Use wget to install packer: wget https://<Artifactory DNS>/artifactory/virtual-misc/packer/1.4.3/packer_1.4.3_linux_amd64.zip tar -zxvf packer_1.4.3_linux_amd64.zip chmod +x packer sudo mv packer /usr/bin Install Terraform \u00b6 To Learn more about Terraform, in the Learn the Concepts topic, see Testing with Terraform and InSpec . Use the following commands to install Terraform: wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform/0.11.8/terraform_0.11.8_linux_amd64.zip tar -zxvf terraform_0.11.8_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/bin # installing terraform providers wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-aws/2.25.0/terraform-provider-aws_v2.25.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-external/1.0.0_x4/terraform-provider-external_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-helm/0.10.2/terraform-provider-helm_v0.10.2_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-kubernetes/1.8.1/terraform-provider-kubernetes_v1.8.1_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-local/1.3.0/terraform-provider-local_v1.3.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-null/1.0.0_x4/terraform-provider-null_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-template/1.0.0_x4/terraform-provider-template_v1.0.0_x4 chmod +x terraform-provider-* mv terraform-provider-* /usr/local Install Git \u00b6 Git is used for version control. To install git, run the command: sudo yum -y install git Configure bundler and bash profile \u00b6 Configure bundler and bash profiles with Artifactory credentials. Configure the bash profile with (file: ~/.bash_profile) Supermarket URL, Artifactory API KEY, Artifactory Username export supermarket_url = <Artifactory DNS>/artifactory/webapp/api/chef/virtual-supermarket export artifactory_username = <Artifactory Username> export artifactory_api_key = <Artifactory API Key> Install Ruby with RVM \u00b6 wget https://<Artifactory DNS>/artifactory/virtual-misc/rvm/1.29.3/rvm-1.29.3.tar mkdir rvm && cd rvm tar --strip-components = 1 -xzf ../rvm-stable.tar.gz ./install --auto-dotfiles source ~/.rvm/scripts/rvm wget https://<Artifactory DNS>/artifactory/virtual-misc/ruby/2.5.7/ruby-2.5.7.tar.bz2 # Save these packages for offline use by storing them in the rvm archive folder $rvm_path /archives/ # An alternate archive folder can be specified in the .rvmrc file echo rvm_archives_path = /path/to/tarballs/ >> ~/.rvmrc # Disable automatic dependencies (\"requirements\") fetching: rvm autolibs read-fail # Clean default gems: echo \"\" > ~/.rvm/gemsets/default.gems # Clean global gems: echo \"\" > ~/.rvm/gemsets/global.gems rvm install 2 .5.7 # (this may require sudo password for autolibs) # Set default Ruby version: rvm use 2 .5.7 --default # Restart your terminal for RVM to work Configure Gem sources \u00b6 Configuring Gem sources to use Artifactory. gem source -a https://$artifactory_username:$artifactory_api_key@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/ To remove the community gem repo, use: gem sources -r https://rubygems.org If you want to setup the credentials for your gem tool either include your API*KEY in the ~/.gem/credentials file, or run the following command: curl -u$artifactory_username:$artifactory_api_key https://<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials Configure Bundler \u00b6 Execute the command in the terminal to mirror the system to use real-artifactory bundle config mirror.https://rubygems.org https://$artifactory_username:$artifactory_api_key@supermarket_url Configure Python \u00b6 Add the following configuration to ~/.pip/pip.conf [ global ] index-url = https:// $artifactory_username : $artifactory_api_key @<artifactory DNS>/artifactory/api/pypi/virtual-pypi/simple Configure your workstation: macOS \u00b6 Option 1: Using the dev_env docker container \u00b6 Download and install Docker engine on your system. Follow the instructions at Using the dev_env docker container . Option 2: Configuring your workstation from scratch \u00b6 Install the following tools: ChefDK Packer Terraform Brew git envrc AWS-cli Python-pip AWS-Vault You must have administrator computer privileges to install Brew. Install Brew by pasting below command in a macOS Terminal prompt /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Install wget using homebrew brew install wget Installation of packages can be done by using the command or by downloading from official websites of respective packages Download ChefDK from the below link and install it To Install the package go the Downloads folder and double click on the chefdk-3.11.3-1.dmg file. Provide credentials if necessary. wget https://<artifactory DNS>/artifactory/virtual-misc/chefdk/3.18.14-1/chefdk-3.11.3-1.dmg ~/Downloads/ Configure version of Ruby that is included in ChefDK # Lets use chef version of ruby and gems for development. It is not compulsory if you know how to play with ruby please use your own methods. echo 'export PATH=\"/opt/chefdk/embedded/bin:\\$PATH\"' >> ~/.bash_profilee && source ~/.bash_profile Install AWS CLI to do API calls to AWS # Installing AWS CLI to do API calls to AWS brew install awscli Check if AWS CLI is installed using aws -\u2014version Configure AWS credentials using command # Get your AWS access key and secret key from the AWS console # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region name [ None ] : < select a region ie.. ` us-east-1 ` > Default output format [ None ] : < we prefer ` json ` > AWS credentials: (Put in access key, secret access key, region and format) Install packer using brew using the command line # Installing packer which is used to build AMI's brew install packer Download and install Terraform #download terraform wget https://releases.hashicorp.com/terraform/0.11.8/terraform_0.11.8_darwin_amd64.zip ~/Downloads/ unzip ~/Downloads/terraform_0.11.8_darwin_amd64.zip sudo mv ~/Downloads/terraform /usr/local/bin/terraform # To verify the installation terraform version Install direnv using the brew command # use brew to install direnv # if you need more information please refer to https://direnv.net/ brew install direnv Add the following line at the end of the ~/.bash_profile file and use direnv allow to load environment variables for that particular session bash eval \"$(direnv hook bash) Install git brew install git - Install aws-vault A better way to login into AWS console and generation of temporary credentials. for more information pleas visit https://github.com/99designs/aws-vault brew cask install aws-vault Configure bundler and bash profile with artifactory credentials Configure the bash_profile with (file: ~/.bash_profile) Supermarket url, Artifactory password API, Artifactory Username export ARTIFACTORY_USERNAME = <artifactory-username> export ARTIFACTORY_API_KEY = <artifactory-api-key> export ARTIFACTORY_ENDPOINT = <artifactory DNS> export ARTIFACTORY_SUPERMARKET_URL = \"https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/virtual-supermarket/\" After the Artifactory credentials are added above, you either need to reload the bash_profile or move a directory up and back into the project and type direnv allow to load the new ENV variables. Execute the command in the terminal to configure the bundler to use artifactory bundle config mirror.https://rubygems.org \"https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/\" bundle config mirror.http://rubygems.org \"http:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/\" Check if bundler is configured correctly with bundle config Configure gems upload and download from the artifactory #For your gem client to upload and download Gems from this repository you need to add it to your ~/.gemrc file using the following command: gem source -a https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/ curl -u $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY https:// $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials # install framework gems using the command below--note this may take awhile to run gem install pipeline-tasks Install the Python and pip brew install python Configure Python pip Add the following configuration to ~/.pip/pip.conf . [ global ] index-url = https://<ARTIFACTORY_USERNAME>:<ARTIFACTORY_API_KEY>@<artifactory DNS>/artifactory/api/pypi/virtual-pypi/simple Configure your workstation: Windows \u00b6 Option 1: Using the dev_env docker container \u00b6 Download and install Docker engine on your system. Follow the instructions at Configure the dev_env container . Option 2: Using a Linux virtual machine \u00b6 The configuration of DevSecOps can be done on a Linux/ Unix, or a Mac machine, and for a Windows machine you need to install a Linux Virtual Machine(VM). For more information on installation of Linux VM, see Windows Linux Virtual Machine . Configure your Linux workstation \u00b6 In the Configure your Workstation section, follow the instructions in Configuring workstation from scratch . Additional resources \u00b6 VirtualBox How to install CentOS on VirtualBox VMware workstation VirtualBox Images VMware Images","title":"Configure a DevSecOps workstation"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-a-devsecops-workstation","text":"You must first configure a machine for creating a DevSecOps project, and then building and testing the pipelines created in that project. The configuration of a DevSecOps workstation can be done on a Linux/ Unix, or a Mac machine, and for a Windows machine you need to install a Linux Virtual Machine(VM).","title":"Configure a DevSecOps Workstation"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#prerequisites","text":"Before you start with the configuration of the workstation, following are the prerequisites: You have access to an active AWS Account. You know how to launch an EC2 instance in AWS. You have the following details of your AWS Account: AWS Account ID AWS Access Key AWS Secret Key VPC-ID Subnet-ID Security Groups for SSH and egress SSH key to login Bastion host (if key is deployed on a private subnet) Bastion key You have an Artifactory account created for you. You have the following Artifactory details: Artifactory URL Artifactory user name Artifactory API key Notes You need an Artifactory API Key and Artifactory username for configuring your system to use artifactory. You need an Artifactory DNS for downloading packages from the artifactory and installing them on your workstation.","title":"Prerequisites"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-your-workstation-linux-or-unix","text":"The Linux configuration can be done using in one of the following options: Using the dev_env docker container Configuring workstation from scratch","title":"Configure your workstation: Linux or Unix"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#using-the-dev_env-docker-container","text":"The dev_env docker container contains all the prerequisites setup required for the DevSecOps workstation. This configuration process has the following steps: Download and install docker engine Setup environment variable for private docker repository Download the dev_env docker image Launch the docker image Update gem sources an`d bundle configuration","title":"Using the dev_env docker container"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#download-and-install-docker-engine","text":"Install the Docker Engine - Community for CentOS. For information on how to install Docker, see Docker Engine Documentation .","title":"Download and install docker engine"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#set-environment-variable-for-private-docker-repository","text":"After downloading and installing the docker engine, set the private artifactory URL to an environment variable. To create environment variables, it is recommended to use direnv , which can provide project specific functionality. To learn more about direnv , in the Learn the Concepts section, see Managing Environment Variables with direnv If you have not installed direnv , then use the export VARIABLE_NAME=value command to get the private docker registry repository. Add the registry location with the repository name. The docker environment variable DOCKER-REPO will be based off the Artifactory DNS. Example of using this format for an artifactory URL is shown below: export DOCKER-REPO=virtual-docker-reanplatform.<Artifactory DNS>","title":"Set environment variable for private docker repository"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#download-the-dev_env-docker-image","text":"Use your artifactory credentials to download the docker image (dev_env) from the artifactory Login to the artifactory-based docker registry with the artifactory credentials: # login into to docker registry with read-only credentials docker login $DOCKER -REPO Enter your artifactory username and password after running the login command. Pull the docker image # pull the dev_env image docker pull $DOCKER -REPO/reanplatform-builder_image:latest","title":"Download the dev_env docker image"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#mount-the-local-directories","text":"Before running the docker image you have to mount the directories. We recommend using two locations. All binaries and any other install related files are in one location and all platform repos are in another location. In the example below we are mounting the binaries to /opt/scratch and the platform repos to /opt/media . docker run -it -d -v <local-directory>/opt/media:Z -v <local-directory>/opt/scratch:Z $DOCKER-REPO/reanplatform-builder_image:latest /bin/bash --login","title":"Mount the local directories"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#launch-the-docker-image","text":"Once you start the container you can login anytime using following command: # to get the list of running container docker ps # get the container-id of the dev_env and then login into it docker exec -it <container-id> /bin/bash All required DevOps tools are already installed in the dev_env docker image.","title":"Launch the docker image"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#update-your-gem-sources-and-bundle-config","text":"The following steps must be followed inside the docker container: Configure Gem sources to use the Artifactory rubygems repo gem source -a https://<USERNAME>:<API_KEY>@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/ Remove the community gem repo gem sources -r https://rubygems.org Run the command to setup your gem tool curl -u<USERNAME>:<API_KEY> https://<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials Configure bundler bundle config mirror.https://rubygems.org https://<USERNAME>:<API_KEY>@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/","title":"Update your gem sources and bundle config"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configuring-workstation-from-scratch","text":"The following instructions are applicable to RedHat compatible Linux distributions. To configure the Linux workstation from scratch use the following steps: Update Yum configuration Install Python-pip Install AWS CLI Configure AWS CLI Install ChefDK Install Packer Install Terraform Install Git Configure bundle and bash profile Install Ruby with RVM Configure Gem sources Configure Bundler Configure Python","title":"Configuring workstation from scratch"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#update-yum-configuration","text":"You must use the Artifactory to install the yum packages. To resolve .rpm files using the yum client, edit or create the artifactory.repo file with root privileges: sudo vi /etc/yum.repos.d/artifactory.repo update file with following contents: [ Artifactory ] name = Artifactory baseurl = https://<URL_ENCODED_USERNAME>:<PASSWORD>@<Artifactory DNS>/artifactory/virtual-yum-rhel7 enabled = 1 gpgcheck = 0","title":"Update Yum Configuration"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-python-pip","text":"Python-pip is the standard package manager for Python. If you don't have Python 3, please use instructions below to install: Python Installation in CentOS/RHEL: sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python3.6 -V Installation of Python-pip: curl -O https://bootstrap.pypa.io/get-pip.py # if you are using python2.X python get-pip.py --user # if you are using python 3.X python3 get-pip.py --user export PATH = ~/.local/bin: $PATH source ~/.bash_profile Test to verify if pip is installed correctly. pip3 --version","title":"Install Python-pip"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-aws-cli","text":"AWS CLI is required to execute AWS commands. Use the following command to install AWS CLI: pip3 install awscli --upgrade --user When you use the --user switch, pip installs the AWS CLI to ~/.local/bin . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/1.16.116 Python/3.6.8 Linux/4.14.77-81.59-amzn2.x86_64 botocore/1.12.106","title":"Install AWS CLI"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-aws-cli","text":"After AWS CLI is installed, configure AWS credentials using the command below. # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region: <Region of AWS> Default output format [ None ] : < we prefer ` json ` >","title":"Configure AWS CLI"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-chefdk","text":"sudo yum -y install chefdk","title":"Install ChefDK"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-packer","text":"To Learn more about Packer, in the Learn the Concepts topic, see Building with Packer Use wget to install packer: wget https://<Artifactory DNS>/artifactory/virtual-misc/packer/1.4.3/packer_1.4.3_linux_amd64.zip tar -zxvf packer_1.4.3_linux_amd64.zip chmod +x packer sudo mv packer /usr/bin","title":"Install Packer"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-terraform","text":"To Learn more about Terraform, in the Learn the Concepts topic, see Testing with Terraform and InSpec . Use the following commands to install Terraform: wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform/0.11.8/terraform_0.11.8_linux_amd64.zip tar -zxvf terraform_0.11.8_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/bin # installing terraform providers wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-aws/2.25.0/terraform-provider-aws_v2.25.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-external/1.0.0_x4/terraform-provider-external_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-helm/0.10.2/terraform-provider-helm_v0.10.2_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-kubernetes/1.8.1/terraform-provider-kubernetes_v1.8.1_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-local/1.3.0/terraform-provider-local_v1.3.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-null/1.0.0_x4/terraform-provider-null_v1.0.0_x4 wget https://<Artifactory DNS>/artifactory/virtual-misc/terraform-provider-template/1.0.0_x4/terraform-provider-template_v1.0.0_x4 chmod +x terraform-provider-* mv terraform-provider-* /usr/local","title":"Install Terraform"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-git","text":"Git is used for version control. To install git, run the command: sudo yum -y install git","title":"Install Git"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-bundler-and-bash-profile","text":"Configure bundler and bash profiles with Artifactory credentials. Configure the bash profile with (file: ~/.bash_profile) Supermarket URL, Artifactory API KEY, Artifactory Username export supermarket_url = <Artifactory DNS>/artifactory/webapp/api/chef/virtual-supermarket export artifactory_username = <Artifactory Username> export artifactory_api_key = <Artifactory API Key>","title":"Configure bundler and bash profile"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#install-ruby-with-rvm","text":"wget https://<Artifactory DNS>/artifactory/virtual-misc/rvm/1.29.3/rvm-1.29.3.tar mkdir rvm && cd rvm tar --strip-components = 1 -xzf ../rvm-stable.tar.gz ./install --auto-dotfiles source ~/.rvm/scripts/rvm wget https://<Artifactory DNS>/artifactory/virtual-misc/ruby/2.5.7/ruby-2.5.7.tar.bz2 # Save these packages for offline use by storing them in the rvm archive folder $rvm_path /archives/ # An alternate archive folder can be specified in the .rvmrc file echo rvm_archives_path = /path/to/tarballs/ >> ~/.rvmrc # Disable automatic dependencies (\"requirements\") fetching: rvm autolibs read-fail # Clean default gems: echo \"\" > ~/.rvm/gemsets/default.gems # Clean global gems: echo \"\" > ~/.rvm/gemsets/global.gems rvm install 2 .5.7 # (this may require sudo password for autolibs) # Set default Ruby version: rvm use 2 .5.7 --default # Restart your terminal for RVM to work","title":"Install Ruby with RVM"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-gem-sources","text":"Configuring Gem sources to use Artifactory. gem source -a https://$artifactory_username:$artifactory_api_key@<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/ To remove the community gem repo, use: gem sources -r https://rubygems.org If you want to setup the credentials for your gem tool either include your API*KEY in the ~/.gem/credentials file, or run the following command: curl -u$artifactory_username:$artifactory_api_key https://<Artifactory DNS>/artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials","title":"Configure Gem sources"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-bundler","text":"Execute the command in the terminal to mirror the system to use real-artifactory bundle config mirror.https://rubygems.org https://$artifactory_username:$artifactory_api_key@supermarket_url","title":"Configure Bundler"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-python","text":"Add the following configuration to ~/.pip/pip.conf [ global ] index-url = https:// $artifactory_username : $artifactory_api_key @<artifactory DNS>/artifactory/api/pypi/virtual-pypi/simple","title":"Configure Python"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-your-workstation-macos","text":"","title":"Configure your workstation: macOS"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#option-1-using-the-dev_env-docker-container","text":"Download and install Docker engine on your system. Follow the instructions at Using the dev_env docker container .","title":"Option 1: Using the dev_env docker container"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#option-2-configuring-your-workstation-from-scratch","text":"Install the following tools: ChefDK Packer Terraform Brew git envrc AWS-cli Python-pip AWS-Vault You must have administrator computer privileges to install Brew. Install Brew by pasting below command in a macOS Terminal prompt /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Install wget using homebrew brew install wget Installation of packages can be done by using the command or by downloading from official websites of respective packages Download ChefDK from the below link and install it To Install the package go the Downloads folder and double click on the chefdk-3.11.3-1.dmg file. Provide credentials if necessary. wget https://<artifactory DNS>/artifactory/virtual-misc/chefdk/3.18.14-1/chefdk-3.11.3-1.dmg ~/Downloads/ Configure version of Ruby that is included in ChefDK # Lets use chef version of ruby and gems for development. It is not compulsory if you know how to play with ruby please use your own methods. echo 'export PATH=\"/opt/chefdk/embedded/bin:\\$PATH\"' >> ~/.bash_profilee && source ~/.bash_profile Install AWS CLI to do API calls to AWS # Installing AWS CLI to do API calls to AWS brew install awscli Check if AWS CLI is installed using aws -\u2014version Configure AWS credentials using command # Get your AWS access key and secret key from the AWS console # Command to configure AWS credentials aws configure --profile <account-name> AWS Access Key ID [ None ] : <your access key ID> AWS Secret Access Key [ None ] : <your secret key> Default region name [ None ] : < select a region ie.. ` us-east-1 ` > Default output format [ None ] : < we prefer ` json ` > AWS credentials: (Put in access key, secret access key, region and format) Install packer using brew using the command line # Installing packer which is used to build AMI's brew install packer Download and install Terraform #download terraform wget https://releases.hashicorp.com/terraform/0.11.8/terraform_0.11.8_darwin_amd64.zip ~/Downloads/ unzip ~/Downloads/terraform_0.11.8_darwin_amd64.zip sudo mv ~/Downloads/terraform /usr/local/bin/terraform # To verify the installation terraform version Install direnv using the brew command # use brew to install direnv # if you need more information please refer to https://direnv.net/ brew install direnv Add the following line at the end of the ~/.bash_profile file and use direnv allow to load environment variables for that particular session bash eval \"$(direnv hook bash) Install git brew install git - Install aws-vault A better way to login into AWS console and generation of temporary credentials. for more information pleas visit https://github.com/99designs/aws-vault brew cask install aws-vault Configure bundler and bash profile with artifactory credentials Configure the bash_profile with (file: ~/.bash_profile) Supermarket url, Artifactory password API, Artifactory Username export ARTIFACTORY_USERNAME = <artifactory-username> export ARTIFACTORY_API_KEY = <artifactory-api-key> export ARTIFACTORY_ENDPOINT = <artifactory DNS> export ARTIFACTORY_SUPERMARKET_URL = \"https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/virtual-supermarket/\" After the Artifactory credentials are added above, you either need to reload the bash_profile or move a directory up and back into the project and type direnv allow to load the new ENV variables. Execute the command in the terminal to configure the bundler to use artifactory bundle config mirror.https://rubygems.org \"https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/\" bundle config mirror.http://rubygems.org \"http:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/\" Check if bundler is configured correctly with bundle config Configure gems upload and download from the artifactory #For your gem client to upload and download Gems from this repository you need to add it to your ~/.gemrc file using the following command: gem source -a https:// $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY @ $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/ curl -u $ARTIFACTORY_USERNAME : $ARTIFACTORY_API_KEY https:// $ARTIFACTORY_ENDPOINT /artifactory/api/gems/virtual-rubygems/api/v1/api_key.yaml > ~/.gem/credentials # install framework gems using the command below--note this may take awhile to run gem install pipeline-tasks Install the Python and pip brew install python Configure Python pip Add the following configuration to ~/.pip/pip.conf . [ global ] index-url = https://<ARTIFACTORY_USERNAME>:<ARTIFACTORY_API_KEY>@<artifactory DNS>/artifactory/api/pypi/virtual-pypi/simple","title":"Option 2: Configuring your workstation from scratch"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-your-workstation-windows","text":"","title":"Configure your workstation: Windows"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#option-1-using-the-dev_env-docker-container_1","text":"Download and install Docker engine on your system. Follow the instructions at Configure the dev_env container .","title":"Option 1: Using the dev_env docker container"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#option-2-using-a-linux-virtual-machine","text":"The configuration of DevSecOps can be done on a Linux/ Unix, or a Mac machine, and for a Windows machine you need to install a Linux Virtual Machine(VM). For more information on installation of Linux VM, see Windows Linux Virtual Machine .","title":"Option 2: Using a Linux virtual machine"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#configure-your-linux-workstation","text":"In the Configure your Workstation section, follow the instructions in Configuring workstation from scratch .","title":"Configure your Linux workstation"},{"location":"devsecops/Configuring-a-DevSecOps-Workstation/#additional-resources","text":"VirtualBox How to install CentOS on VirtualBox VMware workstation VirtualBox Images VMware Images","title":"Additional resources"},{"location":"devsecops/Creating-Projects/","text":"Create Projects \u00b6 The project generator creates a Ruby-based project providing AMI creation capabilities. In the further sections of this document we will explain how to customize the project to successfully create AMIs. Contents \u00b6 Prerequisites Step by step guide Generating a Project Prerequisites \u00b6 Before you start creating a project, ensure that you have: An access to Artifactory that has all the required artifacts for the project. An access to Git server. AWS credentials for Packer. For more information on how to use AWS credentials with Packer, visit Packer Documentation, and see AWS Authentication . Step-by-step guide \u00b6 The following steps are required for creating AMI pipelines: Generate project using project generator Set Basic Configuration in Config.rb Creating a Packer template Add required server configuration using chef cookbooks Add Inspec tests to validate server configuration Commit to Git Additional Commands Generating a project \u00b6 The project creation depends on the pipeline-generator ruby gem To install the gem locally, use the command: gem install pipeline-generator To generate the custom project on your local machine, use the command: generate pipeline [APP_NAME] [DIR] --style=[STYLE] pipeline : indicates that we're creating a new pipeline codebase [APP_NAME] : name of the pipeline For example, clientname-app-pipeline. [DIR] : directory with the generated output. It can be any directory in the file system. --style (-s) : allows us to specify the type of pipeline we're creating, for AMIs this should be set to image/packer Examples \u00b6 Generating project in a parent directory In this example, the project generator creates files with the application name base-rhel7 in the companyxyz-rhel7 directory in the parent directory. generate pipeline base-rhel7 ../companyxyz-rhel7 -s image/packer . \u2514\u2500\u2500 parent directory \u251c\u2500\u2500 current directory - the command is executed here \u2514\u2500\u2500 companyxyz-rhel7 - this is the directory with generated files Generate project in a subdirectory In this example, the project generator creates files with the application name base-rhel7 in the companyabc-rhel7 subdirectory of the current directory. generate pipeline base-rhel7 companyabc-rhel7 -s image/packer . \u2514\u2500\u2500 parent directory \u2514\u2500\u2500 current directory - the command is executed here \u2514\u2500\u2500 companyabc-rhel7 - this is the directory with generated files Understanding the generated files \u00b6 The generate pipeline command for image/packer creates the following files: File name File type Purpose Requires customizations Rakefile Rake Loads the AMI rake tasks No config.rb Ruby Configures the AMI pipeline Yes (when changing generated values) build.json JSON The AMI packer template Yes jenkins/Build Jenkinsfile Builds AMI in the core account No jenkins/Deliver Jenkinsfile Delivers AMI to the workload accounts No jenkins/Release/Start Jenkinsfile Starts the release process No jenkins/Release/Build Jenkinsfile Builds the AMI in the context of the release process No jenkins/Release/Deliver Jenkinsfile Delivers the built AMI in the context of the release process No jenkins/Release/Finish Jenkinsfile Finishes the release process No test.tf Terraform Creates EC2 instance for server and compliance scanning No The Jenkins files define Jenkins pipelines using the provided shared Jenkins library. These pipelines execute underlying rake tasks defined for AMI projects. Note: The framework also creates Gemfile and Gemfile.lock . These files describe the Ruby gem dependencies. Working with the Configuration files \u00b6 The config.rb file in the project directory contains the project configuration details. The framework automatically loads config.rb by default. The framework requires the :application configuration setting. The project generator automatically sets :application to the value passed in application name. To change your application name, set :application to a new value. The framework also support loading additional configuration data from other configuration files based on PIPELINE_ENV environment variable. This allows different configuration for different environments, for example development vs production. The additional configuration files must be located under config directory in the project root and the file names have to consist of PIPELINE_ENV values and the .rb file extension. The values in the additional configuration files overrides the values in config.rb . For example, if the same parameter :iamaparameter is set in both config.rb and config/dev.rb and PIPELINE_ENV is set to dev , the value of :iamaparameter in config/dev.rb will be used. In this example, we have three additional configuration files. . \u2514\u2500\u2500 project - project directory \u2514\u2500\u2500 config - config directory \u251c\u2500\u2500 dev.rb - this file is loaded when PIPELINE_ENV is dev \u251c\u2500\u2500 qa.rb - this file is loaded when PIPELINE_ENV is qa \u2514\u2500\u2500 prod.rb - this file is loaded when PIPELINE_ENV is prod PIPELINE_ENV defaults to dev if it hasn't been set. For more information on the parameters in the config.rb file, in the Configuring Projects topic, see Working with config.rb . Creating a Packer template \u00b6 Packer uses a template JSON file to create the Amazon Machine Images (AMIs). In the project generator framework, this template file is build.json and this file is located in the project root folder. To learn more about Packer and the various features this tool offers, in the Learn the Concepts topic, Building with Packer . Overriding config.rb values with build.json values \u00b6 To modify or provide values for the variables that are null in the variables section of build.json ; we can add these variables and values to the config.rb file. Any values provided in the config.rb file for a packer build will override values in the build.json and local vars.json files. For example, you can set a region variable in the config.rb file using the following command: set :aws_region , 'us-east-1' This value of us-east-1 will be used by the packer template during the build process. Mandatory Packer variables \u00b6 When using the project generator, a build.json template is available by default. Following are the basic mandatory parameters for an AMI packer build: variable purpose build.json template location name Name of the builder such as EBS 1-- only needed if multiple builders are used builders type Type of builder being used such as amazon-ebs builders ami_name Name of the AMI being built, a combination of user supplied name, env name and timestamp variables as null, user function variable in builders instance_type The EC2 instance type to use when building the AMI such as t2.small variables source_ami The source AMI whose root volume will be copied and used to provision variables as null, user function variable in builders Adding Chef cookbooks \u00b6 Packer provisioners use built-in and third-party software to install and configure the machine image after booting. For more information see, Packer docs . Installing Chef \u00b6 This step should be done only if the image being built does not have Chef already installed. Packer will automatically attempt to install Chef by default. For more information, in Packer documentation, see Chef Solo Provisioner . If Chef is not installed by default, use the custom script to install Chef. For more information, in Packer documentation, see Chef Solo Install Commands . Creating a Berksfile \u00b6 If a Berksfile is present in the project root the framework assumes that the project uses Chef to provision the image. For more information on how to add Chef cookbook dependencies, in Chef documentation, see About Berksfile . When build task in invoked, the framework cleans any existing cookbooks already vendored, then vendors all the dependent cookbooks to the vendor-cookbooks directory. Adding a Chef solo provisioner \u00b6 Chef-Solo is an open source tool that runs locally and allows to provision guest machines using Chef cookbooks without any Chef configuration. For more information on adding Chef templates for Packer provisioners, in Packer documentation, see Chef Solo Provisioner . The only change that needs to be done for the chef-solo provisioner is to point it to the local folder containing the cookbooks, in this case vendor-cookbooks directory. { \"type\" : \"chef-solo\" , \"cookbook_paths\" : [ \"vendor-cookbooks\" ] } To learn more about Chef, in the Learn the Concepts section, see Provisioning with Chef . After these provisioners and files are created, the packer template AMI can be built by running the rake build command. rake build This command will create an AMI template that will install ChefDK on the image and run the cookbooks. Adding InSpec tests \u00b6 InSpec is a framework for testing and auditing applications and infrastructure. To learn more about InSpec, in the Learn the Concepts section, see Testing with Terraform and Inspec . The InSpec testing profiles are located in the test folder of the project. Inspec profile settings \u00b6 When InSpec profiles are used for server testing, set :test_suite to the directory with the InSpec profile. If :test_suite is not set, it defaults to :application . # in this example, we should have our InSpec profile in \"test/my-inspec-profile\" set :test_suite , 'my-inspec-profile' Create a local InSpec profile \u00b6 Add InSpec profiles to your project by creating your own profiles or including profiles from the Chef Supermarket repository in the Artifactory. To learn more about creating or adding inSpec profiles, visit the Chef InSpec documentation The InSpec profile contains the inspec.yml file, which includes the required name key, and other optional keys such as description, title, summary, and other parameters. The InSpec profile also contains controls files. These directories contains all the tests. The InSpec tests are stored in its respective repository and released to artifactory to be pulled down to the project. When you execute a local profile, the inspec.yml file will be read in order to source any profile dependencies. It will then cache the dependencies locally and generate an inspec.lock file. In the project root folder, create a folder named test In the test folder create another folder for the InSpec profile. The folder name must match :test_suite in config.rb. Add the inspec.yml file. In this example, we have a sample inspec.yml : # test/<test_suite>/inspec.yml name : 'example-application-name' title : Inspec for azure application maintainer : John Doe copyright : ABC Corp copyright_email : john.doe@abccorp.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 supports : - os-family : redhat - os-family : centos Adding local controls to an InSpec profile \u00b6 Controls added directly to the InSpec profile are local to a project/pipeline and are not globally available to other pipelines. To use a common set of InSpec controls that can be shared by multiple pipelines, see the next topic Adding shared controls to an InSpec profile. After the local InSpec profile is created inside the test directory with appropriate inspec.yml file, add another folder called controls to the profile. Now create a <control_file>.rb and start writing InSpec tests. In this example, we have InSpec control properties.rb checking for the file presence: # test/<test_suite>/controls/properties.rb title '/tmp/properties profile' # you add controls here control \"properties\" do describe file ( '/tmp' ) do it { should be_directory } end describe file ( '/tmp/properties' ) do it { should exist } its ( 'mode' ) { should cmp '00755' } end end For information on different resources like the file resource mentioned above, in the InSpec documentation, see Inspec resources . Adding shared controls to an InSpec profile \u00b6 We can add shared profiles under depends section of inspec.yml, so that they can be fetched and used at runtime. For example, name : 'example-application-name' title : Inspec for azure application maintainer : John Doe copyright : ABC Corp copyright_email : john.doe@abccorp.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 inspec_version : - '> 4.0' supports : - os-family : redhat - os-family : centos - os-family : bsd depends : - name : generic-app git : https://github.com/abccorp/inspec-generic-app.git In our local profile we can use the dependent profile controls. For example, # test/<test_suite>/controls/shared_controls.rb include_controls 'generic-app' do end Passing Inputs to InSpec \u00b6 For more information Chef InSpec Input, see InSpec Documentation InSpec profiles can optionally have inputs values passed at runtime. We can set them up in a local file test/<test_suite>/inspec_inputs.yml . An example InSpec Inputs file is below: --- # test/<test_suite>/inspec_inputs.yml chefdk_version : 3.3.23 # ChefDK version chef_bins : # Installed chef binaries - chef - chef-client - berks - inspec ca : true The framework will automatically use inspec_inputs.yml as the InSpec Inputs data. Committing to Git \u00b6 Commit and Push the Code \u00b6 A Git repository is a virtual storage of your project. If you need to setup authentication with the Git server Run git config --global user.name <name> to set up your Git username for all commits Run git config --global user.email <email> to set up your Git email for all commits Run ssh-keygen -t rsa -C \"<email>\" -P \"\" -q -f ~/.ssh/git_rsa to generate a ssh key pair It generates two keys: Private key ~/.ssh/git_rsa Public key ~/.ssh/git_rsa.pub Register the public key ~/.ssh/git_rsa.pub in the Git server Update the local ~/.ssh/config file with this section Host <the Git host> AddKeysToAgent yes PreferredAuthentications publickey IdentityFile ~/.ssh/git_rsa Create a private Git repository on the Git server The repository name should match the generated application name (e.g. clientname-myapp-pipeline ) The remote URL will be used in one of the steps below as <remote_repo_url> Open a shell prompt in the newly-generated code directory Run git init It will create an empty Git repository Run git add . it will stage all files in the directory Run git commit -m \"Initial Commit\" it will commit all staged files Run git remote add origin <remote_repo_url> it will set up the upstream branch for the local branches Run git push -u origin develop it will push the local commit to the remote develop branch Additional Commands \u00b6 Following are some additional commands that you can use in the project: To Check your configuration, use rake -T To create an unencrypted AMI, use rake build To run InSpec test suite and compliance scans, use rake test To create the encrypted AMI,use rake deliver","title":"Configure project"},{"location":"devsecops/Creating-Projects/#create-projects","text":"The project generator creates a Ruby-based project providing AMI creation capabilities. In the further sections of this document we will explain how to customize the project to successfully create AMIs.","title":"Create Projects"},{"location":"devsecops/Creating-Projects/#contents","text":"Prerequisites Step by step guide Generating a Project","title":"Contents"},{"location":"devsecops/Creating-Projects/#prerequisites","text":"Before you start creating a project, ensure that you have: An access to Artifactory that has all the required artifacts for the project. An access to Git server. AWS credentials for Packer. For more information on how to use AWS credentials with Packer, visit Packer Documentation, and see AWS Authentication .","title":"Prerequisites"},{"location":"devsecops/Creating-Projects/#step-by-step-guide","text":"The following steps are required for creating AMI pipelines: Generate project using project generator Set Basic Configuration in Config.rb Creating a Packer template Add required server configuration using chef cookbooks Add Inspec tests to validate server configuration Commit to Git Additional Commands","title":"Step-by-step guide"},{"location":"devsecops/Creating-Projects/#generating-a-project","text":"The project creation depends on the pipeline-generator ruby gem To install the gem locally, use the command: gem install pipeline-generator To generate the custom project on your local machine, use the command: generate pipeline [APP_NAME] [DIR] --style=[STYLE] pipeline : indicates that we're creating a new pipeline codebase [APP_NAME] : name of the pipeline For example, clientname-app-pipeline. [DIR] : directory with the generated output. It can be any directory in the file system. --style (-s) : allows us to specify the type of pipeline we're creating, for AMIs this should be set to image/packer","title":"Generating a project"},{"location":"devsecops/Creating-Projects/#examples","text":"Generating project in a parent directory In this example, the project generator creates files with the application name base-rhel7 in the companyxyz-rhel7 directory in the parent directory. generate pipeline base-rhel7 ../companyxyz-rhel7 -s image/packer . \u2514\u2500\u2500 parent directory \u251c\u2500\u2500 current directory - the command is executed here \u2514\u2500\u2500 companyxyz-rhel7 - this is the directory with generated files Generate project in a subdirectory In this example, the project generator creates files with the application name base-rhel7 in the companyabc-rhel7 subdirectory of the current directory. generate pipeline base-rhel7 companyabc-rhel7 -s image/packer . \u2514\u2500\u2500 parent directory \u2514\u2500\u2500 current directory - the command is executed here \u2514\u2500\u2500 companyabc-rhel7 - this is the directory with generated files","title":"Examples"},{"location":"devsecops/Creating-Projects/#understanding-the-generated-files","text":"The generate pipeline command for image/packer creates the following files: File name File type Purpose Requires customizations Rakefile Rake Loads the AMI rake tasks No config.rb Ruby Configures the AMI pipeline Yes (when changing generated values) build.json JSON The AMI packer template Yes jenkins/Build Jenkinsfile Builds AMI in the core account No jenkins/Deliver Jenkinsfile Delivers AMI to the workload accounts No jenkins/Release/Start Jenkinsfile Starts the release process No jenkins/Release/Build Jenkinsfile Builds the AMI in the context of the release process No jenkins/Release/Deliver Jenkinsfile Delivers the built AMI in the context of the release process No jenkins/Release/Finish Jenkinsfile Finishes the release process No test.tf Terraform Creates EC2 instance for server and compliance scanning No The Jenkins files define Jenkins pipelines using the provided shared Jenkins library. These pipelines execute underlying rake tasks defined for AMI projects. Note: The framework also creates Gemfile and Gemfile.lock . These files describe the Ruby gem dependencies.","title":"Understanding the generated files"},{"location":"devsecops/Creating-Projects/#working-with-the-configuration-files","text":"The config.rb file in the project directory contains the project configuration details. The framework automatically loads config.rb by default. The framework requires the :application configuration setting. The project generator automatically sets :application to the value passed in application name. To change your application name, set :application to a new value. The framework also support loading additional configuration data from other configuration files based on PIPELINE_ENV environment variable. This allows different configuration for different environments, for example development vs production. The additional configuration files must be located under config directory in the project root and the file names have to consist of PIPELINE_ENV values and the .rb file extension. The values in the additional configuration files overrides the values in config.rb . For example, if the same parameter :iamaparameter is set in both config.rb and config/dev.rb and PIPELINE_ENV is set to dev , the value of :iamaparameter in config/dev.rb will be used. In this example, we have three additional configuration files. . \u2514\u2500\u2500 project - project directory \u2514\u2500\u2500 config - config directory \u251c\u2500\u2500 dev.rb - this file is loaded when PIPELINE_ENV is dev \u251c\u2500\u2500 qa.rb - this file is loaded when PIPELINE_ENV is qa \u2514\u2500\u2500 prod.rb - this file is loaded when PIPELINE_ENV is prod PIPELINE_ENV defaults to dev if it hasn't been set. For more information on the parameters in the config.rb file, in the Configuring Projects topic, see Working with config.rb .","title":"Working with the Configuration files"},{"location":"devsecops/Creating-Projects/#creating-a-packer-template","text":"Packer uses a template JSON file to create the Amazon Machine Images (AMIs). In the project generator framework, this template file is build.json and this file is located in the project root folder. To learn more about Packer and the various features this tool offers, in the Learn the Concepts topic, Building with Packer .","title":"Creating a Packer template"},{"location":"devsecops/Creating-Projects/#overriding-configrb-values-with-buildjson-values","text":"To modify or provide values for the variables that are null in the variables section of build.json ; we can add these variables and values to the config.rb file. Any values provided in the config.rb file for a packer build will override values in the build.json and local vars.json files. For example, you can set a region variable in the config.rb file using the following command: set :aws_region , 'us-east-1' This value of us-east-1 will be used by the packer template during the build process.","title":"Overriding config.rb values with build.json values"},{"location":"devsecops/Creating-Projects/#mandatory-packer-variables","text":"When using the project generator, a build.json template is available by default. Following are the basic mandatory parameters for an AMI packer build: variable purpose build.json template location name Name of the builder such as EBS 1-- only needed if multiple builders are used builders type Type of builder being used such as amazon-ebs builders ami_name Name of the AMI being built, a combination of user supplied name, env name and timestamp variables as null, user function variable in builders instance_type The EC2 instance type to use when building the AMI such as t2.small variables source_ami The source AMI whose root volume will be copied and used to provision variables as null, user function variable in builders","title":"Mandatory Packer variables"},{"location":"devsecops/Creating-Projects/#adding-chef-cookbooks","text":"Packer provisioners use built-in and third-party software to install and configure the machine image after booting. For more information see, Packer docs .","title":"Adding Chef cookbooks"},{"location":"devsecops/Creating-Projects/#installing-chef","text":"This step should be done only if the image being built does not have Chef already installed. Packer will automatically attempt to install Chef by default. For more information, in Packer documentation, see Chef Solo Provisioner . If Chef is not installed by default, use the custom script to install Chef. For more information, in Packer documentation, see Chef Solo Install Commands .","title":"Installing Chef"},{"location":"devsecops/Creating-Projects/#creating-a-berksfile","text":"If a Berksfile is present in the project root the framework assumes that the project uses Chef to provision the image. For more information on how to add Chef cookbook dependencies, in Chef documentation, see About Berksfile . When build task in invoked, the framework cleans any existing cookbooks already vendored, then vendors all the dependent cookbooks to the vendor-cookbooks directory.","title":"Creating a Berksfile"},{"location":"devsecops/Creating-Projects/#adding-a-chef-solo-provisioner","text":"Chef-Solo is an open source tool that runs locally and allows to provision guest machines using Chef cookbooks without any Chef configuration. For more information on adding Chef templates for Packer provisioners, in Packer documentation, see Chef Solo Provisioner . The only change that needs to be done for the chef-solo provisioner is to point it to the local folder containing the cookbooks, in this case vendor-cookbooks directory. { \"type\" : \"chef-solo\" , \"cookbook_paths\" : [ \"vendor-cookbooks\" ] } To learn more about Chef, in the Learn the Concepts section, see Provisioning with Chef . After these provisioners and files are created, the packer template AMI can be built by running the rake build command. rake build This command will create an AMI template that will install ChefDK on the image and run the cookbooks.","title":"Adding a Chef solo provisioner"},{"location":"devsecops/Creating-Projects/#adding-inspec-tests","text":"InSpec is a framework for testing and auditing applications and infrastructure. To learn more about InSpec, in the Learn the Concepts section, see Testing with Terraform and Inspec . The InSpec testing profiles are located in the test folder of the project.","title":"Adding InSpec tests"},{"location":"devsecops/Creating-Projects/#inspec-profile-settings","text":"When InSpec profiles are used for server testing, set :test_suite to the directory with the InSpec profile. If :test_suite is not set, it defaults to :application . # in this example, we should have our InSpec profile in \"test/my-inspec-profile\" set :test_suite , 'my-inspec-profile'","title":"Inspec profile settings"},{"location":"devsecops/Creating-Projects/#create-a-local-inspec-profile","text":"Add InSpec profiles to your project by creating your own profiles or including profiles from the Chef Supermarket repository in the Artifactory. To learn more about creating or adding inSpec profiles, visit the Chef InSpec documentation The InSpec profile contains the inspec.yml file, which includes the required name key, and other optional keys such as description, title, summary, and other parameters. The InSpec profile also contains controls files. These directories contains all the tests. The InSpec tests are stored in its respective repository and released to artifactory to be pulled down to the project. When you execute a local profile, the inspec.yml file will be read in order to source any profile dependencies. It will then cache the dependencies locally and generate an inspec.lock file. In the project root folder, create a folder named test In the test folder create another folder for the InSpec profile. The folder name must match :test_suite in config.rb. Add the inspec.yml file. In this example, we have a sample inspec.yml : # test/<test_suite>/inspec.yml name : 'example-application-name' title : Inspec for azure application maintainer : John Doe copyright : ABC Corp copyright_email : john.doe@abccorp.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 supports : - os-family : redhat - os-family : centos","title":"Create a local InSpec profile"},{"location":"devsecops/Creating-Projects/#adding-local-controls-to-an-inspec-profile","text":"Controls added directly to the InSpec profile are local to a project/pipeline and are not globally available to other pipelines. To use a common set of InSpec controls that can be shared by multiple pipelines, see the next topic Adding shared controls to an InSpec profile. After the local InSpec profile is created inside the test directory with appropriate inspec.yml file, add another folder called controls to the profile. Now create a <control_file>.rb and start writing InSpec tests. In this example, we have InSpec control properties.rb checking for the file presence: # test/<test_suite>/controls/properties.rb title '/tmp/properties profile' # you add controls here control \"properties\" do describe file ( '/tmp' ) do it { should be_directory } end describe file ( '/tmp/properties' ) do it { should exist } its ( 'mode' ) { should cmp '00755' } end end For information on different resources like the file resource mentioned above, in the InSpec documentation, see Inspec resources .","title":"Adding local controls to an InSpec profile"},{"location":"devsecops/Creating-Projects/#adding-shared-controls-to-an-inspec-profile","text":"We can add shared profiles under depends section of inspec.yml, so that they can be fetched and used at runtime. For example, name : 'example-application-name' title : Inspec for azure application maintainer : John Doe copyright : ABC Corp copyright_email : john.doe@abccorp.com license : All Rights Reserved summary : An InSpec Compliance Profile version : 0.1.0 inspec_version : - '> 4.0' supports : - os-family : redhat - os-family : centos - os-family : bsd depends : - name : generic-app git : https://github.com/abccorp/inspec-generic-app.git In our local profile we can use the dependent profile controls. For example, # test/<test_suite>/controls/shared_controls.rb include_controls 'generic-app' do end","title":"Adding shared controls to an InSpec profile"},{"location":"devsecops/Creating-Projects/#passing-inputs-to-inspec","text":"For more information Chef InSpec Input, see InSpec Documentation InSpec profiles can optionally have inputs values passed at runtime. We can set them up in a local file test/<test_suite>/inspec_inputs.yml . An example InSpec Inputs file is below: --- # test/<test_suite>/inspec_inputs.yml chefdk_version : 3.3.23 # ChefDK version chef_bins : # Installed chef binaries - chef - chef-client - berks - inspec ca : true The framework will automatically use inspec_inputs.yml as the InSpec Inputs data.","title":"Passing Inputs to InSpec"},{"location":"devsecops/Creating-Projects/#committing-to-git","text":"","title":"Committing to Git"},{"location":"devsecops/Creating-Projects/#commit-and-push-the-code","text":"A Git repository is a virtual storage of your project. If you need to setup authentication with the Git server Run git config --global user.name <name> to set up your Git username for all commits Run git config --global user.email <email> to set up your Git email for all commits Run ssh-keygen -t rsa -C \"<email>\" -P \"\" -q -f ~/.ssh/git_rsa to generate a ssh key pair It generates two keys: Private key ~/.ssh/git_rsa Public key ~/.ssh/git_rsa.pub Register the public key ~/.ssh/git_rsa.pub in the Git server Update the local ~/.ssh/config file with this section Host <the Git host> AddKeysToAgent yes PreferredAuthentications publickey IdentityFile ~/.ssh/git_rsa Create a private Git repository on the Git server The repository name should match the generated application name (e.g. clientname-myapp-pipeline ) The remote URL will be used in one of the steps below as <remote_repo_url> Open a shell prompt in the newly-generated code directory Run git init It will create an empty Git repository Run git add . it will stage all files in the directory Run git commit -m \"Initial Commit\" it will commit all staged files Run git remote add origin <remote_repo_url> it will set up the upstream branch for the local branches Run git push -u origin develop it will push the local commit to the remote develop branch","title":"Commit and Push the Code"},{"location":"devsecops/Creating-Projects/#additional-commands","text":"Following are some additional commands that you can use in the project: To Check your configuration, use rake -T To create an unencrypted AMI, use rake build To run InSpec test suite and compliance scans, use rake test To create the encrypted AMI,use rake deliver","title":"Additional Commands"},{"location":"devsecops/Customizing-projects/","text":"Customize projects \u00b6 Customize the projects by modifying config.rb and adding new rake tasks. config.rb is automatically loaded by the framework. It is the best option for loading your configuration files and set up your custom variables. In the example below, the following variables are defined: :application , :application_type , ::custom_var , and :source_ami . # config.rb # Set the application name. set :application , 'custom-ami-123' # Set the application type to AMI. set :application_type , :ami # Set the custom variable set :custom_var , '12345' # Set the source AMI. This AWS AMI will be used to build the custom AMI. # This code block will # read the 'SOURCE_IMAGE_FILE' environment variable to get the file name # read the JSON file returning the 'ami_id' value set :source_ami , lambda { ENV . fetch ( 'SOURCE_IMAGE' ) do ami_file = ENV . fetch ( 'SOURCE_IMAGE_FILE' ) { raise 'Either SOURCE_IMAGE or SOURCE_IMAGE_FILE env var must be set!' } read_json ( ami_file ) [ 'ami_id' ] end } Contents \u00b6 Adding configuration settings Read JSON and YAML files Initializing AWS clients Adding server and website objects Uploading files Using Rake Tasks Adding before and after hooks Adding configuration settings \u00b6 The framework stores current configuration in the global configuration hash. It includes internally provisioned configuration values and values provisioned directly with setters. The global configuration hash should be accessed only via the helper methods. fetch(key, default=nil) Retrieves the value from the hash of globally defined values. If the key is not found, it will return the default value. set(key, value) Adds the key-value pair to the hash of globally defined values. set_if_empty(key, value) Adds the key-value pair to the globally defined values only if the key is not configured. Setters \u00b6 The framework provides the following methods for setting configuration values. set set_if_empty Let's look at the set example: # Sets :application to `not-my-application` set :application , 'not-my-application' # Sets :application to `my-application`, the older value is overwritten set :application , 'my-application' Let's look at the set_if_empty example: # Sets :application to `my-application` set :application , 'my-application' # This set command has no effect because :application is already set set_if_empty :application , 'the-greatest-application-ever-created' Getters \u00b6 The framework provides the following methods for getting the configuration values. # In this example, we are retrieving the `:application` value application = fetch ( :application ) Reading JSON and YAML files \u00b6 JSON and YAML files are very popular ways of storing configuration and data. The framework supports reading files in JSON and YAML formats. Read JSON files \u00b6 The method read_json is used for reading JSON files. read_json(path) Returns the JSON file content, if the file exists Returns nil, if the file doesn't exits # if this example, we are reading the content of 'target/infra-core-bucket.json' # and we are looking for the 'platform_bucket' element config_bucket = read_json ( target ( \"infra-core-bucket.json\" )) [ 'platform_bucket' ] Read YAML files \u00b6 The method read_yaml is used for reading YAML files. read_yaml(path) Returns the YAML file content, if the file exists Returns nil, if the file doesn't exits # if this example, we are reading the content of 'config/customer.yml' customer = read_yaml ( 'config/customer.yml' ) Initializing AWS clients \u00b6 The framework supports connections to multiple AWS accounts/regions at the same time by means of named AWS clients. AWS connections will always use the credentials that were used at the initialization time. If the credentials haven't been set, the AWS clients will use the credentials loaded from: - AWS environment variables - AWS instance profile - AWS_PROFILE environment variable - AWS profile If the AWS region haven't been set, the AWS clients will use the default AWS region: - AWS_REGION environment variable - AWS_DEFAULT_REGION environment variable - The region associated with the AWS profile The framework supports four methods of AWS client initialization: S3 Client, AWS Client, AWS Client Advanced, and Generic Cloud Client. S3 Client \u00b6 s3_client is a very simple method with a single argument. This method uses the default AWS credentials. The parameter value for the s3_client is: region: [String] AWS Region. Defaults to nil. # In this example, we call the `s3_client` method to get an AWS S3 client in the 'us-east-1' region. s3_client = s3_client ( 'us-east-1' ) AWS Client \u00b6 aws_client method creates an AWS client for the specific AWS API. This method uses the default AWS credentials. The parameter values for the aws_client are: api: [String] AWS API region: [String] AWS Region. Defaults to nil. # In this example, we call the `aws_client` method to get an AWS S3 client in the 'us-east-1' region. s3_client = aws_client ( :S3 , 'us-east-1' ) AWS Client Advanced \u00b6 aws_client_advanced method creates an AWS client for the specific AWS API. This methods gives an option to use named clients allowing connections to multiple regions. This method uses the default AWS credentials. The parameter values for aws_client_advanced are: name: [String] client name. Defaults to :default. client_type: [Symbol] AWS API region: [String] AWS Region. Defaults to nil. # In this example, we call `aws_client_advanced` to get named AWS EC2 clients for calls to EC2 services in `us-east-1` and `us-west-1`. ec2_useast1 = :: PipelineTasks :: Dsl :: CloudHelper . aws_client_advanced ( name : :useast1 , client_type : :EC2 , region : 'us-east-1' ) ec2_uswest1 = :: PipelineTasks :: Dsl :: CloudHelper . aws_client_advanced ( name : :uswest1 , client_type : :EC2 , region : 'us-west-1' ) Generic Cloud Client \u00b6 The generic cloud client or cloud_client is called to create AWS or Azure cloud clients. This methods allows connections to multiple AWS regions using named clients and connections using multiple AWS accounts with the help of AWS profiles. The basic parameter values passed into the cloud_client for AWS connections are: client_type: [Symbol] client API type (:EC2, :AutoScaling, :S3, etc.) name: [Symbol] client name, defaults to :default provider : [Symbol] cloud provider, :aws or :azure. Defaults to AWS. region : [String] AWS region aws_profile : [String] Named AWS profile (in the config/credentials files) # In this example, we call the `cloud_client` method to get an AWS EC2 client in the default AWS region. ec2_client = :: PipelineTasks :: Dsl :: CloudHelper . cloud_client ( client_type : :EC2 ) # In this example, we call the `cloud_client` method to get an AWS EC2 client in the us-east-1 AWS region using the 'abc' profile ec2_client = :: PipelineTasks :: Dsl :: CloudHelper . cloud_client ( client_type : :EC2 , region : 'us-east-1' , aws_profile : 'abc' ) Adding server and website objects \u00b6 About servers \u00b6 server is a Ruby object defining a logical server. The object contains all information necessary to connect to the underlying server. Using server \u00b6 The server creation requires a name and a variety of options depending on the configuration details. name: [String/Symbol] server name protocol : [String] server protocol (winrm, winrms, wsman, wsmans, ssh, docker, local) host : [String] server host name or IP address port : [Integer/String] server port user : [String] server user name password : [String] server password. Either password or SSH keys is required. keys : [String] file path to the SSH key. Either password or SSH keys is required. bastion_user : [String] bastion user name. all bastion options are optional. bastion_host : [String] bastion host name or IP address. all bastion options are optional. bastion_port : [String] bastion port. all bastion options are optional. test_profile : [String] test profile. This is used for inspec tests. The value must be set if server_test_tool is :inspec. # in this example, we are defining 'server_no_bastion' server 'server_no_bastion' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' # in this example, we are defining 'server_with_bastion' server 'server_with_bastion' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' , bastion_user : \"ec2-user\" , bastion_host : '1.1.1.2' , bastion_port : 22 , test_profile : 'test' # in this example, we are defining 'server_efg' that is using server 'bastion_123' server 'bastion_123' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , keys : 'ssh_key_path' server 'server_efg' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' , bastion : 'bastion123' test_profile : 'test' Using server_source \u00b6 The server creation requires a name, a provider, and a resource_type and a variety of properties depending on the configuration details. If the resource type is aws_instance , the server name should be set to the value of either tag:Name or instance_id to allow the host to be fetched automatically. name: [String/Symbol] server name provider : [String] should be set to aws for AWS pipelines resource_type : [String] should be either aws_instance or aws_autoscaling_group protocol : [String] server protocol (winrm, winrms, wsman, wsmans, ssh, docker, local) host : [String] server host name or IP address port : [Integer/String] server port user : [String] server user name password : [String] server password. Either password or SSH keys is required. keys : [String] file path to the server's SSH key. Either password or SSH keys is required. # in this example, we are defining a new server 'my_server_123' that is using the bastion options bastion_options = { name : 'bastion_123' , provider : 'aws' , resource_type : 'aws_autoscaling_group' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , keys : 'ssh_key_path' } server_source 'my_server_123' , provider : 'aws' , resource_type : 'aws_instance' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , password : 'password' , bastion : bastion_options , test_profile : 'common' # in this example, we are defining 'my_server_456' that is using server 'bastion_asg' server_source 'bastion_asg' , provider : 'aws' , resource_type : 'aws_autoscaling_group' , user : 'ec2-user' , protocol : 'ssh' , port : 22 , keys : 'ssh_key_path' server_source 'my_server_456' , provider : 'aws' , resource_type : 'aws_instance' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , password : 'password' , bastion : 'bastion_asg' , test_profile : 'common' Looking at the server_source example, we are not providing the host parameter for the bastion. The framework uses AWS APIs to get host(s) at the execution time based on provider , resource_type , and the server name. website \u00b6 website is a Ruby object defining a logical web site. The object contains all necessary information to produce the URL. The website creation requires a name and a variety of options depending on the configuration details. name: [String/Symbol] web site name protocol : [String] web site protocol (http, https). http is the default. host : [String] web site host name port : [Integer/String] web site port user : [String] web site user name password : [String] web site password path : [String] web site path # in this example, we are creating a web site, which will be reachable by https://www.iamawebsite.com/ website :classic , protocol : :https , host : 'www.iamawebsite.com' Uploading files \u00b6 Uploading files to servers \u00b6 scp_upload Upload local files to a remote server, using scp . scp_upload parameters are: server: [String] server locals: [Array ] array of local files remote : [String] remote path # in this example, we are uploading the file at local_file_path to server123 at remote_path scp_upload server123 , [ 'local_file_path' ] , 'remote_path' train_upload Upload local files to a remote server, using train . We recommend using scp_upload instead of this method. train_upload parameters are: server: [String] server locals: [Array ] array of local files remote : [String] remote path # in this example, we are uploading the file at local_file_path to server123 at remote_path train_upload server123 , [ 'local_file_path' ] , 'remote_path' Uploading files to S3 \u00b6 s3_folder_upload! uploads a folder to an S3 bucket. This method uses the default AWS region. s3_folder_upload! parameters are: bucket: [String] S3 bucket local_folder: [String] local folder (source) remote_folder : [String] remote path thread_count : [Integer] Maximum number of simultaneous threads. Defaults to 5. # in this example, we are uploading the content of the /opt/options/config-files directory # to the s3_destination_bucket under 'config-files' s3_folder_upload! ( 's3_destination_bucket' , '/opt/options/config-files' , 'config-files' ) Using Rake Tasks \u00b6 See the following link for additional information about Rake Learn the tools: Running builds with Rake . Adding new rake tasks \u00b6 New rake tasks can be added directly to the Rakefile. Additional rake files (with the file extension .rake) can be added to the rakelib directory located at the top project level (i.e. the same directory that contains the main Rakefile). In the following example, we are adding one new rake task. # in this example, we are adding a new rake task directly to Rakefile load 'packer-tasks/rake/image/ami.rake' task :do_something do # the task code goes here end In the following example, we are adding two new rake tasks in do.rake and another task in qa.rake . # This is the file content of 'rakelib/do.rake' # in this example, we are adding new rake tasks to a .rake file in rakelib task :do_something do # the task code goes here end task :do_something_else do # the task code goes here end # This is the file content of 'rakelib/qa.rake' # in this example, we are adding a new rake task to a .rake file in rakelib task :qa do # the task code goes here end rake namespaces \u00b6 Rake namespaces are used to avoid clashes between tasks with the same names. # the build task in the main namespace namespace 'main' do task :build do # the task code goes here end end # the build task in the samples namespace namespace 'samples' do task :build do # the task code goes here end end # the top level build task now has dependencies on two other tasks task build : %w[main:build samples:build] We can call each task individually or we can use prerequisites to define another task (see the example above). # calling the main:build task rake main:build # calling the samples:build task rake samples:build # calling the top level build task. the build task depends on main:build and samples:build rake build Enhancing rake tasks \u00b6 A rake task may be defined more than once. Each definition adds its prerequisites and actions to the existing definition. # in this example, we are showing different ways to define the same task # the following is equivalent to the single task specification given below task :name task name : :prereq1 task name : %w[prereq2] task :name do | t | # actions end # the following is equivalent to the multiple task specifications given above task name : [ :prereq1 , :prereq2 ] do | t | # actions (may reference t) end Using framework's DSL methods in rake tasks \u00b6 If you want to use framework's DSL (Domain Specific Language) in your rake tasks, you just need to import the framework into your .rake files. # in this example, we are using the pipeline-task framework methods require 'pipeline-tasks' set :a_custom_variable , 'value_123' set_if_empty :do_compex_test , false Adding before and after hooks \u00b6 About before and after hooks for Rake tasks \u00b6 The before and after hooks allow customization by enabling any rake tasks to call a task (or code block) before or after its own execution. If you don't have much experience with Ruby Rake management, think of a rake task as implementation steps packaged into a task scope that can be called from the command line. As this may be obvious, the order of the arguments matters when methods such as before and after are called. In both the before and after hooks, the current task is first and then following by the prerequisite or post task and any additional arguments or do-end code blocks. before hook \u00b6 The before hook should be created in the following way: before(task, prerequisite, *args, &block) . before parameters are: task: [String] task prerequisite: [String] this task should be executed before task args : [String] prerequisite task arguments block : [Object] optional code block In the example below, :starting would be the task to be called and :ensure_user would be the task to run before :starting is invoked. # :ensure_user will be called before :starting before :starting , :ensure_user # in this example, 'infra:destroy:core-bucket-pre' will be executed before 'infra:destroy:core-bucket:destroy' # the code block (the code between do-end) contains some additional code that we want to execute before 'infra:destroy:core-bucket:destroy' , 'infra:destroy:core-bucket-pre' do # Identify the bucket that will hold remote state. backend_bucket = fetch ( :terraform_backend_bucket ) # Build the TF args that target the core account. options = { environment : fetch ( :accounts , {}) [ fetch ( :core_vars , {}) [ 'account_code' ]] } # Remove the bucket from the TF state. status \"Removing bucket: #{ backend_bucket } from terraform state\" cd 'terraform/core-bucket' do terraform 'state' , nil , 'rm' , 'aws_s3_bucket.platform' , options . merge ( ignore_failure : true ) end end after hook \u00b6 The after hook should be created in the following way: after(task, post_task, *args, &block) . after parameters are: task: [String] task post_task: [String] this task should be executed after task args : [String] post_task task arguments block : [Object] optional code block The after hook example below shows the after method name and then the arguments being passed with the first argument as security:compliance being the initial task to be called and the next argument security:check_audit_results being the post task to be called after the first task is completed. # in this example, `security:check_audit_results` will be called after `security:compliance` after \"security:compliance\" , \"security:check_audit_results\" # in this example, 'config:bootstrap:upload' will be called after 'infra:deploy:core-bucket:apply' # the code block will ensure we always upload generated configuration to the bucket by invoking task 'config:customer.yml:upload!' after 'infra:deploy:core-bucket:apply' , 'config:bootstrap:upload' do begin invoke 'config:customer.yml:upload!' rescue RuntimeError => e error e . message end end","title":"Customize projects"},{"location":"devsecops/Customizing-projects/#customize-projects","text":"Customize the projects by modifying config.rb and adding new rake tasks. config.rb is automatically loaded by the framework. It is the best option for loading your configuration files and set up your custom variables. In the example below, the following variables are defined: :application , :application_type , ::custom_var , and :source_ami . # config.rb # Set the application name. set :application , 'custom-ami-123' # Set the application type to AMI. set :application_type , :ami # Set the custom variable set :custom_var , '12345' # Set the source AMI. This AWS AMI will be used to build the custom AMI. # This code block will # read the 'SOURCE_IMAGE_FILE' environment variable to get the file name # read the JSON file returning the 'ami_id' value set :source_ami , lambda { ENV . fetch ( 'SOURCE_IMAGE' ) do ami_file = ENV . fetch ( 'SOURCE_IMAGE_FILE' ) { raise 'Either SOURCE_IMAGE or SOURCE_IMAGE_FILE env var must be set!' } read_json ( ami_file ) [ 'ami_id' ] end }","title":"Customize projects"},{"location":"devsecops/Customizing-projects/#contents","text":"Adding configuration settings Read JSON and YAML files Initializing AWS clients Adding server and website objects Uploading files Using Rake Tasks Adding before and after hooks","title":"Contents"},{"location":"devsecops/Customizing-projects/#adding-configuration-settings","text":"The framework stores current configuration in the global configuration hash. It includes internally provisioned configuration values and values provisioned directly with setters. The global configuration hash should be accessed only via the helper methods. fetch(key, default=nil) Retrieves the value from the hash of globally defined values. If the key is not found, it will return the default value. set(key, value) Adds the key-value pair to the hash of globally defined values. set_if_empty(key, value) Adds the key-value pair to the globally defined values only if the key is not configured.","title":"Adding configuration settings"},{"location":"devsecops/Customizing-projects/#setters","text":"The framework provides the following methods for setting configuration values. set set_if_empty Let's look at the set example: # Sets :application to `not-my-application` set :application , 'not-my-application' # Sets :application to `my-application`, the older value is overwritten set :application , 'my-application' Let's look at the set_if_empty example: # Sets :application to `my-application` set :application , 'my-application' # This set command has no effect because :application is already set set_if_empty :application , 'the-greatest-application-ever-created'","title":"Setters"},{"location":"devsecops/Customizing-projects/#getters","text":"The framework provides the following methods for getting the configuration values. # In this example, we are retrieving the `:application` value application = fetch ( :application )","title":"Getters"},{"location":"devsecops/Customizing-projects/#reading-json-and-yaml-files","text":"JSON and YAML files are very popular ways of storing configuration and data. The framework supports reading files in JSON and YAML formats.","title":"Reading JSON and YAML files"},{"location":"devsecops/Customizing-projects/#read-json-files","text":"The method read_json is used for reading JSON files. read_json(path) Returns the JSON file content, if the file exists Returns nil, if the file doesn't exits # if this example, we are reading the content of 'target/infra-core-bucket.json' # and we are looking for the 'platform_bucket' element config_bucket = read_json ( target ( \"infra-core-bucket.json\" )) [ 'platform_bucket' ]","title":"Read JSON files"},{"location":"devsecops/Customizing-projects/#read-yaml-files","text":"The method read_yaml is used for reading YAML files. read_yaml(path) Returns the YAML file content, if the file exists Returns nil, if the file doesn't exits # if this example, we are reading the content of 'config/customer.yml' customer = read_yaml ( 'config/customer.yml' )","title":"Read YAML files"},{"location":"devsecops/Customizing-projects/#initializing-aws-clients","text":"The framework supports connections to multiple AWS accounts/regions at the same time by means of named AWS clients. AWS connections will always use the credentials that were used at the initialization time. If the credentials haven't been set, the AWS clients will use the credentials loaded from: - AWS environment variables - AWS instance profile - AWS_PROFILE environment variable - AWS profile If the AWS region haven't been set, the AWS clients will use the default AWS region: - AWS_REGION environment variable - AWS_DEFAULT_REGION environment variable - The region associated with the AWS profile The framework supports four methods of AWS client initialization: S3 Client, AWS Client, AWS Client Advanced, and Generic Cloud Client.","title":"Initializing AWS clients"},{"location":"devsecops/Customizing-projects/#s3-client","text":"s3_client is a very simple method with a single argument. This method uses the default AWS credentials. The parameter value for the s3_client is: region: [String] AWS Region. Defaults to nil. # In this example, we call the `s3_client` method to get an AWS S3 client in the 'us-east-1' region. s3_client = s3_client ( 'us-east-1' )","title":"S3 Client"},{"location":"devsecops/Customizing-projects/#aws-client","text":"aws_client method creates an AWS client for the specific AWS API. This method uses the default AWS credentials. The parameter values for the aws_client are: api: [String] AWS API region: [String] AWS Region. Defaults to nil. # In this example, we call the `aws_client` method to get an AWS S3 client in the 'us-east-1' region. s3_client = aws_client ( :S3 , 'us-east-1' )","title":"AWS Client"},{"location":"devsecops/Customizing-projects/#aws-client-advanced","text":"aws_client_advanced method creates an AWS client for the specific AWS API. This methods gives an option to use named clients allowing connections to multiple regions. This method uses the default AWS credentials. The parameter values for aws_client_advanced are: name: [String] client name. Defaults to :default. client_type: [Symbol] AWS API region: [String] AWS Region. Defaults to nil. # In this example, we call `aws_client_advanced` to get named AWS EC2 clients for calls to EC2 services in `us-east-1` and `us-west-1`. ec2_useast1 = :: PipelineTasks :: Dsl :: CloudHelper . aws_client_advanced ( name : :useast1 , client_type : :EC2 , region : 'us-east-1' ) ec2_uswest1 = :: PipelineTasks :: Dsl :: CloudHelper . aws_client_advanced ( name : :uswest1 , client_type : :EC2 , region : 'us-west-1' )","title":"AWS Client Advanced"},{"location":"devsecops/Customizing-projects/#generic-cloud-client","text":"The generic cloud client or cloud_client is called to create AWS or Azure cloud clients. This methods allows connections to multiple AWS regions using named clients and connections using multiple AWS accounts with the help of AWS profiles. The basic parameter values passed into the cloud_client for AWS connections are: client_type: [Symbol] client API type (:EC2, :AutoScaling, :S3, etc.) name: [Symbol] client name, defaults to :default provider : [Symbol] cloud provider, :aws or :azure. Defaults to AWS. region : [String] AWS region aws_profile : [String] Named AWS profile (in the config/credentials files) # In this example, we call the `cloud_client` method to get an AWS EC2 client in the default AWS region. ec2_client = :: PipelineTasks :: Dsl :: CloudHelper . cloud_client ( client_type : :EC2 ) # In this example, we call the `cloud_client` method to get an AWS EC2 client in the us-east-1 AWS region using the 'abc' profile ec2_client = :: PipelineTasks :: Dsl :: CloudHelper . cloud_client ( client_type : :EC2 , region : 'us-east-1' , aws_profile : 'abc' )","title":"Generic Cloud Client"},{"location":"devsecops/Customizing-projects/#adding-server-and-website-objects","text":"","title":"Adding server and website objects"},{"location":"devsecops/Customizing-projects/#about-servers","text":"server is a Ruby object defining a logical server. The object contains all information necessary to connect to the underlying server.","title":"About servers"},{"location":"devsecops/Customizing-projects/#using-server","text":"The server creation requires a name and a variety of options depending on the configuration details. name: [String/Symbol] server name protocol : [String] server protocol (winrm, winrms, wsman, wsmans, ssh, docker, local) host : [String] server host name or IP address port : [Integer/String] server port user : [String] server user name password : [String] server password. Either password or SSH keys is required. keys : [String] file path to the SSH key. Either password or SSH keys is required. bastion_user : [String] bastion user name. all bastion options are optional. bastion_host : [String] bastion host name or IP address. all bastion options are optional. bastion_port : [String] bastion port. all bastion options are optional. test_profile : [String] test profile. This is used for inspec tests. The value must be set if server_test_tool is :inspec. # in this example, we are defining 'server_no_bastion' server 'server_no_bastion' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' # in this example, we are defining 'server_with_bastion' server 'server_with_bastion' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' , bastion_user : \"ec2-user\" , bastion_host : '1.1.1.2' , bastion_port : 22 , test_profile : 'test' # in this example, we are defining 'server_efg' that is using server 'bastion_123' server 'bastion_123' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , keys : 'ssh_key_path' server 'server_efg' , protocol : \"ssh\" , user : \"ec2-user\" , port : 22 , host : '1.1.1.1' , keys : 'sshkey' , bastion : 'bastion123' test_profile : 'test'","title":"Using server"},{"location":"devsecops/Customizing-projects/#using-server_source","text":"The server creation requires a name, a provider, and a resource_type and a variety of properties depending on the configuration details. If the resource type is aws_instance , the server name should be set to the value of either tag:Name or instance_id to allow the host to be fetched automatically. name: [String/Symbol] server name provider : [String] should be set to aws for AWS pipelines resource_type : [String] should be either aws_instance or aws_autoscaling_group protocol : [String] server protocol (winrm, winrms, wsman, wsmans, ssh, docker, local) host : [String] server host name or IP address port : [Integer/String] server port user : [String] server user name password : [String] server password. Either password or SSH keys is required. keys : [String] file path to the server's SSH key. Either password or SSH keys is required. # in this example, we are defining a new server 'my_server_123' that is using the bastion options bastion_options = { name : 'bastion_123' , provider : 'aws' , resource_type : 'aws_autoscaling_group' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , keys : 'ssh_key_path' } server_source 'my_server_123' , provider : 'aws' , resource_type : 'aws_instance' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , password : 'password' , bastion : bastion_options , test_profile : 'common' # in this example, we are defining 'my_server_456' that is using server 'bastion_asg' server_source 'bastion_asg' , provider : 'aws' , resource_type : 'aws_autoscaling_group' , user : 'ec2-user' , protocol : 'ssh' , port : 22 , keys : 'ssh_key_path' server_source 'my_server_456' , provider : 'aws' , resource_type : 'aws_instance' , protocol : 'ssh' , user : 'ec2-user' , port : 22 , password : 'password' , bastion : 'bastion_asg' , test_profile : 'common' Looking at the server_source example, we are not providing the host parameter for the bastion. The framework uses AWS APIs to get host(s) at the execution time based on provider , resource_type , and the server name.","title":"Using server_source"},{"location":"devsecops/Customizing-projects/#website","text":"website is a Ruby object defining a logical web site. The object contains all necessary information to produce the URL. The website creation requires a name and a variety of options depending on the configuration details. name: [String/Symbol] web site name protocol : [String] web site protocol (http, https). http is the default. host : [String] web site host name port : [Integer/String] web site port user : [String] web site user name password : [String] web site password path : [String] web site path # in this example, we are creating a web site, which will be reachable by https://www.iamawebsite.com/ website :classic , protocol : :https , host : 'www.iamawebsite.com'","title":"website"},{"location":"devsecops/Customizing-projects/#uploading-files","text":"","title":"Uploading  files"},{"location":"devsecops/Customizing-projects/#uploading-files-to-servers","text":"scp_upload Upload local files to a remote server, using scp . scp_upload parameters are: server: [String] server locals: [Array ] array of local files remote : [String] remote path # in this example, we are uploading the file at local_file_path to server123 at remote_path scp_upload server123 , [ 'local_file_path' ] , 'remote_path' train_upload Upload local files to a remote server, using train . We recommend using scp_upload instead of this method. train_upload parameters are: server: [String] server locals: [Array ] array of local files remote : [String] remote path # in this example, we are uploading the file at local_file_path to server123 at remote_path train_upload server123 , [ 'local_file_path' ] , 'remote_path'","title":"Uploading files to servers"},{"location":"devsecops/Customizing-projects/#uploading-files-to-s3","text":"s3_folder_upload! uploads a folder to an S3 bucket. This method uses the default AWS region. s3_folder_upload! parameters are: bucket: [String] S3 bucket local_folder: [String] local folder (source) remote_folder : [String] remote path thread_count : [Integer] Maximum number of simultaneous threads. Defaults to 5. # in this example, we are uploading the content of the /opt/options/config-files directory # to the s3_destination_bucket under 'config-files' s3_folder_upload! ( 's3_destination_bucket' , '/opt/options/config-files' , 'config-files' )","title":"Uploading files to S3"},{"location":"devsecops/Customizing-projects/#using-rake-tasks","text":"See the following link for additional information about Rake Learn the tools: Running builds with Rake .","title":"Using Rake Tasks"},{"location":"devsecops/Customizing-projects/#adding-new-rake-tasks","text":"New rake tasks can be added directly to the Rakefile. Additional rake files (with the file extension .rake) can be added to the rakelib directory located at the top project level (i.e. the same directory that contains the main Rakefile). In the following example, we are adding one new rake task. # in this example, we are adding a new rake task directly to Rakefile load 'packer-tasks/rake/image/ami.rake' task :do_something do # the task code goes here end In the following example, we are adding two new rake tasks in do.rake and another task in qa.rake . # This is the file content of 'rakelib/do.rake' # in this example, we are adding new rake tasks to a .rake file in rakelib task :do_something do # the task code goes here end task :do_something_else do # the task code goes here end # This is the file content of 'rakelib/qa.rake' # in this example, we are adding a new rake task to a .rake file in rakelib task :qa do # the task code goes here end","title":"Adding new rake tasks"},{"location":"devsecops/Customizing-projects/#rake-namespaces","text":"Rake namespaces are used to avoid clashes between tasks with the same names. # the build task in the main namespace namespace 'main' do task :build do # the task code goes here end end # the build task in the samples namespace namespace 'samples' do task :build do # the task code goes here end end # the top level build task now has dependencies on two other tasks task build : %w[main:build samples:build] We can call each task individually or we can use prerequisites to define another task (see the example above). # calling the main:build task rake main:build # calling the samples:build task rake samples:build # calling the top level build task. the build task depends on main:build and samples:build rake build","title":"rake namespaces"},{"location":"devsecops/Customizing-projects/#enhancing-rake-tasks","text":"A rake task may be defined more than once. Each definition adds its prerequisites and actions to the existing definition. # in this example, we are showing different ways to define the same task # the following is equivalent to the single task specification given below task :name task name : :prereq1 task name : %w[prereq2] task :name do | t | # actions end # the following is equivalent to the multiple task specifications given above task name : [ :prereq1 , :prereq2 ] do | t | # actions (may reference t) end","title":"Enhancing rake tasks"},{"location":"devsecops/Customizing-projects/#using-frameworks-dsl-methods-in-rake-tasks","text":"If you want to use framework's DSL (Domain Specific Language) in your rake tasks, you just need to import the framework into your .rake files. # in this example, we are using the pipeline-task framework methods require 'pipeline-tasks' set :a_custom_variable , 'value_123' set_if_empty :do_compex_test , false","title":"Using framework's DSL methods in rake tasks"},{"location":"devsecops/Customizing-projects/#adding-before-and-after-hooks","text":"","title":"Adding before and after hooks"},{"location":"devsecops/Customizing-projects/#about-before-and-after-hooks-for-rake-tasks","text":"The before and after hooks allow customization by enabling any rake tasks to call a task (or code block) before or after its own execution. If you don't have much experience with Ruby Rake management, think of a rake task as implementation steps packaged into a task scope that can be called from the command line. As this may be obvious, the order of the arguments matters when methods such as before and after are called. In both the before and after hooks, the current task is first and then following by the prerequisite or post task and any additional arguments or do-end code blocks.","title":"About before and after hooks for Rake tasks"},{"location":"devsecops/Customizing-projects/#before-hook","text":"The before hook should be created in the following way: before(task, prerequisite, *args, &block) . before parameters are: task: [String] task prerequisite: [String] this task should be executed before task args : [String] prerequisite task arguments block : [Object] optional code block In the example below, :starting would be the task to be called and :ensure_user would be the task to run before :starting is invoked. # :ensure_user will be called before :starting before :starting , :ensure_user # in this example, 'infra:destroy:core-bucket-pre' will be executed before 'infra:destroy:core-bucket:destroy' # the code block (the code between do-end) contains some additional code that we want to execute before 'infra:destroy:core-bucket:destroy' , 'infra:destroy:core-bucket-pre' do # Identify the bucket that will hold remote state. backend_bucket = fetch ( :terraform_backend_bucket ) # Build the TF args that target the core account. options = { environment : fetch ( :accounts , {}) [ fetch ( :core_vars , {}) [ 'account_code' ]] } # Remove the bucket from the TF state. status \"Removing bucket: #{ backend_bucket } from terraform state\" cd 'terraform/core-bucket' do terraform 'state' , nil , 'rm' , 'aws_s3_bucket.platform' , options . merge ( ignore_failure : true ) end end","title":"before hook"},{"location":"devsecops/Customizing-projects/#after-hook","text":"The after hook should be created in the following way: after(task, post_task, *args, &block) . after parameters are: task: [String] task post_task: [String] this task should be executed after task args : [String] post_task task arguments block : [Object] optional code block The after hook example below shows the after method name and then the arguments being passed with the first argument as security:compliance being the initial task to be called and the next argument security:check_audit_results being the post task to be called after the first task is completed. # in this example, `security:check_audit_results` will be called after `security:compliance` after \"security:compliance\" , \"security:check_audit_results\" # in this example, 'config:bootstrap:upload' will be called after 'infra:deploy:core-bucket:apply' # the code block will ensure we always upload generated configuration to the bucket by invoking task 'config:customer.yml:upload!' after 'infra:deploy:core-bucket:apply' , 'config:bootstrap:upload' do begin invoke 'config:customer.yml:upload!' rescue RuntimeError => e error e . message end end","title":"after hook"},{"location":"devsecops/External-Configuration/","text":"External Configuration \u00b6 This feature allows the framework to use json configuration files from an external source. The external source is usually a GIT repository (referred as pipeline-config below). The framework automatically clones the repository (i.e. makes a local copy of the files in the repository) in the external_config_dir , which defaults to target/pipeline-config . The external configuration files are combined into specific configurations. The default values for Packer build is present in the build.json file. Packer is is a tool for creating machine images for multiple platforms from a single source configuration. This single source is in the form of a template file in JSON format and in this case we use a build.json file. To learn more about Packer and JSON templates, see Learn the Concepts . The build.json file contains much of the variables and configurations for the packer build of the AMI you are building. In the build.json file you will find builders (contains what type of AMI is being built), variables, provisioners (processes and software to run after server is up and running) and post-process (tasks for what should be done after the AMI is built). Contents \u00b6 External variables in config.rb External Configuration environment variables What does external configuration do How to disable external configuration How to use external configuration How external configuration is used in the pipeline builds External configuration key Searching external configuration files Sample External Configuration External Configuration Directory Structure External JSON files and config pull The default values for Packer build is present in the build.json file. Example of build.json \u00b6 # code truncated for brevity \"builders\": [ { \"ssh_bastion_host\": \"<Bastion_public_ip>\", \"ssh_bastion_private_key_file\": \"<path_of_SSH_key>\", \"ssh_bastion_username\": \"ec2-user\", ... Variables in config.rb \u00b6 All external configuration parameters are defined in config.rb with variable names starting with external_config Parameter Description Required :external_config_url The URL of the external configuration source. It is used when external_config_type is not set to :none . Yes :external_config_ref The external configuration GIT branch Must be set for GIT sources :external_config_dir The external configuration directory Yes :external_config_prefix The base configuration directory under :external_config_dir . If this parameter is not set, the base configuration directory is :external_config_dir . No :external_config_key Determine all possible directories to load configuration from the least specific to the most specific No :external_config_type External configuration type. Defaults to :git for AMIs No The GIT repository is pulled when the config:pull task is executed. External Configuration environment variables \u00b6 The framework will read certain external configuration values from environment variables Parameter Environment Variable Default :external_config_url PIPELINE_CONFIG_URL :external_config_ref PIPELINE_CONFIG_REF master What does external configuration do \u00b6 The framework pulls the external configuration files from GIT (where the git repository is :external_config_url and the branch is :external_config_ref ) to the :external_config_dir directory. The framework can use the external configuration feature to pull configuration files for a given project (i.e. base rhel7 AMI) and for a given AWS region (i.e. us-east-1). How to disable external configuration \u00b6 Set :external_config_type to :none and make sure :external_config_dir is not set. How to use external configuration \u00b6 To use this feature directly, call external_config_files or external_config_file in your configuration files. # this call will return the list of all vars.json files from the least specific to the most specific set :customer_app_var_files , -> { external_config_files ( 'vars.json' ) } How external configuration is used in the pipeline builds \u00b6 This feature allows image and application pipelines to use pipeline configuration files from pipeline-config and in the case of packer AMI image pipelines the configuration file is the vars.json file. Below shows an example of a vars.json file specific to an AMI project { \"source_ami\" : \"ami-6871a115\" , \"chefdk_version\" : \"3.3.23-1\" , \"chefdk_rpm\" : \"chefdk-3.3.23-1.el7.x86_64.rpm\" , \"nessus_agent_package_name\" : \"NessusAgent\" , \"nessus_agent_package_version\" : \"6.11.1-es7\" , \"mcafee_package_name\" : \"mcafee-epo-agent\" , \"mcafee_package_version\" : \"4.8.0\" , \"aws_cli_package_name\" : \"awscli-bundle\" , \"aws_cli_package_version\" : \"20180913\" } External configuration key \u00b6 The external configuration key ( :external_config_key ) is automatically created based on the the application name. The key is constructed by splitting :application on - , reversing it, and joining it back with a new separator / . For instance if the :application name is baseos-rhel7 then the external configuration key would be rhel7/baseos for the location. Or using another example with the :application value below: set :application , 'customer-app123' # :external_config_key is set to `app123/customer` The key can be overridden by setting the value directly in the application's config.rb . The key determines the directories where the framework will look for external configuration files. Searching external configuration files \u00b6 The search is driven by the external configuration key. The search determines all configuration files matching the given name from the least specific to the most specific. The least specific configuration files are located in the top level directory and the most specific configuration files are located in the lowest level directory. Let's say the base configuration directory is pipeline-config and the external configuration key is set to customer/app123 . The framework will start searching in the pipeline-config directory, moving on to the pipeline-config/app123 , and finishing the search in the pipeline-config/customer/app123 directory. Sample External Configuration \u00b6 set :external_config_dir , target ( 'pipeline-config' ) set :external_config_url , 'git@github.com:customer123/rean-pipeline-config.git' set :external_config_ref , 'branch456' set :external_config_prefix , 'pipelines' set :external_config_type , :git set :external_config_key , 'customerapp/jenkins' This external configuration will cause the framework to get the external configuration files from the GIT branch 'branch456' and the GIT repository at ' git@github.com :customer123/rean-pipeline-config.git' into a directory under ./target/pipeline-config . External Configuration Directory Structure \u00b6 Let's look at the example of pipeline-config directory structure. pipeline-config/ \u251c\u2500\u2500 amis #:external_config_prefix \u2502 \u251c\u2500\u2500 core_account.json \u2502 \u251c\u2500\u2500 rhel7 \u2502 \u2502 \u251c\u2500\u2500 artifactory #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml #values for inspec attributes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml #Rules that should be skipped for a approved period of timeframe, reasons, request and approval \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml #Rules that should be skipped, reasons, request and approval \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 baseos #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 dockerhost \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u251c\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u251c\u2500\u2500 win2012r2 \u2502 \u2502 \u251c\u2500\u2500 baseos ##:external_config_key - 'win2012r2/baseos' \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2514\u2500\u2500 reantestclient ##:external_config_key - 'win2012r2/reantestclient' \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u251c\u2500\u2500 selenium \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u251c\u2500\u2500 testcomplete \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u251c\u2500\u2500 uft \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2514\u2500\u2500 workload_accounts.json \u251c\u2500\u2500 docker-images #:external_config_prefix \u2502 \u2514\u2500\u2500 rhel7 \u2502 \u251c\u2500\u2500 baseos #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json # vars for docker build task \u2502 \u251c\u2500\u2500 bootstrap #:external_config_key - 'rhel7/bootstrap' \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 stigtool_inspec_attributes.yml \u2502 \u2514\u2500\u2500 tomcat \u2502 \u251c\u2500\u2500 audit \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2514\u2500\u2500 vars.json \u2514\u2500\u2500 pipelines #:external_config_prefix \u2514\u2500\u2500 reanplatform \u2514\u2500\u2500 bootstrap #:external_config_key - 'reanplatform/bootstrap' \u2514\u2500\u2500 customer.yml In the directory structure above, you may have noticed the file workload_accounts.json . Workload accounts are where tasks and builds can happen; however the deployment would take place using the core account. For AMIs and Docker Images the vars.json at the root folder i.e, the order in which a variable will be taken for 'baseos-rhel7' is 1. ami/rhel7/baseos/vars.json 2. ami/rhel7/vars.json The framework supports cloud provider and region specific vars. The variable :aws_region must be set in config.rb . In this example, the cloud provider is aws . - ami/rhel7/vars.json - ami/rhel7/baseos/vars.json - ami/rhel7/baseos/aws.vars.json - ami/rhel7/baseos/aws.us-east-1.vars.json - ami/rhel7/baseos/aws.us-gov-west1.vars.json - ami/rhel7/baseos/aws.us-west2.vars.json If :aws_region or AWS_REGION or AWS_DEFAULT_REGION is 'us-east-1', the order of preference is taken as : 1. ami/rhel7/baseos/aws.us-east-1.vars.json 2. ami/rhel7/baseos/aws.vars.json 3. ami/rhel7/baseos/vars.json 4. ami/rhel7/vars.json If the same variable is declared in all four files, the value in the most specific file (in the first file) will be used. External JSON files and config pull \u00b6 Extra variable files get passed to the AMI Packer build (in addition to the build.json) in the following order , with each subsequent file overriding variables of the same name or adding new variables- Loaded First: \u00b6 target/pipeline-config/amis/rhel7/baseos/vars-final.json: The relevant JSON files from the external pipeline configuration(target/pipeline-config/amis ) are combined into a vars-final.json file. the vars-final.json is created by the * bundle exec rake config:pull ** command and pulls down json files from the remote repository based upon the :application name* Loaded Second: \u00b6 vars.json: local vars.json file for packer build Loaded Third: \u00b6 target/image-inputs.json: contains data generated from config.rb, environment variables, etc. Example for variable precedence \u00b6 An example of this would be if the config.rb file had a configuration value set for \"aws_region\" as \"us-east-1\" and the vars.json file in the pipeline-config/amis had \"aws_region\" as a value of \"us-gov-west-1\" . The value set in config.rb for \"aws_region\" would override/replace the value set in the JSON file of the pipeline-config/amis because this would be loaded afterwards. The key point is here is the external pipeline-config repository is a central location for common configuration values of different AMI types. And these values can be overridden or added to locally when configuring the pipeline and only pulling down the values pertaining to the specific AMI based upon the :application value. Related Topics \u00b6 Configure Projects","title":"External configurations"},{"location":"devsecops/External-Configuration/#external-configuration","text":"This feature allows the framework to use json configuration files from an external source. The external source is usually a GIT repository (referred as pipeline-config below). The framework automatically clones the repository (i.e. makes a local copy of the files in the repository) in the external_config_dir , which defaults to target/pipeline-config . The external configuration files are combined into specific configurations. The default values for Packer build is present in the build.json file. Packer is is a tool for creating machine images for multiple platforms from a single source configuration. This single source is in the form of a template file in JSON format and in this case we use a build.json file. To learn more about Packer and JSON templates, see Learn the Concepts . The build.json file contains much of the variables and configurations for the packer build of the AMI you are building. In the build.json file you will find builders (contains what type of AMI is being built), variables, provisioners (processes and software to run after server is up and running) and post-process (tasks for what should be done after the AMI is built).","title":"External Configuration"},{"location":"devsecops/External-Configuration/#contents","text":"External variables in config.rb External Configuration environment variables What does external configuration do How to disable external configuration How to use external configuration How external configuration is used in the pipeline builds External configuration key Searching external configuration files Sample External Configuration External Configuration Directory Structure External JSON files and config pull The default values for Packer build is present in the build.json file.","title":"Contents"},{"location":"devsecops/External-Configuration/#example-of-buildjson","text":"# code truncated for brevity \"builders\": [ { \"ssh_bastion_host\": \"<Bastion_public_ip>\", \"ssh_bastion_private_key_file\": \"<path_of_SSH_key>\", \"ssh_bastion_username\": \"ec2-user\", ...","title":"Example of build.json"},{"location":"devsecops/External-Configuration/#variables-in-configrb","text":"All external configuration parameters are defined in config.rb with variable names starting with external_config Parameter Description Required :external_config_url The URL of the external configuration source. It is used when external_config_type is not set to :none . Yes :external_config_ref The external configuration GIT branch Must be set for GIT sources :external_config_dir The external configuration directory Yes :external_config_prefix The base configuration directory under :external_config_dir . If this parameter is not set, the base configuration directory is :external_config_dir . No :external_config_key Determine all possible directories to load configuration from the least specific to the most specific No :external_config_type External configuration type. Defaults to :git for AMIs No The GIT repository is pulled when the config:pull task is executed.","title":"Variables in config.rb"},{"location":"devsecops/External-Configuration/#external-configuration-environment-variables","text":"The framework will read certain external configuration values from environment variables Parameter Environment Variable Default :external_config_url PIPELINE_CONFIG_URL :external_config_ref PIPELINE_CONFIG_REF master","title":"External Configuration environment variables"},{"location":"devsecops/External-Configuration/#what-does-external-configuration-do","text":"The framework pulls the external configuration files from GIT (where the git repository is :external_config_url and the branch is :external_config_ref ) to the :external_config_dir directory. The framework can use the external configuration feature to pull configuration files for a given project (i.e. base rhel7 AMI) and for a given AWS region (i.e. us-east-1).","title":"What does external configuration do"},{"location":"devsecops/External-Configuration/#how-to-disable-external-configuration","text":"Set :external_config_type to :none and make sure :external_config_dir is not set.","title":"How to disable external configuration"},{"location":"devsecops/External-Configuration/#how-to-use-external-configuration","text":"To use this feature directly, call external_config_files or external_config_file in your configuration files. # this call will return the list of all vars.json files from the least specific to the most specific set :customer_app_var_files , -> { external_config_files ( 'vars.json' ) }","title":"How to use external configuration"},{"location":"devsecops/External-Configuration/#how-external-configuration-is-used-in-the-pipeline-builds","text":"This feature allows image and application pipelines to use pipeline configuration files from pipeline-config and in the case of packer AMI image pipelines the configuration file is the vars.json file. Below shows an example of a vars.json file specific to an AMI project { \"source_ami\" : \"ami-6871a115\" , \"chefdk_version\" : \"3.3.23-1\" , \"chefdk_rpm\" : \"chefdk-3.3.23-1.el7.x86_64.rpm\" , \"nessus_agent_package_name\" : \"NessusAgent\" , \"nessus_agent_package_version\" : \"6.11.1-es7\" , \"mcafee_package_name\" : \"mcafee-epo-agent\" , \"mcafee_package_version\" : \"4.8.0\" , \"aws_cli_package_name\" : \"awscli-bundle\" , \"aws_cli_package_version\" : \"20180913\" }","title":"How external configuration is used in the pipeline builds"},{"location":"devsecops/External-Configuration/#external-configuration-key","text":"The external configuration key ( :external_config_key ) is automatically created based on the the application name. The key is constructed by splitting :application on - , reversing it, and joining it back with a new separator / . For instance if the :application name is baseos-rhel7 then the external configuration key would be rhel7/baseos for the location. Or using another example with the :application value below: set :application , 'customer-app123' # :external_config_key is set to `app123/customer` The key can be overridden by setting the value directly in the application's config.rb . The key determines the directories where the framework will look for external configuration files.","title":"External configuration key"},{"location":"devsecops/External-Configuration/#searching-external-configuration-files","text":"The search is driven by the external configuration key. The search determines all configuration files matching the given name from the least specific to the most specific. The least specific configuration files are located in the top level directory and the most specific configuration files are located in the lowest level directory. Let's say the base configuration directory is pipeline-config and the external configuration key is set to customer/app123 . The framework will start searching in the pipeline-config directory, moving on to the pipeline-config/app123 , and finishing the search in the pipeline-config/customer/app123 directory.","title":"Searching external configuration files"},{"location":"devsecops/External-Configuration/#sample-external-configuration","text":"set :external_config_dir , target ( 'pipeline-config' ) set :external_config_url , 'git@github.com:customer123/rean-pipeline-config.git' set :external_config_ref , 'branch456' set :external_config_prefix , 'pipelines' set :external_config_type , :git set :external_config_key , 'customerapp/jenkins' This external configuration will cause the framework to get the external configuration files from the GIT branch 'branch456' and the GIT repository at ' git@github.com :customer123/rean-pipeline-config.git' into a directory under ./target/pipeline-config .","title":"Sample External Configuration"},{"location":"devsecops/External-Configuration/#external-configuration-directory-structure","text":"Let's look at the example of pipeline-config directory structure. pipeline-config/ \u251c\u2500\u2500 amis #:external_config_prefix \u2502 \u251c\u2500\u2500 core_account.json \u2502 \u251c\u2500\u2500 rhel7 \u2502 \u2502 \u251c\u2500\u2500 artifactory #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml #values for inspec attributes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml #Rules that should be skipped for a approved period of timeframe, reasons, request and approval \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml #Rules that should be skipped, reasons, request and approval \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 baseos #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 dockerhost \u2502 \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u251c\u2500\u2500 vars.json #packer vars for packer build task \u2502 \u251c\u2500\u2500 win2012r2 \u2502 \u2502 \u251c\u2500\u2500 baseos ##:external_config_key - 'win2012r2/baseos' \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2514\u2500\u2500 reantestclient ##:external_config_key - 'win2012r2/reantestclient' \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u251c\u2500\u2500 selenium \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u251c\u2500\u2500 testcomplete \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u251c\u2500\u2500 uft \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2514\u2500\u2500 workload_accounts.json \u251c\u2500\u2500 docker-images #:external_config_prefix \u2502 \u2514\u2500\u2500 rhel7 \u2502 \u251c\u2500\u2500 baseos #:external_config_key - 'rhel7/baseos' \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json # vars for docker build task \u2502 \u251c\u2500\u2500 bootstrap #:external_config_key - 'rhel7/bootstrap' \u2502 \u2502 \u251c\u2500\u2500 audit #:audit_dir \u2502 \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u2502 \u2514\u2500\u2500 vars.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 stigtool_inspec_attributes.yml \u2502 \u2514\u2500\u2500 tomcat \u2502 \u251c\u2500\u2500 audit \u2502 \u2502 \u251c\u2500\u2500 filter.yml \u2502 \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2502 \u251c\u2500\u2500 poam.yml \u2502 \u2502 \u2514\u2500\u2500 skip.yml \u2502 \u251c\u2500\u2500 inspec_attributes.yml \u2502 \u2514\u2500\u2500 vars.json \u2514\u2500\u2500 pipelines #:external_config_prefix \u2514\u2500\u2500 reanplatform \u2514\u2500\u2500 bootstrap #:external_config_key - 'reanplatform/bootstrap' \u2514\u2500\u2500 customer.yml In the directory structure above, you may have noticed the file workload_accounts.json . Workload accounts are where tasks and builds can happen; however the deployment would take place using the core account. For AMIs and Docker Images the vars.json at the root folder i.e, the order in which a variable will be taken for 'baseos-rhel7' is 1. ami/rhel7/baseos/vars.json 2. ami/rhel7/vars.json The framework supports cloud provider and region specific vars. The variable :aws_region must be set in config.rb . In this example, the cloud provider is aws . - ami/rhel7/vars.json - ami/rhel7/baseos/vars.json - ami/rhel7/baseos/aws.vars.json - ami/rhel7/baseos/aws.us-east-1.vars.json - ami/rhel7/baseos/aws.us-gov-west1.vars.json - ami/rhel7/baseos/aws.us-west2.vars.json If :aws_region or AWS_REGION or AWS_DEFAULT_REGION is 'us-east-1', the order of preference is taken as : 1. ami/rhel7/baseos/aws.us-east-1.vars.json 2. ami/rhel7/baseos/aws.vars.json 3. ami/rhel7/baseos/vars.json 4. ami/rhel7/vars.json If the same variable is declared in all four files, the value in the most specific file (in the first file) will be used.","title":"External Configuration Directory Structure"},{"location":"devsecops/External-Configuration/#external-json-files-and-config-pull","text":"Extra variable files get passed to the AMI Packer build (in addition to the build.json) in the following order , with each subsequent file overriding variables of the same name or adding new variables-","title":"External JSON files and config pull"},{"location":"devsecops/External-Configuration/#loaded-first","text":"target/pipeline-config/amis/rhel7/baseos/vars-final.json: The relevant JSON files from the external pipeline configuration(target/pipeline-config/amis ) are combined into a vars-final.json file. the vars-final.json is created by the * bundle exec rake config:pull ** command and pulls down json files from the remote repository based upon the :application name*","title":"Loaded First:"},{"location":"devsecops/External-Configuration/#loaded-second","text":"vars.json: local vars.json file for packer build","title":"Loaded Second:"},{"location":"devsecops/External-Configuration/#loaded-third","text":"target/image-inputs.json: contains data generated from config.rb, environment variables, etc.","title":"Loaded Third:"},{"location":"devsecops/External-Configuration/#example-for-variable-precedence","text":"An example of this would be if the config.rb file had a configuration value set for \"aws_region\" as \"us-east-1\" and the vars.json file in the pipeline-config/amis had \"aws_region\" as a value of \"us-gov-west-1\" . The value set in config.rb for \"aws_region\" would override/replace the value set in the JSON file of the pipeline-config/amis because this would be loaded afterwards. The key point is here is the external pipeline-config repository is a central location for common configuration values of different AMI types. And these values can be overridden or added to locally when configuring the pipeline and only pulling down the values pertaining to the specific AMI based upon the :application value.","title":"Example for variable precedence"},{"location":"devsecops/External-Configuration/#related-topics","text":"Configure Projects","title":"Related Topics"},{"location":"devsecops/Learn-the-concepts/","text":"Learn the concepts \u00b6 DevOps Concepts \u00b6 DevOps is the practice of operations and development engineers participating together in the entire service lifecycle, from design through the development process to production support. Continuous Integration (CI) Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. Continuous Delivery (CD) Continuous Delivery is the ability to get changes of all types\u2014including new features, configuration changes, bug fixes and experiments\u2014into production, or into the hands of users, safely and quickly in a sustainable way. DevOps learning resources: What is DevOps More about DevOps Continuous Integration Continuous Delivery DevSecOps Concepts \u00b6 DevSecOps is the philosophy of integrating security practices within the DevOps process. DevSecOps is about built-in security, not security that functions as a perimeter around apps and data. All people involved in the development process are accountable for security. What is DevSecOps Building with Packer \u00b6 Packer is a tool for creating machine images for multiple platforms from a single source configuration. The JSON configuration file used to define what image we want built and how is called a template in Packer terminology. The template has one or more builders which are Packer components that are able to create a machine image for a single platform (for example, amazon-ebs is the Amazon EC2 AMI builder that ships with Packer). Builders read in some configuration and use that to run and generate a machine image. A builder is invoked as part of a build in order to create the actual resulting images. Packer learning resources: Packer documentation Learn DevOps: Infrastructure Automation With Terraform Provisioning with Chef \u00b6 Chef is a powerful automation platform that transforms infrastructure into code. You create and test your code on your workstation before you deploy it to other environments. When you write your code, you use resources to describe your infrastructure. A resource corresponds to some piece of infrastructure, such as a file, a template, or a package. Each resource declares what state a part of the system should be in, but not how to get there. Chef handles these complexities for you. Chef provides many resources that are ready for you to use. You can also utilize resources shipped in community cookbooks, or write your own resources specific to your infrastructure. Chef learning resources: Basic Chef Fluency Badge Chef Local Cookbook Development Badge Chef Fundamentals: A Recipe for Automating Infrastructure Managing Environment Variables with direnv \u00b6 Direnv is an extension for your shell. It augments existing shells with a new feature that can load and unload environment variables depending on the current directory. To see all the Environment variables available and their values--as well as the one you just created--you can type printenv in the terminal. If using direnv, you will most likely have to type direnv allow after creating the new variable. Testing with Terraform and InSpec \u00b6 Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. Chef InSpec is a free and open-source framework for testing and auditing your applications and infrastructure. Chef InSpec works by comparing the actual state of your system with the desired state that you express in easy-to-read and easy-to-write Chef InSpec code. Chef InSpec detects violations and displays findings in the form of a report, but puts you in control of remediation. For testing our images, we use Terraform to create temporary resources ( EC2 instances ) and InSpec profiles to test the resources. Terraform and InSpec learning resources: Terraform variable interpolation Using Terraform to Manage Applications and Infrastructure Learn DevOps: Infrastructure Automation With Terraform Compliance Automation with InSpec How to create custom InSpec profiles Example InSpec profile Running builds with Rake \u00b6 Rake is a make-like build utility for Ruby with tasks and dependencies specified in standard Ruby syntax. To see the rake tasks run the following command on the command line. bundle exec rake -T We can use rake to execute individual pipeline tasks or to execute rake tasks which have dependencies on other tasks. In this example, we are executing one rake task. bundle exec rake expire # This task expires machine images built by Packer In this example, we are executing another rake task with a lot of dependencies. bundle exec rake test # This task executes pre, post, and actual testing steps In this example, we are executing a rake task in a namespace. bundle exec rake build:validate # the namespace is build and the task is validate Rake learning resources: Rake Rakefile Rake Namespaces Understanding git and git flow \u00b6 Git is a free and open source distributed version control system. Git flow is a Git workflow design which defines a strict branching model and it dictates what kind of branches to set up and how to merge them together. Git flow uses two branches to record the history of the project. The master branch stores the official release history, and the develop branch serves as an integration branch for features. It's also convenient to tag all commits in the master branch with a version number. Each new feature should reside in its own branch, which can be pushed to the central repository for backup/collaboration. But, instead of branching off of master , feature branches use develop as their parent branch. When a feature is complete, it gets merged back into develop. Features should never interact directly with master . Git and Git Flow learning resources: Git Git Flow Cloud credentials: AWS \u00b6 The AWS Command Line Interface (AWS CLI) is an open source tool that interacts with AWS services using commands in your command-line shell. The easiest way to setup AWS credentials is to run the following command in your shell. aws configure TODO: Teach the user how to set up their AWS credentials INI file, and hyperlink them to external documentation about the topic. TODO: Teach the user about the relevant AWS environment variables to select At the very least, the framework needs the following environment variables: AWS_ACCESS_KEY_ID - an AWS access key associated with an IAM user or role AWS_SECRET_ACCESS_KEY - the \"password\" for the access key AWS_DEFAULT_REGION - AWS will send requests to this AWS Region AWS learning resources: AWS Command Line Interface Configure the AWS CLI AWS CLI environment variables AWS Region Security and Compliance: Security Technical Implementation Guide \u00b6 The Security Technical Implementation Guides (STIGs) contain technical guidance to \"lock down\" information systems/software that might otherwise be vulnerable to a malicious computer attack. The framework creates compliance reports by executing the InSpec profiles based on the STIG specifications. For compliance scanning using stigtool one can write skip.yml , poam.yml , filter.yml and inspec_attributes.yml . Below we explain what the purpose of using POAM, Skip and Filter are using for when testing STIG specifications. POAM - Plan of Action and Milestones. A POAM is temporary exclusion in an inspec test. For example, if Red Hat Linux introduces an error in the latest version of a software package, we may have to downgrade. This will cause the compliance scan to fail validation because the STIG insists software should be updated. In order to work around the issue, we would create a POAM for it with an expiration date. Once the date in the POAM has passed, the InSpec test will fail. Skip - InSpec also supports skipping tests entirely. For example, we rely on Amazon to pass several STIGs regarding datacenter security. These tests are skipped permanently, and the reason logged. Filter - Nessus does not support skips and POAMs, instead it uses a filter to exclude tests that are not applicable. STIGs cover many topics, some of which we rely on Amazon's certification process to cover. Some STIGs do not apply at all. For example, we can't check bios settings on an EC2 instance, so we can't validate any bios settings called out in the STIG. They will never apply, so they can be permanently skipped. Rather than removing them from the code base and invalidating the comprehensive STIG list we generated, we skip them. There are also occasionally temporary conditions that force us to bypass an Inspec test. If there's an issue with a RedHat package and we have to downgrade it until a patch comes out, the build would fail the Inspec test that mandates the most recent version. In these situations, we want to skip the check, but we eventually want to run it again. In these situations, we create a POAM for it. A poam looks similar to a skip, but it contains an expiration date. Both contain approval information and an explanation of why it's in place, which are included in the output Skips and POAMs are stored as configuration files in the appropriate audit directory of the external configuration. For example, the base OS RHEL7 packer job skips and POAMs are kept in rean-pipeline-config/amis/rhel7/baseos/audit/ folder of the rean-pipeline-config repo. These files are required to skip STIGs if STIGs are not valid or STIGs cause application issues on the machine. STIGs learning resources: STIGs Network connectivity: AWS image pipelines \u00b6 If a subnet associated with a route table that has a route to an Internet Gateway, it is known as a public subnet. Otherwise, it is a private subnet. AWS networking learning resources: AWS VPCs and Subnets \u00b6 \u00b6","title":"Learn the concepts"},{"location":"devsecops/Learn-the-concepts/#learn-the-concepts","text":"","title":"Learn the concepts"},{"location":"devsecops/Learn-the-concepts/#devops-concepts","text":"DevOps is the practice of operations and development engineers participating together in the entire service lifecycle, from design through the development process to production support. Continuous Integration (CI) Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. Continuous Delivery (CD) Continuous Delivery is the ability to get changes of all types\u2014including new features, configuration changes, bug fixes and experiments\u2014into production, or into the hands of users, safely and quickly in a sustainable way. DevOps learning resources: What is DevOps More about DevOps Continuous Integration Continuous Delivery","title":"DevOps Concepts"},{"location":"devsecops/Learn-the-concepts/#devsecops-concepts","text":"DevSecOps is the philosophy of integrating security practices within the DevOps process. DevSecOps is about built-in security, not security that functions as a perimeter around apps and data. All people involved in the development process are accountable for security. What is DevSecOps","title":"DevSecOps Concepts"},{"location":"devsecops/Learn-the-concepts/#building-with-packer","text":"Packer is a tool for creating machine images for multiple platforms from a single source configuration. The JSON configuration file used to define what image we want built and how is called a template in Packer terminology. The template has one or more builders which are Packer components that are able to create a machine image for a single platform (for example, amazon-ebs is the Amazon EC2 AMI builder that ships with Packer). Builders read in some configuration and use that to run and generate a machine image. A builder is invoked as part of a build in order to create the actual resulting images. Packer learning resources: Packer documentation Learn DevOps: Infrastructure Automation With Terraform","title":" Building with Packer"},{"location":"devsecops/Learn-the-concepts/#provisioning-with-chef","text":"Chef is a powerful automation platform that transforms infrastructure into code. You create and test your code on your workstation before you deploy it to other environments. When you write your code, you use resources to describe your infrastructure. A resource corresponds to some piece of infrastructure, such as a file, a template, or a package. Each resource declares what state a part of the system should be in, but not how to get there. Chef handles these complexities for you. Chef provides many resources that are ready for you to use. You can also utilize resources shipped in community cookbooks, or write your own resources specific to your infrastructure. Chef learning resources: Basic Chef Fluency Badge Chef Local Cookbook Development Badge Chef Fundamentals: A Recipe for Automating Infrastructure","title":" Provisioning with Chef"},{"location":"devsecops/Learn-the-concepts/#managing-environment-variables-with-direnv","text":"Direnv is an extension for your shell. It augments existing shells with a new feature that can load and unload environment variables depending on the current directory. To see all the Environment variables available and their values--as well as the one you just created--you can type printenv in the terminal. If using direnv, you will most likely have to type direnv allow after creating the new variable.","title":" Managing Environment Variables with direnv"},{"location":"devsecops/Learn-the-concepts/#testing-with-terraform-and-inspec","text":"Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. Chef InSpec is a free and open-source framework for testing and auditing your applications and infrastructure. Chef InSpec works by comparing the actual state of your system with the desired state that you express in easy-to-read and easy-to-write Chef InSpec code. Chef InSpec detects violations and displays findings in the form of a report, but puts you in control of remediation. For testing our images, we use Terraform to create temporary resources ( EC2 instances ) and InSpec profiles to test the resources. Terraform and InSpec learning resources: Terraform variable interpolation Using Terraform to Manage Applications and Infrastructure Learn DevOps: Infrastructure Automation With Terraform Compliance Automation with InSpec How to create custom InSpec profiles Example InSpec profile","title":" Testing with Terraform and InSpec"},{"location":"devsecops/Learn-the-concepts/#running-builds-with-rake","text":"Rake is a make-like build utility for Ruby with tasks and dependencies specified in standard Ruby syntax. To see the rake tasks run the following command on the command line. bundle exec rake -T We can use rake to execute individual pipeline tasks or to execute rake tasks which have dependencies on other tasks. In this example, we are executing one rake task. bundle exec rake expire # This task expires machine images built by Packer In this example, we are executing another rake task with a lot of dependencies. bundle exec rake test # This task executes pre, post, and actual testing steps In this example, we are executing a rake task in a namespace. bundle exec rake build:validate # the namespace is build and the task is validate Rake learning resources: Rake Rakefile Rake Namespaces","title":" Running builds with Rake"},{"location":"devsecops/Learn-the-concepts/#understanding-git-and-git-flow","text":"Git is a free and open source distributed version control system. Git flow is a Git workflow design which defines a strict branching model and it dictates what kind of branches to set up and how to merge them together. Git flow uses two branches to record the history of the project. The master branch stores the official release history, and the develop branch serves as an integration branch for features. It's also convenient to tag all commits in the master branch with a version number. Each new feature should reside in its own branch, which can be pushed to the central repository for backup/collaboration. But, instead of branching off of master , feature branches use develop as their parent branch. When a feature is complete, it gets merged back into develop. Features should never interact directly with master . Git and Git Flow learning resources: Git Git Flow","title":" Understanding git and git flow"},{"location":"devsecops/Learn-the-concepts/#cloud-credentials-aws","text":"The AWS Command Line Interface (AWS CLI) is an open source tool that interacts with AWS services using commands in your command-line shell. The easiest way to setup AWS credentials is to run the following command in your shell. aws configure TODO: Teach the user how to set up their AWS credentials INI file, and hyperlink them to external documentation about the topic. TODO: Teach the user about the relevant AWS environment variables to select At the very least, the framework needs the following environment variables: AWS_ACCESS_KEY_ID - an AWS access key associated with an IAM user or role AWS_SECRET_ACCESS_KEY - the \"password\" for the access key AWS_DEFAULT_REGION - AWS will send requests to this AWS Region AWS learning resources: AWS Command Line Interface Configure the AWS CLI AWS CLI environment variables AWS Region","title":"Cloud credentials: AWS"},{"location":"devsecops/Learn-the-concepts/#security-and-compliance-security-technical-implementation-guide","text":"The Security Technical Implementation Guides (STIGs) contain technical guidance to \"lock down\" information systems/software that might otherwise be vulnerable to a malicious computer attack. The framework creates compliance reports by executing the InSpec profiles based on the STIG specifications. For compliance scanning using stigtool one can write skip.yml , poam.yml , filter.yml and inspec_attributes.yml . Below we explain what the purpose of using POAM, Skip and Filter are using for when testing STIG specifications. POAM - Plan of Action and Milestones. A POAM is temporary exclusion in an inspec test. For example, if Red Hat Linux introduces an error in the latest version of a software package, we may have to downgrade. This will cause the compliance scan to fail validation because the STIG insists software should be updated. In order to work around the issue, we would create a POAM for it with an expiration date. Once the date in the POAM has passed, the InSpec test will fail. Skip - InSpec also supports skipping tests entirely. For example, we rely on Amazon to pass several STIGs regarding datacenter security. These tests are skipped permanently, and the reason logged. Filter - Nessus does not support skips and POAMs, instead it uses a filter to exclude tests that are not applicable. STIGs cover many topics, some of which we rely on Amazon's certification process to cover. Some STIGs do not apply at all. For example, we can't check bios settings on an EC2 instance, so we can't validate any bios settings called out in the STIG. They will never apply, so they can be permanently skipped. Rather than removing them from the code base and invalidating the comprehensive STIG list we generated, we skip them. There are also occasionally temporary conditions that force us to bypass an Inspec test. If there's an issue with a RedHat package and we have to downgrade it until a patch comes out, the build would fail the Inspec test that mandates the most recent version. In these situations, we want to skip the check, but we eventually want to run it again. In these situations, we create a POAM for it. A poam looks similar to a skip, but it contains an expiration date. Both contain approval information and an explanation of why it's in place, which are included in the output Skips and POAMs are stored as configuration files in the appropriate audit directory of the external configuration. For example, the base OS RHEL7 packer job skips and POAMs are kept in rean-pipeline-config/amis/rhel7/baseos/audit/ folder of the rean-pipeline-config repo. These files are required to skip STIGs if STIGs are not valid or STIGs cause application issues on the machine. STIGs learning resources: STIGs","title":"Security and Compliance: Security Technical Implementation Guide"},{"location":"devsecops/Learn-the-concepts/#network-connectivity-aws-image-pipelines","text":"If a subnet associated with a route table that has a route to an Internet Gateway, it is known as a public subnet. Otherwise, it is a private subnet. AWS networking learning resources: AWS VPCs and Subnets","title":"Network connectivity: AWS image pipelines"},{"location":"devsecops/Learn-the-concepts/#_1","text":"","title":""},{"location":"devsecops/Learn-the-concepts/#_2","text":"","title":""},{"location":"devsecops/Overview/","text":"Overview \u00b6 Why use this tool \u00b6 The machine image pipelines implement industry's best practices and include out of the box functionality which can be included in customer pipelines. For example, server testing, compliance scanning, etc. Machine Images \u00b6 The framework supports Amazon Machine Images and Docker Images. Machine Image Pipelines \u00b6 The machine image pipelines provide building blocks for machine image creation. The machine image pipelines are developed using GIT projects (generated by the pipeline generator) and Jenkins jobs. The pipeline projects implement the git-flow branching model. The master branch always has the latest released code. Machine Image Pipeline Lifecycle \u00b6 The goal of machine image pipelines is to create tested and hardened machine images. The image creation process consists of: 1. build creates an AMI or a docker image 2. test run tests on a temporary EC2 instance created from the AMI or a temporary container created from the docker image 3. deliver creates AMIs in the configured AWS accounts or publishes the docker image The machine image deliver process performs certain actions depending on the image type: - For AMIs, it will produce AMIs in the configured AWS accounts. - For Docker images, it will publish the docker images into the private Docker repository. Jenkins Pipelines for Machine Images \u00b6 Machine image projects are usually developed on a custom git branch, tested using Jenkins ad-hoc job, and released using the project develop branch. Jenkins pipelines provide capabilities to execute pipelines (i.e. build and test ) from several GIT branches (usually ad-hoc and develop ) and to release the project from the project's develop branch. The project configuration determines whether we get an AMI or a docker image. It is a general practice have AdHoc , Develop and Release jobs for each machine image pipeline: - AdHoc builds and tests machine images from any branch. - Develop only builds and tests machine images from the develop branch. - Release builds, tests and delivers machine images from the develop branch. Merges the release to the master branch.","title":"Overview"},{"location":"devsecops/Overview/#overview","text":"","title":"Overview"},{"location":"devsecops/Overview/#why-use-this-tool","text":"The machine image pipelines implement industry's best practices and include out of the box functionality which can be included in customer pipelines. For example, server testing, compliance scanning, etc.","title":"Why use this tool"},{"location":"devsecops/Overview/#machine-images","text":"The framework supports Amazon Machine Images and Docker Images.","title":"Machine Images"},{"location":"devsecops/Overview/#machine-image-pipelines","text":"The machine image pipelines provide building blocks for machine image creation. The machine image pipelines are developed using GIT projects (generated by the pipeline generator) and Jenkins jobs. The pipeline projects implement the git-flow branching model. The master branch always has the latest released code.","title":"Machine Image Pipelines"},{"location":"devsecops/Overview/#machine-image-pipeline-lifecycle","text":"The goal of machine image pipelines is to create tested and hardened machine images. The image creation process consists of: 1. build creates an AMI or a docker image 2. test run tests on a temporary EC2 instance created from the AMI or a temporary container created from the docker image 3. deliver creates AMIs in the configured AWS accounts or publishes the docker image The machine image deliver process performs certain actions depending on the image type: - For AMIs, it will produce AMIs in the configured AWS accounts. - For Docker images, it will publish the docker images into the private Docker repository.","title":"Machine Image Pipeline Lifecycle"},{"location":"devsecops/Overview/#jenkins-pipelines-for-machine-images","text":"Machine image projects are usually developed on a custom git branch, tested using Jenkins ad-hoc job, and released using the project develop branch. Jenkins pipelines provide capabilities to execute pipelines (i.e. build and test ) from several GIT branches (usually ad-hoc and develop ) and to release the project from the project's develop branch. The project configuration determines whether we get an AMI or a docker image. It is a general practice have AdHoc , Develop and Release jobs for each machine image pipeline: - AdHoc builds and tests machine images from any branch. - Develop only builds and tests machine images from the develop branch. - Release builds, tests and delivers machine images from the develop branch. Merges the release to the master branch.","title":"Jenkins Pipelines for Machine Images"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/","text":"Set-up a Linux virtual machine \u00b6 Download and Install VirtualBox \u00b6 Download and install the latest version of VirtualBox Platform. For more information of VirtualBox, see the VirtualBox website . To download the latest version, see Downloads Download CentOS \u00b6 Download the latest CentOS DVD ISO. To download the latest version, see CentOS downloads . Setup VirtualBox \u00b6 Create a new virtual machine in VirtualBox using the CentOS image. For more information on the installation procedure, see VirtualBox installation documentation . Install CentOS \u00b6 Install the CentOS in your virtual machine. For more information, see CentOS Installation Documentation .","title":"Set-up a Linux Virtual Machine"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/#set-up-a-linux-virtual-machine","text":"","title":"Set-up a Linux virtual machine"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/#download-and-install-virtualbox","text":"Download and install the latest version of VirtualBox Platform. For more information of VirtualBox, see the VirtualBox website . To download the latest version, see Downloads","title":"Download and Install VirtualBox"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/#download-centos","text":"Download the latest CentOS DVD ISO. To download the latest version, see CentOS downloads .","title":"Download CentOS"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/#setup-virtualbox","text":"Create a new virtual machine in VirtualBox using the CentOS image. For more information on the installation procedure, see VirtualBox installation documentation .","title":"Setup VirtualBox"},{"location":"devsecops/Windows-Linux-Virtual-Machine-Setup/#install-centos","text":"Install the CentOS in your virtual machine. For more information, see CentOS Installation Documentation .","title":"Install CentOS"},{"location":"migrate/administering/","text":"Administer Migrate Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. This topic describes how to configure Migrate Accelerator. Configuring Migrate Accelerator \u00b6 To update the Migrate Accelerator configuration properties, you must have administrative access to the instance on which Hitachi Cloud Accelerator Platform is deployed. Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-migrate folder. If the application.properties file is not available in this folder, create the file. If the configuration properties that you want to update are not available in the application.properties file, add those properties. Set the value of the appropriate properties in the application.properties file. Save the application.properties file. Restart the rean-migrate service. Migrate Accelerator configuration properties \u00b6 The following table lists the Migrate Accelerator configuration properties that you can update based on your requirements. Property Description com.reancloud.migration.server.linux.agent_folder Applies to Linux servers that Migrate Accelerator has to migrate to the cloud. It defines the full path of the folder in which Migrate Accelerator must automatically install the CloudEndure migration agent. This folder must be accessible on all Linux servers. com.reancloud.migration.server.windows.agent_folder Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the full path of the folder in which Migrate Accelerator must automatically install the CloudEndure migration agent. This folder must be accessible on all Windows servers. com.reancloud.platform.migrate.winrm.port Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the WinRM port through which Migrate Accelerator can initiate installation of the CloudEndure migration agent over an unencrypted communication channel. Default Value: 5985 com.reancloud.platform.migrate.winrm.https.port Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the WinRM port through which Migrate Accelerator can initiate installation of the CloudEndure migration agent over a secure communication channel. Default Value: 5986 com.reancloud.platform.migrate.migration_status_thread_pool_size Maximum size of the pool of worker threads that Migrate Accelerator must dedicate to the migration management actions. For example, initiating CloudEndure agent installation and retrieving status of migration from CloudEndure. Default Value: 10 com.reancloud.migrate.server.thread.delay Number of milliseconds that Migrate Accelerator must wait before invoking recurring migration management actions. For example, initiating agent installation and retrieving status of migration from CloudEndure. Default Value: 15,000","title":"Administer"},{"location":"migrate/administering/#administer-migrate-accelerator","text":"Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. This topic describes how to configure Migrate Accelerator.","title":"Administer Migrate Accelerator"},{"location":"migrate/administering/#configuring-migrate-accelerator","text":"To update the Migrate Accelerator configuration properties, you must have administrative access to the instance on which Hitachi Cloud Accelerator Platform is deployed. Connect to the EC2 instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/rean-migrate folder. If the application.properties file is not available in this folder, create the file. If the configuration properties that you want to update are not available in the application.properties file, add those properties. Set the value of the appropriate properties in the application.properties file. Save the application.properties file. Restart the rean-migrate service.","title":"Configuring Migrate Accelerator"},{"location":"migrate/administering/#migrate-accelerator-configuration-properties","text":"The following table lists the Migrate Accelerator configuration properties that you can update based on your requirements. Property Description com.reancloud.migration.server.linux.agent_folder Applies to Linux servers that Migrate Accelerator has to migrate to the cloud. It defines the full path of the folder in which Migrate Accelerator must automatically install the CloudEndure migration agent. This folder must be accessible on all Linux servers. com.reancloud.migration.server.windows.agent_folder Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the full path of the folder in which Migrate Accelerator must automatically install the CloudEndure migration agent. This folder must be accessible on all Windows servers. com.reancloud.platform.migrate.winrm.port Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the WinRM port through which Migrate Accelerator can initiate installation of the CloudEndure migration agent over an unencrypted communication channel. Default Value: 5985 com.reancloud.platform.migrate.winrm.https.port Applies to Windows servers that Migrate Accelerator has to migrate to the cloud. It defines the WinRM port through which Migrate Accelerator can initiate installation of the CloudEndure migration agent over a secure communication channel. Default Value: 5986 com.reancloud.platform.migrate.migration_status_thread_pool_size Maximum size of the pool of worker threads that Migrate Accelerator must dedicate to the migration management actions. For example, initiating CloudEndure agent installation and retrieving status of migration from CloudEndure. Default Value: 10 com.reancloud.migrate.server.thread.delay Number of milliseconds that Migrate Accelerator must wait before invoking recurring migration management actions. For example, initiating agent installation and retrieving status of migration from CloudEndure. Default Value: 15,000","title":"Migrate Accelerator configuration properties"},{"location":"migrate/getting-started/","text":"Overview of Migrate Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to easily migrate applications that are running in your existing data centers to the cloud as servers or containers in a easy and automated way. You can use these migrated server images in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) and build and deploy the infrastructure. Migrate Accelerator brings in data from Discovery tools and creates logical application groups of the discovered servers. However, you can change the default grouping of servers based on your requirements. Migrate Accelerator also enables you to generate a build plan for each application group of the discovered servers. In the build plan, Migrate Accelerator automatically populates server details, such as host name and operating system. You can download and use this build plan to capture other details that are required to plan the migration of your application infrastructure to the cloud. After you have finalized the servers in an application group, you can create a migration job in Migrate Accelerator, which uses a migration tool to start the server replication process. You can test that the servers have been successfully replicated and the migrated application is working correctly and then perform the cutover and create the final server images. However, before the cutover, you must carefully plan the downtime for the application that is being migrated. For information about using Migrate Accelerator, see the Discover and migrate resources topic.","title":"Overview"},{"location":"migrate/getting-started/#overview-of-migrate-accelerator","text":"Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to easily migrate applications that are running in your existing data centers to the cloud as servers or containers in a easy and automated way. You can use these migrated server images in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) and build and deploy the infrastructure. Migrate Accelerator brings in data from Discovery tools and creates logical application groups of the discovered servers. However, you can change the default grouping of servers based on your requirements. Migrate Accelerator also enables you to generate a build plan for each application group of the discovered servers. In the build plan, Migrate Accelerator automatically populates server details, such as host name and operating system. You can download and use this build plan to capture other details that are required to plan the migration of your application infrastructure to the cloud. After you have finalized the servers in an application group, you can create a migration job in Migrate Accelerator, which uses a migration tool to start the server replication process. You can test that the servers have been successfully replicated and the migrated application is working correctly and then perform the cutover and create the final server images. However, before the cutover, you must carefully plan the downtime for the application that is being migrated. For information about using Migrate Accelerator, see the Discover and migrate resources topic.","title":"Overview of Migrate Accelerator"},{"location":"migrate/using/","text":"Discover and migrate resources \u00b6 Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. It brings in data from Discovery tools, enables you to pick and choose the servers for each application group, uses a migration tool for server replication, and then creates server images. This topic describes how you can use Migrate Accelerator to migrate discovered servers from your data centers to the cloud and show the migrated resources in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). Important: Migrate Accelerator currently supports the migration of servers from your data center to Amazon Web Services (AWS) only. Contents \u00b6 How Migrate Accelerator works End-to-end scenario for migrating servers from a data center to the cloud Supported discovery and migration tools Signing in to Migrate Accelerator Preparing to discover and migrate servers Discovering and migrating servers Viewing the overall migration status for application groups How Migrate Accelerator works \u00b6 Migrate Accelerator enables you to pick and choose what you migrate and track at the Data Center level. You can add these migrated servers on the Deploy Accelerator canvas and then build and deploy the infrastructure. The following image shows the end-to-end steps for using Migrate Accelerator to create resources in Deploy Accelerator. Note: When you deploy Migrate Accelerator on an AWS EC2 instance, you must ensure that the Migrate Accelerator instance can connect to servers in your data center. End-to-end scenario for migrating servers from a data center to the cloud \u00b6 Consider a scenario in which you want to migrate your application from a data center to the cloud. You can use Migrate Accelerator, the supported discovery and migration tools , and Deploy Accelerator to perform the following end-to-end steps: Use the supported discovery tool to discover servers in your data center. Alternatively, manually create a CSV file that contains an inventory of the servers in your data center. Bring in this discovered data into Migrate Accelerator and create logical application groups that include all servers that are required to run the application. For example, you can add the database server and web server used by an application into a logical application group. Use the supported migration tool to initiate server replication and test that all servers in the application group have been successfully replicated. Use Migrate Accelerator to create images of the target test servers. Launch an instance from this image to confirm that the server is replicated correctly. Plan and announce the downtime for your application. Use the supported migration tool to cutover the servers in the application group to stop further server replication. After the cutover servers are launched successfully, use Migrate Accelerator to create final images of all servers in the application group. Launch instances from the final images in the subnet in which you want to host your application. Point users to the migrated application in the cloud before the end of the planned downtime. Supported discovery and migration tools \u00b6 Migrate Accelerator supports the following tools for discovering servers in your data center and migrating them to the cloud. Discovery : RISC Networks CloudScape Note: If you do not want to use CloudScape, you can also manually create a CSV file that contains an inventory of the servers in your data center. Migration : CloudEndure Signing in to Migrate Accelerator \u00b6 The Migrate Accelerator and Admin Console deployment process creates a default administrator who can perform all required actions. This administrator can then create additional users or other users can create their own Hitachi Cloud Accelerator Platform account . In a browser, type https:// ipAddress . ipAddress refers to the IP address of the instance on which Migrate Accelerator is deployed. Enter your user name or email address and your password. Click SIGN IN . Until you create at least one discovery job, you are prompted to create a new discovery job each time you sign in to Migrate Accelerator. When you sign in to Migrate Accelerator after you have created one or more discovery jobs, the discovery job that you had previously opened appears by default. Note: You can perform various actions in Migrate Accelerator only if your administrator has granted you the appropriate permissions. You can access only the jobs, connections, and migration credentials that you have created. Preparing to migrate servers \u00b6 Before you create discovery and migration jobs in Migrate Accelerator, you must perform the following actions: Discover servers in your data center. Configure CloudEndure to migrate servers from your data center to AWS. Configure providers to store authentication details of the cloud service provider account in which Migrate Accelerator must create images of the migrated servers. Create migration service credentials to store the user name and password that Migrate Accelerator must use to connect to a migration service, such as CloudEndure. For each migration job, Migrate Accelerator communicates with the selected migration service to initiate server migration, track migration status, and perform other actions. Create migration configurations to store a predefined set of migration service, credentials to access that migration service, and the appropriate cloud service provider. While creating a migration job , you must select the appropriate migration configuration. Configure connections to store the user name and password that Migrate Accelerator can use to connect to the servers in your data center and install the CloudEndure agent. While creating a migration job , you can select the appropriate connections. Note: You have to configure connections only if you want Migrate Accelerator to auto-install CloudEndure agents on the servers in your data center. Discover servers in your datacenter \u00b6 Before you create a discovery job in Migrate Accelerator, you must generate an inventory of the servers (or assets) in your data center by using one of the following methods. Bring in server data from CloudScape Manually create an inventory of server data Important: The CSV file that you import into Migrate Accelerator can include servers of any operating system. However, you can successfully run a migration job only for Microsoft Windows and Linux servers. Bring in server data from CloudScape \u00b6 If you are using CloudScape to discover servers in your data center, you can bring in this discovered data into Migrate Accelerator in one of the following ways: Use RISC APIs: While creating a RISC discovery job , specify the CloudScape account credentials, API endpoint and key, and the assessment. Migrate Accelerator brings in discovered data inventory directly from CloudScape. Import data in a CSV format: While creating a RISC_EXPORTED_DISCOVERY discovery job , specify the CSV file that you want to import into Migrate Accelerator. To export the discovered data from CloudScape in a CSV format, use the Consume Intelligence > Foundation > Assets option. For information about using CloudScape to discover servers in your data center, see the RISC Networks documentation . Manually create an inventory of server data \u00b6 Create a CSV file that contains a manual inventory of the servers that need to be migrated. While creating a CSV discovery job , specify this CSV file to be imported into Migrate Accelerator. The first row of your CSV file must contain the following data (or column headers): HostName,IPAddress,ServerId,OSType,Group Note: The Group column enables you to group servers that belong to the same application. Configure CloudEndure \u00b6 Before you create a migration job in Migrate Accelerator, perform the following steps to configure CloudEndure: Register for the CloudEndure Live Migration solution through the AWS Marketplace website with your AWS account. The license package that you select must cover the number of CloudEndure agents that you need to install on the servers in your data center. For more information, see the CloudEndure documentation . To migrate servers from your data center to AWS, create a Live Migration project in CloudEndure. For information about setting up a Live Migration project, see the CloudEndure documentation . Note: While creating a migration job in Migrate Accelerator, you must select this Live Migration project. In the Live Migration project, configure the following details: Configure the replication details and the credentials of the AWS account in which you want to create the infrastructure. Ensure that the license that you select for the Live Migration project covers the number of CloudEndure agents that you need to install in your data center. You can either manually install an agent on each server that you want to migrate or enable Migrate Accelerator to install agents on the servers. Ensure that the appropriate CloudEndure IAM policy is attached to the AWS user whose credentials you have specified in the Live Migration project. Configure providers \u00b6 When you start a migration job for a group of servers in the data center, CloudEndure creates test instances in AWS and replicates data from these servers on the test instances. After the data replication has completed successfully, you can create images of the test instances by using the provider that is configured in the migration job. Providers enable you to specify the cloud provider and credentials for accessing the account in which the images must be created. You can create multiple providers in the Admin Console and select the appropriate provider while creating a migration job. The providers that you create are accessible only to you by default. However, you can choose to share your provider with other users who belong to specific Migrate Accelerator groups. Important: Migrate Accelerator can create images only in the same AWS account in which CloudEndure launches test instances. Therefore, the provider that you select in a migration job must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project. Create a provider \u00b6 On the Home page, click the Accelerator icon ( in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Add Provider , click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). Note: Migrate Accelerator currently supports only AWS as a cloud provider. In Select Accelerator list, select Migrate Accelerator . This allows creating provider for Migrate Accelerator in the Admin console. In Provider Details , specify authentication details (in JSON format) of the AWS account in which Migrate Accelerator must create the server images. Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a secure way for cross-account access. Consider a scenario in which Migrate Accelerator is deployed in an AWS account that is different from the accounts in which Migrate Accelerator must create the server images. Instead of storing the access credentials for all these accounts in Migrate Accelerator, you can attach a role to the instance on which Migrate Accelerator is deployed. Migrate Accelerator can then assume a role in the other accounts and create images. For more information about assuming roles, see the AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Migrate Accelerator is deployed. For the other accounts, you must specify a role that Migrate Accelerator can assume to create the images. The role that you specify must have permissions to create images. It must also define the account in which Migrate Accelerator is deployed as a trusted entity. Sample JSON { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Migrate Accelerator must create the server images. However, you must use this method only if a role is attached to the instance on which Migrate Accelerator is deployed. This role must have the permissions to create images. To use the Instance Profile method, you must specify the region in which the images must be created. Sample JSON { \"region\" : \"xx-xxxx-x\" } Basic Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Migrate Accelerator is deployed in an AWS account that is different from the accounts in which Migrate Accelerator must create the server images. Instead of storing access credentials for all these accounts in Migrate Accelerator, you can specify long-term access credentials for only the parent account. Migrate Accelerator can then assume a role in the other accounts and create images. For more information about assuming roles, see the AWS documentation . To use the Basic Credentials with Assume Role method, you must specify the credentials for only the parent account. For all other accounts, you must specify a role that Migrate Accelerator can assume to create images. The role that you specify must have access to create images. It must also define the parent account as a trusted entity. Sample JSON { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Basic Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Migrate Accelerator must create the server images. The IAM user whose credentials you specify must have access to create images. Sample JSON { \"access_key\" : \"XXXXXXXXXXXXXXX\" , \"secret_key\" : \"XXXXXXXXXXXXX\" , \"region\" : \"xx-xxxx-x\" } In Optional Metadata , specify additional metadata for the provider. Make sure that you do not enter confidential data in the metadata as it is displayed in plain-text. { \"description\": \"This provider is for 001 and 002 blueprints\", \"migration_name\": \"Migration 1\" } Click SAVE . A new provider appears under the Provider List section. Edit a provider \u00b6 On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Provider List , in the Actions column for the provider that you want to edit, click Edit ( ). Under Edit Provider , update the provider name or provider details. Note: If you edit a provider, you must re-enter all values in the provider details before you click UPDATE . Otherwise, an error message might be shown while using this provider to create images of migrated servers. Click UPDATE . Share a provider \u00b6 You can share a provider with a Migrate Accelerator user-group in the Admin console. Sharing a provider assigns that provider to all the users in the selected user-group. This action can reduce the need for users to create their own providers. To share a provider with a specific user group, perform the following actions: On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Provider List , in the Actions column for the provider that you want to share, click Share Provider ( ). In the Share Provider dialog box, for a user group, click the Add Permissions icon ( ). Select the permissions you want to assign, and click DONE . In the Share Provider dialog box, click SUBMIT . Create migration service credentials \u00b6 Migrate Accelerator uses migration credentials to communicate with the migration service to initiate migration, track migration status, and perform other actions. While configuring migration credentials, you must specify the migration service and credentials that Migrate Accelerator must use to migrate application groups from the data center. While creating a migration configuration , you must select the credentials that Migrate Accelerator must use to communicate with the selected migration service. To configure migration service credentials, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) icon in the top right corner. Click Migration Configurations . Click the MIGRATION SERVICE CREDENTIALS tab. Under Add/Edit Migration Service Credentials , click NEW . Enter a unique name for the migration service credentials. From the Migration Service list, select the appropriate migration service. Note: Currently, Migrate Accelerator supports only CloudEndure as a migration service. Enter the credentials (user and password) that Migrate Accelerator must use to access the selected migration service. Click SAVE . The new migration service credentials appear under the Migration Service Credentials List section. Create migration configurations \u00b6 Migration configurations allow you to store a predefined set of configurations that are required while creating a migration job . Each migration configuration comprises the following details: Migration service that Migrate Accelerator must use to migrate servers from the data center to the selected cloud service provider. Credentials to access the migration service. Authentication details for the cloud service provider account in which Migrate Accelerator must create images of the servers. (Only for the CloudEndure migration service) Live Migration project from which Migrate Accelerator must migrate servers. You can specify a migration configuration once and use it in multiple migration jobs. Also, if your migration service credentials or cloud service provider authentication details change, you just need to update the appropriate migration configurations instead of editing multiple migration jobs. To create a migration configuration, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) icon in the top right corner. Click Migration Configurations . Click the MIGRATION CONFIGURATION tab. Under Add/Edit Migration Configuration , click NEW . Enter a unique name and description for the migration configuration. From the Migration Service list, select the appropriate migration service. Note: Currently, Migrate Accelerator supports only CloudEndure as a migration service. From the Migration Service Credentials list, select the appropriate credentials. All credentials for the selected migration service appear in the list. For information about creating the credentials, see Create migration service credentials . To fetch all projects from the CloudEndure migration service, perform the following actions: Click GET PROJECTS . From the Project list, select the appropriate Live Migration project in CloudEndure. Important: Ensure that the selected project includes the license to install agents on all servers in the application group. From the Cloud Service Provider list, select the appropriate provider for creating images of the migrated servers. Ensure that the credentials in the selected provider are the same as the AWS credentials that are configured in the selected Live Migration project. For information about creating providers, see Configure providers . Click SAVE . The new migration configuration appears under the Migration Configuration List section. Configure connections \u00b6 Migrate Accelerator uses connections to connect to servers that are running in your data center so that it can install the CloudEndure agent on these servers. You must create an SSH connection to connect to servers that have the Linux operating system. Similarly, you must create a WinRM connection to connect to servers that have the Microsoft Windows operating system. Note: You have to configure connections only if you want Migrate Accelerator to auto-install CloudEndure agents on the servers in your data center. When you select the Auto install agent option while creating a migration job , you must also specify the appropriate connection to connect to the servers in your data center. To configure a connection, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) in the top-right corner. Click Connections . Under Add/Edit Connection , click NEW . Enter the connection name and select the appropriate connection type. Based on the connection type that you have selected, perform the following actions: Connection type Actions to perform SSH Enter the user and password or SSH key that Migrate Accelerator must use to connect to the servers. Note: Ensure that the user that you specify has Read-Write access to the /tmp directory on the servers. Also, ensure that this user has root access or is in the sudoers list. WinRM Enter the user and password that Migrate Accelerator must use to connect to the servers. To set up a secure connection, select Https . Migrate Accelerator uses the CA certificates that were copied to the /root/rean-platform/ca-certs folder while deploying Migrate Accelerator. Click SAVE . A new connection appears under the Connection List section. Discovering and migrating servers \u00b6 The end-to-end process to bring in discovered servers into Migrate Accelerator, perform server replication, and migrate server images to the cloud includes multiple steps, as shown in the following image. Discover servers Plan migration of servers Migrate servers Use migrated resources in Deploy Accelerator Discover servers \u00b6 Create a discovery job \u00b6 To create a discovery job, perform one of the following actions: If you have not created any discovery job, in the dialog box that appears, click ADD DISCOVERY . If you have previously created a discovery job, on the Home page of Migrate Accelerator, click the Create Discovery icon ( ). On the Create Discovery page, enter the discovery name and description. From the Discovery Tool list, select the appropriate discovery tool. If you have selected RISC as the discovery tool, perform the following actions: Enter your CloudScape API endpoint, API key, user, and password. Click GET ASSESSMENTS . From the CloudScape Assessments list, select the assessment that contains the discovered servers that you want to migrate. (Optional) To retrieve asset data based on tags defined in RISC, perform the following actions: Click ADD TAG . In RISC, a tag is represented by a key-value pair. A key can be associated with multiple values, but the key, in combination with each value, represents a distinct tag. The same tag can be applied to multiple assets and multiple tags can be applied to an asset. To bring in asset data in Migrate Accelerator based on tags defined in RISC, you must specify the key-value pairs representing those tags while creating a discovery job. In Tag Key , type the tag key that is applied to assets that you want to bring into Migrate Accelerator. Important: Ensure that the tag keys and values that you specify in Migrate Accelerator exactly match the tag keys and values defined in RISC. Otherwise, the discovery job will not complete successfully and no assets will be brought into Migrate Accelerator. In Tag values , type the appropriate tag values that correspond to the tag key you have specified. For example, say the OS key in RISC has two values - Windows and Linux . And you want to bring in assets that have the OS-Windows and OS-Linux tags (key-value pairs) assigned to them. While creating a discovery job in Migrate Accelerator, you must specify OS as the Tag key and Windows and Linux as the corresponding Tag values , as shown in the following image. Note: Ensure that you specify a unique set of tag values for a tag key--you cannot type the same tag value twice for a tag key. To add another tag, click ADD TAG and then type the tag key and corresponding tag values. You can add multiple tag keys and corresponding tag values for a discovery job. Important: You cannot modify or delete tag key values once they are saved. However, you can later edit the discovery job to add more tag keys or tag values. If you have selected CSV as the discovery tool, perform the following actions: Click Choose File . Select the CSV file from your local machine and click Upload . The first row of your CSV file must contain the following data (or column headers): HostName,IPAddress,ServerId,OSType,Group The following image shows the Create Discovery page for the CSV discovery tool. If you have selected RISC_EXPORTED_DISCOVERY as the discovery tool, perform the following actions: Click Choose File , select the CSV file that you have exported from RISC CloudScape, and click Upload . (Optional) Click MAP . The Discovery Field column displays the fields in which Migrate Accelerator stores the discovered data. In CSV Column , the drop-down list displays all columns from the CSV file that you have uploaded. For each field in the Discovery Field column, select the CSV column from which the data must be imported, as shown in the following image. (Optional) To view how the discovered data will be mapped to the Migrate Accelerator fields, move through to the end of the Discovery Field column and click PREVIEW . Click CREATE DISCOVERY . Application groups within the discovery job appear in the left panel. Note: After you create a RISC or RISC_EXPORTED_DISCOVERY discovery job, you can manually upload the connectivity data that RISC CloudScape has generated for your assessment. View the status of RISC discovery jobs \u00b6 In the case of RISC discovery jobs, Migrate Accelerator fetches data incrementally from CloudScape. While the data retrieval is still in progress, you can see the already retrieved assets in the Migrate Accelerator UI. The banner for RISC discovery jobs shows the status, timestamps (for last fetch, last new asset, and last updated assets), and impacted actions. Until all assets have been successfully retrieved, it is recommended that you do not perform the following actions: Manually upload the connectivity data Regroup servers in an application group Download the build plan for an application group Migrate servers in an application group Note: To view updates to the discovery job status in the banner, you must manually refresh your browser. The following image shows an example of the banner that is shown for RISC discovery jobs. To hide the banner, click X in the top-right corner. To show the banner again, click the Show RISC progress icon ( ). Manually upload connectivity data \u00b6 The option to manually upload connectivity data is available only for RISC and RISC_EXPORTED_CSV discovery jobs. The connectivity data helps to identify the inter-dependencies between servers in an application group. For example, it can highlight the dependency between the web and database servers of an application. Migrate Accelerator includes this connectivity data in the build plan, which can be used to plan the migration of application infrastructure to the cloud. Before you begin Download the connectivity data for the appropriate assessment in CloudScape (use the Add Intelligence > Available Reports option and then export the Detailed Application Dependency Data report as a CSV file). If required, customize the connectivity data in the CSV file based on the following considerations: Ensure that all the servers listed in the CSV file are a part of the same application group in Migrate Accelerator. Otherwise, none of the connectivity data is saved in the Migrate Accelerator database. If the CSV file size is too large, you might get an error while uploading the file in Migrate Accelerator. In this case, you can break up the connectivity data into multiple files. Migrate Accelerator consolidates the connectivity data across these multiple files. Important: You must ensure that the same connectivity entry (which is a combination of source server, destination server, destination port, and connection protocol) is not listed in multiple files. Otherwise, duplicate connectivity entries are created in the database. Migrate Accelerator requires data from only the following three columns in the CSV file that is downloaded from CloudScape: destport destaddr scraddr To reduce the file size, you can choose to remove all other columns from the CSV file. To manually upload connectivity data, perform the following actions: In the Pick Discovery Job box at the top, locate the discovery job for which you want to upload the connectivity data. Next to the appropriate discovery job, click the Upload icon ( ). In the Manual Upload RISC Connectivity Data window, click Choose File . Locate and select the CSV file that contains the connectivity data for the discovery job. Click SUBMIT . The Upload icon for the discovery job is not available until the CSV file has been uploaded successfully. Note: In case of any error, none of the connectivity data in the CSV file is saved in the Migrate Accelerator database. Edit a discovery job \u00b6 In the Pick Discovery Job box at the top, locate the discovery job that you want to edit. Next to the appropriate discovery job, click the Edit icon ( ). In the Edit Discovery window, make the required updates to the discovery job. For CSV and RISC_EXPORTED_CSV discovery jobs, you can edit only the description. For RISC discovery jobs, you can edit the description, CloudScape API Endpoint, and CloudScape credentials (password and API key). You can also add a new tag key and corresponding tag values, or add new tag values to an existing tag key. For information about adding tag keys and tag values, see Create a discovery job . Note: To save any updates that you make to a RISC discovery job, you must specify the CloudScape password and API key again. Click SAVE . If you have added a new tag key or tag value to a completed RISC discovery job, its status changes to In Progress again. If you have added a tag key or tag value to an in-progress RISC discovery job, Migrate Accelerator waits for the job to complete and then again changes its status to In Progress . In both these cases, the discovery job retrieves asset data that corresponds to the new tag keys and tag values. It also retrieves any new asset data that corresponds to existing tag keys and tag values. Plan migration of servers \u00b6 Migrate Accelerator migrates servers as a part of an application group. An application group is a logical grouping of discovered servers that can be migrated to the cloud at the same time. A few discovery tools logically group servers as a part of the discovery process. For example, the database server and the web server used by an application can be a part of one logical application group. By default, Migrate Accelerator groups the servers in an application group based on the discovery tool that you have selected. However, you can change the default grouping of servers based on your requirements. As a part of planning the migration of application groups, you can perform the following tasks: View details of discovered servers Create a new application group Regroup servers in an application group Download the build plan for an application group Supported operating systems \u00b6 Migrate Accelerator supports the migration of servers that use the following operating systems: Operating system Supported versions Windows - 2016 - 2012 - 2008 Ubuntu - 16.04 - 14.04 Red Hat Linux 7.6 Note: Migrate Accelerator does not support auto-installation of the CloudEndure agent on Windows 2008. View details of discovered servers \u00b6 On the Home page, from the left panel, select an application group. All servers that are a part of the selected application group appear in the Servers for Group panel. Note: The Ungrouped group contains servers that are not a part of any application group. Click the server whose details you want to view. A panel with details of the selected server appears on the right, as shown in the image below. (Optional) To hide the panel, click X in the top-right corner. Create a new application group \u00b6 Ensure that the discovery job for which you want to create a new application group is selected in the Pick Discovery Job box at the top. On the Home page, click Create Group . In the Create Group panel, enter the group name. The group name that you specify must be unique for the selected discovery job. To move servers to the new group, perform the following actions: In the Groups panel, select an existing application group. Servers in the selected application group appear in the Servers for Group panel. In the Servers for Group panel, select the servers that you want to move to the new group. The selected servers also appear in the Create Group panel. Note: The Search option in the Servers for Group panel filters servers listed on the current page only. (Optional) Repeat the above actions to move servers from other existing groups to the new group. The Ungrouped group contains servers that are not a part of any application group. To create a new application group, you must add at least one server in that group. Each server can be a part of only one group. Click CREATE GROUP . In the confirmation box, click YES . A new application group appears in the left panel on the Home page. Regroup servers in an application group \u00b6 Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, click Regroup . In the Regroup panel, from the Group Name list, select the application group to which you want to add servers. Only application groups that contain at least one server appear in the list. To add servers to the selected application group, perform the following actions: In the Groups panel, select an existing application group. Servers in the selected application group appear in the Servers for Group panel. In the Servers for Group panel, select the servers that you want to move to the group selected in the Regroup panel. The selected servers also appear in the Regroup panel. Note: The Search option in the Servers for Group panel filters servers listed on the current page only. (Optional) Repeat the above actions to move servers from other existing groups. Each server can be a part of only one group. Click REGROUP . In the confirmation box, click YES . Download the build plan for an application group \u00b6 Migrate Accelerator generates a build plan for each application group. This build plan is a specification document that you can download and use to plan the migration of your application infrastructure to the cloud. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, from the left panel, select the application group for which you want to download the build plan. Click the Download Build Plan icon ( ). The build plan is downloaded to your local machine. Migrate Accelerator automatically populates data in a few columns across multiple tabs in the build plan. You can identify these columns based on the green background color. Note: By default, Migrate Accelerator generates a generic build plan that is not customized for any specific cloud provider. However, your administrator can configure Migrate Accelerator to generate a build plan that is customized for AWS. (Optional) Enter details in the other sheets based on your requirements, add your existing application architecture diagram, and capture the dependencies for the various resources of the application that you are migrating. A cloud engineer can use this build plan to create the application infrastructure in Deploy Accelerator. Migrate servers \u00b6 You must perform the following steps for each application group that you want to migrate to the cloud. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, from the left panel, select the application group that you want to migrate. To start the migration process, click the Migrate icon ( ). On the Create Migration Job page, perform the following actions: Enter the migration name and description. From the Migration Service list, select the appropriate migration service. Note: Migrate Accelerator currently supports only CloudEndure as a migration service. From the Migration Configuration list, select the appropriate configuration. The migration configuration is a predefined set of credentials to access the selected migration service and the authentication details for the cloud service provider in which Migrate Accelerator must create images of the servers. For the CloudEndure migration service, it also defines the Live Migration project from which Migrate Accelerator must migrate servers. For information about creating migration configurations, see Create migration configurations . (Optional) To enable Migrate Accelerator to install CloudEndure agents on the servers that need to be migrated, perform the following actions: Select Auto install agents . Ensure that the servers in the data center meet the following requirements for installing the CloudEndure agent. Operating system Supported versions Software requirements Windows - 2016 - 2012 .NET framework 4.5 Ubuntu - 16.04 - 14.04 - Python 2.4 or higher - GNU Wget Red Hat Linux 7.6 - Python 2.4 or higher - GNU Wget Note: Consider the following points if Migrate Accelerator is unable to auto install the CloudEndure agent on the servers in the data center: Ensure that the folder that was specified in the com.reancloud.migration.server.windows.agent_folder property in the application.properties file while deploying Migrate Accelerator exists on the Windows servers in the data center. For secure connection to Windows servers, ensure that the thumbprint of the certificates that were copied to the ca-certs folder while deploying Migrate Accelerator matches the WinRM HTTPS Listener certificate thumbprint. Ensure that the Linux servers in the data center have access to a valid YUM repository. For Ubuntu 16.04 servers, run the following command to set the symbolic link for the appropriate version of Python: ln -s python<Version> python For example: To set the symbolic link for Python 2.7, run the following command: ln -s python2.7 python To enable Migrate Accelerator to connect to Ubuntu and Red Hat Linux servers with the user name and key (instead of password) that is specified in the selected connection, ensure that the user has sudo access with NOPASSWD. To set the PASSWD for the user, run the following command on the Ubuntu or Red Hat Linux server: sudo visudo In the file that opens, enter the following property: ALL = NOPASSWD: ALL (To connect to Linux servers in the data center) From the Linux Connection list, select the appropriate SSH migration connection. (To connect to Windows servers in the data center) From the Windows Connection list, select the appropriate WinRM migration connection. For information about creating connections, see Configure connections . When you start the migration job, Migrate Accelerator connects to the servers in the application group by using the SSH or WinRM connection that you have specified and installs the CloudEndure agent on the servers. Note: If you do not select the Auto install agents check box, ensure that you have manually installed the CloudEndure agents on the servers in the data center. For more information about installing the CloudEndure agents, see the CloudEndure documentation . To start the migration process, click CREATE MIGRATION . You can view the status of your migration job on the following page: To view the status of the agent installation process for a server, click the Action icon ( ) next to that server in the Server List panel and then select Logs . The Migration Tool Status column displays the CloudEndure status for each server in the application group. You can also view the overall migration status in Migrate Accelerator in the chart on the right panel. After the server replication is successful, the Migration Tool Status column displays the Ready for Test status. Perform the following actions for each server in the application group: To launch a test instance for the server, click the Action icon ( ) and select Launch Test Instance . After the test instance is successfully launched, create an image from the test instance by clicking the Action icon and selecting Create Image . Migrate Accelerator uses the provider that is configured in the migration job to create the images. The image ID appears in Image ID the column. To confirm that the server data has been successfully replicated, perform the following actions: Sign in to the AWS Management Console. Launch an EC2 instance by using the image ID (AMI) that you have created through Migrate Accelerator. Connect to the EC2 instance and confirm that the server data has been successfully replicated. After confirming that the server data has been successfully replicated, stop the server data replication by clicking the Action icon and selecting Launch Cutover Instance . This action also uninstalls the agent from the server in your data center. To create the final image of the server, click the Action icon and select Create Image . Migrate Accelerator uses the provider that is configured in the migration job to create the images. The new image ID is updated in the Image ID column. (Optional) Use migrated resources in Deploy Accelerator . Use migrated resources in Deploy Accelerator \u00b6 In a browser, type your Cloud Accelerator Platform URL. Sign in to Cloud Accelerator Platform. On the Home page of Deploy Accelerator, create a new environment for one of the migrated applications. Ensure that you select the same provider and region as your target environment in the Live Migration project in CloudEndure. From the Resources tab in the left panel, drag an EC2 Instance resource to the canvas. Click the resource you have created and in the right panel, enter the AMI ID of your migrated server and other attribute details. Repeat steps 4 and 5 for each migrated server that is a part of the application that you want to deploy. To build your environment, drag additional network and other resources to the canvas. Create appropriate dependencies between these resources. Save the environment. Start a deployment of this environment. For more information about creating and deploying environments in Deploy Accelerator, see the Deploy and manage environments . Viewing the overall migration status for application groups \u00b6 You can view the overall migration status for one or more application groups in a discovery job. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. In the icon panel, click DISCOVERY VIEW . From the left panel, select the application groups for which you want to view the dashboard. You can view the following information on the dashboard: Panel Description DISCOVERED This panel displays the total number of discovered servers in your data center. AGENTS This panel displays the total number of servers on which Migrate Accelerator is currently installing the CloudEndure agents, the total number of servers on which the agents are successfully installed, and the total number of servers on which the agents failed to be installed. Note: When the agent installation process starts, the Discovered panel displays 0 discovered servers. Instead, these servers are displayed as INSTALLING, COMPLETE, or FAILED in the AGENTS panel. MIGRATIONS This panel displays the total number of servers that are being currently migrated, total number of servers that were successfully migrated, and total number of servers that failed to be migrated. Note: When the migration process starts, the AGENTS panel displays 0 COMPLETE servers. Instead, these servers are displayed as MIGRATING, COMPLETE, or FAILED in the MIGRATIONS panel. Migrated jobs This panel displays the migration jobs that were run for the selected application group. You can view the job name, job description, job start time, and the migration tool that was used.","title":"Discover and migrate resources"},{"location":"migrate/using/#discover-and-migrate-resources","text":"Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. It brings in data from Discovery tools, enables you to pick and choose the servers for each application group, uses a migration tool for server replication, and then creates server images. This topic describes how you can use Migrate Accelerator to migrate discovered servers from your data centers to the cloud and show the migrated resources in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). Important: Migrate Accelerator currently supports the migration of servers from your data center to Amazon Web Services (AWS) only.","title":"Discover and migrate resources"},{"location":"migrate/using/#contents","text":"How Migrate Accelerator works End-to-end scenario for migrating servers from a data center to the cloud Supported discovery and migration tools Signing in to Migrate Accelerator Preparing to discover and migrate servers Discovering and migrating servers Viewing the overall migration status for application groups","title":"Contents"},{"location":"migrate/using/#how-migrate-accelerator-works","text":"Migrate Accelerator enables you to pick and choose what you migrate and track at the Data Center level. You can add these migrated servers on the Deploy Accelerator canvas and then build and deploy the infrastructure. The following image shows the end-to-end steps for using Migrate Accelerator to create resources in Deploy Accelerator. Note: When you deploy Migrate Accelerator on an AWS EC2 instance, you must ensure that the Migrate Accelerator instance can connect to servers in your data center.","title":"How Migrate Accelerator works"},{"location":"migrate/using/#end-to-end-scenario-for-migrating-servers-from-a-data-center-to-the-cloud","text":"Consider a scenario in which you want to migrate your application from a data center to the cloud. You can use Migrate Accelerator, the supported discovery and migration tools , and Deploy Accelerator to perform the following end-to-end steps: Use the supported discovery tool to discover servers in your data center. Alternatively, manually create a CSV file that contains an inventory of the servers in your data center. Bring in this discovered data into Migrate Accelerator and create logical application groups that include all servers that are required to run the application. For example, you can add the database server and web server used by an application into a logical application group. Use the supported migration tool to initiate server replication and test that all servers in the application group have been successfully replicated. Use Migrate Accelerator to create images of the target test servers. Launch an instance from this image to confirm that the server is replicated correctly. Plan and announce the downtime for your application. Use the supported migration tool to cutover the servers in the application group to stop further server replication. After the cutover servers are launched successfully, use Migrate Accelerator to create final images of all servers in the application group. Launch instances from the final images in the subnet in which you want to host your application. Point users to the migrated application in the cloud before the end of the planned downtime.","title":"End-to-end scenario for migrating servers from a data center to the cloud"},{"location":"migrate/using/#supported-discovery-and-migration-tools","text":"Migrate Accelerator supports the following tools for discovering servers in your data center and migrating them to the cloud. Discovery : RISC Networks CloudScape Note: If you do not want to use CloudScape, you can also manually create a CSV file that contains an inventory of the servers in your data center. Migration : CloudEndure","title":"Supported discovery and migration tools"},{"location":"migrate/using/#signing-in-to-migrate-accelerator","text":"The Migrate Accelerator and Admin Console deployment process creates a default administrator who can perform all required actions. This administrator can then create additional users or other users can create their own Hitachi Cloud Accelerator Platform account . In a browser, type https:// ipAddress . ipAddress refers to the IP address of the instance on which Migrate Accelerator is deployed. Enter your user name or email address and your password. Click SIGN IN . Until you create at least one discovery job, you are prompted to create a new discovery job each time you sign in to Migrate Accelerator. When you sign in to Migrate Accelerator after you have created one or more discovery jobs, the discovery job that you had previously opened appears by default. Note: You can perform various actions in Migrate Accelerator only if your administrator has granted you the appropriate permissions. You can access only the jobs, connections, and migration credentials that you have created.","title":"Signing in to Migrate Accelerator"},{"location":"migrate/using/#preparing-to-migrate-servers","text":"Before you create discovery and migration jobs in Migrate Accelerator, you must perform the following actions: Discover servers in your data center. Configure CloudEndure to migrate servers from your data center to AWS. Configure providers to store authentication details of the cloud service provider account in which Migrate Accelerator must create images of the migrated servers. Create migration service credentials to store the user name and password that Migrate Accelerator must use to connect to a migration service, such as CloudEndure. For each migration job, Migrate Accelerator communicates with the selected migration service to initiate server migration, track migration status, and perform other actions. Create migration configurations to store a predefined set of migration service, credentials to access that migration service, and the appropriate cloud service provider. While creating a migration job , you must select the appropriate migration configuration. Configure connections to store the user name and password that Migrate Accelerator can use to connect to the servers in your data center and install the CloudEndure agent. While creating a migration job , you can select the appropriate connections. Note: You have to configure connections only if you want Migrate Accelerator to auto-install CloudEndure agents on the servers in your data center.","title":"Preparing to migrate servers"},{"location":"migrate/using/#discover-servers-in-your-datacenter","text":"Before you create a discovery job in Migrate Accelerator, you must generate an inventory of the servers (or assets) in your data center by using one of the following methods. Bring in server data from CloudScape Manually create an inventory of server data Important: The CSV file that you import into Migrate Accelerator can include servers of any operating system. However, you can successfully run a migration job only for Microsoft Windows and Linux servers.","title":"Discover servers in your datacenter"},{"location":"migrate/using/#bring-in-server-data-from-cloudscape","text":"If you are using CloudScape to discover servers in your data center, you can bring in this discovered data into Migrate Accelerator in one of the following ways: Use RISC APIs: While creating a RISC discovery job , specify the CloudScape account credentials, API endpoint and key, and the assessment. Migrate Accelerator brings in discovered data inventory directly from CloudScape. Import data in a CSV format: While creating a RISC_EXPORTED_DISCOVERY discovery job , specify the CSV file that you want to import into Migrate Accelerator. To export the discovered data from CloudScape in a CSV format, use the Consume Intelligence > Foundation > Assets option. For information about using CloudScape to discover servers in your data center, see the RISC Networks documentation .","title":"Bring in server data from CloudScape"},{"location":"migrate/using/#manually-create-an-inventory-of-server-data","text":"Create a CSV file that contains a manual inventory of the servers that need to be migrated. While creating a CSV discovery job , specify this CSV file to be imported into Migrate Accelerator. The first row of your CSV file must contain the following data (or column headers): HostName,IPAddress,ServerId,OSType,Group Note: The Group column enables you to group servers that belong to the same application.","title":"Manually create an inventory of server data"},{"location":"migrate/using/#configure-cloudendure","text":"Before you create a migration job in Migrate Accelerator, perform the following steps to configure CloudEndure: Register for the CloudEndure Live Migration solution through the AWS Marketplace website with your AWS account. The license package that you select must cover the number of CloudEndure agents that you need to install on the servers in your data center. For more information, see the CloudEndure documentation . To migrate servers from your data center to AWS, create a Live Migration project in CloudEndure. For information about setting up a Live Migration project, see the CloudEndure documentation . Note: While creating a migration job in Migrate Accelerator, you must select this Live Migration project. In the Live Migration project, configure the following details: Configure the replication details and the credentials of the AWS account in which you want to create the infrastructure. Ensure that the license that you select for the Live Migration project covers the number of CloudEndure agents that you need to install in your data center. You can either manually install an agent on each server that you want to migrate or enable Migrate Accelerator to install agents on the servers. Ensure that the appropriate CloudEndure IAM policy is attached to the AWS user whose credentials you have specified in the Live Migration project.","title":"Configure CloudEndure"},{"location":"migrate/using/#configure-providers","text":"When you start a migration job for a group of servers in the data center, CloudEndure creates test instances in AWS and replicates data from these servers on the test instances. After the data replication has completed successfully, you can create images of the test instances by using the provider that is configured in the migration job. Providers enable you to specify the cloud provider and credentials for accessing the account in which the images must be created. You can create multiple providers in the Admin Console and select the appropriate provider while creating a migration job. The providers that you create are accessible only to you by default. However, you can choose to share your provider with other users who belong to specific Migrate Accelerator groups. Important: Migrate Accelerator can create images only in the same AWS account in which CloudEndure launches test instances. Therefore, the provider that you select in a migration job must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project.","title":"Configure providers"},{"location":"migrate/using/#create-a-provider","text":"On the Home page, click the Accelerator icon ( in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Add Provider , click NEW . Enter the provider name and select the appropriate provider type (for example, AWS). Note: Migrate Accelerator currently supports only AWS as a cloud provider. In Select Accelerator list, select Migrate Accelerator . This allows creating provider for Migrate Accelerator in the Admin console. In Provider Details , specify authentication details (in JSON format) of the AWS account in which Migrate Accelerator must create the server images. Consider the following points while selecting the option to specify authentication details. Instance Profile with Assume Role This method provides a secure way for cross-account access. Consider a scenario in which Migrate Accelerator is deployed in an AWS account that is different from the accounts in which Migrate Accelerator must create the server images. Instead of storing the access credentials for all these accounts in Migrate Accelerator, you can attach a role to the instance on which Migrate Accelerator is deployed. Migrate Accelerator can then assume a role in the other accounts and create images. For more information about assuming roles, see the AWS documentation . To use the Instance Profile with Assume Role method, you must attach a role to the instance on which Migrate Accelerator is deployed. For the other accounts, you must specify a role that Migrate Accelerator can assume to create the images. The role that you specify must have permissions to create images. It must also define the account in which Migrate Accelerator is deployed as a trusted entity. Sample JSON { \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Instance Profile This method provides a secure way of accessing the account in which Migrate Accelerator must create the server images. However, you must use this method only if a role is attached to the instance on which Migrate Accelerator is deployed. This role must have the permissions to create images. To use the Instance Profile method, you must specify the region in which the images must be created. Sample JSON { \"region\" : \"xx-xxxx-x\" } Basic Credentials with Assume Role This method provides another way for cross-account access. Consider a scenario in which Migrate Accelerator is deployed in an AWS account that is different from the accounts in which Migrate Accelerator must create the server images. Instead of storing access credentials for all these accounts in Migrate Accelerator, you can specify long-term access credentials for only the parent account. Migrate Accelerator can then assume a role in the other accounts and create images. For more information about assuming roles, see the AWS documentation . To use the Basic Credentials with Assume Role method, you must specify the credentials for only the parent account. For all other accounts, you must specify a role that Migrate Accelerator can assume to create images. The role that you specify must have access to create images. It must also define the parent account as a trusted entity. Sample JSON { \"access_key\" : \"xxxxxxxxxx\" , \"secret_key\" : \"xxxxxxxxxx\" , \"region\" : \"xx-xxxx-x\" , \"assume_role\" : { \"role_arn\" : \"arn:aws:iam::xxxxxxxxxxx:role/assume_role\" , \"session_name\" : \"SESSION-NAME\" , \"external_id\" : \"assume_role\" } } Basic Credentials To use this method, you must specify the IAM user credentials for accessing the account in which Migrate Accelerator must create the server images. The IAM user whose credentials you specify must have access to create images. Sample JSON { \"access_key\" : \"XXXXXXXXXXXXXXX\" , \"secret_key\" : \"XXXXXXXXXXXXX\" , \"region\" : \"xx-xxxx-x\" } In Optional Metadata , specify additional metadata for the provider. Make sure that you do not enter confidential data in the metadata as it is displayed in plain-text. { \"description\": \"This provider is for 001 and 002 blueprints\", \"migration_name\": \"Migration 1\" } Click SAVE . A new provider appears under the Provider List section.","title":"Create a provider"},{"location":"migrate/using/#edit-a-provider","text":"On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Provider List , in the Actions column for the provider that you want to edit, click Edit ( ). Under Edit Provider , update the provider name or provider details. Note: If you edit a provider, you must re-enter all values in the provider details before you click UPDATE . Otherwise, an error message might be shown while using this provider to create images of migrated servers. Click UPDATE .","title":"Edit a provider"},{"location":"migrate/using/#share-a-provider","text":"You can share a provider with a Migrate Accelerator user-group in the Admin console. Sharing a provider assigns that provider to all the users in the selected user-group. This action can reduce the need for users to create their own providers. To share a provider with a specific user group, perform the following actions: On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Providers . Under Provider List , in the Actions column for the provider that you want to share, click Share Provider ( ). In the Share Provider dialog box, for a user group, click the Add Permissions icon ( ). Select the permissions you want to assign, and click DONE . In the Share Provider dialog box, click SUBMIT .","title":"Share a provider"},{"location":"migrate/using/#create-migration-service-credentials","text":"Migrate Accelerator uses migration credentials to communicate with the migration service to initiate migration, track migration status, and perform other actions. While configuring migration credentials, you must specify the migration service and credentials that Migrate Accelerator must use to migrate application groups from the data center. While creating a migration configuration , you must select the credentials that Migrate Accelerator must use to communicate with the selected migration service. To configure migration service credentials, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) icon in the top right corner. Click Migration Configurations . Click the MIGRATION SERVICE CREDENTIALS tab. Under Add/Edit Migration Service Credentials , click NEW . Enter a unique name for the migration service credentials. From the Migration Service list, select the appropriate migration service. Note: Currently, Migrate Accelerator supports only CloudEndure as a migration service. Enter the credentials (user and password) that Migrate Accelerator must use to access the selected migration service. Click SAVE . The new migration service credentials appear under the Migration Service Credentials List section.","title":"Create migration service credentials"},{"location":"migrate/using/#create-migration-configurations","text":"Migration configurations allow you to store a predefined set of configurations that are required while creating a migration job . Each migration configuration comprises the following details: Migration service that Migrate Accelerator must use to migrate servers from the data center to the selected cloud service provider. Credentials to access the migration service. Authentication details for the cloud service provider account in which Migrate Accelerator must create images of the servers. (Only for the CloudEndure migration service) Live Migration project from which Migrate Accelerator must migrate servers. You can specify a migration configuration once and use it in multiple migration jobs. Also, if your migration service credentials or cloud service provider authentication details change, you just need to update the appropriate migration configurations instead of editing multiple migration jobs. To create a migration configuration, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) icon in the top right corner. Click Migration Configurations . Click the MIGRATION CONFIGURATION tab. Under Add/Edit Migration Configuration , click NEW . Enter a unique name and description for the migration configuration. From the Migration Service list, select the appropriate migration service. Note: Currently, Migrate Accelerator supports only CloudEndure as a migration service. From the Migration Service Credentials list, select the appropriate credentials. All credentials for the selected migration service appear in the list. For information about creating the credentials, see Create migration service credentials . To fetch all projects from the CloudEndure migration service, perform the following actions: Click GET PROJECTS . From the Project list, select the appropriate Live Migration project in CloudEndure. Important: Ensure that the selected project includes the license to install agents on all servers in the application group. From the Cloud Service Provider list, select the appropriate provider for creating images of the migrated servers. Ensure that the credentials in the selected provider are the same as the AWS credentials that are configured in the selected Live Migration project. For information about creating providers, see Configure providers . Click SAVE . The new migration configuration appears under the Migration Configuration List section.","title":" Create migration configurations"},{"location":"migrate/using/#configure-connections","text":"Migrate Accelerator uses connections to connect to servers that are running in your data center so that it can install the CloudEndure agent on these servers. You must create an SSH connection to connect to servers that have the Linux operating system. Similarly, you must create a WinRM connection to connect to servers that have the Microsoft Windows operating system. Note: You have to configure connections only if you want Migrate Accelerator to auto-install CloudEndure agents on the servers in your data center. When you select the Auto install agent option while creating a migration job , you must also specify the appropriate connection to connect to the servers in your data center. To configure a connection, perform the following actions: On the Home page of Migrate Accelerator, click the More options icon ( ) in the top-right corner. Click Connections . Under Add/Edit Connection , click NEW . Enter the connection name and select the appropriate connection type. Based on the connection type that you have selected, perform the following actions: Connection type Actions to perform SSH Enter the user and password or SSH key that Migrate Accelerator must use to connect to the servers. Note: Ensure that the user that you specify has Read-Write access to the /tmp directory on the servers. Also, ensure that this user has root access or is in the sudoers list. WinRM Enter the user and password that Migrate Accelerator must use to connect to the servers. To set up a secure connection, select Https . Migrate Accelerator uses the CA certificates that were copied to the /root/rean-platform/ca-certs folder while deploying Migrate Accelerator. Click SAVE . A new connection appears under the Connection List section.","title":"Configure connections"},{"location":"migrate/using/#discovering-and-migrating-servers","text":"The end-to-end process to bring in discovered servers into Migrate Accelerator, perform server replication, and migrate server images to the cloud includes multiple steps, as shown in the following image. Discover servers Plan migration of servers Migrate servers Use migrated resources in Deploy Accelerator","title":"Discovering and migrating servers"},{"location":"migrate/using/#discover-servers","text":"","title":"Discover servers"},{"location":"migrate/using/#create-a-discovery-job","text":"To create a discovery job, perform one of the following actions: If you have not created any discovery job, in the dialog box that appears, click ADD DISCOVERY . If you have previously created a discovery job, on the Home page of Migrate Accelerator, click the Create Discovery icon ( ). On the Create Discovery page, enter the discovery name and description. From the Discovery Tool list, select the appropriate discovery tool. If you have selected RISC as the discovery tool, perform the following actions: Enter your CloudScape API endpoint, API key, user, and password. Click GET ASSESSMENTS . From the CloudScape Assessments list, select the assessment that contains the discovered servers that you want to migrate. (Optional) To retrieve asset data based on tags defined in RISC, perform the following actions: Click ADD TAG . In RISC, a tag is represented by a key-value pair. A key can be associated with multiple values, but the key, in combination with each value, represents a distinct tag. The same tag can be applied to multiple assets and multiple tags can be applied to an asset. To bring in asset data in Migrate Accelerator based on tags defined in RISC, you must specify the key-value pairs representing those tags while creating a discovery job. In Tag Key , type the tag key that is applied to assets that you want to bring into Migrate Accelerator. Important: Ensure that the tag keys and values that you specify in Migrate Accelerator exactly match the tag keys and values defined in RISC. Otherwise, the discovery job will not complete successfully and no assets will be brought into Migrate Accelerator. In Tag values , type the appropriate tag values that correspond to the tag key you have specified. For example, say the OS key in RISC has two values - Windows and Linux . And you want to bring in assets that have the OS-Windows and OS-Linux tags (key-value pairs) assigned to them. While creating a discovery job in Migrate Accelerator, you must specify OS as the Tag key and Windows and Linux as the corresponding Tag values , as shown in the following image. Note: Ensure that you specify a unique set of tag values for a tag key--you cannot type the same tag value twice for a tag key. To add another tag, click ADD TAG and then type the tag key and corresponding tag values. You can add multiple tag keys and corresponding tag values for a discovery job. Important: You cannot modify or delete tag key values once they are saved. However, you can later edit the discovery job to add more tag keys or tag values. If you have selected CSV as the discovery tool, perform the following actions: Click Choose File . Select the CSV file from your local machine and click Upload . The first row of your CSV file must contain the following data (or column headers): HostName,IPAddress,ServerId,OSType,Group The following image shows the Create Discovery page for the CSV discovery tool. If you have selected RISC_EXPORTED_DISCOVERY as the discovery tool, perform the following actions: Click Choose File , select the CSV file that you have exported from RISC CloudScape, and click Upload . (Optional) Click MAP . The Discovery Field column displays the fields in which Migrate Accelerator stores the discovered data. In CSV Column , the drop-down list displays all columns from the CSV file that you have uploaded. For each field in the Discovery Field column, select the CSV column from which the data must be imported, as shown in the following image. (Optional) To view how the discovered data will be mapped to the Migrate Accelerator fields, move through to the end of the Discovery Field column and click PREVIEW . Click CREATE DISCOVERY . Application groups within the discovery job appear in the left panel. Note: After you create a RISC or RISC_EXPORTED_DISCOVERY discovery job, you can manually upload the connectivity data that RISC CloudScape has generated for your assessment.","title":"Create a discovery job"},{"location":"migrate/using/#view-the-status-of-risc-discovery-jobs","text":"In the case of RISC discovery jobs, Migrate Accelerator fetches data incrementally from CloudScape. While the data retrieval is still in progress, you can see the already retrieved assets in the Migrate Accelerator UI. The banner for RISC discovery jobs shows the status, timestamps (for last fetch, last new asset, and last updated assets), and impacted actions. Until all assets have been successfully retrieved, it is recommended that you do not perform the following actions: Manually upload the connectivity data Regroup servers in an application group Download the build plan for an application group Migrate servers in an application group Note: To view updates to the discovery job status in the banner, you must manually refresh your browser. The following image shows an example of the banner that is shown for RISC discovery jobs. To hide the banner, click X in the top-right corner. To show the banner again, click the Show RISC progress icon ( ).","title":"View the status of RISC discovery jobs"},{"location":"migrate/using/#manually-upload-connectivity-data","text":"The option to manually upload connectivity data is available only for RISC and RISC_EXPORTED_CSV discovery jobs. The connectivity data helps to identify the inter-dependencies between servers in an application group. For example, it can highlight the dependency between the web and database servers of an application. Migrate Accelerator includes this connectivity data in the build plan, which can be used to plan the migration of application infrastructure to the cloud. Before you begin Download the connectivity data for the appropriate assessment in CloudScape (use the Add Intelligence > Available Reports option and then export the Detailed Application Dependency Data report as a CSV file). If required, customize the connectivity data in the CSV file based on the following considerations: Ensure that all the servers listed in the CSV file are a part of the same application group in Migrate Accelerator. Otherwise, none of the connectivity data is saved in the Migrate Accelerator database. If the CSV file size is too large, you might get an error while uploading the file in Migrate Accelerator. In this case, you can break up the connectivity data into multiple files. Migrate Accelerator consolidates the connectivity data across these multiple files. Important: You must ensure that the same connectivity entry (which is a combination of source server, destination server, destination port, and connection protocol) is not listed in multiple files. Otherwise, duplicate connectivity entries are created in the database. Migrate Accelerator requires data from only the following three columns in the CSV file that is downloaded from CloudScape: destport destaddr scraddr To reduce the file size, you can choose to remove all other columns from the CSV file. To manually upload connectivity data, perform the following actions: In the Pick Discovery Job box at the top, locate the discovery job for which you want to upload the connectivity data. Next to the appropriate discovery job, click the Upload icon ( ). In the Manual Upload RISC Connectivity Data window, click Choose File . Locate and select the CSV file that contains the connectivity data for the discovery job. Click SUBMIT . The Upload icon for the discovery job is not available until the CSV file has been uploaded successfully. Note: In case of any error, none of the connectivity data in the CSV file is saved in the Migrate Accelerator database.","title":"Manually upload connectivity data"},{"location":"migrate/using/#edit-a-discovery-job","text":"In the Pick Discovery Job box at the top, locate the discovery job that you want to edit. Next to the appropriate discovery job, click the Edit icon ( ). In the Edit Discovery window, make the required updates to the discovery job. For CSV and RISC_EXPORTED_CSV discovery jobs, you can edit only the description. For RISC discovery jobs, you can edit the description, CloudScape API Endpoint, and CloudScape credentials (password and API key). You can also add a new tag key and corresponding tag values, or add new tag values to an existing tag key. For information about adding tag keys and tag values, see Create a discovery job . Note: To save any updates that you make to a RISC discovery job, you must specify the CloudScape password and API key again. Click SAVE . If you have added a new tag key or tag value to a completed RISC discovery job, its status changes to In Progress again. If you have added a tag key or tag value to an in-progress RISC discovery job, Migrate Accelerator waits for the job to complete and then again changes its status to In Progress . In both these cases, the discovery job retrieves asset data that corresponds to the new tag keys and tag values. It also retrieves any new asset data that corresponds to existing tag keys and tag values.","title":"Edit a discovery job"},{"location":"migrate/using/#plan-migration-of-servers","text":"Migrate Accelerator migrates servers as a part of an application group. An application group is a logical grouping of discovered servers that can be migrated to the cloud at the same time. A few discovery tools logically group servers as a part of the discovery process. For example, the database server and the web server used by an application can be a part of one logical application group. By default, Migrate Accelerator groups the servers in an application group based on the discovery tool that you have selected. However, you can change the default grouping of servers based on your requirements. As a part of planning the migration of application groups, you can perform the following tasks: View details of discovered servers Create a new application group Regroup servers in an application group Download the build plan for an application group","title":"Plan migration of servers"},{"location":"migrate/using/#supported-operating-systems","text":"Migrate Accelerator supports the migration of servers that use the following operating systems: Operating system Supported versions Windows - 2016 - 2012 - 2008 Ubuntu - 16.04 - 14.04 Red Hat Linux 7.6 Note: Migrate Accelerator does not support auto-installation of the CloudEndure agent on Windows 2008.","title":"Supported operating systems"},{"location":"migrate/using/#view-details-of-discovered-servers","text":"On the Home page, from the left panel, select an application group. All servers that are a part of the selected application group appear in the Servers for Group panel. Note: The Ungrouped group contains servers that are not a part of any application group. Click the server whose details you want to view. A panel with details of the selected server appears on the right, as shown in the image below. (Optional) To hide the panel, click X in the top-right corner.","title":"View details of discovered servers"},{"location":"migrate/using/#create-a-new-application-group","text":"Ensure that the discovery job for which you want to create a new application group is selected in the Pick Discovery Job box at the top. On the Home page, click Create Group . In the Create Group panel, enter the group name. The group name that you specify must be unique for the selected discovery job. To move servers to the new group, perform the following actions: In the Groups panel, select an existing application group. Servers in the selected application group appear in the Servers for Group panel. In the Servers for Group panel, select the servers that you want to move to the new group. The selected servers also appear in the Create Group panel. Note: The Search option in the Servers for Group panel filters servers listed on the current page only. (Optional) Repeat the above actions to move servers from other existing groups to the new group. The Ungrouped group contains servers that are not a part of any application group. To create a new application group, you must add at least one server in that group. Each server can be a part of only one group. Click CREATE GROUP . In the confirmation box, click YES . A new application group appears in the left panel on the Home page.","title":"Create a new application group"},{"location":"migrate/using/#regroup-servers-in-an-application-group","text":"Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, click Regroup . In the Regroup panel, from the Group Name list, select the application group to which you want to add servers. Only application groups that contain at least one server appear in the list. To add servers to the selected application group, perform the following actions: In the Groups panel, select an existing application group. Servers in the selected application group appear in the Servers for Group panel. In the Servers for Group panel, select the servers that you want to move to the group selected in the Regroup panel. The selected servers also appear in the Regroup panel. Note: The Search option in the Servers for Group panel filters servers listed on the current page only. (Optional) Repeat the above actions to move servers from other existing groups. Each server can be a part of only one group. Click REGROUP . In the confirmation box, click YES .","title":"Regroup servers in an application group"},{"location":"migrate/using/#download-the-build-plan-for-an-application-group","text":"Migrate Accelerator generates a build plan for each application group. This build plan is a specification document that you can download and use to plan the migration of your application infrastructure to the cloud. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, from the left panel, select the application group for which you want to download the build plan. Click the Download Build Plan icon ( ). The build plan is downloaded to your local machine. Migrate Accelerator automatically populates data in a few columns across multiple tabs in the build plan. You can identify these columns based on the green background color. Note: By default, Migrate Accelerator generates a generic build plan that is not customized for any specific cloud provider. However, your administrator can configure Migrate Accelerator to generate a build plan that is customized for AWS. (Optional) Enter details in the other sheets based on your requirements, add your existing application architecture diagram, and capture the dependencies for the various resources of the application that you are migrating. A cloud engineer can use this build plan to create the application infrastructure in Deploy Accelerator.","title":"Download the build plan for an application group"},{"location":"migrate/using/#migrate-servers","text":"You must perform the following steps for each application group that you want to migrate to the cloud. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. On the Home page, from the left panel, select the application group that you want to migrate. To start the migration process, click the Migrate icon ( ). On the Create Migration Job page, perform the following actions: Enter the migration name and description. From the Migration Service list, select the appropriate migration service. Note: Migrate Accelerator currently supports only CloudEndure as a migration service. From the Migration Configuration list, select the appropriate configuration. The migration configuration is a predefined set of credentials to access the selected migration service and the authentication details for the cloud service provider in which Migrate Accelerator must create images of the servers. For the CloudEndure migration service, it also defines the Live Migration project from which Migrate Accelerator must migrate servers. For information about creating migration configurations, see Create migration configurations . (Optional) To enable Migrate Accelerator to install CloudEndure agents on the servers that need to be migrated, perform the following actions: Select Auto install agents . Ensure that the servers in the data center meet the following requirements for installing the CloudEndure agent. Operating system Supported versions Software requirements Windows - 2016 - 2012 .NET framework 4.5 Ubuntu - 16.04 - 14.04 - Python 2.4 or higher - GNU Wget Red Hat Linux 7.6 - Python 2.4 or higher - GNU Wget Note: Consider the following points if Migrate Accelerator is unable to auto install the CloudEndure agent on the servers in the data center: Ensure that the folder that was specified in the com.reancloud.migration.server.windows.agent_folder property in the application.properties file while deploying Migrate Accelerator exists on the Windows servers in the data center. For secure connection to Windows servers, ensure that the thumbprint of the certificates that were copied to the ca-certs folder while deploying Migrate Accelerator matches the WinRM HTTPS Listener certificate thumbprint. Ensure that the Linux servers in the data center have access to a valid YUM repository. For Ubuntu 16.04 servers, run the following command to set the symbolic link for the appropriate version of Python: ln -s python<Version> python For example: To set the symbolic link for Python 2.7, run the following command: ln -s python2.7 python To enable Migrate Accelerator to connect to Ubuntu and Red Hat Linux servers with the user name and key (instead of password) that is specified in the selected connection, ensure that the user has sudo access with NOPASSWD. To set the PASSWD for the user, run the following command on the Ubuntu or Red Hat Linux server: sudo visudo In the file that opens, enter the following property: ALL = NOPASSWD: ALL (To connect to Linux servers in the data center) From the Linux Connection list, select the appropriate SSH migration connection. (To connect to Windows servers in the data center) From the Windows Connection list, select the appropriate WinRM migration connection. For information about creating connections, see Configure connections . When you start the migration job, Migrate Accelerator connects to the servers in the application group by using the SSH or WinRM connection that you have specified and installs the CloudEndure agent on the servers. Note: If you do not select the Auto install agents check box, ensure that you have manually installed the CloudEndure agents on the servers in the data center. For more information about installing the CloudEndure agents, see the CloudEndure documentation . To start the migration process, click CREATE MIGRATION . You can view the status of your migration job on the following page: To view the status of the agent installation process for a server, click the Action icon ( ) next to that server in the Server List panel and then select Logs . The Migration Tool Status column displays the CloudEndure status for each server in the application group. You can also view the overall migration status in Migrate Accelerator in the chart on the right panel. After the server replication is successful, the Migration Tool Status column displays the Ready for Test status. Perform the following actions for each server in the application group: To launch a test instance for the server, click the Action icon ( ) and select Launch Test Instance . After the test instance is successfully launched, create an image from the test instance by clicking the Action icon and selecting Create Image . Migrate Accelerator uses the provider that is configured in the migration job to create the images. The image ID appears in Image ID the column. To confirm that the server data has been successfully replicated, perform the following actions: Sign in to the AWS Management Console. Launch an EC2 instance by using the image ID (AMI) that you have created through Migrate Accelerator. Connect to the EC2 instance and confirm that the server data has been successfully replicated. After confirming that the server data has been successfully replicated, stop the server data replication by clicking the Action icon and selecting Launch Cutover Instance . This action also uninstalls the agent from the server in your data center. To create the final image of the server, click the Action icon and select Create Image . Migrate Accelerator uses the provider that is configured in the migration job to create the images. The new image ID is updated in the Image ID column. (Optional) Use migrated resources in Deploy Accelerator .","title":"Migrate servers"},{"location":"migrate/using/#use-migrated-resources-in-deploy-accelerator","text":"In a browser, type your Cloud Accelerator Platform URL. Sign in to Cloud Accelerator Platform. On the Home page of Deploy Accelerator, create a new environment for one of the migrated applications. Ensure that you select the same provider and region as your target environment in the Live Migration project in CloudEndure. From the Resources tab in the left panel, drag an EC2 Instance resource to the canvas. Click the resource you have created and in the right panel, enter the AMI ID of your migrated server and other attribute details. Repeat steps 4 and 5 for each migrated server that is a part of the application that you want to deploy. To build your environment, drag additional network and other resources to the canvas. Create appropriate dependencies between these resources. Save the environment. Start a deployment of this environment. For more information about creating and deploying environments in Deploy Accelerator, see the Deploy and manage environments .","title":"Use migrated resources in Deploy Accelerator"},{"location":"migrate/using/#viewing-the-overall-migration-status-for-application-groups","text":"You can view the overall migration status for one or more application groups in a discovery job. Ensure that the appropriate discovery job is selected in the Pick Discovery Job box at the top. In the icon panel, click DISCOVERY VIEW . From the left panel, select the application groups for which you want to view the dashboard. You can view the following information on the dashboard: Panel Description DISCOVERED This panel displays the total number of discovered servers in your data center. AGENTS This panel displays the total number of servers on which Migrate Accelerator is currently installing the CloudEndure agents, the total number of servers on which the agents are successfully installed, and the total number of servers on which the agents failed to be installed. Note: When the agent installation process starts, the Discovered panel displays 0 discovered servers. Instead, these servers are displayed as INSTALLING, COMPLETE, or FAILED in the AGENTS panel. MIGRATIONS This panel displays the total number of servers that are being currently migrated, total number of servers that were successfully migrated, and total number of servers that failed to be migrated. Note: When the migration process starts, the AGENTS panel displays 0 COMPLETE servers. Instead, these servers are displayed as MIGRATING, COMPLETE, or FAILED in the MIGRATIONS panel. Migrated jobs This panel displays the migration jobs that were run for the selected application group. You can view the job name, job description, job start time, and the migration tool that was used.","title":"Viewing the overall migration status for application groups"},{"location":"platform-common/administer/","text":"Administer Cloud Accelerator Platform \u00b6 This topic describes how to manage groups and users in Hitachi Cloud Accelerator Platform and integrate Cloud Accelerator Platform with Microsoft Active Directory. Contents \u00b6 Overview of Admin Console Managing users Managing groups Customizing the password policy for user accounts Customizing user account lockout settings Configuring the session timeout for user accounts Integrating Cloud Accelerator Platform with Active Directory Overview of Admin Console \u00b6 The Admin Console enables Cloud Accelerator Platform administrators to manage a common set of groups and users for Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator), and Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator). Each user is assigned to one or more groups. The administrators assign permissions to each group by either selecting policies or configuring share permissions for specific resources. Based on their assigned groups, users can perform different actions in Deploy Accelerator, Assess Accelerator, Test Accelerator, and Migrate Accelerator. When Cloud Accelerator Platform is successfully deployed, a default administrator is also created with the credentials mentioned in the customer.yml file at the time of deployment. If required, this administrator can create additional administrators. By default, administrators can use the Admin Console to add groups and users and assign users to appropriate groups. If required, administrators can modify users or groups, disable users, and delete groups. Administrators can also integrate Cloud Accelerator Platform with an existing setup of Active Directory and manage users and group membership in Active Directory. Managing users \u00b6 Cloud Accelerator Platform administrators can add and manage users from the Admin Console. They can perform the following actions to manage users: Add a new user Verify a new user Modify an existing user Reset the password of an existing user Unlock an existing user's account Disable an existing user Other users can also create their own Cloud Accelerator Platform account and then request the administrator to assign them to the appropriate group. Note: If you have integrated Cloud Accelerator Platform with Active Directory , you must add and manage users in Active Directory. In that case, administrators can only view user details in the Admin Console. Add a new user \u00b6 Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Add User , enter the full name, user name, email address, and password of the new user. From the Select Groups list, select the groups to which you want to add the user. Cloud Accelerator Platform provides a few default groups that you can assign to users based on the actions that they need to perform across different accelerators. Based on the requirements, administrators can also add new groups . (Optional) To view details of the policies that are a part of the selected groups, select view Policies . Click CREATE USER . A new user appears under the Users List section. To complete the user creation process, verify that user . New users can sign in to Cloud Accelerator Platform only after they are successfully verified. Verify a new user \u00b6 When new users create a Cloud Accelerator Platform account, they receive an email from Cloud Accelerator Platform. To verify their email address and complete the account creation process, users have to click the login link in the email. As an administrator, you can also verify new users who have created their own account. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user you want to verify, click VERIFY . In the confirmation box, click YES . The new user is successfully verified and can sign in to Cloud Accelerator Platform. Modify an existing user \u00b6 Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose details you want to modify, click EDIT . Under Edit User , make the required changes. Click SAVE USER . Reset the password of an existing user \u00b6 Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose password you want to reset, click Reset Password . In the Reset Password window, enter a new password, re-enter the new password, and click SUBMIT . After the password is reset successfully, you can share the new password with the user. Unlock an existing user's account \u00b6 If users fail to sign into their Cloud Accelerator Platform account in three consecutive attempts, their account is locked for two hours. If required, administrators can unlock a user's account during this lockout duration. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose account you want to unlock, click ENABLE . Note: Administrators can also configure default values for the number of failed sign-in attempts and the lockout duration For more information, see Customizing user account lockout settings . Disable an existing user \u00b6 You can disable users to prevent them from signing in to their Cloud Accelerator Platform account. However, the entities that these users have created are not removed. While disabling a user, you can select a policy-based group with which the user's entities can be shared. Other users in the selected group can then access these shared entities. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) icon in the top-right corner and then click Users . Under Users List , in the Actions column for the user that you want to disable, click DISABLE . (Optional) In the Share Entities window, select the policy-based group with which you want to share the following Deploy Accelerator entities that the selected user owns: Environments Deployments Providers Connections Chef Server packages Note: To share test jobs that the user has created in Test Accelerator, you can add the user to a group in which share permissions for the appropriate tests are configured. This action ensures that other users in that group can continue to access the test jobs created by the user you have disabled. For more information, see Managing groups . Click SUBMIT . The user can no longer sign in to Cloud Accelerator Platform. Also, the shared entities of this user can now be accessed by other users in the selected group. The actions that they can perform on the shared entities is based on the policies assigned to the group. Note: To enable a user that you have previously disabled, under the Users List section, in the Actions column for that user, click ENABLE . This user can once again sign in to Cloud Accelerator Platform and access all previously-owned entities. In this case, users in the group that was selected while disabling the user can no longer access the shared entities of this user. Managing groups \u00b6 Cloud Accelerator Platform provides a few default groups that administrators can assign to users. If required, you can also add new groups or modify existing groups from the Admin Console. You can add multiple users in a group and configure permissions for the group in one of the following ways: Assign out-of-the-box policies ( Select Policy ) While adding a new group, you can select one or more policies based on the permissions that you want to assign to users in the group. Cloud Accelerator Platform provides many out-of-the-box policies that provide permissions to perform specific actions. For example, the ManualTestViewCreate policy provides the permission to create and view manual test jobs in Test Accelerator. Configure share permissions for specific resources ( Group-Level Sharing ) While adding a new group, you can configure share permissions for an out-of-the-box list of resources . Based on the assigned share permissions, users within the group can view each other's resources and perform other actions based on the permissions. For example, the View permission for the ManualTest resource in Test Accelerator enables users within the group to view each other's manual test jobs. However, the users within this group must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to view each other's manual test jobs, users must also be assigned to another policy-based group that gives them the basic permission to view manual test jobs. Note: You can continue to add and manage Cloud Accelerator Platform groups in the Admin Console even after you have integrated Cloud Accelerator Platform with Active Directory . Default groups \u00b6 The following table lists the default groups that are available in the Admin Console. While adding users , administrators can assign them to the appropriate groups based on the actions that they need to perform across different accelerators. Based on the requirements, you can also add a new group and configure the appropriate policies (access level) for that group. Group Accelerator Access level ADMIN -- Assess Accelerator -- Deploy Accelerator -- Test Accelerator This group allows users to perform all admin-related tasks, such as manage users and groups in Admin Console, add Chef Servers in Deploy Accelerator, and configure Test Accelerator. CLOUD_ARCHITECT -- Assess Accelerator -- Deploy Accelerator This group allows users to perform multiple operations, such as create environments, import environments, view environments, edit environments, deploy environments, create and manage providers, create and manage connections, destroy environments, delete environments, export environments as blueprints, run assessment policies, view assessment policy reports, download assessment policy reports, and customize assessment policy reports. DEFAULT -- Assess Accelerator -- Deploy Accelerator -- Test Accelerator This group allows users to update their profile information. Note: If users create their own Cloud Accelerator Platform account or if you do not select a group while creating users, the DEFAULT group is automatically assigned to these users. ENVIRONMENT_USER Deploy Accelerator This group allows users to deploy and destroy environments. ENVIRONMENT_VIEWER Deploy Accelerator This group allows users to view all connections, providers, and environments. FREE_TRIALS -- Assess Accelerator -- Deploy Accelerator This group allows users to create a limited number of new connections, providers, and environments. hcaptest-user Test Accelerator This group allows users to run and manage test jobs, view tests results, create and manage job schedules, view provider details, and view configuration details. hcaptest-view Test Accelerator This group allows users to view test jobs, test results, job schedules, provider details, and configuration details in Test Accelerator. HELM_USER Workflow Accelerator This group allows users to create Helm repositories and deploy Helm charts using Workflow Accelerator solution packages. HELM_VIEWER Workflow Accelerator This group allows users to only view Helm repository and Helm chart data defined in Workflow Accelerator solution packages. MIGRATE_USER Migrate Accelerator This group allows users to perform all actions in Migrate Accelerator, such as create discovery and migration jobs, connections, and migration credentials. MIGRATE_VIEWER Migrate Accelerator This group allows users to only view data in Migrate Accelerator. PROVIDER_USER Migrate Accelerator This group allows Migrate Accelerator users to create and edit the providers they have created. PROVIDER_VIEWER Migrate Accelerator This group allows Migrate Accelerator users to only view the providers they have previously created. SOLUTION_PACKAGE_DEPLOYMENT_USER Workflow Accelerator This group allows users to deploy registered solution packages. SOLUTION_PACKAGE_DEPLOYMENT_VIEWER Workflow Accelerator This group allows users to only view deployment data for registered solution packages. SOLUTION_PACKAGE_USER Workflow Accelerator This group allows users to perform all operations on solution packages, including creating and registering solution packages SOLUTION_PACKAGE_VIEWER Workflow Accelerator This group allows users to only view data for registered solution packages. VIEW_ALL_USER'S_ENTITIES Deploy Accelerator This group allows users to view the environments and deployments of all users in Deploy Accelerator . It also allows users to view additional entities (providers, connections, and packages) of all users in Deploy Accelerator but only by using API commands. However, to perform any actions on these entities, users must also be members of the ADMIN group. Note: You cannot view or modify the policies that are attached to the VIEW_ALL_USER'S ENTITIES groups. Add a new group \u00b6 While adding a new group, you can either assign out-of-the-box policies to the group or assign share permissions for specific resources. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) icon in the top-right corner and then click Groups . Under Add Group , enter the group name and description. To assign policies to the group, perform the following actions: From the Select Policy list, select the appropriate policies. By selecting a policy, you are adding permissions that are attached to the policy. Note: You can add a new group without selecting a policy. (Optional) To view details of the selected policies, select view Policies . From the Select Users list, select the users you want to add to this group. The selected users get permissions attached to the selected policies. To assign share permissions for specific resources to the group, perform the following actions: Turn on the Group Level Sharing toggle. The resources for which you can configure share permissions appear under the Select Resources section. Important: You can currently configure share permissions for only Test Accelerator resources. Expand the resource for which you want to configure share permissions and select the appropriate permission check boxes. Repeat this step for each resource for which you want to configure share permissions. Note: In the case of Test Accelerator resources, ensure that you select the View permission along with other permissions, such as Stop and Schedule . From the Select Users list, select the users you want to add to this group. The selected users can view the resources created by other users in the group. They can also perform other actions based on the additional permissions that are selected for that resource. Note: The selected users must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to schedule test jobs, users must also be a part of the reantest-user group. Click CREATE GROUP . A new group appears under the Groups List section. Modify an existing group \u00b6 Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Groups . Under Groups List , in the Actions column for the group that you want to modify, perform one of the following actions: To add users to the group, click EDIT , make the required changes under the Edit Group section, and click SAVE GROUP . To delete the group, click DELETE . You can delete a group only if it does not have any selected policies or assigned users. Customizing the password policy for user accounts \u00b6 By default, the following password policy is used for user accounts in Cloud Accelerator Platform: Password must be between 10 and 50 characters in length. Password must be a mix of upper and lower case characters. Password must have at least one numeric character. Administrators can choose to customize the password policy by setting any of the following parameters: Maximum length Minimum length Minimum special characters Minimum digit characters (numeric characters) Minimum upper case characters Minimum lower case characters The values that you set for these parameters do not have to be within any pre-defined range. However, you must ensure that the values that you set do not contradict each other. For example, if you set the value of minimum special, upper case, lower case, and numeric characters to three each, your minimum length value must be at least 12. Note: To customize the password policy for user accounts, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To customize the password policy for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: authnz: environment section. If the PWD_POLICY property is not available in the core.yml file, add the property. For the PWD_POLICY property, set one or more of the following parameters: max_length min_length min_spl_chars min_digits min_upper_case_chars min_lower_case_chars The parameters that you set must be comma separated, as shown in the example below. PWD_POLICY: min_upper_case_chars=3,min_lower_case_chars=6,min_length=9,max_length=20 Save the core.yml file. Restart the rean-core service. Customizing user account lockout settings \u00b6 By default, Cloud Accelerator Platform users are locked out of their account for two hours after three unsuccessful sign-in attempts. If required, administrators can modify the number of failed sign-in attempts and the lockout duration. Note: To customize the lockout settings for user accounts, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To customize lockout settings for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: authnz: environment section. To modify the number of failed login attempts, perform the following actions: If the rean.platform.security.brute.failed_attempts property is not available in the core.yml file, add the property. Update the number of failed sign-in attempts, as shown below: rean.platform.security.brute.failed_attempts=${BRUTE_FAILED_ATTEMPTS:5} To modify the lockout duration, perform the following actions: If the rean.platform.security.brute.locked_duration_in_min property is not available in the core.yml file, add the property. Update the lockout duration (in minutes), as shown below: rean.platform.security.brute.locked_duration_in_min=${BRUTE_LOCKED_DURATION_IN_MIN:60} Save the core.yml file. Restart the rean-core service. Configuring the session timeout for user accounts \u00b6 By default, Cloud Accelerator Platform users are automatically logged out of their account if they do not perform any action for 30 minutes. If required, administrators can modify this session timeout duration (in minutes). If users close their browser tab or window, their working session remains active for the duration that is configured. Note: To configure the session timeout duration, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To configure the session timeout for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: apigateway: environment section. If the com.reancloud.platform.token.expiration.time property is not available in the core.yml file, add the property. Update the session timeout duration (in minutes), as shown below: com.reancloud.platform.token.expiration.time=45 Save the core.yml file. Restart the rean-core service. Integrating Cloud Accelerator Platform with Active Directory \u00b6 Cloud Accelerator Platform enables you to use your existing Microsoft Active Directory setup to authenticate users in Cloud Accelerator Platform and provide user-group authorization. In this case, you can only view users in the Admin Console. As a part of configuring this integration, administrators must map Active Directory groups to the appropriate Cloud Accelerator Platform groups. At least one Active Directory group must be mapped to the ADMIN group in Cloud Accelerator Platform. Users who are members of this Active Directory group can access the Admin Console. Going forward, they can manage groups, update the group mapping, and view users in the Admin Console. However, they can add and delete users only in Active Directory. After the Active Directory integration is completed, users need to sign in to Cloud Accelerator Platform with their Active Directory email ID and password. Cloud Accelerator Platform uses Active Directory to authenticate users. The authenticated users can perform actions based on the C loud Accelerator Platform group that is mapped to their Active Directory group. If Cloud Accelerator Platform is in use before it is integrated with Active Directory, you must migrate existing users from Cloud Accelerator Platform to Active Directory . Note: Administrators can integrate Cloud Accelerator Platform with only one Active Directory server. Before you begin \u00b6 Before you configure the Active Directory server in the Admin Console, ensure that you have performed the following actions: Whitelisted the Cloud Accelerator Platform server's IP address in the Active Directory server's security group. Ensured that the email ID of the default administrator in Cloud Accelerator Platform is added in Active Directory. Configured the LDAP_ADMIN_USERDN and LDAP_ADMIN_PASSWORD properties in the customer.yml file, as shown in the following example: LDAP_ADMIN_USERDN=CN=Administrator,CN=Users,DC=reancloud,DC=com LDAP_ADMIN_PASSWORD=D$jn*JHG) (Only if you want to use LDAP over SSL) Copied the certificate that you have generated while setting up Active Directory to the cacert folder on the Cloud Accelerator Platform instance. Restarted the Cloud Accelerator Platform services after updating the customer.yml file and copying the certificate to the cacert folder. Configure the Active Directory server \u00b6 Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options ( ) in the top-right corner and then click LDAP Configuration . Under LDAP Configuration , enter the following information: Field Description Server URL URL of the server on which Active Directory is configured. To use LDAP over SSL, you must specify the URL in the following format: ldaps:// HostName :636 User DN A unique user name that is used to find the administrator user in Active Directory. Password Password of the administrator user specified in User DN . Base DN A Base DN in Active Directory that is used to authenticate users. Search Base DN A Base DN in Active Directory that is used to search for groups. Click VALIDATE AND SAVE . In the confirmation message box, click SAVE to enable Active Directory integration. After you have successfully integrated Cloud Accelerator Platform with Active Directory, the Active Directory groups that are available in the Organization Unit (OU) specified in Search Base DN appear under the Group Mappings section. Map Active Directory and Cloud Accelerator Platform groups . Map Active Directory and Cloud Accelerator Platform groups \u00b6 After you have successfully integrated Cloud Accelerator Platform with Active Directory, the Active Directory groups appear under the Group Mappings section based on your configuration. You must map these Active Directory groups to the appropriate Cloud Accelerator Platform groups. Important: You must map at least one Active Directory group to the ADMIN group in Cloud Accelerator Platform. This Active Directory group must contain users who should be given administrative permissions in Cloud Accelerator Platform. To map Active Directory and Cloud Accelerator Platform groups, perform the following actions: Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click LDAP Configuration . Under Group Mappings , in the Local Group column for an Active Directory group, select the appropriate Cloud Accelerator Platform group. Repeat step 4 for each Active Directory group that is listed in the LDAP Group column. Click SAVE MAPPINGS . Note: You can continue to add and manage groups in the Admin Console even after you have integrated Cloud Accelerator Platform with Active Directory. View Active Directory users \u00b6 After the Active Directory integration is completed, you can only view user details in the Admin Console. You must add and manage users in Active Directory. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , select the user whose details you want to view. Migrate users from Cloud Accelerator Platform to Active Directory \u00b6 If Cloud Accelerator Platform is in use before it is integrated with Active Directory, you must migrate the existing Cloud Accelerator Platform users to Active Directory. The data owned by the Cloud Accelerator Platform users must be available after the Active Directory integration is completed. Migrate the default administrator user to Active Directory \u00b6 Consider the following points while migrating the default administrator user: The user whose email ID was provided while deploying Cloud Accelerator Platform is considered as the default administrator of Cloud Accelerator Platform, irrespective of authorization groups association. The user who is integrating Cloud Accelerator Platform with Active Directory must ensure that the Active Directory has a user with the email ID of default administrator. After Active Directory integration is completed, the default administrator user must use the Active Directory credentials to sign in to Cloud Accelerator Platform. Migrate existing users to Active Directory \u00b6 Consider the following points while migrating existing Cloud Accelerator Platform users: If the email IDs of existing Cloud Accelerator Platform users are the same as in Active Directory, the Active Directory users automatically get mapped to Cloud Accelerator Platform users. All the data (environments, providers, connections, and so on) associated with the existing Cloud Accelerator Platform users is available to the Active Directory users with the same email IDs. The existing Cloud Accelerator Platform users must use the Active Directory credentials to sign in to Cloud Accelerator Platform. If the email IDs of existing Cloud Accelerator Platform users are not available in Active Directory, the existing Cloud Accelerator Platform users are not able to sign in to Cloud Accelerator Platform.","title":"Administer"},{"location":"platform-common/administer/#administer-cloud-accelerator-platform","text":"This topic describes how to manage groups and users in Hitachi Cloud Accelerator Platform and integrate Cloud Accelerator Platform with Microsoft Active Directory.","title":"Administer Cloud Accelerator Platform"},{"location":"platform-common/administer/#contents","text":"Overview of Admin Console Managing users Managing groups Customizing the password policy for user accounts Customizing user account lockout settings Configuring the session timeout for user accounts Integrating Cloud Accelerator Platform with Active Directory","title":"Contents"},{"location":"platform-common/administer/#overview-of-admin-console","text":"The Admin Console enables Cloud Accelerator Platform administrators to manage a common set of groups and users for Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator), and Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator). Each user is assigned to one or more groups. The administrators assign permissions to each group by either selecting policies or configuring share permissions for specific resources. Based on their assigned groups, users can perform different actions in Deploy Accelerator, Assess Accelerator, Test Accelerator, and Migrate Accelerator. When Cloud Accelerator Platform is successfully deployed, a default administrator is also created with the credentials mentioned in the customer.yml file at the time of deployment. If required, this administrator can create additional administrators. By default, administrators can use the Admin Console to add groups and users and assign users to appropriate groups. If required, administrators can modify users or groups, disable users, and delete groups. Administrators can also integrate Cloud Accelerator Platform with an existing setup of Active Directory and manage users and group membership in Active Directory.","title":"Overview of Admin Console"},{"location":"platform-common/administer/#managing-users","text":"Cloud Accelerator Platform administrators can add and manage users from the Admin Console. They can perform the following actions to manage users: Add a new user Verify a new user Modify an existing user Reset the password of an existing user Unlock an existing user's account Disable an existing user Other users can also create their own Cloud Accelerator Platform account and then request the administrator to assign them to the appropriate group. Note: If you have integrated Cloud Accelerator Platform with Active Directory , you must add and manage users in Active Directory. In that case, administrators can only view user details in the Admin Console.","title":"Managing users"},{"location":"platform-common/administer/#add-a-new-user","text":"Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Add User , enter the full name, user name, email address, and password of the new user. From the Select Groups list, select the groups to which you want to add the user. Cloud Accelerator Platform provides a few default groups that you can assign to users based on the actions that they need to perform across different accelerators. Based on the requirements, administrators can also add new groups . (Optional) To view details of the policies that are a part of the selected groups, select view Policies . Click CREATE USER . A new user appears under the Users List section. To complete the user creation process, verify that user . New users can sign in to Cloud Accelerator Platform only after they are successfully verified.","title":"Add a new user"},{"location":"platform-common/administer/#verify-a-new-user","text":"When new users create a Cloud Accelerator Platform account, they receive an email from Cloud Accelerator Platform. To verify their email address and complete the account creation process, users have to click the login link in the email. As an administrator, you can also verify new users who have created their own account. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user you want to verify, click VERIFY . In the confirmation box, click YES . The new user is successfully verified and can sign in to Cloud Accelerator Platform.","title":"Verify a new user"},{"location":"platform-common/administer/#modify-an-existing-user","text":"Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose details you want to modify, click EDIT . Under Edit User , make the required changes. Click SAVE USER .","title":"Modify an existing user"},{"location":"platform-common/administer/#reset-the-password-of-an-existing-user","text":"Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose password you want to reset, click Reset Password . In the Reset Password window, enter a new password, re-enter the new password, and click SUBMIT . After the password is reset successfully, you can share the new password with the user.","title":"Reset the password of an existing user"},{"location":"platform-common/administer/#unlock-an-existing-users-account","text":"If users fail to sign into their Cloud Accelerator Platform account in three consecutive attempts, their account is locked for two hours. If required, administrators can unlock a user's account during this lockout duration. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , in the Actions column for the user whose account you want to unlock, click ENABLE . Note: Administrators can also configure default values for the number of failed sign-in attempts and the lockout duration For more information, see Customizing user account lockout settings .","title":"Unlock an existing user's account"},{"location":"platform-common/administer/#disable-an-existing-user","text":"You can disable users to prevent them from signing in to their Cloud Accelerator Platform account. However, the entities that these users have created are not removed. While disabling a user, you can select a policy-based group with which the user's entities can be shared. Other users in the selected group can then access these shared entities. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) icon in the top-right corner and then click Users . Under Users List , in the Actions column for the user that you want to disable, click DISABLE . (Optional) In the Share Entities window, select the policy-based group with which you want to share the following Deploy Accelerator entities that the selected user owns: Environments Deployments Providers Connections Chef Server packages Note: To share test jobs that the user has created in Test Accelerator, you can add the user to a group in which share permissions for the appropriate tests are configured. This action ensures that other users in that group can continue to access the test jobs created by the user you have disabled. For more information, see Managing groups . Click SUBMIT . The user can no longer sign in to Cloud Accelerator Platform. Also, the shared entities of this user can now be accessed by other users in the selected group. The actions that they can perform on the shared entities is based on the policies assigned to the group. Note: To enable a user that you have previously disabled, under the Users List section, in the Actions column for that user, click ENABLE . This user can once again sign in to Cloud Accelerator Platform and access all previously-owned entities. In this case, users in the group that was selected while disabling the user can no longer access the shared entities of this user.","title":"Disable an existing user"},{"location":"platform-common/administer/#managing-groups","text":"Cloud Accelerator Platform provides a few default groups that administrators can assign to users. If required, you can also add new groups or modify existing groups from the Admin Console. You can add multiple users in a group and configure permissions for the group in one of the following ways: Assign out-of-the-box policies ( Select Policy ) While adding a new group, you can select one or more policies based on the permissions that you want to assign to users in the group. Cloud Accelerator Platform provides many out-of-the-box policies that provide permissions to perform specific actions. For example, the ManualTestViewCreate policy provides the permission to create and view manual test jobs in Test Accelerator. Configure share permissions for specific resources ( Group-Level Sharing ) While adding a new group, you can configure share permissions for an out-of-the-box list of resources . Based on the assigned share permissions, users within the group can view each other's resources and perform other actions based on the permissions. For example, the View permission for the ManualTest resource in Test Accelerator enables users within the group to view each other's manual test jobs. However, the users within this group must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to view each other's manual test jobs, users must also be assigned to another policy-based group that gives them the basic permission to view manual test jobs. Note: You can continue to add and manage Cloud Accelerator Platform groups in the Admin Console even after you have integrated Cloud Accelerator Platform with Active Directory .","title":"Managing groups"},{"location":"platform-common/administer/#default-groups","text":"The following table lists the default groups that are available in the Admin Console. While adding users , administrators can assign them to the appropriate groups based on the actions that they need to perform across different accelerators. Based on the requirements, you can also add a new group and configure the appropriate policies (access level) for that group. Group Accelerator Access level ADMIN -- Assess Accelerator -- Deploy Accelerator -- Test Accelerator This group allows users to perform all admin-related tasks, such as manage users and groups in Admin Console, add Chef Servers in Deploy Accelerator, and configure Test Accelerator. CLOUD_ARCHITECT -- Assess Accelerator -- Deploy Accelerator This group allows users to perform multiple operations, such as create environments, import environments, view environments, edit environments, deploy environments, create and manage providers, create and manage connections, destroy environments, delete environments, export environments as blueprints, run assessment policies, view assessment policy reports, download assessment policy reports, and customize assessment policy reports. DEFAULT -- Assess Accelerator -- Deploy Accelerator -- Test Accelerator This group allows users to update their profile information. Note: If users create their own Cloud Accelerator Platform account or if you do not select a group while creating users, the DEFAULT group is automatically assigned to these users. ENVIRONMENT_USER Deploy Accelerator This group allows users to deploy and destroy environments. ENVIRONMENT_VIEWER Deploy Accelerator This group allows users to view all connections, providers, and environments. FREE_TRIALS -- Assess Accelerator -- Deploy Accelerator This group allows users to create a limited number of new connections, providers, and environments. hcaptest-user Test Accelerator This group allows users to run and manage test jobs, view tests results, create and manage job schedules, view provider details, and view configuration details. hcaptest-view Test Accelerator This group allows users to view test jobs, test results, job schedules, provider details, and configuration details in Test Accelerator. HELM_USER Workflow Accelerator This group allows users to create Helm repositories and deploy Helm charts using Workflow Accelerator solution packages. HELM_VIEWER Workflow Accelerator This group allows users to only view Helm repository and Helm chart data defined in Workflow Accelerator solution packages. MIGRATE_USER Migrate Accelerator This group allows users to perform all actions in Migrate Accelerator, such as create discovery and migration jobs, connections, and migration credentials. MIGRATE_VIEWER Migrate Accelerator This group allows users to only view data in Migrate Accelerator. PROVIDER_USER Migrate Accelerator This group allows Migrate Accelerator users to create and edit the providers they have created. PROVIDER_VIEWER Migrate Accelerator This group allows Migrate Accelerator users to only view the providers they have previously created. SOLUTION_PACKAGE_DEPLOYMENT_USER Workflow Accelerator This group allows users to deploy registered solution packages. SOLUTION_PACKAGE_DEPLOYMENT_VIEWER Workflow Accelerator This group allows users to only view deployment data for registered solution packages. SOLUTION_PACKAGE_USER Workflow Accelerator This group allows users to perform all operations on solution packages, including creating and registering solution packages SOLUTION_PACKAGE_VIEWER Workflow Accelerator This group allows users to only view data for registered solution packages. VIEW_ALL_USER'S_ENTITIES Deploy Accelerator This group allows users to view the environments and deployments of all users in Deploy Accelerator . It also allows users to view additional entities (providers, connections, and packages) of all users in Deploy Accelerator but only by using API commands. However, to perform any actions on these entities, users must also be members of the ADMIN group. Note: You cannot view or modify the policies that are attached to the VIEW_ALL_USER'S ENTITIES groups.","title":"Default groups"},{"location":"platform-common/administer/#add-a-new-group","text":"While adding a new group, you can either assign out-of-the-box policies to the group or assign share permissions for specific resources. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) icon in the top-right corner and then click Groups . Under Add Group , enter the group name and description. To assign policies to the group, perform the following actions: From the Select Policy list, select the appropriate policies. By selecting a policy, you are adding permissions that are attached to the policy. Note: You can add a new group without selecting a policy. (Optional) To view details of the selected policies, select view Policies . From the Select Users list, select the users you want to add to this group. The selected users get permissions attached to the selected policies. To assign share permissions for specific resources to the group, perform the following actions: Turn on the Group Level Sharing toggle. The resources for which you can configure share permissions appear under the Select Resources section. Important: You can currently configure share permissions for only Test Accelerator resources. Expand the resource for which you want to configure share permissions and select the appropriate permission check boxes. Repeat this step for each resource for which you want to configure share permissions. Note: In the case of Test Accelerator resources, ensure that you select the View permission along with other permissions, such as Stop and Schedule . From the Select Users list, select the users you want to add to this group. The selected users can view the resources created by other users in the group. They can also perform other actions based on the additional permissions that are selected for that resource. Note: The selected users must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to schedule test jobs, users must also be a part of the reantest-user group. Click CREATE GROUP . A new group appears under the Groups List section.","title":"Add a new group"},{"location":"platform-common/administer/#modify-an-existing-group","text":"Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Groups . Under Groups List , in the Actions column for the group that you want to modify, perform one of the following actions: To add users to the group, click EDIT , make the required changes under the Edit Group section, and click SAVE GROUP . To delete the group, click DELETE . You can delete a group only if it does not have any selected policies or assigned users.","title":"Modify an existing group"},{"location":"platform-common/administer/#customizing-the-password-policy-for-user-accounts","text":"By default, the following password policy is used for user accounts in Cloud Accelerator Platform: Password must be between 10 and 50 characters in length. Password must be a mix of upper and lower case characters. Password must have at least one numeric character. Administrators can choose to customize the password policy by setting any of the following parameters: Maximum length Minimum length Minimum special characters Minimum digit characters (numeric characters) Minimum upper case characters Minimum lower case characters The values that you set for these parameters do not have to be within any pre-defined range. However, you must ensure that the values that you set do not contradict each other. For example, if you set the value of minimum special, upper case, lower case, and numeric characters to three each, your minimum length value must be at least 12. Note: To customize the password policy for user accounts, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To customize the password policy for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: authnz: environment section. If the PWD_POLICY property is not available in the core.yml file, add the property. For the PWD_POLICY property, set one or more of the following parameters: max_length min_length min_spl_chars min_digits min_upper_case_chars min_lower_case_chars The parameters that you set must be comma separated, as shown in the example below. PWD_POLICY: min_upper_case_chars=3,min_lower_case_chars=6,min_length=9,max_length=20 Save the core.yml file. Restart the rean-core service.","title":"Customizing the password policy for user accounts"},{"location":"platform-common/administer/#customizing-user-account-lockout-settings","text":"By default, Cloud Accelerator Platform users are locked out of their account for two hours after three unsuccessful sign-in attempts. If required, administrators can modify the number of failed sign-in attempts and the lockout duration. Note: To customize the lockout settings for user accounts, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To customize lockout settings for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: authnz: environment section. To modify the number of failed login attempts, perform the following actions: If the rean.platform.security.brute.failed_attempts property is not available in the core.yml file, add the property. Update the number of failed sign-in attempts, as shown below: rean.platform.security.brute.failed_attempts=${BRUTE_FAILED_ATTEMPTS:5} To modify the lockout duration, perform the following actions: If the rean.platform.security.brute.locked_duration_in_min property is not available in the core.yml file, add the property. Update the lockout duration (in minutes), as shown below: rean.platform.security.brute.locked_duration_in_min=${BRUTE_LOCKED_DURATION_IN_MIN:60} Save the core.yml file. Restart the rean-core service.","title":"Customizing user account lockout settings"},{"location":"platform-common/administer/#configuring-the-session-timeout-for-user-accounts","text":"By default, Cloud Accelerator Platform users are automatically logged out of their account if they do not perform any action for 30 minutes. If required, administrators can modify this session timeout duration (in minutes). If users close their browser tab or window, their working session remains active for the duration that is configured. Note: To configure the session timeout duration, you must have administrative access to the instance on which Cloud Accelerator Platform is deployed. To configure the session timeout for user accounts, perform the following actions: Connect to the instance on which Cloud Accelerator Platform is deployed. Locate the ${PLATFORM_HOME}/deployment folder. Open the core.yml file and locate the services: apigateway: environment section. If the com.reancloud.platform.token.expiration.time property is not available in the core.yml file, add the property. Update the session timeout duration (in minutes), as shown below: com.reancloud.platform.token.expiration.time=45 Save the core.yml file. Restart the rean-core service.","title":"Configuring the session timeout for user accounts"},{"location":"platform-common/administer/#integrating-cloud-accelerator-platform-with-active-directory","text":"Cloud Accelerator Platform enables you to use your existing Microsoft Active Directory setup to authenticate users in Cloud Accelerator Platform and provide user-group authorization. In this case, you can only view users in the Admin Console. As a part of configuring this integration, administrators must map Active Directory groups to the appropriate Cloud Accelerator Platform groups. At least one Active Directory group must be mapped to the ADMIN group in Cloud Accelerator Platform. Users who are members of this Active Directory group can access the Admin Console. Going forward, they can manage groups, update the group mapping, and view users in the Admin Console. However, they can add and delete users only in Active Directory. After the Active Directory integration is completed, users need to sign in to Cloud Accelerator Platform with their Active Directory email ID and password. Cloud Accelerator Platform uses Active Directory to authenticate users. The authenticated users can perform actions based on the C loud Accelerator Platform group that is mapped to their Active Directory group. If Cloud Accelerator Platform is in use before it is integrated with Active Directory, you must migrate existing users from Cloud Accelerator Platform to Active Directory . Note: Administrators can integrate Cloud Accelerator Platform with only one Active Directory server.","title":"Integrating Cloud Accelerator Platform with Active Directory"},{"location":"platform-common/administer/#before-you-begin","text":"Before you configure the Active Directory server in the Admin Console, ensure that you have performed the following actions: Whitelisted the Cloud Accelerator Platform server's IP address in the Active Directory server's security group. Ensured that the email ID of the default administrator in Cloud Accelerator Platform is added in Active Directory. Configured the LDAP_ADMIN_USERDN and LDAP_ADMIN_PASSWORD properties in the customer.yml file, as shown in the following example: LDAP_ADMIN_USERDN=CN=Administrator,CN=Users,DC=reancloud,DC=com LDAP_ADMIN_PASSWORD=D$jn*JHG) (Only if you want to use LDAP over SSL) Copied the certificate that you have generated while setting up Active Directory to the cacert folder on the Cloud Accelerator Platform instance. Restarted the Cloud Accelerator Platform services after updating the customer.yml file and copying the certificate to the cacert folder.","title":"Before you begin"},{"location":"platform-common/administer/#configure-the-active-directory-server","text":"Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options ( ) in the top-right corner and then click LDAP Configuration . Under LDAP Configuration , enter the following information: Field Description Server URL URL of the server on which Active Directory is configured. To use LDAP over SSL, you must specify the URL in the following format: ldaps:// HostName :636 User DN A unique user name that is used to find the administrator user in Active Directory. Password Password of the administrator user specified in User DN . Base DN A Base DN in Active Directory that is used to authenticate users. Search Base DN A Base DN in Active Directory that is used to search for groups. Click VALIDATE AND SAVE . In the confirmation message box, click SAVE to enable Active Directory integration. After you have successfully integrated Cloud Accelerator Platform with Active Directory, the Active Directory groups that are available in the Organization Unit (OU) specified in Search Base DN appear under the Group Mappings section. Map Active Directory and Cloud Accelerator Platform groups .","title":"Configure the Active Directory server"},{"location":"platform-common/administer/#map-active-directory-and-cloud-accelerator-platform-groups","text":"After you have successfully integrated Cloud Accelerator Platform with Active Directory, the Active Directory groups appear under the Group Mappings section based on your configuration. You must map these Active Directory groups to the appropriate Cloud Accelerator Platform groups. Important: You must map at least one Active Directory group to the ADMIN group in Cloud Accelerator Platform. This Active Directory group must contain users who should be given administrative permissions in Cloud Accelerator Platform. To map Active Directory and Cloud Accelerator Platform groups, perform the following actions: Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click LDAP Configuration . Under Group Mappings , in the Local Group column for an Active Directory group, select the appropriate Cloud Accelerator Platform group. Repeat step 4 for each Active Directory group that is listed in the LDAP Group column. Click SAVE MAPPINGS . Note: You can continue to add and manage groups in the Admin Console even after you have integrated Cloud Accelerator Platform with Active Directory.","title":"Map Active Directory and Cloud Accelerator Platform groups"},{"location":"platform-common/administer/#view-active-directory-users","text":"After the Active Directory integration is completed, you can only view user details in the Admin Console. You must add and manage users in Active Directory. Sign in to Cloud Accelerator Platform as an administrator. On the Home page, click the Accelerator icon ( ) in the top-left corner and select Admin Console . Click the More options icon ( ) in the top-right corner and then click Users . Under Users List , select the user whose details you want to view.","title":"View Active Directory users"},{"location":"platform-common/administer/#migrate-users-from-cloud-accelerator-platform-to-active-directory","text":"If Cloud Accelerator Platform is in use before it is integrated with Active Directory, you must migrate the existing Cloud Accelerator Platform users to Active Directory. The data owned by the Cloud Accelerator Platform users must be available after the Active Directory integration is completed.","title":"Migrate users from Cloud Accelerator Platform to Active Directory"},{"location":"platform-common/administer/#migrate-the-default-administrator-user-to-active-directory","text":"Consider the following points while migrating the default administrator user: The user whose email ID was provided while deploying Cloud Accelerator Platform is considered as the default administrator of Cloud Accelerator Platform, irrespective of authorization groups association. The user who is integrating Cloud Accelerator Platform with Active Directory must ensure that the Active Directory has a user with the email ID of default administrator. After Active Directory integration is completed, the default administrator user must use the Active Directory credentials to sign in to Cloud Accelerator Platform.","title":"Migrate the default administrator user to Active Directory"},{"location":"platform-common/administer/#migrate-existing-users-to-active-directory","text":"Consider the following points while migrating existing Cloud Accelerator Platform users: If the email IDs of existing Cloud Accelerator Platform users are the same as in Active Directory, the Active Directory users automatically get mapped to Cloud Accelerator Platform users. All the data (environments, providers, connections, and so on) associated with the existing Cloud Accelerator Platform users is available to the Active Directory users with the same email IDs. The existing Cloud Accelerator Platform users must use the Active Directory credentials to sign in to Cloud Accelerator Platform. If the email IDs of existing Cloud Accelerator Platform users are not available in Active Directory, the existing Cloud Accelerator Platform users are not able to sign in to Cloud Accelerator Platform.","title":"Migrate existing users to Active Directory"},{"location":"platform-common/cliTool/","text":"Install and use Cloud Accelerator Platform CLI \u00b6 This topic helps you to install, configure, and use the Hitachi Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI), which is a tool that enables you to perform various actions in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Workflow (Workflow Accelerator), and the Admin Console. You can use the Cloud Accelerator Platform CLI to deploy environments, run tests, manage users, and perform many other actions. Contents \u00b6 Supported operating systems Prerequisites Installing the Cloud Accelerator Platform CLI Configuring the Cloud Accelerator Platform CLI Verifying supported versions of the accelerators Platform CLI command reference Supported operating systems \u00b6 The Cloud Accelerator Platform CLI is supported on the following operating systems: Linux macOS Prerequisites \u00b6 Before installing the Cloud Accelerator Platform CLI, you must perform the following actions: Ensure that the computer on which you plan to install the Cloud Accelerator Platform CLI meets the following requirements: Python and PIP 3.5 or later is installed on the computer. Cloud Accelerator Platform Artifactory is accessible from the computer. Create a user for accessing Cloud Accelerator Platform Artifactory and generate the API key from the Cloud Accelerator Platform Artifactory. Note: If you cannot access Cloud Accelerator Platform Artifactory or create your user, contact the Cloud Accelerator Platform Support team . Installing the Cloud Accelerator Platform CLI \u00b6 On the computer on which you want to install the Cloud Accelerator Platform CLI, perform one of the following actions: To install the latest version of the Cloud Accelerator Platform CLI, use the following command. pip3 install reanplatform-cli --index-url https://<Artifactory_user_name>:<Artifactory password or api_key>@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple Example: pip3 install reanplatform-cli --index-url https://m.connor:AVCdaxyzRam79HC3TByNw2vVd5XMKv4vpyZjhAsyhsANC78B0XXXviud@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple To install a specific version of the Cloud Accelerator Platform CLI, run the following command**.** pip3 install reanplatform-cli == <cli_version> --index-url https://<Artifactory_user_name>:<Artifactory password or api_key>@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple Example: pip3 install reanplatform-cli == 2 .28.0 --index-url https://m.connor:AVCdaxyzRam79HC3TByNw2vVd5XMKv4vpyZjhAsyhsANC78B0XXXviud@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple (Optional) To verify the version of the Cloud Accelerator Platform CLI that is installed, run the following command: pip3 list | grep reanplatform-cli Configuring the Cloud Accelerator Platform CLI \u00b6 After installing the Cloud Accelerator Platform CLI, you can configure access to the Cloud Accelerator Platform instance by using one of the following methods: Set environment variables Use an existing configuration file Create a configuration file Note: The precedence order for fetching configuration details is environment variables, existing configuration file, and finally the configuration file that you create. Set environment variables \u00b6 To configure access to a Cloud Accelerator Platform instance by setting environment variables, run the following commands: export REAN_PLATFORM_USER_NAME = \"<username>\" export REAN_PLATFORM_PASSWORD = \"<password>\" export REAN_PLATFORM_BASE_URL = \"<PlatformURL>\" Example export REAN_PLATFORM_USER_NAME = \"m.connor@companyname.com\" export REAN_PLATFORM_PASSWORD = \"Password@1234\" export REAN_PLATFORM_BASE_URL = \"https://rean-platform.reancloud.com\" SSL certificate verification By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can set the following optional environment variables. To configure the certificate path, set the REAN_PLATFORM_SSL_CERTIFICATE_PATH environment variable to the absolute path of the certificate, as shown in the example below. export REAN_PLATFORM_SSL_CERTIFICATE_PATH = \"/Users/mconner/gitRepos/companyname/cli/rootCA.pem\" To skip verifying the SSL certificate, set the REAN_PLATFORM_VERIFY_SSL environment variable to False , as shown below. export REAN_PLATFORM_VERIFY_SSL = False Note: The default value of the REAN_PLATFORM_VERIFY_SSL environment variable is true . Use an existing configuration file \u00b6 To configure access to a Cloud Accelerator Platform instance by using an existing configuration file, perform the following actions: Ensure that your existing configuration file (YAML file) is in the following format: <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> SSL certificate verification By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can add the following optional parameters in the configuration file. To configure the certificate path, set the ssl_certificate_path parameter to the absolute path of the certificate, as shown in the example below. <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> ssl_certificate_path : /Users/mconner/gitRepos/companyname/cli/rootCA To skip verifying the SSL certificate, set the verify_ssl_certificate parameter to false , as shown below. <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> verify_ssl_certificate : false Note: To later enable SSL certificate verification, you can set the verify_ssl_certificate parameter to true . To export the configuration file path through an environment variable, run the following command: export REAN_PLATFORM_CONFIG_FILE_PATH = <Path_to_configuration_yaml_file> Create a configuration file \u00b6 Run one of the following commands: rean-platform configure --platform_base_url <platform_url> --username <platform_username> rean-platform configure -url <platform_url> -u <platform_username> Example rean-platform configure --platform_base_url https://rean-platform.reancloud.com --username m.connor@companyname.com Store credentials in an encrypted format To store the credentials in an encrypted format, use the --encrypt_credentials or -e parameter in the command, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --encrypt_credentials rean-platform configure -url <platform_url> -u <platform_username> -e Verify SSL certificate By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can use the following optional parameters in the command. To configure the certificate path when SSL certificate verification is enabled, use the --ssl_certificate_path parameter and specify the absolute path of the certificate, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --ssl_certificate_path <certificate_absolute_path> To skip verifying the SSL certificate, use the --ignore_ssl_verification or -i parameter, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --ignore_ssl_verification rean-platform configure -url <platform_url> -u <platform_username> -i Enter the password for the user name that you have specified. Verifying supported versions of the accelerators \u00b6 Each release of Cloud Accelerator Platform CLI supports a specific version of Admin Console, Deploy Accelerator, Test Accelerator, and Workflow Accelerator. To find out which accelerator versions are supported by the Cloud Accelerator Platform CLI that you have installed, run the following commands: Admin Console rean-auth --version Deploy Accelerator rean-deploy --version Test Accelerator rean-test --version Workflow Accelerator rean-workflow --version","title":"Install and use Platform CLI"},{"location":"platform-common/cliTool/#install-and-use-cloud-accelerator-platform-cli","text":"This topic helps you to install, configure, and use the Hitachi Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI), which is a tool that enables you to perform various actions in Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator), Hitachi Cloud Accelerator Platform - Test (Test Accelerator), Hitachi Cloud Accelerator Platform - Workflow (Workflow Accelerator), and the Admin Console. You can use the Cloud Accelerator Platform CLI to deploy environments, run tests, manage users, and perform many other actions.","title":"Install and use Cloud Accelerator Platform CLI"},{"location":"platform-common/cliTool/#contents","text":"Supported operating systems Prerequisites Installing the Cloud Accelerator Platform CLI Configuring the Cloud Accelerator Platform CLI Verifying supported versions of the accelerators Platform CLI command reference","title":"Contents"},{"location":"platform-common/cliTool/#supported-operating-systems","text":"The Cloud Accelerator Platform CLI is supported on the following operating systems: Linux macOS","title":"Supported operating systems"},{"location":"platform-common/cliTool/#prerequisites","text":"Before installing the Cloud Accelerator Platform CLI, you must perform the following actions: Ensure that the computer on which you plan to install the Cloud Accelerator Platform CLI meets the following requirements: Python and PIP 3.5 or later is installed on the computer. Cloud Accelerator Platform Artifactory is accessible from the computer. Create a user for accessing Cloud Accelerator Platform Artifactory and generate the API key from the Cloud Accelerator Platform Artifactory. Note: If you cannot access Cloud Accelerator Platform Artifactory or create your user, contact the Cloud Accelerator Platform Support team .","title":"Prerequisites"},{"location":"platform-common/cliTool/#installing-the-cloud-accelerator-platform-cli","text":"On the computer on which you want to install the Cloud Accelerator Platform CLI, perform one of the following actions: To install the latest version of the Cloud Accelerator Platform CLI, use the following command. pip3 install reanplatform-cli --index-url https://<Artifactory_user_name>:<Artifactory password or api_key>@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple Example: pip3 install reanplatform-cli --index-url https://m.connor:AVCdaxyzRam79HC3TByNw2vVd5XMKv4vpyZjhAsyhsANC78B0XXXviud@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple To install a specific version of the Cloud Accelerator Platform CLI, run the following command**.** pip3 install reanplatform-cli == <cli_version> --index-url https://<Artifactory_user_name>:<Artifactory password or api_key>@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple Example: pip3 install reanplatform-cli == 2 .28.0 --index-url https://m.connor:AVCdaxyzRam79HC3TByNw2vVd5XMKv4vpyZjhAsyhsANC78B0XXXviud@artifactory.reancloud.com/artifactory/api/pypi/virtual-pypi/simple (Optional) To verify the version of the Cloud Accelerator Platform CLI that is installed, run the following command: pip3 list | grep reanplatform-cli","title":"Installing the Cloud Accelerator Platform CLI"},{"location":"platform-common/cliTool/#configuring-the-cloud-accelerator-platform-cli","text":"After installing the Cloud Accelerator Platform CLI, you can configure access to the Cloud Accelerator Platform instance by using one of the following methods: Set environment variables Use an existing configuration file Create a configuration file Note: The precedence order for fetching configuration details is environment variables, existing configuration file, and finally the configuration file that you create.","title":"Configuring the Cloud Accelerator Platform CLI"},{"location":"platform-common/cliTool/#set-environment-variables","text":"To configure access to a Cloud Accelerator Platform instance by setting environment variables, run the following commands: export REAN_PLATFORM_USER_NAME = \"<username>\" export REAN_PLATFORM_PASSWORD = \"<password>\" export REAN_PLATFORM_BASE_URL = \"<PlatformURL>\" Example export REAN_PLATFORM_USER_NAME = \"m.connor@companyname.com\" export REAN_PLATFORM_PASSWORD = \"Password@1234\" export REAN_PLATFORM_BASE_URL = \"https://rean-platform.reancloud.com\" SSL certificate verification By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can set the following optional environment variables. To configure the certificate path, set the REAN_PLATFORM_SSL_CERTIFICATE_PATH environment variable to the absolute path of the certificate, as shown in the example below. export REAN_PLATFORM_SSL_CERTIFICATE_PATH = \"/Users/mconner/gitRepos/companyname/cli/rootCA.pem\" To skip verifying the SSL certificate, set the REAN_PLATFORM_VERIFY_SSL environment variable to False , as shown below. export REAN_PLATFORM_VERIFY_SSL = False Note: The default value of the REAN_PLATFORM_VERIFY_SSL environment variable is true .","title":"Set environment variables"},{"location":"platform-common/cliTool/#use-an-existing-configuration-file","text":"To configure access to a Cloud Accelerator Platform instance by using an existing configuration file, perform the following actions: Ensure that your existing configuration file (YAML file) is in the following format: <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> SSL certificate verification By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can add the following optional parameters in the configuration file. To configure the certificate path, set the ssl_certificate_path parameter to the absolute path of the certificate, as shown in the example below. <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> ssl_certificate_path : /Users/mconner/gitRepos/companyname/cli/rootCA To skip verifying the SSL certificate, set the verify_ssl_certificate parameter to false , as shown below. <any-value-specific-to-environment> : base_url : <base_url> username : <username> password : <password> verify_ssl_certificate : false Note: To later enable SSL certificate verification, you can set the verify_ssl_certificate parameter to true . To export the configuration file path through an environment variable, run the following command: export REAN_PLATFORM_CONFIG_FILE_PATH = <Path_to_configuration_yaml_file>","title":"Use an existing configuration file"},{"location":"platform-common/cliTool/#create-a-configuration-file","text":"Run one of the following commands: rean-platform configure --platform_base_url <platform_url> --username <platform_username> rean-platform configure -url <platform_url> -u <platform_username> Example rean-platform configure --platform_base_url https://rean-platform.reancloud.com --username m.connor@companyname.com Store credentials in an encrypted format To store the credentials in an encrypted format, use the --encrypt_credentials or -e parameter in the command, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --encrypt_credentials rean-platform configure -url <platform_url> -u <platform_username> -e Verify SSL certificate By default, SSL certificate verification is enabled for Cloud Accelerator Platform CLI. Based on your requirements, you can use the following optional parameters in the command. To configure the certificate path when SSL certificate verification is enabled, use the --ssl_certificate_path parameter and specify the absolute path of the certificate, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --ssl_certificate_path <certificate_absolute_path> To skip verifying the SSL certificate, use the --ignore_ssl_verification or -i parameter, as shown below. rean-platform configure --platform_base_url <platform_url> --username <platform_username> --ignore_ssl_verification rean-platform configure -url <platform_url> -u <platform_username> -i Enter the password for the user name that you have specified.","title":"Create a configuration file"},{"location":"platform-common/cliTool/#verifying-supported-versions-of-the-accelerators","text":"Each release of Cloud Accelerator Platform CLI supports a specific version of Admin Console, Deploy Accelerator, Test Accelerator, and Workflow Accelerator. To find out which accelerator versions are supported by the Cloud Accelerator Platform CLI that you have installed, run the following commands: Admin Console rean-auth --version Deploy Accelerator rean-deploy --version Test Accelerator rean-test --version Workflow Accelerator rean-workflow --version","title":"Verifying supported versions of the accelerators"},{"location":"platform-common/createAndAccessAccount/","text":"Create and access Cloud Accelerator Platform account \u00b6 This topic helps you to create a Hitachi Cloud Accelerator Platform account. It also describes how to sign in to Cloud Accelerator Platform, reset a forgotten password, change your password, and set user preferences. Contents \u00b6 Creating a Cloud Accelerator Platform account Signing in to Cloud Accelerator Platform Resetting a forgotten password Changing your password Setting user preferences Note: Make sure that you use a browser that Cloud Accelerator Platform supports. For the complete list of browsers, see Supported browsers and resolution . Creating a Cloud Accelerator Platform account \u00b6 In a browser, type the Cloud Accelerator Platform URL. Click Create New Account . Enter your full name, user name, email address, and password. Select I agree to the Terms and Conditions . Click SIGN UP . An email is sent to the email ID that you have specified. Sign in to the email account that you have specified and open the email that you have received from Cloud Accelerator Platform. To verify your email address and complete your account creation, select the login link in the email. Contact your Cloud Accelerator Platform administrator and request for the appropriate permissions to perform various actions in the following accelerators: Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) Hitachi Cloud Accelerator Platform - Test (Test Accelerator) Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) For more information about user permissions, see Managing users . Signing in to Cloud Accelerator Platform \u00b6 After signing in to Cloud Accelerator Platform, you can perform various actions only if your administrator has granted you the appropriate permissions. In a browser, type the Cloud Accelerator Platform URL. Enter your user name or email address and your password. Click SIGN IN . After you are signed in, you can see the Home page of Deploy Accelerator. Note: By default, if you fail to sign in to your account in three consecutive attempts, you are locked out of your account for two hours. However, your administrator might have customized the user account lockout settings . If required, you can request your administrator to unlock your user account . (Optional) To access other accelerators, click the Accelerator icon ( ) in the top-left corner and select one of the following accelerators: Hitachi Cloud Accelerator Platform - Test Hitachi Cloud Accelerator Platform - Assess Hitachi Cloud Accelerator Platform - Migrate !!! Note: You are automatically signed out of Cloud Accelerator Platform if you do not perform any action for 30 minutes or for the session timeout duration that your administrator has configured . If you close the browser tab or window, your working session remains active for the duration that is configured. Resetting a forgotten password \u00b6 In a browser, type the Cloud Accelerator Platform URL. Click Forgot Password . Note: The Forgot Password link is not available if your administrator has integrated Microsoft Active Directory with Cloud Accelerator Platform. You can submit a request for a new password to your Active Directory administrator. In the Forgot Password window, enter your registered email address and click RESET . A reset password email is sent to your registered email address. Sign in to your registered email account and open the email that you have received from Cloud Accelerator Platform. In the email, select the link to reset your password. On the Reset Password page, enter your new password and click RESET PASSWORD . Sign in to Cloud Accelerator Platform with your new password. Changing your password \u00b6 On the Home page, click the User icon ( ). Click your User name . On the Welcome page, click CHANGE PASSWORD . Note: The CHANGE PASSWORD link is not available if your administrator has integrated Active Directory with Cloud Accelerator Platform. You can submit a request to change your password to your Active Directory administrator. In the Change Password window, enter your current password and new password, and then re-enter your new password. Click SUBMIT . (Optional) On the Welcome page, edit your full name and click UPDATE USER . Setting user preferences \u00b6 On the Home page, click the User icon ( ). Click Preferences . In the User Preferences window, on the Deploy tab, set your preferences. Currently, you can set your preferences only for email notifications that are sent when deployments are started or destroyed in Deploy Accelerator. Note: The environment-level settings have the highest precedence, followed by user-level settings, and finally application-level (default) settings. To set your preferences for email notifications, perform the following actions: To receive all email notifications for completion of the deploy/destroy process, in Enable Deploy/Destroy Complete Notifications , select True . To receive email notifications for initiation of the deploy/destroy process, in Enable Deploy/Destroy Initiation Notifications , select True . To not receive email notifications for any of the above notifications, select False . To apply application level settings, select Default . To revert your preferences to the last-saved settings, click DISCARD . To reset your preferences to their default value, perform the following actions: To reset a specific preference, click the Reset link next to that preference and then click SAVE . To reset all your preferences, click RESET ALL PREFERENCES . In the confirmation message box, click YES .","title":"Create & access account"},{"location":"platform-common/createAndAccessAccount/#create-and-access-cloud-accelerator-platform-account","text":"This topic helps you to create a Hitachi Cloud Accelerator Platform account. It also describes how to sign in to Cloud Accelerator Platform, reset a forgotten password, change your password, and set user preferences.","title":"Create and access Cloud Accelerator Platform account"},{"location":"platform-common/createAndAccessAccount/#contents","text":"Creating a Cloud Accelerator Platform account Signing in to Cloud Accelerator Platform Resetting a forgotten password Changing your password Setting user preferences Note: Make sure that you use a browser that Cloud Accelerator Platform supports. For the complete list of browsers, see Supported browsers and resolution .","title":"Contents"},{"location":"platform-common/createAndAccessAccount/#creating-a-cloud-accelerator-platform-account","text":"In a browser, type the Cloud Accelerator Platform URL. Click Create New Account . Enter your full name, user name, email address, and password. Select I agree to the Terms and Conditions . Click SIGN UP . An email is sent to the email ID that you have specified. Sign in to the email account that you have specified and open the email that you have received from Cloud Accelerator Platform. To verify your email address and complete your account creation, select the login link in the email. Contact your Cloud Accelerator Platform administrator and request for the appropriate permissions to perform various actions in the following accelerators: Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) Hitachi Cloud Accelerator Platform - Test (Test Accelerator) Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) For more information about user permissions, see Managing users .","title":"Creating a Cloud Accelerator Platform account"},{"location":"platform-common/createAndAccessAccount/#signing-in-to-cloud-accelerator-platform","text":"After signing in to Cloud Accelerator Platform, you can perform various actions only if your administrator has granted you the appropriate permissions. In a browser, type the Cloud Accelerator Platform URL. Enter your user name or email address and your password. Click SIGN IN . After you are signed in, you can see the Home page of Deploy Accelerator. Note: By default, if you fail to sign in to your account in three consecutive attempts, you are locked out of your account for two hours. However, your administrator might have customized the user account lockout settings . If required, you can request your administrator to unlock your user account . (Optional) To access other accelerators, click the Accelerator icon ( ) in the top-left corner and select one of the following accelerators: Hitachi Cloud Accelerator Platform - Test Hitachi Cloud Accelerator Platform - Assess Hitachi Cloud Accelerator Platform - Migrate !!! Note: You are automatically signed out of Cloud Accelerator Platform if you do not perform any action for 30 minutes or for the session timeout duration that your administrator has configured . If you close the browser tab or window, your working session remains active for the duration that is configured.","title":"Signing in to Cloud Accelerator Platform"},{"location":"platform-common/createAndAccessAccount/#resetting-a-forgotten-password","text":"In a browser, type the Cloud Accelerator Platform URL. Click Forgot Password . Note: The Forgot Password link is not available if your administrator has integrated Microsoft Active Directory with Cloud Accelerator Platform. You can submit a request for a new password to your Active Directory administrator. In the Forgot Password window, enter your registered email address and click RESET . A reset password email is sent to your registered email address. Sign in to your registered email account and open the email that you have received from Cloud Accelerator Platform. In the email, select the link to reset your password. On the Reset Password page, enter your new password and click RESET PASSWORD . Sign in to Cloud Accelerator Platform with your new password.","title":"Resetting a forgotten password"},{"location":"platform-common/createAndAccessAccount/#changing-your-password","text":"On the Home page, click the User icon ( ). Click your User name . On the Welcome page, click CHANGE PASSWORD . Note: The CHANGE PASSWORD link is not available if your administrator has integrated Active Directory with Cloud Accelerator Platform. You can submit a request to change your password to your Active Directory administrator. In the Change Password window, enter your current password and new password, and then re-enter your new password. Click SUBMIT . (Optional) On the Welcome page, edit your full name and click UPDATE USER .","title":"Changing your password"},{"location":"platform-common/createAndAccessAccount/#setting-user-preferences","text":"On the Home page, click the User icon ( ). Click Preferences . In the User Preferences window, on the Deploy tab, set your preferences. Currently, you can set your preferences only for email notifications that are sent when deployments are started or destroyed in Deploy Accelerator. Note: The environment-level settings have the highest precedence, followed by user-level settings, and finally application-level (default) settings. To set your preferences for email notifications, perform the following actions: To receive all email notifications for completion of the deploy/destroy process, in Enable Deploy/Destroy Complete Notifications , select True . To receive email notifications for initiation of the deploy/destroy process, in Enable Deploy/Destroy Initiation Notifications , select True . To not receive email notifications for any of the above notifications, select False . To apply application level settings, select Default . To revert your preferences to the last-saved settings, click DISCARD . To reset your preferences to their default value, perform the following actions: To reset a specific preference, click the Reset link next to that preference and then click SAVE . To reset all your preferences, click RESET ALL PREFERENCES . In the confirmation message box, click YES .","title":"Setting user preferences"},{"location":"platform-common/deployAndConfigure/","text":"Deploy and configure Cloud Accelerator Platform \u00b6 Hitachi Cloud Accelerator Platform can be deployed and configured using bootstrap in both online and offline modes. In the offline mode, Cloud Accelerator Platform does not require Internet connectivity to perform various operations. For more information about deploying and configuring Cloud Accelerator Platform, contact the Cloud Accelerator Platform team.","title":"Deploy and configure"},{"location":"platform-common/deployAndConfigure/#deploy-and-configure-cloud-accelerator-platform","text":"Hitachi Cloud Accelerator Platform can be deployed and configured using bootstrap in both online and offline modes. In the offline mode, Cloud Accelerator Platform does not require Internet connectivity to perform various operations. For more information about deploying and configuring Cloud Accelerator Platform, contact the Cloud Accelerator Platform team.","title":"Deploy and configure Cloud Accelerator Platform"},{"location":"platform-common/faqs/","text":"Frequently Asked Questions (FAQ) \u00b6 Hitachi Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps Principles to deliver Enterprise Workloads in the Cloud. It comprises a set of tools that help you to accelerate the Deploy, Verify, and Manage phases of your journey toward Cloud Adoption. This topic provides answers to frequently asked questions about accelerators. Contents \u00b6 Cloud Accelerator Platform FAQs Admin Console FAQs Deploy Accelerator FAQs Test Accelerator FAQs Assess Accelerator FAQs Migrate Accelerator FAQs Cloud Accelerator Platform FAQs \u00b6 The following table provides answers to frequently asked questions about Cloud Accelerator Platform. Question Answer What access permissions do I get when I create a new Cloud Accelerator Platform account? If you create your own Cloud Accelerator Platform account , the DEFAULT group is automatically assigned to you. This group allows you to only update your profile information. What is the password policy for Cloud Accelerator Platform accounts? Consider the following points while setting the password of a Cloud Accelerator Platform account: -- Password must be between 10 and 50 characters in length. -- Password must be a mix of upper and lower case characters. -- Password must have at least one numeric character. Is a default administrator account created while deploying Cloud Accelerator Platform? When Cloud Accelerator Platform is successfully deployed, a Cloud Accelerator Platform administrator can sign in with the credentials mentioned in customer.yml file at the time of deployment. If required, the default administrator can change this password after signing in for the first time. What browsers does Cloud Accelerator Platform support? Cloud Accelerator Platform supports multiple browsers. To view the list of supported browsers for each accelerator, see Supported browsers and resolution . Admin Console FAQs \u00b6 The Admin Console enables Cloud Accelerator Platform administrators to manage a common set of groups and users for Deploy Accelerator, Test Accelerator, and Assess Accelerator. For more information about the Admin Console, see Overview of Admin Console . The following table provides answers to frequently asked questions about the Admin Console. Question Answer Why am I not able to delete a group? Confirm that the group you are trying to delete does not have any members. You can delete a group only if no users are assigned to that group. Can there be multiple administrators in Cloud Accelerator Platform? When Cloud Accelerator Platform is successfully deployed, a Cloud Accelerator Platform administrator can sign in with the credentials mentioned in customer.yml file at the time of deployment. If required, this administrator can create additional Cloud Accelerator Platform administrators by assigning other users to the appropriate group. Can administrators change the access level for users who have created their own Cloud Accelerator Platform account? Policies defined for a group control the operations that group members can perform in Cloud Accelerator Platform. The Cloud Accelerator Platform administrator can change the DEFAULT group that is assigned to users who have created their own account. For more information about configuring access level for users, see managing groups and managing users . Can I create a group of users who can view each others resources? Yes, you can create a group, add users to this group, and configure share permissions for specific resources. Users within the group can view each other's resources and perform other actions based on the assigned share permissions. However, the users within this group must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to view each other's manual test jobs, users must also be assigned to another policy-based group that gives them the basic permission to view manual test jobs. You can currently configure share permissions for only Test Accelerator resources. For more information, see managing groups . Can I integrate Cloud Accelerator Platform with Microsoft Active Directory for authentication and authorization? Yes, Cloud Accelerator Platform enables you to use your existing Microsoft Active Directory setup to authenticate users in Cloud Accelerator Platform and provide user-group authorization. You can integrate Cloud Accelerator Platform with only one setup of Active Directory. For more information, see Integrating Cloud Accelerator Platform with Active Directory . Does Cloud Accelerator Platform support authentication against more than one Active Directory? No, Cloud Accelerator Platform can be integrated with only one setup of Active Directory. What permission does the Cloud Accelerator Platform service account in Active Directory need? The Cloud Accelerator Platform service account in Active Directory does not require any specific permission. However, ensure that the Cloud Accelerator Platform server's IP address is whitelisted in the Active Directory server's security group. Do I need to set up any specific group structure in Active Directory for Cloud Accelerator Platform? No, there is no specific group structure that you must set up in Active Directory for Cloud Accelerator Platform. The Group Mappings section on the LDAP Configuration page displays groups that are available in the Organization Unit (OU) that is specified in the Search Base DN field. Map at least one Active Directory group to the ADMIN group in Cloud Accelerator Platform. This Active Directory group must contain users who must be given administrative permissions in Cloud Accelerator Platform. Can existing users continue to access Cloud Accelerator Platform after the integration with Active Directory? After the Active Directory integration is completed, existing users and administrators cannot sign in to Cloud Accelerator Platform with their original credentials. Cloud Accelerator Platform now uses Active Directory to authenticate users. Existing users can sign in to Cloud Accelerator Platform only if they are a member of integrated Active Directory. These users need to sign in with their Active Directory email ID and password. The users can view their existing data in Cloud Accelerator Platform only if their Active Directory email ID is same as the email ID they used to log in to the Platform before the integration with Active Directory. This lets the Platform identify the data (environments, deployments, etc.) ownership correctly. Why are administrators not able to log in to Cloud Accelerator Platform after the integration with Active Directory? After the Active Directory integration is completed, Cloud Accelerator Platform uses Active Directory to authenticate users. Therefore, existing administrators are not able to sign in to Cloud Accelerator Platform. While configuring the Active Directory integration, at least one Active Directory group must be mapped to the ADMIN group in Cloud Accelerator Platform. This ensures that users who are members of this Active Directory group can access the Admin Console. Is it possible to manage users in Cloud Accelerator Platform after the integration with Active Directory? No, it is not possible to add or manage users in Cloud Accelerator Platform after the integration with Active Directory. You can only view user details in the Admin Console. Can I view a list of Active Directory users in Cloud Accelerator Platform? After the Active Directory integration is completed, you can only view user details in the Admin Console but you must create and manage users in Active Directory. How can I add new users in Cloud Accelerator Platform after the integration with Active Directory? After Cloud Accelerator Platform is integrated with Active Directory, an Administrator user in the integrated Active Directory can add new users in Active Directory. Once the new users are created in Active Directory, these users can log in to Cloud Accelerator Platform with their Active Directory credentials. Is it possible to create new groups in Cloud Accelerator Platform after the integration with Active Directory? Yes, you can create and manage groups in the Admin Console after you have integrated Cloud Accelerator Platform with Active Directory. You must also map the appropriate Active Directory group to any new group that you create in the Admin Console. How can I map Active Directory users to Cloud Accelerator Platform groups? After the Active Directory integration is completed, the Group Mappings section displays the Active Directory groups based on your configuration. You must map these Active Directory groups to the appropriate Cloud Accelerator Platform groups. For more information see Map Active Directory and Cloud Accelerator Platform groups . Deploy Accelerator FAQs \u00b6 Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. It supports many cloud providers out-of-the-box, including Amazon Web Services (AWS) and Microsoft Azure. Deploy Accelerator also leverages major infrastructure automation tools such as Chef (Chef Server and Chef Solo) and Puppet to configure resources. For information about using Deploy Accelerator, see Deploy and manage environments . The following sections provide answers to frequently asked questions about Deploy Accelerator. Providers Environments Deployments Resources Packages Providers (Deploy Accelerator) \u00b6 Question Answer Can I create multiple accounts in Deploy Accelerator for the same cloud provider? Yes, you can create multiple accounts in Deploy Accelerator for the same cloud provider. In Deploy Accelerator, providers enable you to specify the cloud provider, the account in which to deploy the infrastructure, and credentials to access the account. For example, you can configure a provider for each AWS account and specify different account credentials in each provider. Can I create accounts in Deploy Accelerator across multiple cloud providers? Yes, you can create accounts in Deploy Accelerator across multiple cloud providers. In Deploy Accelerator, providers enable you to specify the cloud provider, the account in which to deploy the infrastructure, and credentials to access the account. You can configure providers in Deploy Accelerator for AWS, Google Cloud Platform, Microsoft Azure, and many other cloud providers. Can I configure AWS as a provider without storing the access key and secret key in Deploy Accelerator? While configuring an AWS provider , you can use the Instance Profile or Assume Role method for a more secure way of accessing the account that Deploy Accelerator must use to deploy an environment. Can I share my provider with other users in Deploy Accelerator? You can share your provider with one or more groups of users in Deploy Accelerator. You can also assign appropriate permissions to each group such as view, edit, share, and delete. All users in that group can use the shared provider for deploying an environment. Environments (Deploy Accelerator) \u00b6 Question Answer Can I use Deploy Accelerator to deploy an application infrastructure that spans multiple cloud providers? Yes, Deploy Accelerator enables you to deploy an application infrastructure that spans multiple cloud providers. However, each environment in Deploy Accelerator is specific to a single cloud provider. To create an application infrastructure that spans multiple cloud providers, you must create a separate environment for each cloud provider and then create dependencies between these environments. For example, you can create an environment for the LDAP infrastructure in a private cloud and create another environment for the web application infrastructure in a public cloud. You can then create a dependency between these two environments. For more information, see Overview of layered environments . Can I reference environment details from another environment? You can use the Depends On and Output resources to refer to values from another environment. For an example, see Create a layered environment . Can I choose not to display output values in the resource logs of an environment? To prevent an output value from appearing in the resource logs of an environment, add the sensitive parameter for that output in the Output resource, as shown in the example below: Why am I not able to create an environment? When you create a Cloud Accelerator Platform account , the default group assigned to you does not have the access to create an environment. Contact your Cloud Accelerator Platform administrator and confirm that you have been added to the appropriate group. Can I collaborate with other users on the same environment? You can collaborate with other users on an environment by sharing that environment with one or more groups. You can also assign each group appropriate permissions such as view, create, edit, deploy, destroy, delete, import and export. All users in that group can work on the shared environment at the same time. Why does the blueprint of a layered environment that I have exported not contain all the environments? When you export an environment as a blueprint , only the environments on which it is dependent are included. Therefore, while exporting a layered environment as a blueprint, ensure that you are exporting the environment that is in the lowest layer. This action ensures that the environments in all other layers are also included in the blueprint. Can I find out if an environment has a dependency on other environments? You can use the Environment Dependency View to view the dependency of an environment on other environments. You can also switch to another environment in the Environment Dependency View window by clicking that environment name. Can I create a new environment version by importing a blueprint? While creating new environments from blueprints , you have to enter a unique combination of name and version for the environment that you are importing. If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Why am I not able to delete some environment versions? You might not be able to delete a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether the environment version that you are trying to delete has any deployments. You cannot delete environment versions that are already deployed. Can I easily identify the differences between two environments? You can compare differences between two environments or two versions of the same environment . This comparison detects environment configuration-level and resource-level differences between the base and target environments. Deployments (Deploy Accelerator) \u00b6 Question Answer Why am I not able to deploy an environment? You might not be able to deploy a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether the environment that you want to deploy is dependent on other environments. You can deploy child deployments only if their parent deployments are available Can I test an environment version before deploying it in production? Each environment version can have multiple deployments. To test an environment version, you can start a new deployment . Based on the test results, you can update the resources and packages in this environment version and redeploy the existing deployment . When you are ready to deploy the environment in production, you can start another new deployment. While starting each new deployment , you can specify a different connection, provider, and input variables. For example, you can specify different VPC IDs for your Staging and Production deployments. Can I view all deployments of an environment? If an environment has multiple deployments, you can view the list of all your deployments and other deployments that are shared with you, along with their status. The deployments drop-down on the canvas categorizes deployments based on environment versions. By default, the most recent deployment from the list for the selected environment version appears. How can I upgrade an existing deployment of an environment? To upgrade an existing deployment, you can redeploy the existing deployment with either the same or a different environment version. In this case, only the changes made in the selected environment version are provisioned. Deployed resources that have not been updated in the selected environment version are not impacted. You can plan and redeploy an existing deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. Before redeploying an existing deployment of an environment, can I preview the changes that will be made to the existing deployment? Before you redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed when you redeploy the deployment. You can plan and redeploy an existing deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. Can I upgrade a parent deployment without impacting its child deployments? You can upgrade a parent deployment by redeploying that deployment with a new or updated environment version. This action impacts its child deployments only if they reference any updated resource values in the parent deployment. Can I upgrade a child deployment without impacting its parent deployments? You can upgrade a child deployment without impacting any of its parent deployments. Why am I not able to deploy an environment that uses the S3 reference type in the Depends On resource? You might not be able to deploy an environment that uses the S3 reference type in the Depends On resource because of the following reasons: -- The provider that you have selected to deploy the environment does not have access to the S3 bucket that you have specified. -- The URL that you have specified does not use the s3://bucketname/tfstatefilename format, in which bucketname is the name of the S3 bucket and tfstatefilename is the Terraform remote state file of the environment that is to be referred. -- If you have defined variables for the S3 bucket and Terraform state file names in the Input Variables resource, you have not used the following interpolation syntax to reference these variables: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } Why am I not able to destroy a deployment of an environment? You might not be able to destroy a deployment of a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether deployments in other environments have a dependency on the deployment that you are trying to destroy. You must destroy the child (or dependent) deployments before you can destroy the parent deployment. Also, you can destroy a deployment that is shared with you only if the deployment owner has given you the Destroy permission for that deployment. Can I identify deployments that are shared with me? Deployments that other users have shared with you appear in a different color in the deployments list on the canvas. Why are users not able to see the deployment that I have shared with their group? When you share a a deployment of an environment version with selected groups, ensure that the environment version is also shared with the same groups. Otherwise, users cannot view the shared deployments. Why am I not able to redeploy a deployment that is shared with me? You can redeploy a deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. In addition, the deployment owner must also share with you the provider that is configured in the shared deployment. Why am I not able to plan a deployment that is shared with me? You can plan a deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. The Redeploy permission of a deployment also includes the permission to plan that deployment. Can I share my deployment only if its status is Deployed? You can share any of your deployments with other groups, irrespective of the deployment state (Deploying, Deployed, Failed, or Stop state). Why do I get an error message that the deployment name that I have specified is not unique even though I cannot see that deployment name in the list on the canvas? The name that you specify for a deployment must be unique across all versions of the environment. If another user already has a deployment with the name that you have specified, you get an error message. However, you can see that deployment name in the list on the canvas only if the deployment and its associated environment version is shared with you. Why are parent deployments not visible when I try to redeploy the child deployment that is shared with me? While redeploying a child deployment that is shared with you, parent deployments are visible in the Parent Deployment Mapping section only if the deployment owner has also shared the parent deployments with you. If the parent deployments are not visible in the Review and Re-Deploy window, you cannot redeploy the child deployment that is shared with you. When I share some environments, why are all its deployments also automatically shared with the same groups with which the environment is shared? If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the same groups with which the environments are shared. The permissions given for the deployments are based on the permissions assigned to the environments. Based on your requirements, you can manually update the sharing permissions for these deployments. Resources (Deploy Accelerator) \u00b6 Question Answer Can I define variables that are available for all resources in an environment? You can use the Input Variable resource to define variables that can be used in all other resources in the same environment. For an example, see Configure variables in your environment . Can I rename a resource that has been added to an environment? You can rename a resource that has been added to an environment. The resource name is automatically updated in other resources within the environment that reference it by using the interpolation syntax or the Depends On attribute. Can I create a copy of an existing resource? You can copy an existing resource from either the same environment or a different environment. The attribute values of the new resource are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The packages that were added to the original resource are also copied to the new resource. Can I view the dependencies between various resources in an environment? You can view the dependencies that are created when resources use the Depends On attribute or the interpolation syntax to reference other resources in an environment. To view these dependencies, click the Links icon ( ) on the Deploy Accelerator canvas. Can I create multiple resources based on the same configuration? You can use the Count attribute of a resource to specify the number of identical resources that you want Deploy Accelerator to create while deploying the environment. Can I control access to the AWS resources that are created when I deploy an environment? To control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can configure a custom tag , which contains a set of AWS key-pair tags, and select that custom tag while deploying an environment . Can I provision packages on servers that are not provisioned by using Deploy Accelerator? You can use the Existing VM resource to represent existing servers in an environment. Based on your requirements, you can add packages to this resource. Why am I not able to add resources to an environment version? You might not be able to add resources to a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether you are trying to add resources to a released environment version. No updates can be made to a released version. You must create a new environment version and then add resources to the new version. Packages (Deploy Accelerator) \u00b6 Question Answer Can I add user-defined packages in Deploy Accelerator? Deploy Accelerator enables you to add your own custom (or user-defined) Chef Solo and Puppet packages. For more information, see Adding user-defined packages . Why is the user-defined package that I have uploaded not visible? You must refresh the Deploy Accelerator web page to see the new package on the Packages tab in the left panel of the Home page. Can I run custom scripts on resources in an environment? You can run custom scripts on resources by using the execute-script package in Deploy Accelerator. Can administrators configure the environment package types that appear while creating an environment? Deploy Accelerator provides out-of-the-box support for Chef Solo, Chef Server, and Puppet. However, only Chef Solo and Chef Server are configured as the default environment package types. Based on requirements, administrators can choose to configure only Chef Solo, only Chef Server, only Puppet, or a combination of these three as the environment package types. For more information, see Configuring the environment package types . Can administrators control which users have access to Chef Server packages and roles? By default, Chef Server packages (or cookbooks) and roles are not shared with any users in Deploy Accelerator. Administrators must configure the groups that can access each package and role. For more information, see Sharing Chef Server packages and roles . Are new packages and roles added in the registered Chef Server automatically available in Deploy Accelerator? Deploy Accelerator polls the registered Chef Server every 30 minutes for new and updated packages (or cookbooks) and roles. However, to view the latest packages and roles at any given time, Administrators can manually update the Chef Server in Deploy Accelerator. Are groups from the registered Chef Server organization automatically available in Deploy Accelerator? When Administrators register a Chef Server in Deploy Accelerator in Deploy Accelerator , groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. However, administrators must ensure that the clients group in the Chef Server organization is a member of all other groups in that organization. The naming convention used for these groups is organizationName - groupName . Cloud Accelerator Platform administrators must modify these groups to add the appropriate users and then Deploy Accelerator administrators can share Chef Server packages with these groups . Are new groups added in a registered Chef Server automatically available in Deploy Accelerator? Deploy Accelerator administrators can view new groups that are added in a Chef Server organization only after manually updating that Chef Server in Deploy Accelerator. Test Accelerator FAQs \u00b6 Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, and scale (load) testing. You can also integrate your existing automation code with Test Accelerator and run automated tests on your web applications. For information about using Test Accelerator, see Run tests . The following sections provide answers to frequently asked questions about Test Accelerator. Configuration Tests Test results Configuration (Test Accelerator) \u00b6 Question Answer Can I use Test Accelerator for a web application that is running inside an intranet? Yes, you can use Test Accelerator to test a web application that is running inside an intranet. In this case, the IP or host (primarily a web server) on which the web application is running is not exposed as a public IP. To use Test Accelerator to test a web application that is running inside an intranet, you can use one of the following approaches: -- Use Proxy: You can set up a proxy for the internal web server with a publicly-exposed IP. For information about setting up a forward proxy in case of an Apache web server, see the Apache documentation . For information about setting up a proxy pass in NGINX, see the NGINX documentation . To set up a proxy for any other web server that you might be using for the public IP, refer to the documentation for that web server. -- Use ngrok: You can use ngrok to expose the web server running on a local machine to the Internet. For more information, see the ngrok documentation . You can use the ngrok-mapped Internet URL while running the URL, manual, automated, or scale tests in Test Accelerator. It is recommended that you finalize the approach only after discussing both these approaches with your internal IT team. Tests (Test Accelerator) \u00b6 Question Answer What type of tests can I perform on web applications by using Test Accelerator? Test Accelerator enables you to perform the following different types of tests on your web applications: -- URL test : This test can instantly check the availability of a web application on different browsers. -- Manual test : This test quickly provisions Kubernetes pods with the selected browsers and reduces the effort to create test setups. You can concentrate on testing the actual use cases. -- Security test : This test performs a security check of your web application by running different types of security packs on the application. -- API test : This test performs a test of the exposed APIs for your application. -- Automated (Cross-browser) test : This test automatically provisions browsers of your choice on Kubernetes pods and then runs your existing automation test suites in the selected browsers. -- Scale test : This test performs the scale testing of your web application across a range of browsers. It enables you to configure multiple browsers, the browser count to test the load, the automation test suite, and the run time in hours. Can I use Test Accelerator to test my infrastructure? The infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, and routers. Test Accelerator supports the following specification types: -- InSpec and Serverspec enable you to check if your servers are configured correctly and adhere to your policy requirements. -- Awspec allows you to check if AWS resources have been deployed with the appropriate attribute values. -- Azurespec allows you to check if Azure resources have been deployed with the appropriate attribute values. For more information, see Running infrastructure tests . Test Accelerator also allows you to use the Default Azure Spec code and Default AWS Spec code to test your Azure and AWS resources. The default code, which is provided out-of-the-box by Test Accelerator, saves time and resources spent on writing your own custom spec code. For more information, see Running codeless infrastructure tests . What is the difference between Apache JMeter and ScaleNow? The major difference between JMeter and ScaleNow is that JMeter uses the stimulated environment for load testing to run the test plans whereas ScaleNow creates the Load testing environment with actual machines to run the test suites. Test results (Test Accelerator) \u00b6 Question Answer Can I view reports for the tests that I have run? On the Home page of Test Accelerator, you can view reports for the tests that you have run. For more information, see Viewing and downloading test results . Can I download test reports? You can download the HTML report of a test run either for a single browser or for all selected browsers. You can also download a summary report of the test run for all browsers in the Excel or CSV format. For more information, see Viewing and downloading test results . Note: The infrastructure test results can be downloaded in the Excel format only. Assess Accelerator FAQs \u00b6 Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) is a tool that automates the assessment of your cloud infrastructure against relevant security and compliance standards. It contains several out-of-the-box policies that you can run to assess the security and compliance level of your cloud infrastructure. For information about using Assess Accelerator, see Run assessment policies . The following sections provide answers to frequently asked questions about Assess Accelerator. Configuration Assessment jobs Reports Configuration (Assess Accelerator) \u00b6 Question Answer How can I access Assess Accelerator? To access Assess Accelerator, sign in to Cloud Accelerator Platform and then click the icon in the top-left corner and select Hitachi Cloud Accelerator - Assess . For information about logging on to Cloud Accelerator Platform, see Create & access account . Can I retain the Docker containers that are created for each assessment job? After an assessment job has completed, Assess Accelerator automatically deletes the Docker container that was created for the job. However, if required, you can retain the Docker containers that are created for assessment jobs by performing the following actions: 1) Use SSH to connect to the instance on which Cloud Accelerator Platform is deployed. 2) Navigate to the {PLATFORM_HOME}/rean-deploy/conf folder. 3) Create the assessnow.properties file and add the terminate.container.on.completion property in the file. 4) Set the value of the terminate.container.on.completion property to false , as shown below: terminate.container.on.completion=false To once again start deleting the Docker containers, update the value of the assess_now property to true . 5) Save the assessnow.properties file. 6) Stop and restart Cloud Accelerator Platform. Assessment jobs (Assess Accelerator) \u00b6 Question Answer Can I specify the same job name multiple times? You can specify the same job name multiple times. However, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. I have run multiple jobs for the same provider. How can I know which is the latest job that I have run? On the My Assessment tab, jobs are listed in the descending order, with the last-run job at the top of the list. Also, if you move your mouse over the status icons, you can see the date and time when a job was started. Can I create an assessment job for multiple providers? Yes, you can select multiple providers in the same assessment job. Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . On the My Assessment tab, each job is listed separately. The Provider column displays the provider name, the Job Name column displays the job name for that provider, and the Group column displays the assessment job name. Do I get a notification email after my assessment job completes? No, you receive an email notification only when your assessment job starts. In addition, you are sent this notification email only if you have selected the Enable Notification check box and provided your email address while creating the assessment job. Reports (Assess Accelerator) \u00b6 Question Answer Can I specify the location for downloading assessment reports? By default, Assess Accelerator downloads the reports to your local computer. While creating the assessment job, you can also specify an AWS S3 bucket to which Assess Accelerator can upload the assessment reports (docx format) that are generated. Why are reports not being uploaded to the S3 bucket specified in the assessment job? For each selected provider in the assessment job, Assess Accelerator appends the specified S3 bucket name with the account ID ( BucketName-AccountID ). If this bucket does not exist, Assess Accelerator creates a new bucket. However, Assess Accelerator requires the s3:PutObject permission to upload assessment reports to an S3 bucket and the s3:CreateObject permission to create a new S3 bucket. Ensure that these permissions have been added to the IAM policies that are used in each selected provider. Which template should I select while downloading an assessment report (docx format)? What is the difference between the templates? To download a detailed report, select the Assess_Template option. To download a summary report, select the Assess_Template_Without_Output option. The summary report lists the assessment criteria, their status (failed or passed) and severity level (CRITICAL, MAJOR, or WARNING), and possible solutions for resolving the identified issues. In addition to the information provided in the summary report, the detailed report includes an appendix that lists the actual instances in your infrastructure that have not met the requirements for each criterion. To download reports without the Current State and Proposed State sections, select the Custom_Assess_Template or Custom_Assess_Template_Without_Output option. Why does an assessment report display no data for a policy? The IAM user or role specified in the selected provider must have the permissions to run a policy on the infrastructure being assessed. Otherwise, the report does not contain any data. For more information about the permissions required for an AWS provider, see Before you begin . How can I interpret the data in the sunburst chart? The sunburst chart provides a graphical summary of the policies that were run for the selected job, along with their status. This chart consists of the following layers: -- The center layer provides a summary of the assessment job. You can see the total number of checks that were run and the number of checks that failed. -- The next layer represents the policies that were run for the selected job. For example, security policy, cost optimization policy, and operations policy. -- The third layer represents different categories of assessment checks within each policy. For example, Security Assessment for Monitoring Compliance is a category of assessment check within the security policy. -- The outermost layer represents each check that is included in a policy. For example, \"Ensure no S3 buckets are public\" is a specific check within the security policy. Click any layer to drill down into a specific set of data. Click the center layer to get back to the previous view. Can I compare the assessment results across multiple jobs or providers? Yes, you can view and download the comparison report to view assessment results across multiple jobs or providers. For more information, see Viewing comparison reports . Why am I not able to view the comparison report on the Assess Accelerator UI? You can see the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If have selected more than 6 jobs, you can download the comparison report. Can I download assessment reports for multiple jobs or providers at the same time? Yes, you can download assessment reports (docx format) for multiple jobs or providers at the same time. For more information, see Downloading comparison reports . Migrate Accelerator FAQs \u00b6 Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. It brings in data from Discovery tools, enables you to pick and choose the servers for each application group, uses a migration tool for server replication, and then creates server images. For information about using Migrate Accelerator, see Discover and migrate resources . The following sections provide answers to frequently asked questions about Migrate Accelerator. General Configuration Server migration General (Migrate Accelerator) \u00b6 Question Answer What discovery and migration tools does Migrate Accelerator support? Migrate Accelerator currently supports RISC Networks CloudScape as a discovery tool and CloudEndure as a migration tool. If required, Migrate Accelerator can also import a list of discovered servers from a CSV file. Does Migrate Accelerator support the migration of servers to AWS only? For now, Migrate Accelerator supports migration of servers to Amazon Web Services (AWS) only. However, if you want to migrate servers to other cloud providers, such as Microsoft Azure, contact the Cloud Accelerator Platform team. Configuration (Migrate Accelerator) \u00b6 Question Answer Can I deploy Migrate Accelerator in AWS instead of the data center from which I want to migrate servers? You can deploy Migrate Accelerator in an AWS account but you must ensure that Migrate Accelerator can communicate with servers in the data center. What are migration credentials? Migrate Accelerator supports CloudEndure as a migration tool. To run a migration job, Migrate Accelerator requires access to the migration tool that you have configured. While configuring migration credentials, you must specify credentials of the migration tool account that Migrate Accelerator must use to migrate application groups. Why do I need to create a connection? If you select Auto install agents while creating a migration job, Migrate Accelerator automates installation of the migration tool agent on servers that are running in your data center. To install these agents, Migrate Accelerator requires SSH connection details for Linux servers and WinRM connection details for Windows servers. Why do I need to create a provider? Providers enable you to specify the cloud provider and credentials for accessing the account in which Migrate Accelerator must create images of the servers you are migrating. When you start a migration job for a group of servers in the data center, CloudEndure creates test instances in AWS and replicates data from these servers on the test instances. After the data replication has completed successfully, Migrate Accelerator uses the provider that is configured in the migration job to create images of the test instances. Note: Migrate Accelerator can create images only in the same AWS account in which CloudEndure launches test instances. Therefore, the provider that you select in a migration job must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project. Server migration (Migrate Accelerator) \u00b6 Question Answer How can I verify that all data on the migrated server has been successfully replicated? On the Server List page, the status of a server changes to Ready for Test after the data is successfully migrated. Migrate Accelerator enables you to launch a test instance and verify that the data has been correctly replicated on the server. On the Server List page, from the Action column for the server whose data you want to verify, click Ready for Test . You can view the instance launch status in the Migration Tool Status column. Does Migrate Accelerator maintain logs of the auto-agent installation process? Migrate Accelerator maintains a log of the auto-agent installation process for each server that is migrated. On the Server List page, from the Action column for a server, click Logs . Can I view a list of all discovery and migration jobs in Migrate Accelerator? To view a list of all discovery jobs, click the Pick Discovery Job box at the top. To view a list of all migration jobs in Migrate Accelerator, click the icon in the top-right corner and then click Migration Jobs .","title":"Frequently Asked Questions"},{"location":"platform-common/faqs/#frequently-asked-questions-faq","text":"Hitachi Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps Principles to deliver Enterprise Workloads in the Cloud. It comprises a set of tools that help you to accelerate the Deploy, Verify, and Manage phases of your journey toward Cloud Adoption. This topic provides answers to frequently asked questions about accelerators.","title":"Frequently Asked Questions (FAQ)"},{"location":"platform-common/faqs/#contents","text":"Cloud Accelerator Platform FAQs Admin Console FAQs Deploy Accelerator FAQs Test Accelerator FAQs Assess Accelerator FAQs Migrate Accelerator FAQs","title":"Contents"},{"location":"platform-common/faqs/#cloud-accelerator-platform-faqs","text":"The following table provides answers to frequently asked questions about Cloud Accelerator Platform. Question Answer What access permissions do I get when I create a new Cloud Accelerator Platform account? If you create your own Cloud Accelerator Platform account , the DEFAULT group is automatically assigned to you. This group allows you to only update your profile information. What is the password policy for Cloud Accelerator Platform accounts? Consider the following points while setting the password of a Cloud Accelerator Platform account: -- Password must be between 10 and 50 characters in length. -- Password must be a mix of upper and lower case characters. -- Password must have at least one numeric character. Is a default administrator account created while deploying Cloud Accelerator Platform? When Cloud Accelerator Platform is successfully deployed, a Cloud Accelerator Platform administrator can sign in with the credentials mentioned in customer.yml file at the time of deployment. If required, the default administrator can change this password after signing in for the first time. What browsers does Cloud Accelerator Platform support? Cloud Accelerator Platform supports multiple browsers. To view the list of supported browsers for each accelerator, see Supported browsers and resolution .","title":"Cloud Accelerator Platform FAQs"},{"location":"platform-common/faqs/#admin-console-faqs","text":"The Admin Console enables Cloud Accelerator Platform administrators to manage a common set of groups and users for Deploy Accelerator, Test Accelerator, and Assess Accelerator. For more information about the Admin Console, see Overview of Admin Console . The following table provides answers to frequently asked questions about the Admin Console. Question Answer Why am I not able to delete a group? Confirm that the group you are trying to delete does not have any members. You can delete a group only if no users are assigned to that group. Can there be multiple administrators in Cloud Accelerator Platform? When Cloud Accelerator Platform is successfully deployed, a Cloud Accelerator Platform administrator can sign in with the credentials mentioned in customer.yml file at the time of deployment. If required, this administrator can create additional Cloud Accelerator Platform administrators by assigning other users to the appropriate group. Can administrators change the access level for users who have created their own Cloud Accelerator Platform account? Policies defined for a group control the operations that group members can perform in Cloud Accelerator Platform. The Cloud Accelerator Platform administrator can change the DEFAULT group that is assigned to users who have created their own account. For more information about configuring access level for users, see managing groups and managing users . Can I create a group of users who can view each others resources? Yes, you can create a group, add users to this group, and configure share permissions for specific resources. Users within the group can view each other's resources and perform other actions based on the assigned share permissions. However, the users within this group must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to view each other's manual test jobs, users must also be assigned to another policy-based group that gives them the basic permission to view manual test jobs. You can currently configure share permissions for only Test Accelerator resources. For more information, see managing groups . Can I integrate Cloud Accelerator Platform with Microsoft Active Directory for authentication and authorization? Yes, Cloud Accelerator Platform enables you to use your existing Microsoft Active Directory setup to authenticate users in Cloud Accelerator Platform and provide user-group authorization. You can integrate Cloud Accelerator Platform with only one setup of Active Directory. For more information, see Integrating Cloud Accelerator Platform with Active Directory . Does Cloud Accelerator Platform support authentication against more than one Active Directory? No, Cloud Accelerator Platform can be integrated with only one setup of Active Directory. What permission does the Cloud Accelerator Platform service account in Active Directory need? The Cloud Accelerator Platform service account in Active Directory does not require any specific permission. However, ensure that the Cloud Accelerator Platform server's IP address is whitelisted in the Active Directory server's security group. Do I need to set up any specific group structure in Active Directory for Cloud Accelerator Platform? No, there is no specific group structure that you must set up in Active Directory for Cloud Accelerator Platform. The Group Mappings section on the LDAP Configuration page displays groups that are available in the Organization Unit (OU) that is specified in the Search Base DN field. Map at least one Active Directory group to the ADMIN group in Cloud Accelerator Platform. This Active Directory group must contain users who must be given administrative permissions in Cloud Accelerator Platform. Can existing users continue to access Cloud Accelerator Platform after the integration with Active Directory? After the Active Directory integration is completed, existing users and administrators cannot sign in to Cloud Accelerator Platform with their original credentials. Cloud Accelerator Platform now uses Active Directory to authenticate users. Existing users can sign in to Cloud Accelerator Platform only if they are a member of integrated Active Directory. These users need to sign in with their Active Directory email ID and password. The users can view their existing data in Cloud Accelerator Platform only if their Active Directory email ID is same as the email ID they used to log in to the Platform before the integration with Active Directory. This lets the Platform identify the data (environments, deployments, etc.) ownership correctly. Why are administrators not able to log in to Cloud Accelerator Platform after the integration with Active Directory? After the Active Directory integration is completed, Cloud Accelerator Platform uses Active Directory to authenticate users. Therefore, existing administrators are not able to sign in to Cloud Accelerator Platform. While configuring the Active Directory integration, at least one Active Directory group must be mapped to the ADMIN group in Cloud Accelerator Platform. This ensures that users who are members of this Active Directory group can access the Admin Console. Is it possible to manage users in Cloud Accelerator Platform after the integration with Active Directory? No, it is not possible to add or manage users in Cloud Accelerator Platform after the integration with Active Directory. You can only view user details in the Admin Console. Can I view a list of Active Directory users in Cloud Accelerator Platform? After the Active Directory integration is completed, you can only view user details in the Admin Console but you must create and manage users in Active Directory. How can I add new users in Cloud Accelerator Platform after the integration with Active Directory? After Cloud Accelerator Platform is integrated with Active Directory, an Administrator user in the integrated Active Directory can add new users in Active Directory. Once the new users are created in Active Directory, these users can log in to Cloud Accelerator Platform with their Active Directory credentials. Is it possible to create new groups in Cloud Accelerator Platform after the integration with Active Directory? Yes, you can create and manage groups in the Admin Console after you have integrated Cloud Accelerator Platform with Active Directory. You must also map the appropriate Active Directory group to any new group that you create in the Admin Console. How can I map Active Directory users to Cloud Accelerator Platform groups? After the Active Directory integration is completed, the Group Mappings section displays the Active Directory groups based on your configuration. You must map these Active Directory groups to the appropriate Cloud Accelerator Platform groups. For more information see Map Active Directory and Cloud Accelerator Platform groups .","title":"Admin Console FAQs"},{"location":"platform-common/faqs/#deploy-accelerator-faqs","text":"Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) is a deployment automation platform that is used to deploy and configure environments reliably and consistently. It supports many cloud providers out-of-the-box, including Amazon Web Services (AWS) and Microsoft Azure. Deploy Accelerator also leverages major infrastructure automation tools such as Chef (Chef Server and Chef Solo) and Puppet to configure resources. For information about using Deploy Accelerator, see Deploy and manage environments . The following sections provide answers to frequently asked questions about Deploy Accelerator. Providers Environments Deployments Resources Packages","title":"Deploy Accelerator FAQs"},{"location":"platform-common/faqs/#providers-deploy-accelerator","text":"Question Answer Can I create multiple accounts in Deploy Accelerator for the same cloud provider? Yes, you can create multiple accounts in Deploy Accelerator for the same cloud provider. In Deploy Accelerator, providers enable you to specify the cloud provider, the account in which to deploy the infrastructure, and credentials to access the account. For example, you can configure a provider for each AWS account and specify different account credentials in each provider. Can I create accounts in Deploy Accelerator across multiple cloud providers? Yes, you can create accounts in Deploy Accelerator across multiple cloud providers. In Deploy Accelerator, providers enable you to specify the cloud provider, the account in which to deploy the infrastructure, and credentials to access the account. You can configure providers in Deploy Accelerator for AWS, Google Cloud Platform, Microsoft Azure, and many other cloud providers. Can I configure AWS as a provider without storing the access key and secret key in Deploy Accelerator? While configuring an AWS provider , you can use the Instance Profile or Assume Role method for a more secure way of accessing the account that Deploy Accelerator must use to deploy an environment. Can I share my provider with other users in Deploy Accelerator? You can share your provider with one or more groups of users in Deploy Accelerator. You can also assign appropriate permissions to each group such as view, edit, share, and delete. All users in that group can use the shared provider for deploying an environment.","title":"Providers (Deploy Accelerator)"},{"location":"platform-common/faqs/#environments-deploy-accelerator","text":"Question Answer Can I use Deploy Accelerator to deploy an application infrastructure that spans multiple cloud providers? Yes, Deploy Accelerator enables you to deploy an application infrastructure that spans multiple cloud providers. However, each environment in Deploy Accelerator is specific to a single cloud provider. To create an application infrastructure that spans multiple cloud providers, you must create a separate environment for each cloud provider and then create dependencies between these environments. For example, you can create an environment for the LDAP infrastructure in a private cloud and create another environment for the web application infrastructure in a public cloud. You can then create a dependency between these two environments. For more information, see Overview of layered environments . Can I reference environment details from another environment? You can use the Depends On and Output resources to refer to values from another environment. For an example, see Create a layered environment . Can I choose not to display output values in the resource logs of an environment? To prevent an output value from appearing in the resource logs of an environment, add the sensitive parameter for that output in the Output resource, as shown in the example below: Why am I not able to create an environment? When you create a Cloud Accelerator Platform account , the default group assigned to you does not have the access to create an environment. Contact your Cloud Accelerator Platform administrator and confirm that you have been added to the appropriate group. Can I collaborate with other users on the same environment? You can collaborate with other users on an environment by sharing that environment with one or more groups. You can also assign each group appropriate permissions such as view, create, edit, deploy, destroy, delete, import and export. All users in that group can work on the shared environment at the same time. Why does the blueprint of a layered environment that I have exported not contain all the environments? When you export an environment as a blueprint , only the environments on which it is dependent are included. Therefore, while exporting a layered environment as a blueprint, ensure that you are exporting the environment that is in the lowest layer. This action ensures that the environments in all other layers are also included in the blueprint. Can I find out if an environment has a dependency on other environments? You can use the Environment Dependency View to view the dependency of an environment on other environments. You can also switch to another environment in the Environment Dependency View window by clicking that environment name. Can I create a new environment version by importing a blueprint? While creating new environments from blueprints , you have to enter a unique combination of name and version for the environment that you are importing. If you enter an existing environment name and a unique version for that environment, the imported environment is added as a version of that existing environment. Why am I not able to delete some environment versions? You might not be able to delete a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether the environment version that you are trying to delete has any deployments. You cannot delete environment versions that are already deployed. Can I easily identify the differences between two environments? You can compare differences between two environments or two versions of the same environment . This comparison detects environment configuration-level and resource-level differences between the base and target environments.","title":"Environments (Deploy Accelerator)"},{"location":"platform-common/faqs/#deployments-deploy-accelerator","text":"Question Answer Why am I not able to deploy an environment? You might not be able to deploy a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether the environment that you want to deploy is dependent on other environments. You can deploy child deployments only if their parent deployments are available Can I test an environment version before deploying it in production? Each environment version can have multiple deployments. To test an environment version, you can start a new deployment . Based on the test results, you can update the resources and packages in this environment version and redeploy the existing deployment . When you are ready to deploy the environment in production, you can start another new deployment. While starting each new deployment , you can specify a different connection, provider, and input variables. For example, you can specify different VPC IDs for your Staging and Production deployments. Can I view all deployments of an environment? If an environment has multiple deployments, you can view the list of all your deployments and other deployments that are shared with you, along with their status. The deployments drop-down on the canvas categorizes deployments based on environment versions. By default, the most recent deployment from the list for the selected environment version appears. How can I upgrade an existing deployment of an environment? To upgrade an existing deployment, you can redeploy the existing deployment with either the same or a different environment version. In this case, only the changes made in the selected environment version are provisioned. Deployed resources that have not been updated in the selected environment version are not impacted. You can plan and redeploy an existing deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. Before redeploying an existing deployment of an environment, can I preview the changes that will be made to the existing deployment? Before you redeploy an existing deployment, you can view the plan that Deploy Accelerator generates. This plan provides a list of resources that will be created, updated, or destroyed when you redeploy the deployment. You can plan and redeploy an existing deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. Can I upgrade a parent deployment without impacting its child deployments? You can upgrade a parent deployment by redeploying that deployment with a new or updated environment version. This action impacts its child deployments only if they reference any updated resource values in the parent deployment. Can I upgrade a child deployment without impacting its parent deployments? You can upgrade a child deployment without impacting any of its parent deployments. Why am I not able to deploy an environment that uses the S3 reference type in the Depends On resource? You might not be able to deploy an environment that uses the S3 reference type in the Depends On resource because of the following reasons: -- The provider that you have selected to deploy the environment does not have access to the S3 bucket that you have specified. -- The URL that you have specified does not use the s3://bucketname/tfstatefilename format, in which bucketname is the name of the S3 bucket and tfstatefilename is the Terraform remote state file of the environment that is to be referred. -- If you have defined variables for the S3 bucket and Terraform state file names in the Input Variables resource, you have not used the following interpolation syntax to reference these variables: s3:// \\({var.*bucketNameInputVariable*}/\\) {var. tfstateFileNameInputVariable } Why am I not able to destroy a deployment of an environment? You might not be able to destroy a deployment of a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether deployments in other environments have a dependency on the deployment that you are trying to destroy. You must destroy the child (or dependent) deployments before you can destroy the parent deployment. Also, you can destroy a deployment that is shared with you only if the deployment owner has given you the Destroy permission for that deployment. Can I identify deployments that are shared with me? Deployments that other users have shared with you appear in a different color in the deployments list on the canvas. Why are users not able to see the deployment that I have shared with their group? When you share a a deployment of an environment version with selected groups, ensure that the environment version is also shared with the same groups. Otherwise, users cannot view the shared deployments. Why am I not able to redeploy a deployment that is shared with me? You can redeploy a deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. In addition, the deployment owner must also share with you the provider that is configured in the shared deployment. Why am I not able to plan a deployment that is shared with me? You can plan a deployment that is shared with you only if the deployment owner has given you the Redeploy permission for that deployment. The Redeploy permission of a deployment also includes the permission to plan that deployment. Can I share my deployment only if its status is Deployed? You can share any of your deployments with other groups, irrespective of the deployment state (Deploying, Deployed, Failed, or Stop state). Why do I get an error message that the deployment name that I have specified is not unique even though I cannot see that deployment name in the list on the canvas? The name that you specify for a deployment must be unique across all versions of the environment. If another user already has a deployment with the name that you have specified, you get an error message. However, you can see that deployment name in the list on the canvas only if the deployment and its associated environment version is shared with you. Why are parent deployments not visible when I try to redeploy the child deployment that is shared with me? While redeploying a child deployment that is shared with you, parent deployments are visible in the Parent Deployment Mapping section only if the deployment owner has also shared the parent deployments with you. If the parent deployments are not visible in the Review and Re-Deploy window, you cannot redeploy the child deployment that is shared with you. When I share some environments, why are all its deployments also automatically shared with the same groups with which the environment is shared? If you have upgraded from a version earlier than 2.9.0, and you later share any environment version that was created before the upgrade, all its deployments are also automatically shared with the same groups with which the environments are shared. The permissions given for the deployments are based on the permissions assigned to the environments. Based on your requirements, you can manually update the sharing permissions for these deployments.","title":"Deployments (Deploy Accelerator)"},{"location":"platform-common/faqs/#resources-deploy-accelerator","text":"Question Answer Can I define variables that are available for all resources in an environment? You can use the Input Variable resource to define variables that can be used in all other resources in the same environment. For an example, see Configure variables in your environment . Can I rename a resource that has been added to an environment? You can rename a resource that has been added to an environment. The resource name is automatically updated in other resources within the environment that reference it by using the interpolation syntax or the Depends On attribute. Can I create a copy of an existing resource? You can copy an existing resource from either the same environment or a different environment. The attribute values of the new resource are the same as the original resource that you have copied. However, the name attribute value and tags are not copied to the new resource. The packages that were added to the original resource are also copied to the new resource. Can I view the dependencies between various resources in an environment? You can view the dependencies that are created when resources use the Depends On attribute or the interpolation syntax to reference other resources in an environment. To view these dependencies, click the Links icon ( ) on the Deploy Accelerator canvas. Can I create multiple resources based on the same configuration? You can use the Count attribute of a resource to specify the number of identical resources that you want Deploy Accelerator to create while deploying the environment. Can I control access to the AWS resources that are created when I deploy an environment? To control access to AWS resources in an environment, you can assign tags to the resources. AWS IAM policies can leverage resource tags to define the resources that users can access. Deploy Accelerator simplifies the process of adding common tags to all resources in an environment. You can configure a custom tag , which contains a set of AWS key-pair tags, and select that custom tag while deploying an environment . Can I provision packages on servers that are not provisioned by using Deploy Accelerator? You can use the Existing VM resource to represent existing servers in an environment. Based on your requirements, you can add packages to this resource. Why am I not able to add resources to an environment version? You might not be able to add resources to a shared environment if the owner has not given you the appropriate permission. If you are not working on a shared environment, confirm whether you are trying to add resources to a released environment version. No updates can be made to a released version. You must create a new environment version and then add resources to the new version.","title":"Resources (Deploy Accelerator)"},{"location":"platform-common/faqs/#packages-deploy-accelerator","text":"Question Answer Can I add user-defined packages in Deploy Accelerator? Deploy Accelerator enables you to add your own custom (or user-defined) Chef Solo and Puppet packages. For more information, see Adding user-defined packages . Why is the user-defined package that I have uploaded not visible? You must refresh the Deploy Accelerator web page to see the new package on the Packages tab in the left panel of the Home page. Can I run custom scripts on resources in an environment? You can run custom scripts on resources by using the execute-script package in Deploy Accelerator. Can administrators configure the environment package types that appear while creating an environment? Deploy Accelerator provides out-of-the-box support for Chef Solo, Chef Server, and Puppet. However, only Chef Solo and Chef Server are configured as the default environment package types. Based on requirements, administrators can choose to configure only Chef Solo, only Chef Server, only Puppet, or a combination of these three as the environment package types. For more information, see Configuring the environment package types . Can administrators control which users have access to Chef Server packages and roles? By default, Chef Server packages (or cookbooks) and roles are not shared with any users in Deploy Accelerator. Administrators must configure the groups that can access each package and role. For more information, see Sharing Chef Server packages and roles . Are new packages and roles added in the registered Chef Server automatically available in Deploy Accelerator? Deploy Accelerator polls the registered Chef Server every 30 minutes for new and updated packages (or cookbooks) and roles. However, to view the latest packages and roles at any given time, Administrators can manually update the Chef Server in Deploy Accelerator. Are groups from the registered Chef Server organization automatically available in Deploy Accelerator? When Administrators register a Chef Server in Deploy Accelerator in Deploy Accelerator , groups from the selected Chef Server organization are automatically created in Cloud Accelerator Platform. However, administrators must ensure that the clients group in the Chef Server organization is a member of all other groups in that organization. The naming convention used for these groups is organizationName - groupName . Cloud Accelerator Platform administrators must modify these groups to add the appropriate users and then Deploy Accelerator administrators can share Chef Server packages with these groups . Are new groups added in a registered Chef Server automatically available in Deploy Accelerator? Deploy Accelerator administrators can view new groups that are added in a Chef Server organization only after manually updating that Chef Server in Deploy Accelerator.","title":"Packages (Deploy Accelerator)"},{"location":"platform-common/faqs/#test-accelerator-faqs","text":"Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, and scale (load) testing. You can also integrate your existing automation code with Test Accelerator and run automated tests on your web applications. For information about using Test Accelerator, see Run tests . The following sections provide answers to frequently asked questions about Test Accelerator. Configuration Tests Test results","title":"Test Accelerator FAQs"},{"location":"platform-common/faqs/#configuration-test-accelerator","text":"Question Answer Can I use Test Accelerator for a web application that is running inside an intranet? Yes, you can use Test Accelerator to test a web application that is running inside an intranet. In this case, the IP or host (primarily a web server) on which the web application is running is not exposed as a public IP. To use Test Accelerator to test a web application that is running inside an intranet, you can use one of the following approaches: -- Use Proxy: You can set up a proxy for the internal web server with a publicly-exposed IP. For information about setting up a forward proxy in case of an Apache web server, see the Apache documentation . For information about setting up a proxy pass in NGINX, see the NGINX documentation . To set up a proxy for any other web server that you might be using for the public IP, refer to the documentation for that web server. -- Use ngrok: You can use ngrok to expose the web server running on a local machine to the Internet. For more information, see the ngrok documentation . You can use the ngrok-mapped Internet URL while running the URL, manual, automated, or scale tests in Test Accelerator. It is recommended that you finalize the approach only after discussing both these approaches with your internal IT team.","title":"Configuration (Test Accelerator)"},{"location":"platform-common/faqs/#tests-test-accelerator","text":"Question Answer What type of tests can I perform on web applications by using Test Accelerator? Test Accelerator enables you to perform the following different types of tests on your web applications: -- URL test : This test can instantly check the availability of a web application on different browsers. -- Manual test : This test quickly provisions Kubernetes pods with the selected browsers and reduces the effort to create test setups. You can concentrate on testing the actual use cases. -- Security test : This test performs a security check of your web application by running different types of security packs on the application. -- API test : This test performs a test of the exposed APIs for your application. -- Automated (Cross-browser) test : This test automatically provisions browsers of your choice on Kubernetes pods and then runs your existing automation test suites in the selected browsers. -- Scale test : This test performs the scale testing of your web application across a range of browsers. It enables you to configure multiple browsers, the browser count to test the load, the automation test suite, and the run time in hours. Can I use Test Accelerator to test my infrastructure? The infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, and routers. Test Accelerator supports the following specification types: -- InSpec and Serverspec enable you to check if your servers are configured correctly and adhere to your policy requirements. -- Awspec allows you to check if AWS resources have been deployed with the appropriate attribute values. -- Azurespec allows you to check if Azure resources have been deployed with the appropriate attribute values. For more information, see Running infrastructure tests . Test Accelerator also allows you to use the Default Azure Spec code and Default AWS Spec code to test your Azure and AWS resources. The default code, which is provided out-of-the-box by Test Accelerator, saves time and resources spent on writing your own custom spec code. For more information, see Running codeless infrastructure tests . What is the difference between Apache JMeter and ScaleNow? The major difference between JMeter and ScaleNow is that JMeter uses the stimulated environment for load testing to run the test plans whereas ScaleNow creates the Load testing environment with actual machines to run the test suites.","title":"Tests (Test Accelerator)"},{"location":"platform-common/faqs/#test-results-test-accelerator","text":"Question Answer Can I view reports for the tests that I have run? On the Home page of Test Accelerator, you can view reports for the tests that you have run. For more information, see Viewing and downloading test results . Can I download test reports? You can download the HTML report of a test run either for a single browser or for all selected browsers. You can also download a summary report of the test run for all browsers in the Excel or CSV format. For more information, see Viewing and downloading test results . Note: The infrastructure test results can be downloaded in the Excel format only.","title":"Test results (Test Accelerator)"},{"location":"platform-common/faqs/#assess-accelerator-faqs","text":"Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) is a tool that automates the assessment of your cloud infrastructure against relevant security and compliance standards. It contains several out-of-the-box policies that you can run to assess the security and compliance level of your cloud infrastructure. For information about using Assess Accelerator, see Run assessment policies . The following sections provide answers to frequently asked questions about Assess Accelerator. Configuration Assessment jobs Reports","title":"Assess Accelerator FAQs"},{"location":"platform-common/faqs/#configuration-assess-accelerator","text":"Question Answer How can I access Assess Accelerator? To access Assess Accelerator, sign in to Cloud Accelerator Platform and then click the icon in the top-left corner and select Hitachi Cloud Accelerator - Assess . For information about logging on to Cloud Accelerator Platform, see Create & access account . Can I retain the Docker containers that are created for each assessment job? After an assessment job has completed, Assess Accelerator automatically deletes the Docker container that was created for the job. However, if required, you can retain the Docker containers that are created for assessment jobs by performing the following actions: 1) Use SSH to connect to the instance on which Cloud Accelerator Platform is deployed. 2) Navigate to the {PLATFORM_HOME}/rean-deploy/conf folder. 3) Create the assessnow.properties file and add the terminate.container.on.completion property in the file. 4) Set the value of the terminate.container.on.completion property to false , as shown below: terminate.container.on.completion=false To once again start deleting the Docker containers, update the value of the assess_now property to true . 5) Save the assessnow.properties file. 6) Stop and restart Cloud Accelerator Platform.","title":"Configuration (Assess Accelerator)"},{"location":"platform-common/faqs/#assessment-jobs-assess-accelerator","text":"Question Answer Can I specify the same job name multiple times? You can specify the same job name multiple times. However, it is recommended that you specify a unique job name that helps you to identify the selected policies and any other details. For example, Operations-May2017-Week1. I have run multiple jobs for the same provider. How can I know which is the latest job that I have run? On the My Assessment tab, jobs are listed in the descending order, with the last-run job at the top of the list. Also, if you move your mouse over the status icons, you can see the date and time when a job was started. Can I create an assessment job for multiple providers? Yes, you can select multiple providers in the same assessment job. Assess Accelerator creates a separate job for each provider in the format JobName_ProviderName . On the My Assessment tab, each job is listed separately. The Provider column displays the provider name, the Job Name column displays the job name for that provider, and the Group column displays the assessment job name. Do I get a notification email after my assessment job completes? No, you receive an email notification only when your assessment job starts. In addition, you are sent this notification email only if you have selected the Enable Notification check box and provided your email address while creating the assessment job.","title":"Assessment jobs (Assess Accelerator)"},{"location":"platform-common/faqs/#reports-assess-accelerator","text":"Question Answer Can I specify the location for downloading assessment reports? By default, Assess Accelerator downloads the reports to your local computer. While creating the assessment job, you can also specify an AWS S3 bucket to which Assess Accelerator can upload the assessment reports (docx format) that are generated. Why are reports not being uploaded to the S3 bucket specified in the assessment job? For each selected provider in the assessment job, Assess Accelerator appends the specified S3 bucket name with the account ID ( BucketName-AccountID ). If this bucket does not exist, Assess Accelerator creates a new bucket. However, Assess Accelerator requires the s3:PutObject permission to upload assessment reports to an S3 bucket and the s3:CreateObject permission to create a new S3 bucket. Ensure that these permissions have been added to the IAM policies that are used in each selected provider. Which template should I select while downloading an assessment report (docx format)? What is the difference between the templates? To download a detailed report, select the Assess_Template option. To download a summary report, select the Assess_Template_Without_Output option. The summary report lists the assessment criteria, their status (failed or passed) and severity level (CRITICAL, MAJOR, or WARNING), and possible solutions for resolving the identified issues. In addition to the information provided in the summary report, the detailed report includes an appendix that lists the actual instances in your infrastructure that have not met the requirements for each criterion. To download reports without the Current State and Proposed State sections, select the Custom_Assess_Template or Custom_Assess_Template_Without_Output option. Why does an assessment report display no data for a policy? The IAM user or role specified in the selected provider must have the permissions to run a policy on the infrastructure being assessed. Otherwise, the report does not contain any data. For more information about the permissions required for an AWS provider, see Before you begin . How can I interpret the data in the sunburst chart? The sunburst chart provides a graphical summary of the policies that were run for the selected job, along with their status. This chart consists of the following layers: -- The center layer provides a summary of the assessment job. You can see the total number of checks that were run and the number of checks that failed. -- The next layer represents the policies that were run for the selected job. For example, security policy, cost optimization policy, and operations policy. -- The third layer represents different categories of assessment checks within each policy. For example, Security Assessment for Monitoring Compliance is a category of assessment check within the security policy. -- The outermost layer represents each check that is included in a policy. For example, \"Ensure no S3 buckets are public\" is a specific check within the security policy. Click any layer to drill down into a specific set of data. Click the center layer to get back to the previous view. Can I compare the assessment results across multiple jobs or providers? Yes, you can view and download the comparison report to view assessment results across multiple jobs or providers. For more information, see Viewing comparison reports . Why am I not able to view the comparison report on the Assess Accelerator UI? You can see the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If have selected more than 6 jobs, you can download the comparison report. Can I download assessment reports for multiple jobs or providers at the same time? Yes, you can download assessment reports (docx format) for multiple jobs or providers at the same time. For more information, see Downloading comparison reports .","title":"Reports (Assess Accelerator)"},{"location":"platform-common/faqs/#migrate-accelerator-faqs","text":"Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) helps you to migrate applications that are running in your existing data centers to the cloud. It brings in data from Discovery tools, enables you to pick and choose the servers for each application group, uses a migration tool for server replication, and then creates server images. For information about using Migrate Accelerator, see Discover and migrate resources . The following sections provide answers to frequently asked questions about Migrate Accelerator. General Configuration Server migration","title":"Migrate Accelerator FAQs"},{"location":"platform-common/faqs/#general-migrate-accelerator","text":"Question Answer What discovery and migration tools does Migrate Accelerator support? Migrate Accelerator currently supports RISC Networks CloudScape as a discovery tool and CloudEndure as a migration tool. If required, Migrate Accelerator can also import a list of discovered servers from a CSV file. Does Migrate Accelerator support the migration of servers to AWS only? For now, Migrate Accelerator supports migration of servers to Amazon Web Services (AWS) only. However, if you want to migrate servers to other cloud providers, such as Microsoft Azure, contact the Cloud Accelerator Platform team.","title":"General (Migrate Accelerator)"},{"location":"platform-common/faqs/#configuration-migrate-accelerator","text":"Question Answer Can I deploy Migrate Accelerator in AWS instead of the data center from which I want to migrate servers? You can deploy Migrate Accelerator in an AWS account but you must ensure that Migrate Accelerator can communicate with servers in the data center. What are migration credentials? Migrate Accelerator supports CloudEndure as a migration tool. To run a migration job, Migrate Accelerator requires access to the migration tool that you have configured. While configuring migration credentials, you must specify credentials of the migration tool account that Migrate Accelerator must use to migrate application groups. Why do I need to create a connection? If you select Auto install agents while creating a migration job, Migrate Accelerator automates installation of the migration tool agent on servers that are running in your data center. To install these agents, Migrate Accelerator requires SSH connection details for Linux servers and WinRM connection details for Windows servers. Why do I need to create a provider? Providers enable you to specify the cloud provider and credentials for accessing the account in which Migrate Accelerator must create images of the servers you are migrating. When you start a migration job for a group of servers in the data center, CloudEndure creates test instances in AWS and replicates data from these servers on the test instances. After the data replication has completed successfully, Migrate Accelerator uses the provider that is configured in the migration job to create images of the test instances. Note: Migrate Accelerator can create images only in the same AWS account in which CloudEndure launches test instances. Therefore, the provider that you select in a migration job must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project.","title":"Configuration (Migrate Accelerator)"},{"location":"platform-common/faqs/#server-migration-migrate-accelerator","text":"Question Answer How can I verify that all data on the migrated server has been successfully replicated? On the Server List page, the status of a server changes to Ready for Test after the data is successfully migrated. Migrate Accelerator enables you to launch a test instance and verify that the data has been correctly replicated on the server. On the Server List page, from the Action column for the server whose data you want to verify, click Ready for Test . You can view the instance launch status in the Migration Tool Status column. Does Migrate Accelerator maintain logs of the auto-agent installation process? Migrate Accelerator maintains a log of the auto-agent installation process for each server that is migrated. On the Server List page, from the Action column for a server, click Logs . Can I view a list of all discovery and migration jobs in Migrate Accelerator? To view a list of all discovery jobs, click the Pick Discovery Job box at the top. To view a list of all migration jobs in Migrate Accelerator, click the icon in the top-right corner and then click Migration Jobs .","title":"Server migration (Migrate Accelerator)"},{"location":"platform-common/getStarted/","text":"Get started with Cloud Accelerator Platform \u00b6 This topic helps you to get started with Hitachi Cloud Accelerator Platform, which drives Enterprise Digital Transformation and enables the adoption of DevOps principles to deliver Enterprise Workloads in the cloud. It provides an overview of Cloud Accelerator Platform and describes how users can get started with Cloud Accelerator Platform. Contents \u00b6 Overview of Cloud Accelerator Platform Supported browsers and screen resolution How to get started as an administrator How to get started as a user Trademarks notice Overview of Cloud Accelerator Platform \u00b6 Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps principles to deliver Enterprise Workloads in the cloud. It comprises a set of tools (accelerators) that help you to accelerate your journey toward cloud adoption in the following ways: You can quickly and easily rehost, refactor, or rebuild your applications in the cloud. You can automate one-time or continuous verification of your infrastructure, applications, and security in the cloud. The following are a few benefits of using Cloud Accelerator Platform: Delivers high-speed cloud adoption by enabling you to automate the deployment of new and existing workloads or applications in the cloud. It supports different cloud adoption strategies, such as rehost, refactor, and rebuild. Provides pre-packaged content across all phases (Deploy, Verify, and Manage) of cloud adoption. This content, which is based on industry standards, includes deployment blueprints and packages. Enables higher ROI for cloud adoption as pre-packaged content eases the cloud and DevOps learning curve for engineers. It reduces the need for specialized resources and the time required to move applications to the cloud. Reduces the time to go to market by providing tools that help you to automate each phase (Deploy, Verify, and Manage) of cloud adoption. Supported browsers and resolution \u00b6 Cloud Accelerator Platform comprises Deploy Accelerator, Test Accelerator, Assess Accelerator, and Migrate Accelerator. The following table lists the supported browsers for all these accelerators. Browser Browser version Google Chrome 88.0 and later Mozilla Firefox 85.0 and later Microsoft Edge 88.0 and later Apple Safari 14.0.3 and later Screen resolution \u00b6 Minimum resolution: 1024 x 768 pixels How to get started as an administrator \u00b6 The following table lists the actions that administrators must perform to get started with Cloud Accelerator Platform and accelerators. Action Steps to perform 1. Deploy Cloud Accelerator Platform. 1a. For information about deploying Cloud Accelerator Platform (Deploy Accelerator, Assess Accelerator, Test Accelerator, and Migrate Accelerator), contact the Cloud Accelerator Platform team. 1b. Sign in to Cloud Accelerator Platform . 2. Create and manage groups and users in Cloud Accelerator Platform. Sign in to Cloud Accelerator Platform and create and manage groups and users in Cloud Accelerator Platform. You can also share the Cloud Accelerator Platform URL with users and ask them to create their own account. However, users can perform various actions in Cloud Accelerator Platform only after the administrator assigns them to a group that has the appropriate permissions. Alternatively, you can integrate Cloud Accelerator Platform with your existing Microsoft Active Directory setup and manage users and user-group authorization in Active Directory. 3. Configure accelerators. -- Administer Deploy Accelerator . -- Administer Test Accelerator. How to get started as a user \u00b6 The following table lists the actions that users must perform to get started with Cloud Accelerator Platform and accelerators. Action Steps to perform 1. Sign in to Cloud Accelerator Platform. 1a. Create a Cloud Accelerator Platform account and request your administrator to add you to the appropriate group. 1b. Sign in to Cloud Accelerator Platform . Note: If your administrator has integrated Cloud Accelerator Platform with Active Directory, you can sign in with your Active Directory user credentials. You can perform various actions in Cloud Accelerator Platform only if your administrator has added you to the appropriate group. 2. Based on your goals, perform the appropriate actions in Deploy Accelerator, Assess Accelerator, Test Accelerator, or Migrate Accelerator. -- Discover and migrate resources . -- Deploy and manage environments . -- Run tests . -- Run assessment policies . Trademarks notice \u00b6 HITACHI is a trademark or registered trademark of Hitachi, Ltd. Microsoft, Active Directory, Azure, Internet Explorer, Microsoft Edge, and Windows are trademarks or registered trademarks of Microsoft Corporation. All other trademarks, service marks, and company names are properties of their respective owners.","title":"Get Started"},{"location":"platform-common/getStarted/#get-started-with-cloud-accelerator-platform","text":"This topic helps you to get started with Hitachi Cloud Accelerator Platform, which drives Enterprise Digital Transformation and enables the adoption of DevOps principles to deliver Enterprise Workloads in the cloud. It provides an overview of Cloud Accelerator Platform and describes how users can get started with Cloud Accelerator Platform.","title":"Get started with Cloud Accelerator Platform"},{"location":"platform-common/getStarted/#contents","text":"Overview of Cloud Accelerator Platform Supported browsers and screen resolution How to get started as an administrator How to get started as a user Trademarks notice","title":"Contents"},{"location":"platform-common/getStarted/#overview-of-cloud-accelerator-platform","text":"Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps principles to deliver Enterprise Workloads in the cloud. It comprises a set of tools (accelerators) that help you to accelerate your journey toward cloud adoption in the following ways: You can quickly and easily rehost, refactor, or rebuild your applications in the cloud. You can automate one-time or continuous verification of your infrastructure, applications, and security in the cloud. The following are a few benefits of using Cloud Accelerator Platform: Delivers high-speed cloud adoption by enabling you to automate the deployment of new and existing workloads or applications in the cloud. It supports different cloud adoption strategies, such as rehost, refactor, and rebuild. Provides pre-packaged content across all phases (Deploy, Verify, and Manage) of cloud adoption. This content, which is based on industry standards, includes deployment blueprints and packages. Enables higher ROI for cloud adoption as pre-packaged content eases the cloud and DevOps learning curve for engineers. It reduces the need for specialized resources and the time required to move applications to the cloud. Reduces the time to go to market by providing tools that help you to automate each phase (Deploy, Verify, and Manage) of cloud adoption.","title":"Overview of Cloud Accelerator Platform"},{"location":"platform-common/getStarted/#supported-browsers-and-resolution","text":"Cloud Accelerator Platform comprises Deploy Accelerator, Test Accelerator, Assess Accelerator, and Migrate Accelerator. The following table lists the supported browsers for all these accelerators. Browser Browser version Google Chrome 88.0 and later Mozilla Firefox 85.0 and later Microsoft Edge 88.0 and later Apple Safari 14.0.3 and later","title":"Supported browsers and resolution"},{"location":"platform-common/getStarted/#screen-resolution","text":"Minimum resolution: 1024 x 768 pixels","title":"Screen resolution"},{"location":"platform-common/getStarted/#how-to-get-started-as-an-administrator","text":"The following table lists the actions that administrators must perform to get started with Cloud Accelerator Platform and accelerators. Action Steps to perform 1. Deploy Cloud Accelerator Platform. 1a. For information about deploying Cloud Accelerator Platform (Deploy Accelerator, Assess Accelerator, Test Accelerator, and Migrate Accelerator), contact the Cloud Accelerator Platform team. 1b. Sign in to Cloud Accelerator Platform . 2. Create and manage groups and users in Cloud Accelerator Platform. Sign in to Cloud Accelerator Platform and create and manage groups and users in Cloud Accelerator Platform. You can also share the Cloud Accelerator Platform URL with users and ask them to create their own account. However, users can perform various actions in Cloud Accelerator Platform only after the administrator assigns them to a group that has the appropriate permissions. Alternatively, you can integrate Cloud Accelerator Platform with your existing Microsoft Active Directory setup and manage users and user-group authorization in Active Directory. 3. Configure accelerators. -- Administer Deploy Accelerator . -- Administer Test Accelerator.","title":"How to get started as an administrator"},{"location":"platform-common/getStarted/#how-to-get-started-as-a-user","text":"The following table lists the actions that users must perform to get started with Cloud Accelerator Platform and accelerators. Action Steps to perform 1. Sign in to Cloud Accelerator Platform. 1a. Create a Cloud Accelerator Platform account and request your administrator to add you to the appropriate group. 1b. Sign in to Cloud Accelerator Platform . Note: If your administrator has integrated Cloud Accelerator Platform with Active Directory, you can sign in with your Active Directory user credentials. You can perform various actions in Cloud Accelerator Platform only if your administrator has added you to the appropriate group. 2. Based on your goals, perform the appropriate actions in Deploy Accelerator, Assess Accelerator, Test Accelerator, or Migrate Accelerator. -- Discover and migrate resources . -- Deploy and manage environments . -- Run tests . -- Run assessment policies .","title":"How to get started as a user"},{"location":"platform-common/getStarted/#trademarks-notice","text":"HITACHI is a trademark or registered trademark of Hitachi, Ltd. Microsoft, Active Directory, Azure, Internet Explorer, Microsoft Edge, and Windows are trademarks or registered trademarks of Microsoft Corporation. All other trademarks, service marks, and company names are properties of their respective owners.","title":"Trademarks notice"},{"location":"platform-common/whatsNew/","text":"What's new \u00b6 Hitachi Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps Principles to deliver Enterprise Workloads in the Cloud. It comprises a set of tools that help you to accelerate the Deploy, Verify, and Manage phases of your journey toward Cloud Adoption. This topic lists the six most-recent Cloud Accelerator Platform omnibus releases and the set of accelerator versions included in each omnibus release. It also lists the various enhancements that are delivered in each release of the accelerators and the Cloud Accelerator Platform CLI. To view the enhancements that were delivered in older releases, see What's new (older releases) . Contents \u00b6 Cloud Accelerator Platform omnibus release updates Admin Console and Platform Common Services release updates Deploy Accelerator release updates Workflow Accelerator release updates Test Accelerator release updates Assess Accelerator release updates Migrate Accelerator release updates Cloud Accelerator Platform CLI release updates Cloud Accelerator Platform omnibus release updates \u00b6 The following table lists the set of accelerator versions that are included in the six most-recent Cloud Accelerator Platform omnibus releases. To view the enhancements delivered in an accelerator version, click the corresponding link. Release date Omnibus version Accelerator versions included in the omnibus release February , 2021 1.9.0 -- Admin Console and Platform Common Services 3.1.0 , 3.2.0 -- Deploy Accelerator 3.1.0 , 3.2.0 -- Workflow Accelerator 0.18.0 -- Test Accelerator 3.3.0 -- Assess Accelerator 3.0.0 January 28, 2021 1.8.1 -- Admin Console and Platform Common Services 3.0.0 , 3.0.2 -- Deploy Accelerator 3.0.0 , 3.0.2 -- Workflow Accelerator 0.15.0 , 0.16.0 , 0.17.0 -- Test Accelerator 3.0.0 , 3.1.0 , 3.2.0 -- Assess Accelerator 2.22.0 , 2.23.0 , 2.24.0 Note: Support for Dashboard Accelerator and Managed Cloud has been discontinued. The documentation for these accelerators is no longer available in the Cloud Accelerator Platform help website. October 29, 2020 1.5.1 -- Admin Console and Platform Common Services 2.28.0 -- Deploy Accelerator 2.28.0 -- Test Accelerator 2.22.3 -- Assess Accelerator 2.20.7 -- Migrate Accelerator 2.12.0 -- Workflow Accelerator 0.14.0 September 30, 2020 1.4.1 -- Admin Console and Platform Common Services 2.26.0 and 2.27.0 -- Deploy Accelerator 2.26.0 and 2.27.0 -- Test Accelerator 2.22.3 -- Assess Accelerator 2.20.6 -- Migrate Accelerator 2.12.0 -- Workflow Accelerator 0.13.0 September 11, 2020 1.3.3 -- Admin Console and Platform Common Services 2.24.0 and 2.25.0 -- Deploy Accelerator 2.24.0 and 2.25.0 -- Migrate Accelerator 2.12.0 -- Test Accelerator 2.22.1 and 2.22.2 -- Assess Accelerator 2.20.5 -- Dashboard Accelerator 2.5.0 and 2.6.0 -- Workflow Accelerator 0.10.0 -- Managed Cloud Services 1.0.0, 1.1.0, 1.2.0, and 1.3.0 This omnibus release supports the deployment of Cloud Accelerator Platform on Amazon Elastic Kubernetes Services (EKS). Important: Earlier versions of Cloud Accelerator Platform were deployed on a Docker Host. If you are upgrading from an earlier version to omnibus 1.3.3 on EKS, your existing data will also need to be migrated. For information about upgrading to omnibus 1.3.3, contact the Cloud Accelerator Platform Support Team. June 2, 2020 1.2.8 -- Admin Console and Platform Common Services 2.23.0 -- Deploy Accelerator 2.23.0 -- Migrate Accelerator 2.10.0 -- Test Accelerator 2.22.0 -- Assess Accelerator 2.20.0 -- Dashboard Accelerator 2.4.0 -- Workflow Accelerator 0.9.1 Admin Console and Platform Common Services release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Admin Console (Admin Console) and Platform Common Services enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Admin Console and Platform Common Services releases, see What's new (older releases) . For more information, see Create and access Cloud Accelerator Platform account and Administer Cloud Accelerator Platform . Release date Version Enhancements February 16, 2021 3.2.0 This release includes a few bug fixes. February 1, 2021 3.1.0 This release includes bug fixes related to the Platform Common Services. January 13, 2021 3.0.2 This release updates the copyright year in the Cloud Accelerator Platform user interface. November 12, 2020 3.0.0 This release includes updates to the Platform Common Services to support Test Accelerator 3.0.0. October 13, 2020 2.28.0 This release includes a few bug fixes. September 2, 2020 2.27.0 This release includes updates to the Platform Common Services to support Assess Accelerator. August 24, 2020 2.26.0 This release includes a new About HCAP box to display the versions of the accelerators installed. July 24, 2020 2.25.0 This release locks users out of their Cloud Accelerator Platform account for two hours if they fail to sign in to their account in three consecutive attempts. Administrators can unlock an existing user's account and customize the default lockout settings in the Admin Console. June 12, 2020 2.24.0 This release includes the following enhancements: -- Users are automatically signed out of Cloud Accelerator Platform if they do not perform any action for 30 minutes. Administrators can configure this default session timeout duration . -- Users can set preferences for their Cloud Accelerator Platform accounts. May 8, 2020 2.23.0 This release includes a few bug fixes. Deploy Accelerator release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Deploy Accelerator releases, see What's new (older releases) . For information about using Deploy Accelerator, see Deploy and manage environments . Release date Version Enhancements February 18, 2021 3.2.0 This release of Deploy Accelerator includes the following enhancements: - Support for adding resources and data sources using Hashicorp Configuration Language. - Support for adding timeout attribute to resources. - Upgrade of provider versions for AWS, Azure, GCP, and OCI providers. February 2, 2021 3.1.0 This release of Deploy Accelerator includes the following enhancements: - Support for viewing parent and child environments dependency. - Support for creating Databricks , Kibana , and Azure DevOps providers. - Ability to search environments using any parameter on the Environments table. - Upgrade of provider versions for GCP, VSphere, Helm, Kubernetes, Azuread, Consul and OCI providers. - Upgrade of Azure environments from 1.44.0 (released). January 13, 2021 3.0.2 This release of Deploy Accelerator includes bug fixes related to the Terraform v0.12 upgrade. The upgrade requires some manual changes for migration of blueprints from previous releases to 3.0.2 . November 12, 2020 3.0.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating Google beta resources in Google environments. -- Upgrade of the Terraform version to 0.12. -- Upgrade of all the providers to use the Terraform version 0.12. -- Automatic upgrade of AWS environments from 1.6 to 2.X. -- Automatic upgrade of Azure environments from 1.44.0 (beta). -- Added provider validation for Google , Azure , and Helm providers. October 13, 2020 2.28.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating a Datadog provider . -- Support for adding a reference provider to the ACME, Helm2, Helm3, and Kubernetes providers. -- Support for creating a Kubernetes provider for a Google Kubernetes Engine (GKE) cluster . September 2, 2020 2.27.0 This release of Deploy Accelerator includes the following enhancements: -- Users can now export multiple environments which are not related to each other in a single blueprint. -- Support for creating ACME Terraform provider (GA release, beta release was in 2.26.0). -- Support for creating OCI registries with Helm 3. -- Provider credentials are removed from the database once it is deleted. August 24, 2020 2.26.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating ACME Terraform provider (beta release), Oracle Cloud Infrastructure provider , Artifactory provider . -- Support for version 1.12.0 of the Terraform provider for Kubernetes. -- Users can now restore deleted environments from the Environment List window. -- Users can view additional deployment parameters in the Deployment Details window. -- A new ID column added to the Providers, Chef Servers, Custom Tags, and Connections tables for displaying additional information. -- Two new fields for Provider Version and Created by added in the Environment Configuration and the Deploy windows for displaying additional information. July 24, 2020 2.25.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating a Consul provider . -- Deploy Accelerator now supports version 2.70.0 of the Terraform provider for AWS. For a complete list of Terraform provider versions that Deploy Accelerator supports, see Supported providers . -- Support for using resources of the Terraform Random provider in an environment. These resources always appear in the Resources panel irrespective of the provider type that is selected for an environment. -- Support for a new data source, HTTP Data Source . For more information about this data source, see the Terraform Documentation . -- Users can now assign share permissions to one or more groups while creating an environment , copying an environment , and creating new environments from blueprints . Also, while creating a new environment version , users can choose to apply share permissions of the base environment. -- For enhanced security, users have to now select a warning check box while sharing environments , sharing deployments , sharing providers , sharing connections , and exporting environments . Deploy Accelerator administrators also have to select the warning check box while sharing Chef Server packages and roles . June 12, 2020 2.24.0 This release of Deploy Accelerator includes the following enhancements: -- Administrators can easily sync with the registered Chef Server and get new and updated packages and groups in Deploy Accelerator. -- Ability to enable or disable email notifications for deployments by setting environment-level or user-level preferences . Administrators can also enable or disable email notifications at the application level . The precedence order for the three settings is environment-level, followed by user-level, and finally application-level. -- Support for the locals resource and the Null Data Source data source, which are available across all provider types in Deploy Accelerator. -- Before creating or updating a provider, Deploy Accelerator validates the JSON syntax in the provider details. In the case of AWS and Kubernetes providers, the authentication details (credentials) are also validated. For more information, see Create a provider . -- Upgraded versions of Terraform providers for Amazon Web Services (AWS), Google Cloud Computing, Oracle, DNS, Vsphere, V-Sphere NSX (Beta)-T, Kubernetes, Azure, Azure Active Directory, and Azure Stack (Beta) are now available. For a list of Terraform provider versions that Deploy Accelerator supports, see Supported providers . -- The Helm provider is renamed as Helm2 and now supports version 0.10.4 of the Terraform provider for Helm. A new Helm3 provider type is also available and supports version 1.2.1 of the Terraform provider for Helm. The provider details that you have to specify for both Helm providers is the same. If you have upgraded to this release, your existing Helm environments will automatically become Helm2 environments. May 8, 2020 2.23.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to sort blueprints in the Blueprint Gallery based on their name or owner. By default, the blueprints are sorted based on their names in the descending order. -- Ability to search for blueprints in the Blueprint Gallery based on their name, description, owner name, or owner email. The metadata.yml file for a blueprint supports two additional attributes: ownerName and ownerEmail . -- Deploy Accelerator now supports version 1.34.0 of the Terraform provider for Azure. -- UI performance improvements by adding browser caching headers and enabling proxy caching. Workflow Accelerator release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Workflow (Workflow Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . For information about Workflow Accelerator, see Overview of Workflow Accelerator . Release date Version Enhancements February 5, 2021 0.18.0 This release of Workflow Accelerator includes the following enhancements: -- Support for creating a default provider . -- Support for the timeout and wait input mapping parameters for the deployHelmChart action. These two parameters are optional. January 20, 2021 0.17.0 This release of Workflow Accelerator includes upgrade support for the deployEnv , deployHelmChart , createHelmRepo , deploySolutionPackage , and createProvider workflow actions. December 17, 2020 0.16.0 This release of Workflow Accelerator includes the following enhancements: -- Support of CLI to support operations like register , get, delete solution package, and also to deploy and destroy solution packages. -- Support for scheduling solution package deployment . November 12, 2020 0.15.0 This release of Workflow Accelerator includes support for \"deploymentParameters\" for the \" deployEnv \" workflow action: -- You can now pass or override the deployment parameters. -- provider, region, connection, and chefEnvironment are supported parameters. -- Supported with schema versions 2.0, 2.1, 2.2, and 2.3. October 13, 2020 0.14.0 This release of Workflow Accelerator includes the following enhancements: -- GKE integration with Helm service: You can now install your helm chart with the underlined cluster as Google Kubernetes Engine. Helm2, Helm3, and Kubernetes are the supported provider types for GKE. Supported with schema versions 2.0, 2.1, and 2.2. -- GKE integration with the foundry: Support for the complete lifecycle of the referenced Foundry solution package. You can can now register the Foundry solution package with the underlined cluster as Google Kubernetes Engine. Supported with schema versions 2.0, 2.1, and 2.2. -- Support for Helm 3 and OCI registries: You can now pass the OCI registry URL in the metadata of the solution package, which is passed as an argument to the Helm Deploy Workflow Action. You can now deploy helm charts and images located in the OCI registry. Supported with schema versions 2.0, 2.1, and 2.2. -- The database locking issue is resolved: The deployment of the solution package is now successful. The issue of unresponsiveness from the activiti is resolved. September 11, 2020 0.13.0 This release of Workflow includes the ability to pre-register a Foundry Solution Package prior to deployment. August 31, 2020 0.12.0 This release of Workflow Accelerator includes the following enhancements: -- Solution Package integration with Helm Service. -- Ability to Deploy or Destroy a Foundry Solution Package from the Solution Catalog. -- Ability to skip a deployment and just read the outputs from a layer. -- Ability to have one Solution Package depend on another Solution Package deployment. August 18, 2020 0.11.0 This release of Workflow Accelerator includes the following enhancements: -- Output support -- Dynamic provider creation August 8, 2020 0.10.1 This release of Workflow Accelerator includes the following enhancement. In the Input mapping section, you must always define the data type with the following constraints: -- The supported data types are List, String, Boolean, Map, Integer, MAP, LIST, STRING and they are case sensitive. You cannot use any other formats than the ones that are specified. -- The default data type value is String . July 15, 2020 0.10.0 This release of Workflow Accelerator includes the following enhancements: -- Optional layers support: New parameters (action, name, condition, and data ) have been introduced to support optional layer configurations. You can now customize or control the skipping of deployments based on your requirements. Supported with schema version 2.0. -- Backward compatibility: You now have flexibility in choosing any schema version from version 1.0 and 2.0. If you do not want to use the optional layer enhancement, you must use schema version 1.0. May 14, 2020 0.9.1 This release of Workflow Accelerator makes the type available in the input mapping section under the input parameter. the supported types are STRING, LIST, MAP, and they are case sensitive. The default value of the parameter is STRING, if not value is entered while creating the Solution Package. Test Accelerator release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Test (Test Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Test Accelerator releases, see What's new (older releases) . For information about using Test Accelerator, see Run tests . Release date Version Enhancements February 17, 2021 3.3.0 This release of Test Accelerator includes bug fixes related to GCP resources for infrastructure tests. January 13, 2021 3.2.0 This release of Test Accelerator includes the following enhancements: -- Ability to run custom infrastructure test with GCP (Google Cloud Provider) Spec code . -- Ability to run codeless infrastructure test with GCP Spec code. December 10, 2020 3.1.0 This release of Test Accelerator includes the ability to edit Test Accelerator configurations in the UI. November 12, 2020 3.0.0 This release of Test Accelerator includes the following enhancements: -- Ability to run API test . -- Ability to run codeless infrastructure test . -- Use of Kubernetes pods for running tests instead of machine instances. -- Ability to view and download logs for tests. Note: If you are upgrading to this release, existing test jobs and reports will not be available after the upgrade. To refer to any existing test job configuration after the upgrade, contact the Cloud Accelerator Platform Support team . September 14, 2020 2.22.3 This release of Test Accelerator includes the following enhancements: --- A new About HCAP box to display the versions of the accelerators installed. -- CLI support for the new accelerator version. July 24, 2020 2.22.2 This release of Test Accelerator includes a few bug fixes. June 12, 2020 2.22.1 This release of Test Accelerator supports a new aws_session_token parameter in the Basic Credentials type for AWS infrastructure tests. This new parameter enables users to specify temporary credentials for an IAM user or an Assume Role. May 8, 2020 2.22.0 This release of Test Accelerator includes a few bug fixes. Assess Accelerator release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Assess Accelerator releases, see What's new (older releases) . For information about using Assess Accelerator, see Run assessment policies . Release date Version Enhancements February 16, 2021 3.0.0 This release of Assess Accelerator includes support for creating Azure assessment jobs . January 14, 2021 2.24.0 This release of Assess Accelerator includes the addition of a CIS ID column to the assessment report template. December 17, 2020 2.23.0 This release of Assess Accelerator includes bug fixes related to report templates. November 12, 2020 2.22.0 This release of Assess Accelerator includes the following enhancements: -- Addition of Prowler tool for conducting assessment. -- Create and manage assessment templates for different types of assessments. October 15, 2020 2.20.7 This release of Assess Accelerator includes the following enhancements: -- New Microsoft Word template for exporting reports. -- More intuitive error messages. September 14, 2020 2.20.6 This release of Assess Accelerator includes a new About HCAP box to display the versions of the accelerators installed. September 10, 2020 2.20.5 This release supports the deployment of Assess Accelerator on Amazon Elastic Kubernetes Services (EKS). January 6, 2020 2.20.0 This release of Assess Accelerator includes the branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Assess has been renamed as Assess Accelerator. Migrate Accelerator release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Migrate Accelerator releases, see What's new (older releases) . For information about using Migrate Accelerator, see Discover and migrate resources . Release date Version Enhancements July 7, 2020 2.12.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to create a migration configuration once and use it in multiple migration jobs. Each migration configuration contains the migration service, credentials for the migration service, and authentication details for the cloud service provider account. For the CloudEndure migration service, it also includes the Live Migration project. -- The Migration Credentials option is no longer available in the More options menu. Instead, users have to select the new Migration Configurations option to create both migration service credentials and migration configurations . -- The Create Migration Job page has been updated to allow users to select a migration service and the appropriate migration configuration. For more information, see Migrate servers . May 8, 2020 2.10.0 This release of Migrate Accelerator provides support for tag-based asset data retrieval in RISC discovery jobs. For more information, see Create a discovery job . Cloud Accelerator Platform CLI release updates \u00b6 The following table lists the Hitachi Cloud Accelerator Platform - Command Line Interface (Cloud Accelerator Platform CLI) enhancements that are delivered in recent releases. To view the enhancements that were delivered in older Cloud Accelerator Platform CLI releases, see What's new (older releases) . For more information, see Install and use Cloud Accelerator Platform CLI . Release date Version Enhancements February 16, 2021 3.2.0 This release of Cloud Accelerator Platform CLI includes bug fixes related to Test Accelerator and Workflow Accelerator. This release supports Deploy Accelerator 3.2.0, Admin Console 3.2.0, Test Accelerator 3.3.0, and Workflow Accelerator 0.18.0. February 1, 2021 3.1.0 This release of Cloud Accelerator Platform CLI includes bug fixes related to Deploy Accelerator. This release supports Deploy Accelerator 3.1.0, Admin Console 3.1.0, Test Accelerator 3.2.0, and Workflow Accelerator 0.17.0. January 13, 2021 3.0.3 This release of Cloud Accelerator Platform CLI adds support for Workflow Accelerator. You can now use Workflow Accelerator CLI commands to register, get, delete, deploy, and destroy solution packages. This release supports Deploy Accelerator 3.0.2, Admin Console 3.0.2, Test Accelerator 3.2.0, and Workflow Accelerator 0.16.0 and 0.17.0. November 12, 2020 3.0.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 3.0.0, Admin Console 3.0.0, and Test Accelerator 2.22.3. October 13, 2020 2.28.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.28.0, Admin Console 2.28.0, and Test Accelerator 2.22.3. September 2, 2020 2.27.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.27.0, Admin Console 2.27.0, and Test Accelerator 2.22.3. August 24, 2020 2.26.0 This release of the Cloud Accelerator Platform CLI includes a few bug fixes. July 24, 2020 2.25.0 This release of Cloud Accelerator Platform CLI supports two new parameters ( group_name and actions ) for the import-blueprint command in Deploy Accelerator. These parameters allow users to share the environments being imported. June 25, 2020 2.24.0 This release of Cloud Accelerator Platform CLI includes the following enhancements: -- Support for passing the aws_session_token parameter in the provider JSON for AWS infrastructure tests in Test Accelerator. -- Support for a new parameter ( export_jobid_path ) in the commands for running all types of tests in Test Accelerator. After a test job completes successfully, this parameter creates a file that contains the job name and job ID. May 8, 2020 2.23.0 This release of Cloud Accelerator Platform CLI fixes issues with the run-infra-test command and the wait parameter in all commands for running test jobs in Test Accelerator. This release supports Deploy Accelerator 2.23.0, Admin Console 2.23.0, and Test Accelerator 2.22.0.","title":"Latest Releases"},{"location":"platform-common/whatsNew/#whats-new","text":"Hitachi Cloud Accelerator Platform drives Enterprise Digital Transformation and enables the adoption of DevOps Principles to deliver Enterprise Workloads in the Cloud. It comprises a set of tools that help you to accelerate the Deploy, Verify, and Manage phases of your journey toward Cloud Adoption. This topic lists the six most-recent Cloud Accelerator Platform omnibus releases and the set of accelerator versions included in each omnibus release. It also lists the various enhancements that are delivered in each release of the accelerators and the Cloud Accelerator Platform CLI. To view the enhancements that were delivered in older releases, see What's new (older releases) .","title":"What's new"},{"location":"platform-common/whatsNew/#contents","text":"Cloud Accelerator Platform omnibus release updates Admin Console and Platform Common Services release updates Deploy Accelerator release updates Workflow Accelerator release updates Test Accelerator release updates Assess Accelerator release updates Migrate Accelerator release updates Cloud Accelerator Platform CLI release updates","title":"Contents"},{"location":"platform-common/whatsNew/#cloud-accelerator-platform-omnibus-release-updates","text":"The following table lists the set of accelerator versions that are included in the six most-recent Cloud Accelerator Platform omnibus releases. To view the enhancements delivered in an accelerator version, click the corresponding link. Release date Omnibus version Accelerator versions included in the omnibus release February , 2021 1.9.0 -- Admin Console and Platform Common Services 3.1.0 , 3.2.0 -- Deploy Accelerator 3.1.0 , 3.2.0 -- Workflow Accelerator 0.18.0 -- Test Accelerator 3.3.0 -- Assess Accelerator 3.0.0 January 28, 2021 1.8.1 -- Admin Console and Platform Common Services 3.0.0 , 3.0.2 -- Deploy Accelerator 3.0.0 , 3.0.2 -- Workflow Accelerator 0.15.0 , 0.16.0 , 0.17.0 -- Test Accelerator 3.0.0 , 3.1.0 , 3.2.0 -- Assess Accelerator 2.22.0 , 2.23.0 , 2.24.0 Note: Support for Dashboard Accelerator and Managed Cloud has been discontinued. The documentation for these accelerators is no longer available in the Cloud Accelerator Platform help website. October 29, 2020 1.5.1 -- Admin Console and Platform Common Services 2.28.0 -- Deploy Accelerator 2.28.0 -- Test Accelerator 2.22.3 -- Assess Accelerator 2.20.7 -- Migrate Accelerator 2.12.0 -- Workflow Accelerator 0.14.0 September 30, 2020 1.4.1 -- Admin Console and Platform Common Services 2.26.0 and 2.27.0 -- Deploy Accelerator 2.26.0 and 2.27.0 -- Test Accelerator 2.22.3 -- Assess Accelerator 2.20.6 -- Migrate Accelerator 2.12.0 -- Workflow Accelerator 0.13.0 September 11, 2020 1.3.3 -- Admin Console and Platform Common Services 2.24.0 and 2.25.0 -- Deploy Accelerator 2.24.0 and 2.25.0 -- Migrate Accelerator 2.12.0 -- Test Accelerator 2.22.1 and 2.22.2 -- Assess Accelerator 2.20.5 -- Dashboard Accelerator 2.5.0 and 2.6.0 -- Workflow Accelerator 0.10.0 -- Managed Cloud Services 1.0.0, 1.1.0, 1.2.0, and 1.3.0 This omnibus release supports the deployment of Cloud Accelerator Platform on Amazon Elastic Kubernetes Services (EKS). Important: Earlier versions of Cloud Accelerator Platform were deployed on a Docker Host. If you are upgrading from an earlier version to omnibus 1.3.3 on EKS, your existing data will also need to be migrated. For information about upgrading to omnibus 1.3.3, contact the Cloud Accelerator Platform Support Team. June 2, 2020 1.2.8 -- Admin Console and Platform Common Services 2.23.0 -- Deploy Accelerator 2.23.0 -- Migrate Accelerator 2.10.0 -- Test Accelerator 2.22.0 -- Assess Accelerator 2.20.0 -- Dashboard Accelerator 2.4.0 -- Workflow Accelerator 0.9.1","title":"Cloud Accelerator Platform omnibus release updates"},{"location":"platform-common/whatsNew/#admin-console-and-platform-common-services-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Admin Console (Admin Console) and Platform Common Services enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Admin Console and Platform Common Services releases, see What's new (older releases) . For more information, see Create and access Cloud Accelerator Platform account and Administer Cloud Accelerator Platform . Release date Version Enhancements February 16, 2021 3.2.0 This release includes a few bug fixes. February 1, 2021 3.1.0 This release includes bug fixes related to the Platform Common Services. January 13, 2021 3.0.2 This release updates the copyright year in the Cloud Accelerator Platform user interface. November 12, 2020 3.0.0 This release includes updates to the Platform Common Services to support Test Accelerator 3.0.0. October 13, 2020 2.28.0 This release includes a few bug fixes. September 2, 2020 2.27.0 This release includes updates to the Platform Common Services to support Assess Accelerator. August 24, 2020 2.26.0 This release includes a new About HCAP box to display the versions of the accelerators installed. July 24, 2020 2.25.0 This release locks users out of their Cloud Accelerator Platform account for two hours if they fail to sign in to their account in three consecutive attempts. Administrators can unlock an existing user's account and customize the default lockout settings in the Admin Console. June 12, 2020 2.24.0 This release includes the following enhancements: -- Users are automatically signed out of Cloud Accelerator Platform if they do not perform any action for 30 minutes. Administrators can configure this default session timeout duration . -- Users can set preferences for their Cloud Accelerator Platform accounts. May 8, 2020 2.23.0 This release includes a few bug fixes.","title":"Admin Console and Platform Common Services release updates"},{"location":"platform-common/whatsNew/#deploy-accelerator-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Deploy Accelerator releases, see What's new (older releases) . For information about using Deploy Accelerator, see Deploy and manage environments . Release date Version Enhancements February 18, 2021 3.2.0 This release of Deploy Accelerator includes the following enhancements: - Support for adding resources and data sources using Hashicorp Configuration Language. - Support for adding timeout attribute to resources. - Upgrade of provider versions for AWS, Azure, GCP, and OCI providers. February 2, 2021 3.1.0 This release of Deploy Accelerator includes the following enhancements: - Support for viewing parent and child environments dependency. - Support for creating Databricks , Kibana , and Azure DevOps providers. - Ability to search environments using any parameter on the Environments table. - Upgrade of provider versions for GCP, VSphere, Helm, Kubernetes, Azuread, Consul and OCI providers. - Upgrade of Azure environments from 1.44.0 (released). January 13, 2021 3.0.2 This release of Deploy Accelerator includes bug fixes related to the Terraform v0.12 upgrade. The upgrade requires some manual changes for migration of blueprints from previous releases to 3.0.2 . November 12, 2020 3.0.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating Google beta resources in Google environments. -- Upgrade of the Terraform version to 0.12. -- Upgrade of all the providers to use the Terraform version 0.12. -- Automatic upgrade of AWS environments from 1.6 to 2.X. -- Automatic upgrade of Azure environments from 1.44.0 (beta). -- Added provider validation for Google , Azure , and Helm providers. October 13, 2020 2.28.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating a Datadog provider . -- Support for adding a reference provider to the ACME, Helm2, Helm3, and Kubernetes providers. -- Support for creating a Kubernetes provider for a Google Kubernetes Engine (GKE) cluster . September 2, 2020 2.27.0 This release of Deploy Accelerator includes the following enhancements: -- Users can now export multiple environments which are not related to each other in a single blueprint. -- Support for creating ACME Terraform provider (GA release, beta release was in 2.26.0). -- Support for creating OCI registries with Helm 3. -- Provider credentials are removed from the database once it is deleted. August 24, 2020 2.26.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating ACME Terraform provider (beta release), Oracle Cloud Infrastructure provider , Artifactory provider . -- Support for version 1.12.0 of the Terraform provider for Kubernetes. -- Users can now restore deleted environments from the Environment List window. -- Users can view additional deployment parameters in the Deployment Details window. -- A new ID column added to the Providers, Chef Servers, Custom Tags, and Connections tables for displaying additional information. -- Two new fields for Provider Version and Created by added in the Environment Configuration and the Deploy windows for displaying additional information. July 24, 2020 2.25.0 This release of Deploy Accelerator includes the following enhancements: -- Support for creating a Consul provider . -- Deploy Accelerator now supports version 2.70.0 of the Terraform provider for AWS. For a complete list of Terraform provider versions that Deploy Accelerator supports, see Supported providers . -- Support for using resources of the Terraform Random provider in an environment. These resources always appear in the Resources panel irrespective of the provider type that is selected for an environment. -- Support for a new data source, HTTP Data Source . For more information about this data source, see the Terraform Documentation . -- Users can now assign share permissions to one or more groups while creating an environment , copying an environment , and creating new environments from blueprints . Also, while creating a new environment version , users can choose to apply share permissions of the base environment. -- For enhanced security, users have to now select a warning check box while sharing environments , sharing deployments , sharing providers , sharing connections , and exporting environments . Deploy Accelerator administrators also have to select the warning check box while sharing Chef Server packages and roles . June 12, 2020 2.24.0 This release of Deploy Accelerator includes the following enhancements: -- Administrators can easily sync with the registered Chef Server and get new and updated packages and groups in Deploy Accelerator. -- Ability to enable or disable email notifications for deployments by setting environment-level or user-level preferences . Administrators can also enable or disable email notifications at the application level . The precedence order for the three settings is environment-level, followed by user-level, and finally application-level. -- Support for the locals resource and the Null Data Source data source, which are available across all provider types in Deploy Accelerator. -- Before creating or updating a provider, Deploy Accelerator validates the JSON syntax in the provider details. In the case of AWS and Kubernetes providers, the authentication details (credentials) are also validated. For more information, see Create a provider . -- Upgraded versions of Terraform providers for Amazon Web Services (AWS), Google Cloud Computing, Oracle, DNS, Vsphere, V-Sphere NSX (Beta)-T, Kubernetes, Azure, Azure Active Directory, and Azure Stack (Beta) are now available. For a list of Terraform provider versions that Deploy Accelerator supports, see Supported providers . -- The Helm provider is renamed as Helm2 and now supports version 0.10.4 of the Terraform provider for Helm. A new Helm3 provider type is also available and supports version 1.2.1 of the Terraform provider for Helm. The provider details that you have to specify for both Helm providers is the same. If you have upgraded to this release, your existing Helm environments will automatically become Helm2 environments. May 8, 2020 2.23.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to sort blueprints in the Blueprint Gallery based on their name or owner. By default, the blueprints are sorted based on their names in the descending order. -- Ability to search for blueprints in the Blueprint Gallery based on their name, description, owner name, or owner email. The metadata.yml file for a blueprint supports two additional attributes: ownerName and ownerEmail . -- Deploy Accelerator now supports version 1.34.0 of the Terraform provider for Azure. -- UI performance improvements by adding browser caching headers and enabling proxy caching.","title":"Deploy Accelerator release updates"},{"location":"platform-common/whatsNew/#workflow-accelerator-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Workflow (Workflow Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . For information about Workflow Accelerator, see Overview of Workflow Accelerator . Release date Version Enhancements February 5, 2021 0.18.0 This release of Workflow Accelerator includes the following enhancements: -- Support for creating a default provider . -- Support for the timeout and wait input mapping parameters for the deployHelmChart action. These two parameters are optional. January 20, 2021 0.17.0 This release of Workflow Accelerator includes upgrade support for the deployEnv , deployHelmChart , createHelmRepo , deploySolutionPackage , and createProvider workflow actions. December 17, 2020 0.16.0 This release of Workflow Accelerator includes the following enhancements: -- Support of CLI to support operations like register , get, delete solution package, and also to deploy and destroy solution packages. -- Support for scheduling solution package deployment . November 12, 2020 0.15.0 This release of Workflow Accelerator includes support for \"deploymentParameters\" for the \" deployEnv \" workflow action: -- You can now pass or override the deployment parameters. -- provider, region, connection, and chefEnvironment are supported parameters. -- Supported with schema versions 2.0, 2.1, 2.2, and 2.3. October 13, 2020 0.14.0 This release of Workflow Accelerator includes the following enhancements: -- GKE integration with Helm service: You can now install your helm chart with the underlined cluster as Google Kubernetes Engine. Helm2, Helm3, and Kubernetes are the supported provider types for GKE. Supported with schema versions 2.0, 2.1, and 2.2. -- GKE integration with the foundry: Support for the complete lifecycle of the referenced Foundry solution package. You can can now register the Foundry solution package with the underlined cluster as Google Kubernetes Engine. Supported with schema versions 2.0, 2.1, and 2.2. -- Support for Helm 3 and OCI registries: You can now pass the OCI registry URL in the metadata of the solution package, which is passed as an argument to the Helm Deploy Workflow Action. You can now deploy helm charts and images located in the OCI registry. Supported with schema versions 2.0, 2.1, and 2.2. -- The database locking issue is resolved: The deployment of the solution package is now successful. The issue of unresponsiveness from the activiti is resolved. September 11, 2020 0.13.0 This release of Workflow includes the ability to pre-register a Foundry Solution Package prior to deployment. August 31, 2020 0.12.0 This release of Workflow Accelerator includes the following enhancements: -- Solution Package integration with Helm Service. -- Ability to Deploy or Destroy a Foundry Solution Package from the Solution Catalog. -- Ability to skip a deployment and just read the outputs from a layer. -- Ability to have one Solution Package depend on another Solution Package deployment. August 18, 2020 0.11.0 This release of Workflow Accelerator includes the following enhancements: -- Output support -- Dynamic provider creation August 8, 2020 0.10.1 This release of Workflow Accelerator includes the following enhancement. In the Input mapping section, you must always define the data type with the following constraints: -- The supported data types are List, String, Boolean, Map, Integer, MAP, LIST, STRING and they are case sensitive. You cannot use any other formats than the ones that are specified. -- The default data type value is String . July 15, 2020 0.10.0 This release of Workflow Accelerator includes the following enhancements: -- Optional layers support: New parameters (action, name, condition, and data ) have been introduced to support optional layer configurations. You can now customize or control the skipping of deployments based on your requirements. Supported with schema version 2.0. -- Backward compatibility: You now have flexibility in choosing any schema version from version 1.0 and 2.0. If you do not want to use the optional layer enhancement, you must use schema version 1.0. May 14, 2020 0.9.1 This release of Workflow Accelerator makes the type available in the input mapping section under the input parameter. the supported types are STRING, LIST, MAP, and they are case sensitive. The default value of the parameter is STRING, if not value is entered while creating the Solution Package.","title":"Workflow Accelerator release updates"},{"location":"platform-common/whatsNew/#test-accelerator-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Test (Test Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Test Accelerator releases, see What's new (older releases) . For information about using Test Accelerator, see Run tests . Release date Version Enhancements February 17, 2021 3.3.0 This release of Test Accelerator includes bug fixes related to GCP resources for infrastructure tests. January 13, 2021 3.2.0 This release of Test Accelerator includes the following enhancements: -- Ability to run custom infrastructure test with GCP (Google Cloud Provider) Spec code . -- Ability to run codeless infrastructure test with GCP Spec code. December 10, 2020 3.1.0 This release of Test Accelerator includes the ability to edit Test Accelerator configurations in the UI. November 12, 2020 3.0.0 This release of Test Accelerator includes the following enhancements: -- Ability to run API test . -- Ability to run codeless infrastructure test . -- Use of Kubernetes pods for running tests instead of machine instances. -- Ability to view and download logs for tests. Note: If you are upgrading to this release, existing test jobs and reports will not be available after the upgrade. To refer to any existing test job configuration after the upgrade, contact the Cloud Accelerator Platform Support team . September 14, 2020 2.22.3 This release of Test Accelerator includes the following enhancements: --- A new About HCAP box to display the versions of the accelerators installed. -- CLI support for the new accelerator version. July 24, 2020 2.22.2 This release of Test Accelerator includes a few bug fixes. June 12, 2020 2.22.1 This release of Test Accelerator supports a new aws_session_token parameter in the Basic Credentials type for AWS infrastructure tests. This new parameter enables users to specify temporary credentials for an IAM user or an Assume Role. May 8, 2020 2.22.0 This release of Test Accelerator includes a few bug fixes.","title":"Test Accelerator release updates"},{"location":"platform-common/whatsNew/#assess-accelerator-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Assess Accelerator releases, see What's new (older releases) . For information about using Assess Accelerator, see Run assessment policies . Release date Version Enhancements February 16, 2021 3.0.0 This release of Assess Accelerator includes support for creating Azure assessment jobs . January 14, 2021 2.24.0 This release of Assess Accelerator includes the addition of a CIS ID column to the assessment report template. December 17, 2020 2.23.0 This release of Assess Accelerator includes bug fixes related to report templates. November 12, 2020 2.22.0 This release of Assess Accelerator includes the following enhancements: -- Addition of Prowler tool for conducting assessment. -- Create and manage assessment templates for different types of assessments. October 15, 2020 2.20.7 This release of Assess Accelerator includes the following enhancements: -- New Microsoft Word template for exporting reports. -- More intuitive error messages. September 14, 2020 2.20.6 This release of Assess Accelerator includes a new About HCAP box to display the versions of the accelerators installed. September 10, 2020 2.20.5 This release supports the deployment of Assess Accelerator on Amazon Elastic Kubernetes Services (EKS). January 6, 2020 2.20.0 This release of Assess Accelerator includes the branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Assess has been renamed as Assess Accelerator.","title":"Assess Accelerator release updates"},{"location":"platform-common/whatsNew/#migrate-accelerator-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator) enhancements that are delivered in the six most-recent Cloud Accelerator Platform omnibus releases . To view the enhancements that were delivered in older Migrate Accelerator releases, see What's new (older releases) . For information about using Migrate Accelerator, see Discover and migrate resources . Release date Version Enhancements July 7, 2020 2.12.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to create a migration configuration once and use it in multiple migration jobs. Each migration configuration contains the migration service, credentials for the migration service, and authentication details for the cloud service provider account. For the CloudEndure migration service, it also includes the Live Migration project. -- The Migration Credentials option is no longer available in the More options menu. Instead, users have to select the new Migration Configurations option to create both migration service credentials and migration configurations . -- The Create Migration Job page has been updated to allow users to select a migration service and the appropriate migration configuration. For more information, see Migrate servers . May 8, 2020 2.10.0 This release of Migrate Accelerator provides support for tag-based asset data retrieval in RISC discovery jobs. For more information, see Create a discovery job .","title":"Migrate Accelerator release updates"},{"location":"platform-common/whatsNew/#cloud-accelerator-platform-cli-release-updates","text":"The following table lists the Hitachi Cloud Accelerator Platform - Command Line Interface (Cloud Accelerator Platform CLI) enhancements that are delivered in recent releases. To view the enhancements that were delivered in older Cloud Accelerator Platform CLI releases, see What's new (older releases) . For more information, see Install and use Cloud Accelerator Platform CLI . Release date Version Enhancements February 16, 2021 3.2.0 This release of Cloud Accelerator Platform CLI includes bug fixes related to Test Accelerator and Workflow Accelerator. This release supports Deploy Accelerator 3.2.0, Admin Console 3.2.0, Test Accelerator 3.3.0, and Workflow Accelerator 0.18.0. February 1, 2021 3.1.0 This release of Cloud Accelerator Platform CLI includes bug fixes related to Deploy Accelerator. This release supports Deploy Accelerator 3.1.0, Admin Console 3.1.0, Test Accelerator 3.2.0, and Workflow Accelerator 0.17.0. January 13, 2021 3.0.3 This release of Cloud Accelerator Platform CLI adds support for Workflow Accelerator. You can now use Workflow Accelerator CLI commands to register, get, delete, deploy, and destroy solution packages. This release supports Deploy Accelerator 3.0.2, Admin Console 3.0.2, Test Accelerator 3.2.0, and Workflow Accelerator 0.16.0 and 0.17.0. November 12, 2020 3.0.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 3.0.0, Admin Console 3.0.0, and Test Accelerator 2.22.3. October 13, 2020 2.28.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.28.0, Admin Console 2.28.0, and Test Accelerator 2.22.3. September 2, 2020 2.27.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.27.0, Admin Console 2.27.0, and Test Accelerator 2.22.3. August 24, 2020 2.26.0 This release of the Cloud Accelerator Platform CLI includes a few bug fixes. July 24, 2020 2.25.0 This release of Cloud Accelerator Platform CLI supports two new parameters ( group_name and actions ) for the import-blueprint command in Deploy Accelerator. These parameters allow users to share the environments being imported. June 25, 2020 2.24.0 This release of Cloud Accelerator Platform CLI includes the following enhancements: -- Support for passing the aws_session_token parameter in the provider JSON for AWS infrastructure tests in Test Accelerator. -- Support for a new parameter ( export_jobid_path ) in the commands for running all types of tests in Test Accelerator. After a test job completes successfully, this parameter creates a file that contains the job name and job ID. May 8, 2020 2.23.0 This release of Cloud Accelerator Platform CLI fixes issues with the run-infra-test command and the wait parameter in all commands for running test jobs in Test Accelerator. This release supports Deploy Accelerator 2.23.0, Admin Console 2.23.0, and Test Accelerator 2.22.0.","title":"Cloud Accelerator Platform CLI release updates"},{"location":"platform-common/whatsNewArchive/","text":"What's new (older releases) \u00b6 This topic is an archive of the older releases of Hitachi Cloud Accelerator Platform. It lists the Cloud Accelerator Platform omnibus releases and individual accelerator releases. You can view the enhancements that were delivered in older releases of the accelerators and the Cloud Accelerator Platform CLI. To view the enhancements delivered in the six most-recent Cloud Accelerator Platform omnibus releases, see What's new . Contents \u00b6 Admin Console and Platform Common Serivces release updates (Older Releases) Deploy Accelerator release updates (Older Releases) Migrate Accelerator release updates (Older Releases) Test Accelerator release updates (Older Releases) Assess Accelerator release updates (Older Releases) Cloud Accelerator Platform CLI release updates (Older Releases) Dashboard Accelerator release updates (Discontinued Support) Managed Cloud Services release updates (Discontinued Support) Managed Cloud Services Legacy release updates (Discontinued Support) Admin Console and Platform Common Services release updates (Older Releases) \u00b6 The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Admin Console (Admin Console) and Platform Common Services. For more information, see Create and access Cloud Accelerator Platform account and Administer Cloud Accelerator Platform . Release date Version Enhancements March 31, 2020 2.22.0 This release includes a few bug fixes. March 5, 2020 2.21.0 This release includes a new group, VIEW_ALL_USER'S_ENTITIES , in the Admin Console. This group allows users to view the environments and deployments of all users in Deploy Accelerator. For more information, see Default groups and Accessing environments and deployments of other users . February 7, 2020 2.20.0 This release enables administrators to customize the password policy for Cloud Accelerator Platform user accounts. December 24, 2019 2.19.0 This release includes a few bug fixes. November 21, 2019 2.18.0 This release includes a few bug fixes. September 26, 2019 2.17.0 This release includes a few bug fixes. September 9, 2019 2.16.0 This release includes a few bug fixes. August 8, 2019 2.15.0 This release includes a few bug fixes. July 16, 2019 2.14.1 This release includes a few bug fixes related to verifying user accounts. July 1, 2019 2.14.0 This release enables Cloud Accelerator Platform administrators to share an existing user's Deploy Accelerator entities with a policy-based group while disabling that user in the Admin Console. Other users in the selected group can then access these shared entities. June 3, 2019 2.13.0 This release includes a few bug fixes. May 10, 2019 2.12.0 This release includes a few bug fixes. March 15, 2019 2.11.0 This release includes the following enhancements: -- When new users verify their own Cloud Accelerator Platform account, an email notification is sent to the default Cloud Accelerator Platform administrator. -- In the Admin Console, the Cloud Accelerator Platform administrators can reset the password of existing users. February 15, 2019 2.10.0 This release includes a few bug fixes. January 21, 2019 2.9.0 This release includes a few bug fixes. December 12, 2018 2.8.0 This release includes a few bug fixes. November 29, 2018 2.7.1 This release includes a few bug fixes. November 19, 2018 2.7.0 This release includes a few bug fixes. September 26, 2018 2.6.2 This release includes a few bug fixes. September 18, 2018 2.6.0 This release includes a few bug fixes. August 6, 2018 2.4.0 This release includes a few bug fixes. July 23, 2018 2.1.0 This release includes a few bug fixes. June 20, 2018 1.7.3 This release enables administrators to integrate Cloud Accelerator Platform with Microsoft Active Directory and manage users and user-group authorization in Active Directory. May 23, 2018 1.6.2 This release enables Cloud Accelerator Platform administrators to view details of the policies that are attached to groups in the Admin Console. April 27, 2018 1.6.1 This release includes a few bug fixes. April 19, 2018 1.6.0 This release includes a few bug fixes. March 28, 2018 1.5.0 This release includes a few bug fixes. March 9, 2018 1.4.0 This release includes a few bug fixes. February 27, 2018 1.3.0 This release includes a few bug fixes. January 24, 2018 1.2.0 This release includes a few bug fixes. January 2, 2018 1.1.0 This release includes a few bug fixes. November 6, 2017 1.0.0 This release enables Cloud Accelerator Platform administrators to create and manage users as well as groups, and attach policies to groups in the Admin Console. September 20, 2017 0.14.0 This release includes a few bug fixes. September 11, 2017 0.13.1 This release includes a few bug fixes. September 7, 2017 0.13.0 This release includes a few bug fixes. August 14, 2017 0.12.1 This release includes a few bug fixes. August 4, 2017 0.12.0 This release includes a few bug fixes. July 21, 2017 0.11.0 This release includes a few bug fixes. June 16, 2017 0.10.2 This release includes a few bug fixes. May 15, 2017 0.10.0 This release includes a few bug fixes. April 27, 2017 0.9.0 This release includes a few bug fixes. March 29, 2017 0.8.0 This release includes the following enhancements: -- The first user who creates an account is assigned the Admin role. -- Administrators can assign roles to other users in the Admin Console. Deploy Accelerator release updates (Older Releases) \u00b6 The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). For information about using Deploy Accelerator, see Deploy and manage environments . Release date Version Enhancements March 31, 2020 2.22.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to sync blueprints in the Blueprint Gallery with the latest updates in the Artifactory. Deploy Accelerator also automatically syncs the blueprint metadata every 30 minutes by default. However, administrators can modify the sync interval based on requirements. -- Metadata.yml now supports a relative path for the image that is shown for a blueprint in the Blueprint Gallery. -- New API to import blueprints from the Artifactory . The new importBlueprintFromArtifactory API allows you to specify the name and version of the blueprint you want to import. You can also choose to specify a prefix for the names of all imported environment, and a group with which to share the imported environments (with the View and Deploy permissions). March 5, 2020 2.21.0 This release of Deploy Accelerator includes the ability to rename environments . February 7, 2020 2.20.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to upgrade the AWS provider version that was used to create an environment to a newer AWS provider version that is supported. -- Support for provisioning Ansible Solo packages on Windows instances. -- Additional branding updates: In the Deploy Accelerator URL that is shown in the browser, reandeploy has been replaced with hcapdeploy . December 24, 2019 2.19.0 This release of Deploy Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Deploy has been renamed as Deploy Accelerator. -- Support for using an existing Terraform Module in an environment . You can add the new Terraform Module resource to an environment and specify the source from where the existing Terraform module can be downloaded. You can also use the output of this Terraform module in other resources within the environment. -- While sharing various entities (such as environments , deployments , providers , and connections ), users can now view only the groups to which they belong. Only administrators can view all groups. -- The /env/userMap API in Deploy Accelerator is no longer available. November 21, 2019 2.18.0 This release of Deploy Accelerator includes the following new features: -- Support for Google Cloud Platform resources. -- Support for adding and using a user-defined Ansible Solo package. This release does not support the provisioning of Ansible Solo packages on Windows instances. September 26, 2019 2.17.0 This release of Deploy Accelerator includes the following new features: -- Support for the Hong Kong region. -- Support for the use of AWS provider credentials to create Helm and Kubernetes (EKS) providers. September 9, 2019 2.16.0 This release of Deploy Accelerator includes the following new features: -- Providers added for Azure Stack, Azure Active Directory, and vSphere NSX-T. -- Helm repository data resource of the Helm provider can be defined while configuring Deploy Accelerator. -- Resources available for the vSphere provider. August 8, 2019 2.15.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to add blueprints in the Blueprint Gallery . Multiple versions of a blueprint can also now be made available in the Blueprint Gallery. -- Ability to compare environment configuration-level differences , such as providers, connections, owners, custom tags, and more. -- Support for two new providers, Kubernetes and Helm. -- Updates to the supported format of environment versions. You can now append the xx.xx.xx format with a dash (-) followed by a custom alphanumeric value based on your requirements. For example: 01.02.00-updated The value that you specify for the environment version must not exceed 40 characters and must not contain a colon (:). -- Additional support for multiple Microsoft Azure resources. July 16, 2019 2.14.1 This release of Deploy Accelerator includes a few bug fixes related to importing blueprints. July 1, 2019 2.14.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports version 1.30.1 of the Terraform provider for Azure. -- Additional support for multiple Microsoft Azure resources. June 3, 2019 2.13.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now provides support for multiple recipes in Chef Solo packages. While adding a user-defined Chef Solo package, all recipes that are defined in the selected Chef cookbook become available in Deploy Accelerator. While adding the Chef Solo package to a compute resource in an environment, you can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. -- Starting with this release, the Global Connection check box is no longer available while creating connections. Instead, you can now share your connections with specific groups and assign View, Edit, and Delete permissions. Users who are members of these groups can now use the same connection for instances in their environments. If you have upgraded to this release, existing Global Connections continue to appear in the list of connections. May 10, 2019 2.12.0 This release of Deploy Accelerator provides support for roles and recipes in Chef Server packages. This enhancement includes the following updates: -- When the administrator registers a Chef Server in Deploy Accelerator, both cookbooks and roles from the specified Chef Server organization become available in Deploy Accelerator. -- While creating a Chef Server environment, users can add both roles and packages (or cookbooks) to a compute resource. -- While adding a Chef Server package, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. March 15, 2019 2.11.0 This release of Deploy Accelerator includes the following enhancements: -- You can view the input variables that were passed in each deployment of an environment. -- On the Packages tab in the left panel, you can move your mouse over a package to view the description. -- If you click the Environment Dependency icon when a deployment is selected on the canvas, you can see the parent deployments and the status of these deployments. -- The Instance resource has been renamed as EC2 Instance . -- Deploy Accelerator supports multiple AWS StorageGateway resources. -- On the Environment List page or in the Search Environments box at the top, you can now see the owner of each environment. -- Managed packages are now available by default after Deploy Accelerator is successfully deployed. The UPDATE button, which was earlier used to synchronize the managed packages, no longer appear on the Package List page. February 15, 2019 2.10.0 This release of Deploy Accelerator includes the following enhancements: -- Support for two new AWS resources, Dynamodb Table and Dynamodb Table Item , and a new data source, Dynamodb Table . -- Administrators can configure Deploy Accelerator to send emails to users from an SMTP server on which authentication is not enabled. -- The Resource Logs window for a deployment displays timestamps. January 21, 2019 2.9.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports Mozilla Firefox, Apple Safari, Microsoft Internet Explorer, and Microsoft Edge in addition to Google Chrome. For a complete list of supported browser versions, see Supported browsers . -- Starting with this release, you can share each deployment of an environment with specific groups and assign View, Redeploy, Destroy, and Stop permissions based on your requirements. If you have upgraded to this release, all deployments of environments that were shared before the upgrade are automatically shared with the same groups with which the environments are shared. In addition, if you later share any environment that was created before the upgrade, all its deployments are also automatically shared with the selected groups. The permissions given for the deployments are based on the permissions assigned to the environments. -- Added support for multiple Microsoft Azure resources. -- The Blueprint View icon has been renamed as Environment Dependency View. December 12, 2018 2.8.0 This release of Deploy Accelerator includes the following enhancements: -- Starting with this release, the Global Provider check box is no longer available while creating providers. Instead, you can now share your providers with specific groups and assign View, Edit, Share, and Delete permissions. Users who are members of these groups can now use the same provider for deploying an environment. If you have upgraded to this release, existing Global Providers continue to appear in the list of providers. -- The SSH Keygen resource has been deprecated and no longer appears in the Resources panel. To dynamically generate a key pair while deploying an environment, you can now use the TLS Private Key Generator resource. If you have upgraded to this release, any existing environments that used the SSH Keygen resource continue to be deployed without any errors. November 29, 2018 2.7.1 This release of Deploy Accelerator enables administrators to configure multiple environment package types (Chef Solo, Chef Server, and Puppet) at the same time. By default, Chef Solo and Chef Server are configured as the environment package types. This enhancement also includes the following updates: -- While creating an environment , you can select one of the supported environment package types. For this environment, the Packages panel displays packages of the selected environment package type only. -- The Package List page displays packages of all the supported environment package types. -- The dnow.package.configuration_type property has been deprecated. To configure the environment package types , administrators must now use the following property: com.reancloud.platform.deploy.environment.configuration_types November 19, 2018 2.7.0 This release supports Deploy Accelerator in an offline mode. In the offline mode, Deploy Accelerator does not require Internet connectivity to perform various operations. All required artifacts are accessed from the artifactory that is configured while deploying Deploy Accelerator. New configuration properties have been added to configure the artifactory. Also, to support the adding of user-defined packages in the offline mode, the artifactory option is now available in the Repository Type drop-down. September 26, 2018 2.6.2 This release of Deploy Accelerator adds support for the following Amazon Web Services (AWS) resources: -- IAM User Login profile -- IAM Group membership -- EC2 Launch template September 18, 2018 2.6.0 This release of Deploy Accelerator includes the following enhancements: -- The Blueprint Gallery enables you to view out-of-the-box blueprints that are available in Deploy Accelerator. These blueprints are templates of industry standard cloud or IT infrastructure architectures. You can create new environments from out-of-the-box blueprints . -- You can now view the list of deployments across all environment versions . -- The Deploy Accelerator API documentation has been released. August 6, 2018 2.4.0 This release of Deploy Accelerator enables you to view the plan for a new deployment of an environment that has existing deployments. July 23, 2018 2.1.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now enables all users to create global providers and connections that are available to all other Deploy Accelerator users. -- The enhanced Package UI enables you to more easily create new packages and add new versions of existing packages . June 20, 2018 1.7.3 This release of Deploy Accelerator includes the following enhancements: -- You can compare differences between two environments in Deploy Accelerator. You can view a list of resources that are added, removed, or edited in the target environment. In the case of edited resources, you can also view differences in attribute values and packages. -- The Parent Deployment Mapping field in the Review and Deploy and Plan windows now displays drop-down lists for each Depends On resource. It lists deployments of only the parent environment version that is selected in the Depends On resource. -- Deploy Accelerator now supports HashiCorp Terraform data sources. To add a data source to an environment, you can drag that data source from the Data Sources section in Resources tab in the left panel to the canvas. -- The Create New Version and Release Version options have been moved to the version drop-down in the icon bar on the Home page. Also, you can now view all deployments of an environment version by clicking on the deployment name tab on the canvas. May 23, 2018 1.6.2 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports FIPS compliant encryption (AES-256) to encrypt sensitive data such as access key and secret key. -- You can now select a specific parent environment version in the Depends On resource of the child environment. April 27, 2018 1.6.1 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports version 0.11.6 of HashiCorp Terraform. -- Deploy Accelerator enables you to use NTLM authentication, instead of basic authentication, in WinRM connections. -- Cloud Accelerator Platform now uses Amazon CloudWatch Logs for all container logging, which enables you to view different logs from your container instances in Amazon Management Console. April 19, 2018 1.6.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator supports the configuration of CA certificates while deploying Cloud Accelerator Platform . -- Deploy Accelerator enables you to simultaneously share multiple packages with one or more groups when Chef Server is configured as the configuration tool. -- You can specify the parent environment version in the Depends On resource of the child environment. Deploy Accelerator exports the specified version of the parent environment while exporting the child environment . March 28, 2018 1.5.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports the versioning of environments. While creating , copying , and importing environments, you have to specify a unique environment version in the xx.xx.xx format. -- Deploy Accelerator also supports multiple deployments of the same environment version. You can define input variables while starting a new deployment or redeploying an existing deployment . -- You can choose to release an environment version after it has been finalized or deployed in production. To make any further changes to this released version, you have to create a new environment version . -- Deploy Accelerator enables you to plan or redeploy an existing deployment based on a different environment version. -- You can view all deployments for an environment version, along with their status: DEPLOYING, DEPLOYED, or FAILED. -- The More menu in the icon bar on the Home page displays three new options: Create New Version , Release Version , and See Deployments . A new version drop-down is also available to navigate between the different versions of an environment. March 9, 2018 1.4.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator supports the automatic creation of groups when Deploy Accelerator administrators add a Chef Server in Deploy Accelerator. Administrators must then share Chef Server managed packages with these groups and assign appropriate permissions. -- Deploy Accelerator now supports version 0.11.3 of HashiCorp Terraform. February 27, 2018 1.3.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator enables you to control access to AWS resources in an environment by using custom tags. You can now configure a custom tag , which contains a set of AWS key-pair tags, and then select that custom tag for an environment. Deploy Accelerator attaches the custom tag to all resources in that environment. -- If Chef Server is configured as the configuration tool for packages, Deploy Accelerator administrators can control the users who have access to the Chef Server managed packages. Administrators can share these packages with specific groups and assign appropriate permissions. January 24, 2018 1.2.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports a new AWS resource: IAM User . -- Deploy Accelerator supports new instance types for the Instance and RDS resources. -- Security enhancements to ensure that environments are more securely deployed. The pre-script, post-script, and local-exec scripts are run in a separate Docker container and do not affect the Deploy Accelerator instance. January 2, 2018 1.1.0 This release of Deploy Accelerator introduces a new icon ( ) for the Depends On resource in the Deploy Accelerator UI. November 6, 2017 1.0.0 This release of Deploy Accelerator includes the following enhancements: -- Containerized deployment of Cloud Accelerator Platform. -- Deploy Accelerator supports the environment collaboration feature, which allow multiple users to perform different tasks on the environment based on the permissions. September 20, 2017 0.14.0 This release of Deploy Accelerator includes the following enhancements: -- You can assess cost of the environment at any point of time. -- Deploy Accelerator now supports edit environment feature. -- Deploy Accelerator supports the following AWS resources: AWS Elasticsearch domain and AWS Elasticsearch domain policy . September 11, 2017 0.13.1 This release of Deploy Accelerator now supports a new provider, OPC (Oracle Public Cloud). September 7, 2017 0.13.0 This release of Deploy Accelerator includes the following enhancements: -- AWS Region can be specified while creating a new environment. -- Deploy Accelerator now supports the Import a resource feature, which helps in importing a resource from the provider-end to Deploy Accelerator and managing it. August 14, 2017 0.12.1 This release of Deploy Accelerator adds support for the following Amazon Web Services (AWS) Web Application Firewall (WAF) resources: -- WAF Byte Match Set -- WAF IP Addresses -- WAF Rule -- WAF Size Constraint Set -- WAF Web ACL -- WAF SQL Injection match Set -- WAF XSS Match Set -- WAF Regional Byte Match Set -- WAF Regional Ip Set August 4, 2017 0.12.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator no longer supports the process of adding Ansible packages to the /opt/dnow/data/ansible-packages folder, updating the packages.json file, and restarting the rean-deploy service. -- Administrators must add user-defined Ansible packages by using the Ansible Packages option. -- The Ansible Packages option now appears when you set Ansible as the supported configuration tool for adding packages . -- The hide_ansible property in the dnow.properties file has been deprecated. -- User-defined Ansible packages are now stored in the database. If you have upgraded to this release, Deploy Accelerator automatically migrates package definitions of existing user-defined Ansible packages to the database. July 21, 2017 0.11.0 This release of Deploy Accelerator includes the following enhancements: -- The Deploy Accelerator UI has been redesigned and includes new icons to create and manage environments. -- To access the Providers , Connections , Environments , and Manage Users options, you now have to click the new icon in the top-right corner. -- While configuring a provider , you can use the Instance Profile and Assume Role (Cross-account Access) methods to specify authentication details of an AWS account. -- The Deploy Accelerator CLI tool supports actions such as deploy, destroy, and export environments. -- Deploy Accelerator supports use of the aws_caller_identity , aws_region , aws_partition , aws_availability_zones , and aws_elb_service_account data sources in blueprints. -- You can copy one or more existing resources from the same environment or a different environment. -- For the local-exec package, you can now specify whether the script must be run after the resource is created or before it is destroyed. -- Deploy Accelerator now takes a backup of your database before starting the upgrade process. -- Managed packages are no longer available by default and need to be synchronized with your instance of Deploy Accelerator. -- Deploy Accelerator supports package versioning for managed and user-defined packages. If you have upgraded to this release, Deploy Accelerator automatically sets the version of all existing packages to 1.0. -- Managed and user-defined packages are now stored in the database instead of JSON files. If you have upgraded to this release, you must reupload your existing user-defined packages. -- Deploy Accelerator now supports only one configuration tool (Chef, Ansible, Puppet, or Chef Server) at a time. The dnow.packages.files property has been deprecated. -- Administrators can use the new Packages option to add and release user-defined packages. -- Administrators can now add Chef Server managed packages in Deploy Accelerator . June 16, 2017 0.10.2 This release of Deploy Accelerator provides a fix for the following issues: -- Packages could not be successfully provisioned on Windows instances. -- The plan could not be successfully generated for deployed environments. May 15, 2017 0.10.0 This release of Deploy Accelerator includes the following enhancements: -- The JBoss package is now available in Deploy Accelerator. -- The new Plan button enables you to view the deployment plan for an environment. However, you can view the plan for child environments only if their parent environments are already deployed. -- The Deploy Accelerator help website is now available. To access this help website, you can click the Help icon ( ). -- The issue with deleting environments has been fixed. April 27, 2017 0.9.0 This release of Deploy Accelerator includes the following enhancements: -- DeployNow has been renamed as REAN Deploy. The new name and logo ( ) appear in the Deploy Accelerator UI. -- To see Deploy Accelerator version, click the User icon ( ) in the top-right corner and then About . -- The new SSH Keygen resource enables you to dynamically generate a key pair (private and public keys) for an instance while deploying an environment. -- The new Use Custom Connection attribute of an instance, along with the Key Pair standard resource, enables you to associate a dynamically-generated key pair with the instance. -- The Count attribute is available for all standard resources and enables you to define the number of identical resources that Deploy Accelerator must create. -- You have to provide an additional confirmation to destroy an environment. March 29, 2017 0.8.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports the Spot Instance resource. -- The default notification email address in Deploy Accelerator has changed to product-info@reancloud.com . -- Administrators can configure the access of Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) from the Deploy Accelerator UI. Migrate Accelerator release updates (Older Releases) \u00b6 The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator). For information about using Migrate Accelerator, see Discover and migrate resources . Release date Version Enhancements April 1, 2020 2.9.0 This release of Migrate Accelerator includes the following enhancements: -- A status banner is shown for RISC discovery jobs. This banner also displays timestamps for asset data retrieval and lists the actions that might be impacted while data is being incrementally retrieved. -- Additional branding updates: In the Migrate Accelerator URL that is shown in the browser, reanmigrate has been replaced with hcapmigrate . March 5, 2020 2.8.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to manually upload connectivity data for RISC_EXPORTED_CSV discovery jobs. -- In the case of RISC discovery jobs, Migrate Accelerator now fetches data incrementally from CloudScape. While the data retrieval is still in progress, you can see the already retrieved assets in the Migrate Accelerator UI. January 22, 2020 2.7.0 This release of Migrate Accelerator includes the following enhancements: -- Enhancements to the Migrate Accelerator logs that are generated in CloudWatch. -- RISC discovery jobs in Migrate Accelerator can now retrieve servers based on their device type or tags applied to them in RISC CloudScape. While deploying Migrate Accelerator, administrators can specify the device type or tag based on which RISC discovery jobs must retrieve servers. If both device type and tag are specified, servers will be retrieved based on the configured tag. December 31, 2019 2.6.0 This release of Migrate Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Migrate has been renamed as Migrate Accelerator. -- Ability to manually upload connectivity data for RISC discovery jobs. -- Updates to the UI for creating new application groups and regrouping servers in existing application groups . December 11, 2019 2.5.0 This release of Migrate Accelerator includes the following enhancements: -- Availability of generic and AWS-specific build plans. By default, Migrate Accelerator generates the generic build plan for all application groups. However, this configuration can be modified while deploying Migrate Accelerator. -- Updates to the page that appears before the first discovery job is created. October 25, 2019 2.4.0 This release of Migrate Accelerator includes support for adding custom metadata for Providers . September 26, 2019 2.3.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to create providers specific to Migrate Accelerator. This allows users to create accelerator specific providers from the Admin Console. -- Support for attribute-based access control for Migrate Accelerator providers. This allows users to share their Migrate Accelerator providers with other user-groups. September 3, 2019 2.2.0 This release of Migrate Accelerator includes a few bug fixes. August 8, 2019 2.1.0 This release of Migrate Accelerator enables users to create and manage multiple providers. While creating a migration job, users can select the appropriate provider. The selected provider is used to create images of the migrated servers in AWS. The multiple-provider support enhancement includes the following updates: -- Migrate Accelerator users can use the Admin Console to configure providers , which contains the cloud provider and credentials for accessing the account in which the server images must be created. -- Two new groups, PROVIDER_USER and PROVIDER_VIEWER , are now available in the Admin Console. The PROVIDER_USER group enables users to create and edit providers while the PROVIDER_VIEWER group enables users to only view providers that they have previously created. -- While creating a migration job , users have to select the provider that Migrate Accelerator must use to create server images. The selected provider must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project. July 1, 2019 2.0.0 This release of Migrate Accelerator fixes a few issues with CSV discovery and provides support for multiple users who can simultaneously log on to the UI and then create and manage their own discovery jobs, connections, migration credentials, and migration jobs. The multiple-user support enhancement includes the following updates: -- The login page of Migrate Accelerator has changed to the Cloud Accelerator Platform login page that is also used to sign in to the Admin Console and other accelerators (Deploy Accelerator, Test Accelerator, and Assess Accelerator). -- Users can create their own account from the login page. However, they can perform various actions in Migrate Accelerator only after the Administrator assigns them to the appropriate groups. -- Administrators can use the Admin Console to manage users for Migrate Accelerator. In this release, the Admin Console is also deployed along with Migrate Accelerator on the same server. -- The MIGRATE_USER and MIGRATE_VIEWER policies are available in the Admin Console to manage access for the Migrate Accelerator users. The MIGRATE_USER policy enables users to perform all actions in Migrate Accelerator while the MIGRATE_VIEWER policy enables users to only view data in Migrate Accelerator. -- All users can currently view only the jobs, connections, and migration credentials that they have created. Note: This release of Migrate Accelerator does not support the Admin Console features of configuring group-level sharing and integrating with Active Directory. Also, when users are disabled, the Migrate Accelerator entities such as discovery jobs, migration jobs, connections, and migration connections, are not shared with the selected group. May 31, 2019 1.2.0 This release of Migrate Accelerator includes the following enhancements: -- Containerized deployment of Migrate Accelerator. For information about deploying Migrate Accelerator, contact the Cloud Accelerator Platform team. -- Support for Federal Information Processing Standard (FIPS) compliant algorithm to encrypt sensitive data such as passwords and AWS credentials. -- Migrate Accelerator now uses Amazon CloudWatch Logs for all container logging. You can view these logs in Amazon Management Console. March 29, 2019 1.0.1 This release of Migrate Accelerator provides a fix for the issue with using CA certificates to connect to the Windows servers that you want to migrate. March 15, 2019 1.0.0 This release of Migrate Accelerator includes the following enhancements: -- The procedure to deploy Migrate Accelerator has been updated. -- While creating a discovery job , you can select RISC_EXPORTED_DISCOVERY as the Discovery Tool. Use this option if you have exported the discovered data from RISC Networks CloudScape in a CSV format. -- While creating a migration job , you can select two different types of connections (SSH and WinRM) based on the operating system of the servers in the application group that you want to migrate. -- You can download a build plan for each application group. August 18, 2017 0.12.1 Initial release of the Migrate Accelerator documentation. Test Accelerator release updates (Older Releases) \u00b6 The following table lists the features and updates that were delivered in older releases of Hitachi Cloud Accelerator Platform - Test (Test Accelerator). For information about using Test Accelerator, see Run tests . Release date Version Enhancements April 1, 2020 2.21.0 This release of Test Accelerator includes bug fixes and the following enhancement. The Test Accelerator bootstrap deployment process now enables you to configure Artifactory details. If these details are not configured during deployment, the administrator must configure the Artifactory properties (URL, user name, API key, and Miscellaneous repository) by using the Configuration option in the Test Accelerator UI. February 12, 2020 2.20.0 This release of Test Accelerator includes the following enhancements: -- The existing reantest-user and reantest-view groups have been renamed as hcaptest-user and hcaptest-view . In case of an upgrade to version 2.20.0, you can see both new ( hcaptest-user and hcaptest-view ) and existing ( reantest-user and reantest-view ) groups. The administrator must manually move users from the existing groups to the new groups and then delete the old groups. All future updates will be available in the new groups. -- Additional branding updates: In the Test Accelerator URL that is shown in the browser, reantest has been replaced with hcaptest . December 24, 2019 2.19.0 This release of Test Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. REAN Test has been renamed as Test Accelerator. -- Infrastructure test job reports can be downloaded in the Excel and HTML formats. -- The infrastructure test job list can be filtered to show only scheduled jobs, only jobs that you have created, or only scheduled jobs that you have created. -- The output section of the JSON file that contains test data for an infrastructure test with default Azure Spec code , now also supports names of the Azure resources. Previously, you could only specify IDs of the Azure resources. November 7, 2019 2.18.0 This release of Test Accelerator includes the following enhancements: -- Support for new AWS Resources. -- Support for new Azure Resources . October 10, 2019 2.17.1 This release of Test Accelerator includes the following enhancements: -- Login support for running Security test. -- Addition of configuration parameter for login support. September 27, 2019 2.17.0 This release of Test Accelerator includes the following enhancements: -- Support for the Hong Kong region. -- Ability to run infrastructure test with default Azure Spec code . September 6, 2019 2.16.0 This release of Test Accelerator includes the following enhancements: -- Support for running custom Azure infrastructure tests . -- Download of test reports for Cross browser, URL, Scale now, Security and UPA in Excel and .csv format. August 8, 2019 2.15.0 This release of Test Accelerator delivers version 1.4 of the Default AWS Spec code, which is used to run AWS infrastructure tests. The Default AWS Spec code 1.4 provides additional support for the following AWS resources: -- aws_ami -- aws_codecommit_repository -- aws_cloudtrail -- aws_iam_group_policy -- aws_iam_role_policy -- aws_key_pair -- aws_kinesis_stream -- aws_kms_alias -- aws_kms_key -- aws_security_group_rule -- aws_swf_domain -- aws_s3_bucket -- aws_volume_attachment For the complete list of AWS resources that the Default AWS Spec code supports, see Supported resources . July 2, 2019 2.14.0 This release of Test Accelerator includes the following enhancements: -- In the case of a Cross Browser, URL, Security, UI Performance, or Scale test, you can now download a ZIP file that contains the test report for each selected browser. You can also download the summary report of a test job in the Excel format. For more information, see Viewing test results . -- A new reantest-viewer default group for Test Accelerator is now available in the Admin Console. This group allows users to view test jobs, job schedules, provider details, and configuration details in Test Accelerator. Also, the existing REANTestUser default group has been renamed as reantest-user . This group allows users to run and manage test jobs, create and manage job schedules, view provider details, and view configuration details. In case of an upgrade to version 2.14.0, you can see the existing REANTestUser group and the new reantest-user group. The reantest-user group has all the policies of the REANTestUser group along with the additional ManageJobAccess policy. The administrator must either assign appropriate users to the new group or add the ManageJobAccess policy to the existing REANTestUser group. -- Cloud Accelerator Platform administrators can create a new group of Test Accelerator users in the Admin Console and configure share permissions for different types of tests and the Job List page. Administrators can assign the View , Stop , and Schedule permissions for different types of test and the View permission for the Job List page. Based on the assigned permissions, users within a group can view, stop, or schedule each other's test jobs. Similarly, on the Job List page and in the left panel, they can view the test jobs created by other users in their group. However, these users must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to schedule test jobs, users must also be a part of the reantest-user group. June 4, 2019 2.13.0 This release of Test Accelerator includes the following enhancements: -- The default awspec code that Test Accelerator provides to run an AWS infrastructure test has been updated. The resources that you list in your input-output JSON file must now be categorized by the HashiCorp Terraform resource names (for example: aws_instance and aws_subnet) instead of custom categories, such as VPC and EC2. For more information, see running an AWS infrastructure test with default awspec code . In case of an upgrade to version 2.13.0, the Test Accelerator administrator must ensure that the value of the Automation Infra Validation-AWSSpec code property ( Artifactory_Details category) is set to /Infra-validation-awspec/1.3/Infra-validation-awspec-1.3.zip . This action ensures that Test Accelerator is using version 1.3 of the default awspec code to run the AWS infrastructure test. Also, it is recommended that users update any existing input-output JSON files to use the new format. -- The Test Accelerator administrator can now get encrypted Browser AMIs by setting the Fetch encrypted AMIs property to true . In addition, the administrator can get the AMIs based on additional user-defined tags that are specified in the Update AMI with external tags property. Both these properties are available in the Miscellaneous category. For information about updating the configuration properties, see Configuring Test Accelerator . May 14, 2019 2.12.0 This release of Test Accelerator includes the following enhancements: -- To access the Infra Dashboard, you must click Infra ( ) in the left panel on the Test Accelerator Home page. You can no longer see Infra Dashboard when you click the icon in the top-right corner. -- In the Infra Dashboard, for an infrastructure job of the awspec type, a consolidated report is available for each AWS resource type. -- You can now view a list of all the test jobs that you have created, along with the job type, creation date, and status. -- While creating a provider , the administrator has to specify only the Instance Profile Name that must be attached to the instances that are launched for test jobs. In the Infrastructure step of adding a provider, the TestInstance Role and Instance Profile ARN fields have been replaced with the Instance Profile Name field. -- The Test Accelerator documentation now provides instructions for running an AWS infrastructure test with default awspec code . -- The Test Accelerator API documentation has been released. March 18, 2019 2.11.0 This release of Test Accelerator includes the following enhancements: -- While creating URL, UPA, and Security tests, you can now specify a unique name for the test in the Job Name field. -- Test Accelerator supports the running of URL and Automation tests on Microsoft Internet Explorer 11. -- Test Accelerator supports the running of tests on Mozilla Firefox 64 and Google Chrome 64, 68, 69, 70, and 71. February 15, 2019 2.10.0 This release of Test Accelerator includes the following enhancements: -- Test Accelerator supports STIG Red Hat Enterprise Linux 7 AMI to launch instances for running tests. -- The Test Accelerator UI is supported on Mozilla Firefox, Microsoft Internet Explorer, Microsoft Edge, and Apple Safari, in addition to Google Chrome. For a complete list of supported browser versions, see Supported browsers . -- While creating an infrastructure test , you can upload a file that Test Accelerator must extract and provide while running your automation code. -- Test Accelerator supports the running of tests on Mozilla Firefox 63. -- The Test Accelerator administrator can create multiple providers and configure the infrastructure details for each provider. The provider that is configured as the default provider is used to launch the test instances. -- Sample automation code is stored in the Artifactory instead of GitHub. The new Artifactory Details section in the Configuration window enables the Test Accelerator administrator to configure the Artifactory access details and the sample automation codes. -- The Test Accelerator administrator can now click a new Refresh icon on the BROWSERS tab in the Configuration window to get the latest Browser AMIs with specific tags from the AWS account that is configured as the default provider. -- Test Accelerator no longer supports spot instances for running tests. February 27, 2018 1.3.0 In this release, the Test Accelerator UI has been redesigned and includes new icons. January 2, 2018 - This release of Test Accelerator includes the following enhancements: -- SmartBear TestComplete has support of stigged AMI. -- Cookbook has been changed to make it compatible with SmartBear TestComplete. September 12, 2017 - This release of Test Accelerator includes the following enhancements: -- Test Accelerator supports Scheduling test jobs . -- In infrastructure testing, you can stop and rerun the process. -- Test Accelerator now supports Chrome browser version 54-60 in URL, manual, and automated tests. Assess Accelerator release updates (Older Releases) \u00b6 The following table lists the features and updates that were delivered in older releases of Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator). For information about using Assess Accelerator, see Run assessment policies . Release date Version Enhancements November 6, 2019 2.18.1 This release of Assess Accelerator includes the following enhancements: -- Over-ride attribute values in the rules by modifying the default values . -- Bug fixes related to security and performance enhancement. September 09, 2019 2.16.0 This release of Assess Accelerator includes now allows you to skip the rules in the assessment policies to create a custom assessment. August 20, 2019 2.14.1 This release of Assess Accelerator includes the following enhancements: -- If an Assess Accelerator user has inherited administrator access from an AWS IAM user, a validation message appears while running an assessment. -- Bug fixes related to errors in reports. February 15, 2019 2.10.0 This release of Assess Accelerator includes the following enhancements: -- The comparison report is enhanced with Dashboard and Summary View sheets. -- Users can re-run the failed jobs from the list of assessment jobs. -- Minor changes in select providers and bug fixes. September 18, 2018 2.6.0 This release of Assess Accelerator includes the following enhancements: -- The new Compare tab enables you to generate, view, and download a report that compares assessment results across multiple jobs or providers. -- You can view the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If you select more than 6 jobs, you have to download the comparison report. -- The comparison report now includes the raw policy data that you can use to generate your own reports and charts. September 4, 2018 2.5.1 This release of Assess Accelerator includes the following enhancements: -- You can now create an assessment job for multiple providers. -- While creating an assessment job , you can specify an AWS S3 bucket to which REAN Assess can upload assessment reports that are generated in the docx format. -- You can compare assessment results across multiple jobs or providers and also download the comparison report. March 9, 2018 1.4.0 This release of REAN Assess includes the following enhancements: -- You can now use the Instance Profile, Static Credentials with Assume Role, and Instance Profile with Assume role methods while configuring a provider in Assess Accelerator. -- The rean-assess-aws-iam-policy.json custom inline policy has been replaced with the rean-assess-aws-iam-policy-1.json , rean-assess-aws-iam-policy-2.json , and rean-assess-aws-iam-policy-3.json files. The permissions in these files have also been updated. For more information about using these files, see Before you begin . January 2, 2018 1.1.0 This release of Assess Accelerator delivers a new and enhanced UI. The help website has also been updated based on the new UI. August 14, 2017 0.12.1 Starting with this release of Assess Accelerator, the Docker container that is created to run each assessment job is automatically deleted after the job is completed. Administrators can choose to retain the Docker containers by updating the value of the terminate.container.on.completion property to false in the assessnow.properties file. Note: If required, you must manually delete Docker containers that were created for assessment jobs before this release. June 9, 2017 0.10.1 This release of Assess Accelerator enables users to receive an email notification when an assessment job is completed. While submitting an assessment job, you can enable email notifications for that job and provide your company name and the email address to which the notification must be sent. The email notification is also sent to the registered email address of the user who submitted the assessment job. May 15, 2017 0.10.0 This release of Assess Accelerator delivers the following new templates for assessment policy reports: - Custom_REANAssess_Template - Custom_REANAssess_Template_Without_Appendix These two templates do not include the Current State and Proposed State sections. April 27, 2017 0.9.0 This release of Assess Accelerator includes the following enhancements: - You can now specify a job name each time you run one or more assessment policies for a provider. - On the MY ASSESSMENT tab, jobs that are appear in the Assessment jobs list are sorted based on the time they were started. The most recent job appears at the top of the list. Cloud Accelerator Platform CLI release updates (Older Releases) \u00b6 The following table lists the enhancements that were delivered in older releases of the Hitachi Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI). For more information, see see Install and use Cloud Accelerator Platform CLI . Release date Version Enhancements March 31, 2020 2.22.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.22.0, Admin Console 2.22.0, and Test Accelerator 2.21.0. March 26, 2020 2.21.0 This release of Cloud Accelerator Platform CLI includes the following enhancements: -- Support for SSL certificate verification in Cloud Accelerator Platform CLI. For more information, see Configuring the Cloud Accelerator Platform CLI . -- Initial release of the CLI command reference . February 11, 2020 2.20.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the get-deployment-resource-ids command in Deploy Accelerator. -- Support for the run-infra-default-azurespec and run-infratest-default-awsspec commands in Test Accelerator. December 24, 2019 2.19.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the update-provider command in Test Accelerator. -- New wait parameter in the run-security-test command in Test Accelerator. This parameter displays the status of the security test. -- Fixed an issue with the JSON output of the export-blueprint-environment command in Deploy Accelerator. November 7, 2019 2.18.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the list-groups and add-user-to-group commands in the Admin Console. -- Support for the get-provider command in Test Accelerator. -- Support for Azure Spec in the run-infra-test command in Test Accelerator. -- New wait parameter in the run-infra-test command in Test Accelerator. This parameter displays the status of the infrastructure test. -- Fixed an issue with the JSON output of the get-deployment-input command in Deploy Accelerator. September 27, 2019 2.17.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for storing credentials in an encrypted format in the configuration file that you create for the Cloud Accelerator Platform CLI . -- Ability to use an existing configuration file for the Cloud Accelerator Platform CLI. -- Support for the get-provider , get-connection , share-entity , and get-entity-actions commands in Deploy Accelerator. -- Support for the get-group and get-group-users commands in the Admin Console -- Fixed an issue with the verify-user command in Admin Console. September 6, 2019 2.16.0 This release of the Cloud Accelerator Platform CLI includes a few bug fixes. August 8, 2019 2.15.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for creating users and groups in the Admin Console ( create-user and create-group commands). -- Parameter updates for many existing Test Accelerator CLIs. To get a list of parameters for a Test Accelerator CLI, run the following command: rean-test commandname --help February 8, 2019 0.1.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Cloud Accelerator Platform CLI installer is available on Cloud Accelerator Platform production Artifactory. -- Support for editing providers in Deploy Accelerator. January 11, 2019 0.0.9 This release of the Cloud Accelerator Platform CLI supports blueprint aggregation in Managed Cloud Services. December 12, 2018 0.0.8 This release of the Cloud Accelerator Platform CLI supports the get-terraform-code command in Deploy Accelerator. October 31, 2018 0.0.7 This release supports the Cloud Accelerator Platform CLI on the Windows operating system. October 19, 2018 0.0.6 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- You can now run the infrastructure test with awspec and add configuration properties and tags in Test Accelerator. -- The get-environment and export-environment commands in Deploy Accelerator are now supported. September 10, 2018 0.0.5 This release of the Cloud Accelerator Platform CLI enables you to save output from accelerators to your local machine. August 30, 2018 0.0.4 This release of the Cloud Accelerator Platform CLI enables you to create multiple providers in Deploy Accelerator. August 7, 2018 0.0.1 Initial release of the Cloud Accelerator Platform CLI. Dashboard Accelerator release updates (Discontinued Support) \u00b6 The following table lists the enhancements that were delivered in each release of Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator). The Dashboard Accelerator is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements July 16, 2020 2.6.0 This release of Dashboard Accelerator includes bug fixes related to the CFO dashboard. June 11, 2020 2.5.0 This release of Dashboard Accelerator allows you to view and filter data in the CFO Dashboard visualizations according to the AWS Account name. May 15, 2020 2.4.0 This release of Dashboard Accelerator includes: - Migration from detailed billing report to AWS Cost and Usage reports for generating visualizations in the CFO dashboard. - Use of Cost and Usage reports for Node Usage Dashboard calculation. - Bug fixes related to performance issues. February 24, 2020 2.3.0 This release of Dashboard Accelerator includes bug fixes related to Platform Node Calculation, CFO, Pipeline, and CISO dashboards. December 18, 2019 2.2.0 This release of Dashboard Accelerator includes: -- Branding changes to Hitachi Cloud Accelerator Platform. -- Addition of the HCAP Node Usage dashboard for calculating node usage of cloud compute services. November 19, 2019 2.1.0 This release of Dashboard Accelerator includes bug fixes related to ALB, ELB, VPC Flow Logs, and CloudFront dashboards. October 23, 2019 2.0.0 This release of Dashboard Accelerator includes: -- Support for the Hong Kong region in AWS. -- Use of AWS ELK stack instead of X-Pack. August 13, 2019 1.9.1 This release of Dashboard Accelerator includes: - New visualizations added in VPC Flow Log dashboard. - Minor bug fixes in CISO dashboard. - Use of X-Pack license disabled for ELK stack in Dashboard Accelerator. - The CFO dashboard logs generated in the EC2 instance will be available in the CloudWatch logs. July 19, 2019 1.9.0 This release of Dashboard Accelerator includes: - CloudFront dashboard. - VPC Flow Logs dashboard May 27, 2019 1.8.0 This release of Dashboard Accelerator includes support for sending violations summary notification to Managed Cloud users. April 30, 2019 1.7.0 This release of Dashboard Accelerator includes the following enhancements: - Visualizations for CFO dashboard are updated. - Minor bug fixes in PMO dashboards. January 28, 2019 1.6.3 This release of Dashboard Accelerator includes the following enhancements: - CISO dashboards and Discovery dashboard are available in offline mode. - Minor updates in CFO dashboard and bug fixes. December 21, 2018 1.6.2 This release of Dashboard Accelerator includes the following enhancements: - CFO dashboard is updated to show data for only the previous month or earlier. - Minor updates in PMO dashboard and bug fixes. November 28, 2018 1.6.1 This release of Dashboard Accelerator includes a new Syslog dashboard under the Operations category. October 17, 2018 1.6.0 This release of Dashboard Accelerator includes the following enhancements: - Two new dashboards, ALB Logs Dashboard and ELB Logs Dashboard, are now available under the Operations category. - Two new columns, Project ID and Project Name , have been added to the PMO Utilization - CFO Compliance table in the PMO Utilization Dashboard. September 24, 2018 1.5.0 This release of Dashboard Accelerator includes updated visualizations for Repair Dashboard. April 28, 2018 1.2.0 This release of Dashboard Accelerator includes the following enhancements: - Dashboard Accelerator has a new Login page. - A new Administer section is added to the Dashboard Accelerator documentation. - An admin user in Dashboard Accelerator can now manage aliases. April 4, 2018 1.1.0 This release of Dashboard Accelerator includes updated visualizations for the following dashboards: -- CTO dashboard -- CISO dashboards -- PMO dashboards -- Discovery dashboard -- REAN Managed Cloud dashboard -- Monitoring dashboard Managed Cloud Services release updates (Discontinued Support) \u00b6 The following table lists the features and updates that were delivered in each release of Managed Cloud Services from Hitachi Vantara. Managed Cloud Services is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements July 27, 2020 1.3.0 This release of Managed Cloud Services includes support for 8 additional AWS rules. Managed Cloud Services now supports 23 AWS rules and 2 Azure rules. July 21, 2020 1.2.0 This release of Managed Cloud Services includes the following enhancements: - Support for 10 additional AWS rules. - Action Scheduling, Consolidation, and Cancellation feature. - Support for receiving email notifications through the SMTP service. - Integration with Dashboard Accelerator. July 3, 2020 1.1.0 This release of Managed Cloud Services includes the following enhancements: - Support for 2 additional AWS rules. - Support for 2 Azure rules. June 11, 2020 1.0.0 This release of Managed Cloud Services includes the following enhancements: - Support for 3 AWS rules. - Support for the Whitelisting capability and Action Failure notification. - Support for Common Data Platform (CDP). Managed Cloud Services now communicates with the CDP API and fetches the cloud accounts data from CDP, which was previously fetched from an AWS service. Managed Cloud Services Legacy release updates (Discontinued Support) \u00b6 The following table lists the features and updates that were delivered in each release of Managed Cloud Services from Hitachi Vantara (Legacy). Managed Cloud Services (Legacy) is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements June 25, 2019 1.2.03 This release of the Managed Cloud Services includes the support for an automatic on-demand scaling for DynamoDB tables. April 3, 2019 1.2.02 This release of Managed Cloud Services includes the following enhancements: - You can now whitelist the resources from the Managed Cloud rules. - Support for CloudWatch periodic rules is added in this release. November 6, 2018 1.1.06 This release of the Managed Cloud Services includes the support for CloudWatch events. May 23, 2018 1.1.0 This release of the Managed Cloud Services includes the integration of Datadog for healing. March 27, 2018 1.0.05 This release of Managed Cloud Services includes the following enhancements: - A new rule Unencrypted snapshots is added. - S3 exposed buckets and Delete unused EBS rules are updated. March 15, 2018 1.0.02 This release of Managed Cloud Services includes the following enhancements: - A new IAM user will get the notification for Check IAM MFA rule. - Managed Cloud Services now supports the SMTP. - Initial release of the Managed Cloud Services documentation. February 18, 2018 1.0.0 This release of Managed Cloud Services includes the following rules: - EC2 required tags - EC2 unused EIP - IAM MFA enabled - RDS required tags - S3 exposed buckets - Delete unused EBS - Delete unused ELB - IAM key age","title":"Archive"},{"location":"platform-common/whatsNewArchive/#whats-new-older-releases","text":"This topic is an archive of the older releases of Hitachi Cloud Accelerator Platform. It lists the Cloud Accelerator Platform omnibus releases and individual accelerator releases. You can view the enhancements that were delivered in older releases of the accelerators and the Cloud Accelerator Platform CLI. To view the enhancements delivered in the six most-recent Cloud Accelerator Platform omnibus releases, see What's new .","title":"What's new (older releases)"},{"location":"platform-common/whatsNewArchive/#contents","text":"Admin Console and Platform Common Serivces release updates (Older Releases) Deploy Accelerator release updates (Older Releases) Migrate Accelerator release updates (Older Releases) Test Accelerator release updates (Older Releases) Assess Accelerator release updates (Older Releases) Cloud Accelerator Platform CLI release updates (Older Releases) Dashboard Accelerator release updates (Discontinued Support) Managed Cloud Services release updates (Discontinued Support) Managed Cloud Services Legacy release updates (Discontinued Support)","title":"Contents"},{"location":"platform-common/whatsNewArchive/#admin-console-and-platform-common-services-release-updates-older-releases","text":"The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Admin Console (Admin Console) and Platform Common Services. For more information, see Create and access Cloud Accelerator Platform account and Administer Cloud Accelerator Platform . Release date Version Enhancements March 31, 2020 2.22.0 This release includes a few bug fixes. March 5, 2020 2.21.0 This release includes a new group, VIEW_ALL_USER'S_ENTITIES , in the Admin Console. This group allows users to view the environments and deployments of all users in Deploy Accelerator. For more information, see Default groups and Accessing environments and deployments of other users . February 7, 2020 2.20.0 This release enables administrators to customize the password policy for Cloud Accelerator Platform user accounts. December 24, 2019 2.19.0 This release includes a few bug fixes. November 21, 2019 2.18.0 This release includes a few bug fixes. September 26, 2019 2.17.0 This release includes a few bug fixes. September 9, 2019 2.16.0 This release includes a few bug fixes. August 8, 2019 2.15.0 This release includes a few bug fixes. July 16, 2019 2.14.1 This release includes a few bug fixes related to verifying user accounts. July 1, 2019 2.14.0 This release enables Cloud Accelerator Platform administrators to share an existing user's Deploy Accelerator entities with a policy-based group while disabling that user in the Admin Console. Other users in the selected group can then access these shared entities. June 3, 2019 2.13.0 This release includes a few bug fixes. May 10, 2019 2.12.0 This release includes a few bug fixes. March 15, 2019 2.11.0 This release includes the following enhancements: -- When new users verify their own Cloud Accelerator Platform account, an email notification is sent to the default Cloud Accelerator Platform administrator. -- In the Admin Console, the Cloud Accelerator Platform administrators can reset the password of existing users. February 15, 2019 2.10.0 This release includes a few bug fixes. January 21, 2019 2.9.0 This release includes a few bug fixes. December 12, 2018 2.8.0 This release includes a few bug fixes. November 29, 2018 2.7.1 This release includes a few bug fixes. November 19, 2018 2.7.0 This release includes a few bug fixes. September 26, 2018 2.6.2 This release includes a few bug fixes. September 18, 2018 2.6.0 This release includes a few bug fixes. August 6, 2018 2.4.0 This release includes a few bug fixes. July 23, 2018 2.1.0 This release includes a few bug fixes. June 20, 2018 1.7.3 This release enables administrators to integrate Cloud Accelerator Platform with Microsoft Active Directory and manage users and user-group authorization in Active Directory. May 23, 2018 1.6.2 This release enables Cloud Accelerator Platform administrators to view details of the policies that are attached to groups in the Admin Console. April 27, 2018 1.6.1 This release includes a few bug fixes. April 19, 2018 1.6.0 This release includes a few bug fixes. March 28, 2018 1.5.0 This release includes a few bug fixes. March 9, 2018 1.4.0 This release includes a few bug fixes. February 27, 2018 1.3.0 This release includes a few bug fixes. January 24, 2018 1.2.0 This release includes a few bug fixes. January 2, 2018 1.1.0 This release includes a few bug fixes. November 6, 2017 1.0.0 This release enables Cloud Accelerator Platform administrators to create and manage users as well as groups, and attach policies to groups in the Admin Console. September 20, 2017 0.14.0 This release includes a few bug fixes. September 11, 2017 0.13.1 This release includes a few bug fixes. September 7, 2017 0.13.0 This release includes a few bug fixes. August 14, 2017 0.12.1 This release includes a few bug fixes. August 4, 2017 0.12.0 This release includes a few bug fixes. July 21, 2017 0.11.0 This release includes a few bug fixes. June 16, 2017 0.10.2 This release includes a few bug fixes. May 15, 2017 0.10.0 This release includes a few bug fixes. April 27, 2017 0.9.0 This release includes a few bug fixes. March 29, 2017 0.8.0 This release includes the following enhancements: -- The first user who creates an account is assigned the Admin role. -- Administrators can assign roles to other users in the Admin Console.","title":"Admin Console and Platform Common Services release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#deploy-accelerator-release-updates-older-releases","text":"The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). For information about using Deploy Accelerator, see Deploy and manage environments . Release date Version Enhancements March 31, 2020 2.22.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to sync blueprints in the Blueprint Gallery with the latest updates in the Artifactory. Deploy Accelerator also automatically syncs the blueprint metadata every 30 minutes by default. However, administrators can modify the sync interval based on requirements. -- Metadata.yml now supports a relative path for the image that is shown for a blueprint in the Blueprint Gallery. -- New API to import blueprints from the Artifactory . The new importBlueprintFromArtifactory API allows you to specify the name and version of the blueprint you want to import. You can also choose to specify a prefix for the names of all imported environment, and a group with which to share the imported environments (with the View and Deploy permissions). March 5, 2020 2.21.0 This release of Deploy Accelerator includes the ability to rename environments . February 7, 2020 2.20.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to upgrade the AWS provider version that was used to create an environment to a newer AWS provider version that is supported. -- Support for provisioning Ansible Solo packages on Windows instances. -- Additional branding updates: In the Deploy Accelerator URL that is shown in the browser, reandeploy has been replaced with hcapdeploy . December 24, 2019 2.19.0 This release of Deploy Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Deploy has been renamed as Deploy Accelerator. -- Support for using an existing Terraform Module in an environment . You can add the new Terraform Module resource to an environment and specify the source from where the existing Terraform module can be downloaded. You can also use the output of this Terraform module in other resources within the environment. -- While sharing various entities (such as environments , deployments , providers , and connections ), users can now view only the groups to which they belong. Only administrators can view all groups. -- The /env/userMap API in Deploy Accelerator is no longer available. November 21, 2019 2.18.0 This release of Deploy Accelerator includes the following new features: -- Support for Google Cloud Platform resources. -- Support for adding and using a user-defined Ansible Solo package. This release does not support the provisioning of Ansible Solo packages on Windows instances. September 26, 2019 2.17.0 This release of Deploy Accelerator includes the following new features: -- Support for the Hong Kong region. -- Support for the use of AWS provider credentials to create Helm and Kubernetes (EKS) providers. September 9, 2019 2.16.0 This release of Deploy Accelerator includes the following new features: -- Providers added for Azure Stack, Azure Active Directory, and vSphere NSX-T. -- Helm repository data resource of the Helm provider can be defined while configuring Deploy Accelerator. -- Resources available for the vSphere provider. August 8, 2019 2.15.0 This release of Deploy Accelerator includes the following enhancements: -- Ability to add blueprints in the Blueprint Gallery . Multiple versions of a blueprint can also now be made available in the Blueprint Gallery. -- Ability to compare environment configuration-level differences , such as providers, connections, owners, custom tags, and more. -- Support for two new providers, Kubernetes and Helm. -- Updates to the supported format of environment versions. You can now append the xx.xx.xx format with a dash (-) followed by a custom alphanumeric value based on your requirements. For example: 01.02.00-updated The value that you specify for the environment version must not exceed 40 characters and must not contain a colon (:). -- Additional support for multiple Microsoft Azure resources. July 16, 2019 2.14.1 This release of Deploy Accelerator includes a few bug fixes related to importing blueprints. July 1, 2019 2.14.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports version 1.30.1 of the Terraform provider for Azure. -- Additional support for multiple Microsoft Azure resources. June 3, 2019 2.13.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now provides support for multiple recipes in Chef Solo packages. While adding a user-defined Chef Solo package, all recipes that are defined in the selected Chef cookbook become available in Deploy Accelerator. While adding the Chef Solo package to a compute resource in an environment, you can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. -- Starting with this release, the Global Connection check box is no longer available while creating connections. Instead, you can now share your connections with specific groups and assign View, Edit, and Delete permissions. Users who are members of these groups can now use the same connection for instances in their environments. If you have upgraded to this release, existing Global Connections continue to appear in the list of connections. May 10, 2019 2.12.0 This release of Deploy Accelerator provides support for roles and recipes in Chef Server packages. This enhancement includes the following updates: -- When the administrator registers a Chef Server in Deploy Accelerator, both cookbooks and roles from the specified Chef Server organization become available in Deploy Accelerator. -- While creating a Chef Server environment, users can add both roles and packages (or cookbooks) to a compute resource. -- While adding a Chef Server package, users can select multiple recipes in the order in which they must be run on the resource. If no recipe is selected, Deploy Accelerator runs the default recipe defined in the cookbook. March 15, 2019 2.11.0 This release of Deploy Accelerator includes the following enhancements: -- You can view the input variables that were passed in each deployment of an environment. -- On the Packages tab in the left panel, you can move your mouse over a package to view the description. -- If you click the Environment Dependency icon when a deployment is selected on the canvas, you can see the parent deployments and the status of these deployments. -- The Instance resource has been renamed as EC2 Instance . -- Deploy Accelerator supports multiple AWS StorageGateway resources. -- On the Environment List page or in the Search Environments box at the top, you can now see the owner of each environment. -- Managed packages are now available by default after Deploy Accelerator is successfully deployed. The UPDATE button, which was earlier used to synchronize the managed packages, no longer appear on the Package List page. February 15, 2019 2.10.0 This release of Deploy Accelerator includes the following enhancements: -- Support for two new AWS resources, Dynamodb Table and Dynamodb Table Item , and a new data source, Dynamodb Table . -- Administrators can configure Deploy Accelerator to send emails to users from an SMTP server on which authentication is not enabled. -- The Resource Logs window for a deployment displays timestamps. January 21, 2019 2.9.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports Mozilla Firefox, Apple Safari, Microsoft Internet Explorer, and Microsoft Edge in addition to Google Chrome. For a complete list of supported browser versions, see Supported browsers . -- Starting with this release, you can share each deployment of an environment with specific groups and assign View, Redeploy, Destroy, and Stop permissions based on your requirements. If you have upgraded to this release, all deployments of environments that were shared before the upgrade are automatically shared with the same groups with which the environments are shared. In addition, if you later share any environment that was created before the upgrade, all its deployments are also automatically shared with the selected groups. The permissions given for the deployments are based on the permissions assigned to the environments. -- Added support for multiple Microsoft Azure resources. -- The Blueprint View icon has been renamed as Environment Dependency View. December 12, 2018 2.8.0 This release of Deploy Accelerator includes the following enhancements: -- Starting with this release, the Global Provider check box is no longer available while creating providers. Instead, you can now share your providers with specific groups and assign View, Edit, Share, and Delete permissions. Users who are members of these groups can now use the same provider for deploying an environment. If you have upgraded to this release, existing Global Providers continue to appear in the list of providers. -- The SSH Keygen resource has been deprecated and no longer appears in the Resources panel. To dynamically generate a key pair while deploying an environment, you can now use the TLS Private Key Generator resource. If you have upgraded to this release, any existing environments that used the SSH Keygen resource continue to be deployed without any errors. November 29, 2018 2.7.1 This release of Deploy Accelerator enables administrators to configure multiple environment package types (Chef Solo, Chef Server, and Puppet) at the same time. By default, Chef Solo and Chef Server are configured as the environment package types. This enhancement also includes the following updates: -- While creating an environment , you can select one of the supported environment package types. For this environment, the Packages panel displays packages of the selected environment package type only. -- The Package List page displays packages of all the supported environment package types. -- The dnow.package.configuration_type property has been deprecated. To configure the environment package types , administrators must now use the following property: com.reancloud.platform.deploy.environment.configuration_types November 19, 2018 2.7.0 This release supports Deploy Accelerator in an offline mode. In the offline mode, Deploy Accelerator does not require Internet connectivity to perform various operations. All required artifacts are accessed from the artifactory that is configured while deploying Deploy Accelerator. New configuration properties have been added to configure the artifactory. Also, to support the adding of user-defined packages in the offline mode, the artifactory option is now available in the Repository Type drop-down. September 26, 2018 2.6.2 This release of Deploy Accelerator adds support for the following Amazon Web Services (AWS) resources: -- IAM User Login profile -- IAM Group membership -- EC2 Launch template September 18, 2018 2.6.0 This release of Deploy Accelerator includes the following enhancements: -- The Blueprint Gallery enables you to view out-of-the-box blueprints that are available in Deploy Accelerator. These blueprints are templates of industry standard cloud or IT infrastructure architectures. You can create new environments from out-of-the-box blueprints . -- You can now view the list of deployments across all environment versions . -- The Deploy Accelerator API documentation has been released. August 6, 2018 2.4.0 This release of Deploy Accelerator enables you to view the plan for a new deployment of an environment that has existing deployments. July 23, 2018 2.1.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now enables all users to create global providers and connections that are available to all other Deploy Accelerator users. -- The enhanced Package UI enables you to more easily create new packages and add new versions of existing packages . June 20, 2018 1.7.3 This release of Deploy Accelerator includes the following enhancements: -- You can compare differences between two environments in Deploy Accelerator. You can view a list of resources that are added, removed, or edited in the target environment. In the case of edited resources, you can also view differences in attribute values and packages. -- The Parent Deployment Mapping field in the Review and Deploy and Plan windows now displays drop-down lists for each Depends On resource. It lists deployments of only the parent environment version that is selected in the Depends On resource. -- Deploy Accelerator now supports HashiCorp Terraform data sources. To add a data source to an environment, you can drag that data source from the Data Sources section in Resources tab in the left panel to the canvas. -- The Create New Version and Release Version options have been moved to the version drop-down in the icon bar on the Home page. Also, you can now view all deployments of an environment version by clicking on the deployment name tab on the canvas. May 23, 2018 1.6.2 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports FIPS compliant encryption (AES-256) to encrypt sensitive data such as access key and secret key. -- You can now select a specific parent environment version in the Depends On resource of the child environment. April 27, 2018 1.6.1 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports version 0.11.6 of HashiCorp Terraform. -- Deploy Accelerator enables you to use NTLM authentication, instead of basic authentication, in WinRM connections. -- Cloud Accelerator Platform now uses Amazon CloudWatch Logs for all container logging, which enables you to view different logs from your container instances in Amazon Management Console. April 19, 2018 1.6.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator supports the configuration of CA certificates while deploying Cloud Accelerator Platform . -- Deploy Accelerator enables you to simultaneously share multiple packages with one or more groups when Chef Server is configured as the configuration tool. -- You can specify the parent environment version in the Depends On resource of the child environment. Deploy Accelerator exports the specified version of the parent environment while exporting the child environment . March 28, 2018 1.5.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports the versioning of environments. While creating , copying , and importing environments, you have to specify a unique environment version in the xx.xx.xx format. -- Deploy Accelerator also supports multiple deployments of the same environment version. You can define input variables while starting a new deployment or redeploying an existing deployment . -- You can choose to release an environment version after it has been finalized or deployed in production. To make any further changes to this released version, you have to create a new environment version . -- Deploy Accelerator enables you to plan or redeploy an existing deployment based on a different environment version. -- You can view all deployments for an environment version, along with their status: DEPLOYING, DEPLOYED, or FAILED. -- The More menu in the icon bar on the Home page displays three new options: Create New Version , Release Version , and See Deployments . A new version drop-down is also available to navigate between the different versions of an environment. March 9, 2018 1.4.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator supports the automatic creation of groups when Deploy Accelerator administrators add a Chef Server in Deploy Accelerator. Administrators must then share Chef Server managed packages with these groups and assign appropriate permissions. -- Deploy Accelerator now supports version 0.11.3 of HashiCorp Terraform. February 27, 2018 1.3.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator enables you to control access to AWS resources in an environment by using custom tags. You can now configure a custom tag , which contains a set of AWS key-pair tags, and then select that custom tag for an environment. Deploy Accelerator attaches the custom tag to all resources in that environment. -- If Chef Server is configured as the configuration tool for packages, Deploy Accelerator administrators can control the users who have access to the Chef Server managed packages. Administrators can share these packages with specific groups and assign appropriate permissions. January 24, 2018 1.2.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports a new AWS resource: IAM User . -- Deploy Accelerator supports new instance types for the Instance and RDS resources. -- Security enhancements to ensure that environments are more securely deployed. The pre-script, post-script, and local-exec scripts are run in a separate Docker container and do not affect the Deploy Accelerator instance. January 2, 2018 1.1.0 This release of Deploy Accelerator introduces a new icon ( ) for the Depends On resource in the Deploy Accelerator UI. November 6, 2017 1.0.0 This release of Deploy Accelerator includes the following enhancements: -- Containerized deployment of Cloud Accelerator Platform. -- Deploy Accelerator supports the environment collaboration feature, which allow multiple users to perform different tasks on the environment based on the permissions. September 20, 2017 0.14.0 This release of Deploy Accelerator includes the following enhancements: -- You can assess cost of the environment at any point of time. -- Deploy Accelerator now supports edit environment feature. -- Deploy Accelerator supports the following AWS resources: AWS Elasticsearch domain and AWS Elasticsearch domain policy . September 11, 2017 0.13.1 This release of Deploy Accelerator now supports a new provider, OPC (Oracle Public Cloud). September 7, 2017 0.13.0 This release of Deploy Accelerator includes the following enhancements: -- AWS Region can be specified while creating a new environment. -- Deploy Accelerator now supports the Import a resource feature, which helps in importing a resource from the provider-end to Deploy Accelerator and managing it. August 14, 2017 0.12.1 This release of Deploy Accelerator adds support for the following Amazon Web Services (AWS) Web Application Firewall (WAF) resources: -- WAF Byte Match Set -- WAF IP Addresses -- WAF Rule -- WAF Size Constraint Set -- WAF Web ACL -- WAF SQL Injection match Set -- WAF XSS Match Set -- WAF Regional Byte Match Set -- WAF Regional Ip Set August 4, 2017 0.12.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator no longer supports the process of adding Ansible packages to the /opt/dnow/data/ansible-packages folder, updating the packages.json file, and restarting the rean-deploy service. -- Administrators must add user-defined Ansible packages by using the Ansible Packages option. -- The Ansible Packages option now appears when you set Ansible as the supported configuration tool for adding packages . -- The hide_ansible property in the dnow.properties file has been deprecated. -- User-defined Ansible packages are now stored in the database. If you have upgraded to this release, Deploy Accelerator automatically migrates package definitions of existing user-defined Ansible packages to the database. July 21, 2017 0.11.0 This release of Deploy Accelerator includes the following enhancements: -- The Deploy Accelerator UI has been redesigned and includes new icons to create and manage environments. -- To access the Providers , Connections , Environments , and Manage Users options, you now have to click the new icon in the top-right corner. -- While configuring a provider , you can use the Instance Profile and Assume Role (Cross-account Access) methods to specify authentication details of an AWS account. -- The Deploy Accelerator CLI tool supports actions such as deploy, destroy, and export environments. -- Deploy Accelerator supports use of the aws_caller_identity , aws_region , aws_partition , aws_availability_zones , and aws_elb_service_account data sources in blueprints. -- You can copy one or more existing resources from the same environment or a different environment. -- For the local-exec package, you can now specify whether the script must be run after the resource is created or before it is destroyed. -- Deploy Accelerator now takes a backup of your database before starting the upgrade process. -- Managed packages are no longer available by default and need to be synchronized with your instance of Deploy Accelerator. -- Deploy Accelerator supports package versioning for managed and user-defined packages. If you have upgraded to this release, Deploy Accelerator automatically sets the version of all existing packages to 1.0. -- Managed and user-defined packages are now stored in the database instead of JSON files. If you have upgraded to this release, you must reupload your existing user-defined packages. -- Deploy Accelerator now supports only one configuration tool (Chef, Ansible, Puppet, or Chef Server) at a time. The dnow.packages.files property has been deprecated. -- Administrators can use the new Packages option to add and release user-defined packages. -- Administrators can now add Chef Server managed packages in Deploy Accelerator . June 16, 2017 0.10.2 This release of Deploy Accelerator provides a fix for the following issues: -- Packages could not be successfully provisioned on Windows instances. -- The plan could not be successfully generated for deployed environments. May 15, 2017 0.10.0 This release of Deploy Accelerator includes the following enhancements: -- The JBoss package is now available in Deploy Accelerator. -- The new Plan button enables you to view the deployment plan for an environment. However, you can view the plan for child environments only if their parent environments are already deployed. -- The Deploy Accelerator help website is now available. To access this help website, you can click the Help icon ( ). -- The issue with deleting environments has been fixed. April 27, 2017 0.9.0 This release of Deploy Accelerator includes the following enhancements: -- DeployNow has been renamed as REAN Deploy. The new name and logo ( ) appear in the Deploy Accelerator UI. -- To see Deploy Accelerator version, click the User icon ( ) in the top-right corner and then About . -- The new SSH Keygen resource enables you to dynamically generate a key pair (private and public keys) for an instance while deploying an environment. -- The new Use Custom Connection attribute of an instance, along with the Key Pair standard resource, enables you to associate a dynamically-generated key pair with the instance. -- The Count attribute is available for all standard resources and enables you to define the number of identical resources that Deploy Accelerator must create. -- You have to provide an additional confirmation to destroy an environment. March 29, 2017 0.8.0 This release of Deploy Accelerator includes the following enhancements: -- Deploy Accelerator now supports the Spot Instance resource. -- The default notification email address in Deploy Accelerator has changed to product-info@reancloud.com . -- Administrators can configure the access of Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator) from the Deploy Accelerator UI.","title":"Deploy Accelerator release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#migrate-accelerator-release-updates-older-releases","text":"The following table lists the enhancements that were delivered in older releases of Hitachi Cloud Accelerator Platform - Migrate (Migrate Accelerator). For information about using Migrate Accelerator, see Discover and migrate resources . Release date Version Enhancements April 1, 2020 2.9.0 This release of Migrate Accelerator includes the following enhancements: -- A status banner is shown for RISC discovery jobs. This banner also displays timestamps for asset data retrieval and lists the actions that might be impacted while data is being incrementally retrieved. -- Additional branding updates: In the Migrate Accelerator URL that is shown in the browser, reanmigrate has been replaced with hcapmigrate . March 5, 2020 2.8.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to manually upload connectivity data for RISC_EXPORTED_CSV discovery jobs. -- In the case of RISC discovery jobs, Migrate Accelerator now fetches data incrementally from CloudScape. While the data retrieval is still in progress, you can see the already retrieved assets in the Migrate Accelerator UI. January 22, 2020 2.7.0 This release of Migrate Accelerator includes the following enhancements: -- Enhancements to the Migrate Accelerator logs that are generated in CloudWatch. -- RISC discovery jobs in Migrate Accelerator can now retrieve servers based on their device type or tags applied to them in RISC CloudScape. While deploying Migrate Accelerator, administrators can specify the device type or tag based on which RISC discovery jobs must retrieve servers. If both device type and tag are specified, servers will be retrieved based on the configured tag. December 31, 2019 2.6.0 This release of Migrate Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. Also, REAN Migrate has been renamed as Migrate Accelerator. -- Ability to manually upload connectivity data for RISC discovery jobs. -- Updates to the UI for creating new application groups and regrouping servers in existing application groups . December 11, 2019 2.5.0 This release of Migrate Accelerator includes the following enhancements: -- Availability of generic and AWS-specific build plans. By default, Migrate Accelerator generates the generic build plan for all application groups. However, this configuration can be modified while deploying Migrate Accelerator. -- Updates to the page that appears before the first discovery job is created. October 25, 2019 2.4.0 This release of Migrate Accelerator includes support for adding custom metadata for Providers . September 26, 2019 2.3.0 This release of Migrate Accelerator includes the following enhancements: -- Ability to create providers specific to Migrate Accelerator. This allows users to create accelerator specific providers from the Admin Console. -- Support for attribute-based access control for Migrate Accelerator providers. This allows users to share their Migrate Accelerator providers with other user-groups. September 3, 2019 2.2.0 This release of Migrate Accelerator includes a few bug fixes. August 8, 2019 2.1.0 This release of Migrate Accelerator enables users to create and manage multiple providers. While creating a migration job, users can select the appropriate provider. The selected provider is used to create images of the migrated servers in AWS. The multiple-provider support enhancement includes the following updates: -- Migrate Accelerator users can use the Admin Console to configure providers , which contains the cloud provider and credentials for accessing the account in which the server images must be created. -- Two new groups, PROVIDER_USER and PROVIDER_VIEWER , are now available in the Admin Console. The PROVIDER_USER group enables users to create and edit providers while the PROVIDER_VIEWER group enables users to only view providers that they have previously created. -- While creating a migration job , users have to select the provider that Migrate Accelerator must use to create server images. The selected provider must contain credentials that match the credentials that are specified in the CloudEndure Live Migration project. July 1, 2019 2.0.0 This release of Migrate Accelerator fixes a few issues with CSV discovery and provides support for multiple users who can simultaneously log on to the UI and then create and manage their own discovery jobs, connections, migration credentials, and migration jobs. The multiple-user support enhancement includes the following updates: -- The login page of Migrate Accelerator has changed to the Cloud Accelerator Platform login page that is also used to sign in to the Admin Console and other accelerators (Deploy Accelerator, Test Accelerator, and Assess Accelerator). -- Users can create their own account from the login page. However, they can perform various actions in Migrate Accelerator only after the Administrator assigns them to the appropriate groups. -- Administrators can use the Admin Console to manage users for Migrate Accelerator. In this release, the Admin Console is also deployed along with Migrate Accelerator on the same server. -- The MIGRATE_USER and MIGRATE_VIEWER policies are available in the Admin Console to manage access for the Migrate Accelerator users. The MIGRATE_USER policy enables users to perform all actions in Migrate Accelerator while the MIGRATE_VIEWER policy enables users to only view data in Migrate Accelerator. -- All users can currently view only the jobs, connections, and migration credentials that they have created. Note: This release of Migrate Accelerator does not support the Admin Console features of configuring group-level sharing and integrating with Active Directory. Also, when users are disabled, the Migrate Accelerator entities such as discovery jobs, migration jobs, connections, and migration connections, are not shared with the selected group. May 31, 2019 1.2.0 This release of Migrate Accelerator includes the following enhancements: -- Containerized deployment of Migrate Accelerator. For information about deploying Migrate Accelerator, contact the Cloud Accelerator Platform team. -- Support for Federal Information Processing Standard (FIPS) compliant algorithm to encrypt sensitive data such as passwords and AWS credentials. -- Migrate Accelerator now uses Amazon CloudWatch Logs for all container logging. You can view these logs in Amazon Management Console. March 29, 2019 1.0.1 This release of Migrate Accelerator provides a fix for the issue with using CA certificates to connect to the Windows servers that you want to migrate. March 15, 2019 1.0.0 This release of Migrate Accelerator includes the following enhancements: -- The procedure to deploy Migrate Accelerator has been updated. -- While creating a discovery job , you can select RISC_EXPORTED_DISCOVERY as the Discovery Tool. Use this option if you have exported the discovered data from RISC Networks CloudScape in a CSV format. -- While creating a migration job , you can select two different types of connections (SSH and WinRM) based on the operating system of the servers in the application group that you want to migrate. -- You can download a build plan for each application group. August 18, 2017 0.12.1 Initial release of the Migrate Accelerator documentation.","title":"Migrate Accelerator release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#test-accelerator-release-updates-older-releases","text":"The following table lists the features and updates that were delivered in older releases of Hitachi Cloud Accelerator Platform - Test (Test Accelerator). For information about using Test Accelerator, see Run tests . Release date Version Enhancements April 1, 2020 2.21.0 This release of Test Accelerator includes bug fixes and the following enhancement. The Test Accelerator bootstrap deployment process now enables you to configure Artifactory details. If these details are not configured during deployment, the administrator must configure the Artifactory properties (URL, user name, API key, and Miscellaneous repository) by using the Configuration option in the Test Accelerator UI. February 12, 2020 2.20.0 This release of Test Accelerator includes the following enhancements: -- The existing reantest-user and reantest-view groups have been renamed as hcaptest-user and hcaptest-view . In case of an upgrade to version 2.20.0, you can see both new ( hcaptest-user and hcaptest-view ) and existing ( reantest-user and reantest-view ) groups. The administrator must manually move users from the existing groups to the new groups and then delete the old groups. All future updates will be available in the new groups. -- Additional branding updates: In the Test Accelerator URL that is shown in the browser, reantest has been replaced with hcaptest . December 24, 2019 2.19.0 This release of Test Accelerator includes the following enhancements: -- Branding changes to Hitachi Cloud Accelerator Platform. REAN Test has been renamed as Test Accelerator. -- Infrastructure test job reports can be downloaded in the Excel and HTML formats. -- The infrastructure test job list can be filtered to show only scheduled jobs, only jobs that you have created, or only scheduled jobs that you have created. -- The output section of the JSON file that contains test data for an infrastructure test with default Azure Spec code , now also supports names of the Azure resources. Previously, you could only specify IDs of the Azure resources. November 7, 2019 2.18.0 This release of Test Accelerator includes the following enhancements: -- Support for new AWS Resources. -- Support for new Azure Resources . October 10, 2019 2.17.1 This release of Test Accelerator includes the following enhancements: -- Login support for running Security test. -- Addition of configuration parameter for login support. September 27, 2019 2.17.0 This release of Test Accelerator includes the following enhancements: -- Support for the Hong Kong region. -- Ability to run infrastructure test with default Azure Spec code . September 6, 2019 2.16.0 This release of Test Accelerator includes the following enhancements: -- Support for running custom Azure infrastructure tests . -- Download of test reports for Cross browser, URL, Scale now, Security and UPA in Excel and .csv format. August 8, 2019 2.15.0 This release of Test Accelerator delivers version 1.4 of the Default AWS Spec code, which is used to run AWS infrastructure tests. The Default AWS Spec code 1.4 provides additional support for the following AWS resources: -- aws_ami -- aws_codecommit_repository -- aws_cloudtrail -- aws_iam_group_policy -- aws_iam_role_policy -- aws_key_pair -- aws_kinesis_stream -- aws_kms_alias -- aws_kms_key -- aws_security_group_rule -- aws_swf_domain -- aws_s3_bucket -- aws_volume_attachment For the complete list of AWS resources that the Default AWS Spec code supports, see Supported resources . July 2, 2019 2.14.0 This release of Test Accelerator includes the following enhancements: -- In the case of a Cross Browser, URL, Security, UI Performance, or Scale test, you can now download a ZIP file that contains the test report for each selected browser. You can also download the summary report of a test job in the Excel format. For more information, see Viewing test results . -- A new reantest-viewer default group for Test Accelerator is now available in the Admin Console. This group allows users to view test jobs, job schedules, provider details, and configuration details in Test Accelerator. Also, the existing REANTestUser default group has been renamed as reantest-user . This group allows users to run and manage test jobs, create and manage job schedules, view provider details, and view configuration details. In case of an upgrade to version 2.14.0, you can see the existing REANTestUser group and the new reantest-user group. The reantest-user group has all the policies of the REANTestUser group along with the additional ManageJobAccess policy. The administrator must either assign appropriate users to the new group or add the ManageJobAccess policy to the existing REANTestUser group. -- Cloud Accelerator Platform administrators can create a new group of Test Accelerator users in the Admin Console and configure share permissions for different types of tests and the Job List page. Administrators can assign the View , Stop , and Schedule permissions for different types of test and the View permission for the Job List page. Based on the assigned permissions, users within a group can view, stop, or schedule each other's test jobs. Similarly, on the Job List page and in the left panel, they can view the test jobs created by other users in their group. However, these users must also be a part of another group that contains appropriate policies to perform the selected actions on the shared resources. For example, to schedule test jobs, users must also be a part of the reantest-user group. June 4, 2019 2.13.0 This release of Test Accelerator includes the following enhancements: -- The default awspec code that Test Accelerator provides to run an AWS infrastructure test has been updated. The resources that you list in your input-output JSON file must now be categorized by the HashiCorp Terraform resource names (for example: aws_instance and aws_subnet) instead of custom categories, such as VPC and EC2. For more information, see running an AWS infrastructure test with default awspec code . In case of an upgrade to version 2.13.0, the Test Accelerator administrator must ensure that the value of the Automation Infra Validation-AWSSpec code property ( Artifactory_Details category) is set to /Infra-validation-awspec/1.3/Infra-validation-awspec-1.3.zip . This action ensures that Test Accelerator is using version 1.3 of the default awspec code to run the AWS infrastructure test. Also, it is recommended that users update any existing input-output JSON files to use the new format. -- The Test Accelerator administrator can now get encrypted Browser AMIs by setting the Fetch encrypted AMIs property to true . In addition, the administrator can get the AMIs based on additional user-defined tags that are specified in the Update AMI with external tags property. Both these properties are available in the Miscellaneous category. For information about updating the configuration properties, see Configuring Test Accelerator . May 14, 2019 2.12.0 This release of Test Accelerator includes the following enhancements: -- To access the Infra Dashboard, you must click Infra ( ) in the left panel on the Test Accelerator Home page. You can no longer see Infra Dashboard when you click the icon in the top-right corner. -- In the Infra Dashboard, for an infrastructure job of the awspec type, a consolidated report is available for each AWS resource type. -- You can now view a list of all the test jobs that you have created, along with the job type, creation date, and status. -- While creating a provider , the administrator has to specify only the Instance Profile Name that must be attached to the instances that are launched for test jobs. In the Infrastructure step of adding a provider, the TestInstance Role and Instance Profile ARN fields have been replaced with the Instance Profile Name field. -- The Test Accelerator documentation now provides instructions for running an AWS infrastructure test with default awspec code . -- The Test Accelerator API documentation has been released. March 18, 2019 2.11.0 This release of Test Accelerator includes the following enhancements: -- While creating URL, UPA, and Security tests, you can now specify a unique name for the test in the Job Name field. -- Test Accelerator supports the running of URL and Automation tests on Microsoft Internet Explorer 11. -- Test Accelerator supports the running of tests on Mozilla Firefox 64 and Google Chrome 64, 68, 69, 70, and 71. February 15, 2019 2.10.0 This release of Test Accelerator includes the following enhancements: -- Test Accelerator supports STIG Red Hat Enterprise Linux 7 AMI to launch instances for running tests. -- The Test Accelerator UI is supported on Mozilla Firefox, Microsoft Internet Explorer, Microsoft Edge, and Apple Safari, in addition to Google Chrome. For a complete list of supported browser versions, see Supported browsers . -- While creating an infrastructure test , you can upload a file that Test Accelerator must extract and provide while running your automation code. -- Test Accelerator supports the running of tests on Mozilla Firefox 63. -- The Test Accelerator administrator can create multiple providers and configure the infrastructure details for each provider. The provider that is configured as the default provider is used to launch the test instances. -- Sample automation code is stored in the Artifactory instead of GitHub. The new Artifactory Details section in the Configuration window enables the Test Accelerator administrator to configure the Artifactory access details and the sample automation codes. -- The Test Accelerator administrator can now click a new Refresh icon on the BROWSERS tab in the Configuration window to get the latest Browser AMIs with specific tags from the AWS account that is configured as the default provider. -- Test Accelerator no longer supports spot instances for running tests. February 27, 2018 1.3.0 In this release, the Test Accelerator UI has been redesigned and includes new icons. January 2, 2018 - This release of Test Accelerator includes the following enhancements: -- SmartBear TestComplete has support of stigged AMI. -- Cookbook has been changed to make it compatible with SmartBear TestComplete. September 12, 2017 - This release of Test Accelerator includes the following enhancements: -- Test Accelerator supports Scheduling test jobs . -- In infrastructure testing, you can stop and rerun the process. -- Test Accelerator now supports Chrome browser version 54-60 in URL, manual, and automated tests.","title":"Test Accelerator release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#assess-accelerator-release-updates-older-releases","text":"The following table lists the features and updates that were delivered in older releases of Hitachi Cloud Accelerator Platform - Assess (Assess Accelerator). For information about using Assess Accelerator, see Run assessment policies . Release date Version Enhancements November 6, 2019 2.18.1 This release of Assess Accelerator includes the following enhancements: -- Over-ride attribute values in the rules by modifying the default values . -- Bug fixes related to security and performance enhancement. September 09, 2019 2.16.0 This release of Assess Accelerator includes now allows you to skip the rules in the assessment policies to create a custom assessment. August 20, 2019 2.14.1 This release of Assess Accelerator includes the following enhancements: -- If an Assess Accelerator user has inherited administrator access from an AWS IAM user, a validation message appears while running an assessment. -- Bug fixes related to errors in reports. February 15, 2019 2.10.0 This release of Assess Accelerator includes the following enhancements: -- The comparison report is enhanced with Dashboard and Summary View sheets. -- Users can re-run the failed jobs from the list of assessment jobs. -- Minor changes in select providers and bug fixes. September 18, 2018 2.6.0 This release of Assess Accelerator includes the following enhancements: -- The new Compare tab enables you to generate, view, and download a report that compares assessment results across multiple jobs or providers. -- You can view the comparison report in the Assess Accelerator UI for a maximum of 6 jobs. If you select more than 6 jobs, you have to download the comparison report. -- The comparison report now includes the raw policy data that you can use to generate your own reports and charts. September 4, 2018 2.5.1 This release of Assess Accelerator includes the following enhancements: -- You can now create an assessment job for multiple providers. -- While creating an assessment job , you can specify an AWS S3 bucket to which REAN Assess can upload assessment reports that are generated in the docx format. -- You can compare assessment results across multiple jobs or providers and also download the comparison report. March 9, 2018 1.4.0 This release of REAN Assess includes the following enhancements: -- You can now use the Instance Profile, Static Credentials with Assume Role, and Instance Profile with Assume role methods while configuring a provider in Assess Accelerator. -- The rean-assess-aws-iam-policy.json custom inline policy has been replaced with the rean-assess-aws-iam-policy-1.json , rean-assess-aws-iam-policy-2.json , and rean-assess-aws-iam-policy-3.json files. The permissions in these files have also been updated. For more information about using these files, see Before you begin . January 2, 2018 1.1.0 This release of Assess Accelerator delivers a new and enhanced UI. The help website has also been updated based on the new UI. August 14, 2017 0.12.1 Starting with this release of Assess Accelerator, the Docker container that is created to run each assessment job is automatically deleted after the job is completed. Administrators can choose to retain the Docker containers by updating the value of the terminate.container.on.completion property to false in the assessnow.properties file. Note: If required, you must manually delete Docker containers that were created for assessment jobs before this release. June 9, 2017 0.10.1 This release of Assess Accelerator enables users to receive an email notification when an assessment job is completed. While submitting an assessment job, you can enable email notifications for that job and provide your company name and the email address to which the notification must be sent. The email notification is also sent to the registered email address of the user who submitted the assessment job. May 15, 2017 0.10.0 This release of Assess Accelerator delivers the following new templates for assessment policy reports: - Custom_REANAssess_Template - Custom_REANAssess_Template_Without_Appendix These two templates do not include the Current State and Proposed State sections. April 27, 2017 0.9.0 This release of Assess Accelerator includes the following enhancements: - You can now specify a job name each time you run one or more assessment policies for a provider. - On the MY ASSESSMENT tab, jobs that are appear in the Assessment jobs list are sorted based on the time they were started. The most recent job appears at the top of the list.","title":"Assess Accelerator release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#cloud-accelerator-platform-cli-release-updates-older-releases","text":"The following table lists the enhancements that were delivered in older releases of the Hitachi Cloud Accelerator Platform Command Line Interface (Cloud Accelerator Platform CLI). For more information, see see Install and use Cloud Accelerator Platform CLI . Release date Version Enhancements March 31, 2020 2.22.0 This release of Cloud Accelerator Platform CLI does not include any enhancements or bug fixes. Only the supported versions of the Deploy Accelerator and Admin Console SDKs have been updated. This release supports Deploy Accelerator 2.22.0, Admin Console 2.22.0, and Test Accelerator 2.21.0. March 26, 2020 2.21.0 This release of Cloud Accelerator Platform CLI includes the following enhancements: -- Support for SSL certificate verification in Cloud Accelerator Platform CLI. For more information, see Configuring the Cloud Accelerator Platform CLI . -- Initial release of the CLI command reference . February 11, 2020 2.20.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the get-deployment-resource-ids command in Deploy Accelerator. -- Support for the run-infra-default-azurespec and run-infratest-default-awsspec commands in Test Accelerator. December 24, 2019 2.19.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the update-provider command in Test Accelerator. -- New wait parameter in the run-security-test command in Test Accelerator. This parameter displays the status of the security test. -- Fixed an issue with the JSON output of the export-blueprint-environment command in Deploy Accelerator. November 7, 2019 2.18.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for the list-groups and add-user-to-group commands in the Admin Console. -- Support for the get-provider command in Test Accelerator. -- Support for Azure Spec in the run-infra-test command in Test Accelerator. -- New wait parameter in the run-infra-test command in Test Accelerator. This parameter displays the status of the infrastructure test. -- Fixed an issue with the JSON output of the get-deployment-input command in Deploy Accelerator. September 27, 2019 2.17.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for storing credentials in an encrypted format in the configuration file that you create for the Cloud Accelerator Platform CLI . -- Ability to use an existing configuration file for the Cloud Accelerator Platform CLI. -- Support for the get-provider , get-connection , share-entity , and get-entity-actions commands in Deploy Accelerator. -- Support for the get-group and get-group-users commands in the Admin Console -- Fixed an issue with the verify-user command in Admin Console. September 6, 2019 2.16.0 This release of the Cloud Accelerator Platform CLI includes a few bug fixes. August 8, 2019 2.15.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Support for creating users and groups in the Admin Console ( create-user and create-group commands). -- Parameter updates for many existing Test Accelerator CLIs. To get a list of parameters for a Test Accelerator CLI, run the following command: rean-test commandname --help February 8, 2019 0.1.0 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- Cloud Accelerator Platform CLI installer is available on Cloud Accelerator Platform production Artifactory. -- Support for editing providers in Deploy Accelerator. January 11, 2019 0.0.9 This release of the Cloud Accelerator Platform CLI supports blueprint aggregation in Managed Cloud Services. December 12, 2018 0.0.8 This release of the Cloud Accelerator Platform CLI supports the get-terraform-code command in Deploy Accelerator. October 31, 2018 0.0.7 This release supports the Cloud Accelerator Platform CLI on the Windows operating system. October 19, 2018 0.0.6 This release of the Cloud Accelerator Platform CLI includes the following enhancements: -- You can now run the infrastructure test with awspec and add configuration properties and tags in Test Accelerator. -- The get-environment and export-environment commands in Deploy Accelerator are now supported. September 10, 2018 0.0.5 This release of the Cloud Accelerator Platform CLI enables you to save output from accelerators to your local machine. August 30, 2018 0.0.4 This release of the Cloud Accelerator Platform CLI enables you to create multiple providers in Deploy Accelerator. August 7, 2018 0.0.1 Initial release of the Cloud Accelerator Platform CLI.","title":"Cloud Accelerator Platform CLI release updates (Older Releases)"},{"location":"platform-common/whatsNewArchive/#dashboard-accelerator-release-updates-discontinued-support","text":"The following table lists the enhancements that were delivered in each release of Hitachi Cloud Accelerator Platform - Dashboard (Dashboard Accelerator). The Dashboard Accelerator is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements July 16, 2020 2.6.0 This release of Dashboard Accelerator includes bug fixes related to the CFO dashboard. June 11, 2020 2.5.0 This release of Dashboard Accelerator allows you to view and filter data in the CFO Dashboard visualizations according to the AWS Account name. May 15, 2020 2.4.0 This release of Dashboard Accelerator includes: - Migration from detailed billing report to AWS Cost and Usage reports for generating visualizations in the CFO dashboard. - Use of Cost and Usage reports for Node Usage Dashboard calculation. - Bug fixes related to performance issues. February 24, 2020 2.3.0 This release of Dashboard Accelerator includes bug fixes related to Platform Node Calculation, CFO, Pipeline, and CISO dashboards. December 18, 2019 2.2.0 This release of Dashboard Accelerator includes: -- Branding changes to Hitachi Cloud Accelerator Platform. -- Addition of the HCAP Node Usage dashboard for calculating node usage of cloud compute services. November 19, 2019 2.1.0 This release of Dashboard Accelerator includes bug fixes related to ALB, ELB, VPC Flow Logs, and CloudFront dashboards. October 23, 2019 2.0.0 This release of Dashboard Accelerator includes: -- Support for the Hong Kong region in AWS. -- Use of AWS ELK stack instead of X-Pack. August 13, 2019 1.9.1 This release of Dashboard Accelerator includes: - New visualizations added in VPC Flow Log dashboard. - Minor bug fixes in CISO dashboard. - Use of X-Pack license disabled for ELK stack in Dashboard Accelerator. - The CFO dashboard logs generated in the EC2 instance will be available in the CloudWatch logs. July 19, 2019 1.9.0 This release of Dashboard Accelerator includes: - CloudFront dashboard. - VPC Flow Logs dashboard May 27, 2019 1.8.0 This release of Dashboard Accelerator includes support for sending violations summary notification to Managed Cloud users. April 30, 2019 1.7.0 This release of Dashboard Accelerator includes the following enhancements: - Visualizations for CFO dashboard are updated. - Minor bug fixes in PMO dashboards. January 28, 2019 1.6.3 This release of Dashboard Accelerator includes the following enhancements: - CISO dashboards and Discovery dashboard are available in offline mode. - Minor updates in CFO dashboard and bug fixes. December 21, 2018 1.6.2 This release of Dashboard Accelerator includes the following enhancements: - CFO dashboard is updated to show data for only the previous month or earlier. - Minor updates in PMO dashboard and bug fixes. November 28, 2018 1.6.1 This release of Dashboard Accelerator includes a new Syslog dashboard under the Operations category. October 17, 2018 1.6.0 This release of Dashboard Accelerator includes the following enhancements: - Two new dashboards, ALB Logs Dashboard and ELB Logs Dashboard, are now available under the Operations category. - Two new columns, Project ID and Project Name , have been added to the PMO Utilization - CFO Compliance table in the PMO Utilization Dashboard. September 24, 2018 1.5.0 This release of Dashboard Accelerator includes updated visualizations for Repair Dashboard. April 28, 2018 1.2.0 This release of Dashboard Accelerator includes the following enhancements: - Dashboard Accelerator has a new Login page. - A new Administer section is added to the Dashboard Accelerator documentation. - An admin user in Dashboard Accelerator can now manage aliases. April 4, 2018 1.1.0 This release of Dashboard Accelerator includes updated visualizations for the following dashboards: -- CTO dashboard -- CISO dashboards -- PMO dashboards -- Discovery dashboard -- REAN Managed Cloud dashboard -- Monitoring dashboard","title":"Dashboard Accelerator release updates (Discontinued Support)"},{"location":"platform-common/whatsNewArchive/#managed-cloud-services-release-updates-discontinued-support","text":"The following table lists the features and updates that were delivered in each release of Managed Cloud Services from Hitachi Vantara. Managed Cloud Services is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements July 27, 2020 1.3.0 This release of Managed Cloud Services includes support for 8 additional AWS rules. Managed Cloud Services now supports 23 AWS rules and 2 Azure rules. July 21, 2020 1.2.0 This release of Managed Cloud Services includes the following enhancements: - Support for 10 additional AWS rules. - Action Scheduling, Consolidation, and Cancellation feature. - Support for receiving email notifications through the SMTP service. - Integration with Dashboard Accelerator. July 3, 2020 1.1.0 This release of Managed Cloud Services includes the following enhancements: - Support for 2 additional AWS rules. - Support for 2 Azure rules. June 11, 2020 1.0.0 This release of Managed Cloud Services includes the following enhancements: - Support for 3 AWS rules. - Support for the Whitelisting capability and Action Failure notification. - Support for Common Data Platform (CDP). Managed Cloud Services now communicates with the CDP API and fetches the cloud accounts data from CDP, which was previously fetched from an AWS service.","title":"Managed Cloud Services release updates (Discontinued Support)"},{"location":"platform-common/whatsNewArchive/#managed-cloud-services-legacy-release-updates-discontinued-support","text":"The following table lists the features and updates that were delivered in each release of Managed Cloud Services from Hitachi Vantara (Legacy). Managed Cloud Services (Legacy) is no longer supported and its documentation is no longer available in the Cloud Accelerator Platform help website. Release date Version Enhancements June 25, 2019 1.2.03 This release of the Managed Cloud Services includes the support for an automatic on-demand scaling for DynamoDB tables. April 3, 2019 1.2.02 This release of Managed Cloud Services includes the following enhancements: - You can now whitelist the resources from the Managed Cloud rules. - Support for CloudWatch periodic rules is added in this release. November 6, 2018 1.1.06 This release of the Managed Cloud Services includes the support for CloudWatch events. May 23, 2018 1.1.0 This release of the Managed Cloud Services includes the integration of Datadog for healing. March 27, 2018 1.0.05 This release of Managed Cloud Services includes the following enhancements: - A new rule Unencrypted snapshots is added. - S3 exposed buckets and Delete unused EBS rules are updated. March 15, 2018 1.0.02 This release of Managed Cloud Services includes the following enhancements: - A new IAM user will get the notification for Check IAM MFA rule. - Managed Cloud Services now supports the SMTP. - Initial release of the Managed Cloud Services documentation. February 18, 2018 1.0.0 This release of Managed Cloud Services includes the following rules: - EC2 required tags - EC2 unused EIP - IAM MFA enabled - RDS required tags - S3 exposed buckets - Delete unused EBS - Delete unused ELB - IAM key age","title":"Managed Cloud Services Legacy release updates (Discontinued Support)"},{"location":"test/administer/","text":"Administer Test Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, scale, and infrastructure testing. Configuring Test Accelerator \u00b6 You can view and edit the Test Accelerator Configurations for multiple categories, such as Mail and SMTP, Artifactory Details. These parameters are set while deploying Test Accelerator. On the Home page of Test Accelerator, click the More options icon ( ) in the top-right corner. Click Configuration . The configurations appear under the Properties tab. Click the appropriate category, and edit one or more properties based on your requirements. The following table describes the categories of properties that you can configure: Category Description Miscellaneous This section contains the Help Documentation URL. Artifactory Details This section contains properties used for configuring the Artifactory URL and authentication details and the location of the ZIP files that contain the sample automation codes. Mail and SMTP This section contains properties used for configuring the Test Accelerator for sending emails to its users, related to test jobs. Kubernetes Properties This section contains properties used for configuring pods in the Kubernetes cluster for Test Accelerator. If the parameters in Kubernetes properties are left blank, then the default values from Kubernetes cluster are used. To save modified values, click SUBMIT . To restore all values to system default, click RESTORE DEFAULT VALUES .","title":"Administer"},{"location":"test/administer/#administer-test-accelerator","text":"Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, scale, and infrastructure testing.","title":"Administer Test Accelerator"},{"location":"test/administer/#configuring-test-accelerator","text":"You can view and edit the Test Accelerator Configurations for multiple categories, such as Mail and SMTP, Artifactory Details. These parameters are set while deploying Test Accelerator. On the Home page of Test Accelerator, click the More options icon ( ) in the top-right corner. Click Configuration . The configurations appear under the Properties tab. Click the appropriate category, and edit one or more properties based on your requirements. The following table describes the categories of properties that you can configure: Category Description Miscellaneous This section contains the Help Documentation URL. Artifactory Details This section contains properties used for configuring the Artifactory URL and authentication details and the location of the ZIP files that contain the sample automation codes. Mail and SMTP This section contains properties used for configuring the Test Accelerator for sending emails to its users, related to test jobs. Kubernetes Properties This section contains properties used for configuring pods in the Kubernetes cluster for Test Accelerator. If the parameters in Kubernetes properties are left blank, then the default values from Kubernetes cluster are used. To save modified values, click SUBMIT . To restore all values to system default, click RESTORE DEFAULT VALUES .","title":"Configuring Test Accelerator"},{"location":"test/api-reference/","text":"Test Accelerator API Reference \u00b6 Hitachi Cloud Accelerator Platform - Test (Test Accelerator) provides a set of APIs (Application Programming Interface) for various components such as test jobs, providers, job scheduler, dashboard, and more. For example, you can use the RunTest APIs to get a list of all test jobs that are running, get job status, get a list of supported browsers, and submit test jobs. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API. Accessing the API documentation \u00b6 To view the API documentation for the version of Test Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcaptest/api-docs/index.html . To view the API documentation for the latest version of Test Accelerator, see the Test Accelerator API Reference website .","title":"API reference"},{"location":"test/api-reference/#test-accelerator-api-reference","text":"Hitachi Cloud Accelerator Platform - Test (Test Accelerator) provides a set of APIs (Application Programming Interface) for various components such as test jobs, providers, job scheduler, dashboard, and more. For example, you can use the RunTest APIs to get a list of all test jobs that are running, get job status, get a list of supported browsers, and submit test jobs. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API.","title":"Test Accelerator API Reference"},{"location":"test/api-reference/#accessing-the-api-documentation","text":"To view the API documentation for the version of Test Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/hcaptest/api-docs/index.html . To view the API documentation for the latest version of Test Accelerator, see the Test Accelerator API Reference website .","title":"Accessing the API documentation"},{"location":"test/getting-started/","text":"Overview of Test Accelerator \u00b6 Testing an application to identify major problems and fixing them is an important part of the Product Release Cycle. The speed of testing is one of the critical pillars of DevOps. Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, and scale (load) testing. You can integrate your existing automation code with Test Accelerator and run automated (cross browser) tests on your web applications. Test Accelerator also enables you to do a URL check or perform manual testing of your web application on different combinations of browsers. In addition, the infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, and routers. For each test job, you can view when the job was last run and the total number of browsers in different states (Configuring, Running, Success, and Failed). You can view an HTML report of a test run for a specific browser and a summary report of the test runs for all browsers. On the Home page of Test Accelerator, you can also view the total number of test cases passed and releasable builds. Test Accelerator runs on Kubernetes and launches Kubernetes pods to run different types of tests. For browser-based tests, pods are launched based on the number of browsers in the test job. For infrastructure and API tests, a pod is launched for each infrastructure or API test job. For information about using Test Accelerator, see the Run tests topic. For information about configuring Test Accelerator, see the Administer Test Acelerator topic. The following image shows the Home page of Test Accelerator:","title":"Overview"},{"location":"test/getting-started/#overview-of-test-accelerator","text":"Testing an application to identify major problems and fixing them is an important part of the Product Release Cycle. The speed of testing is one of the critical pillars of DevOps. Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a cloud-based, DevOps-centric, test automation solution that dynamically creates infrastructure to perform rapid in-parallel cross-browser, functional, and scale (load) testing. You can integrate your existing automation code with Test Accelerator and run automated (cross browser) tests on your web applications. Test Accelerator also enables you to do a URL check or perform manual testing of your web application on different combinations of browsers. In addition, the infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, and routers. For each test job, you can view when the job was last run and the total number of browsers in different states (Configuring, Running, Success, and Failed). You can view an HTML report of a test run for a specific browser and a summary report of the test runs for all browsers. On the Home page of Test Accelerator, you can also view the total number of test cases passed and releasable builds. Test Accelerator runs on Kubernetes and launches Kubernetes pods to run different types of tests. For browser-based tests, pods are launched based on the number of browsers in the test job. For infrastructure and API tests, a pod is launched for each infrastructure or API test job. For information about using Test Accelerator, see the Run tests topic. For information about configuring Test Accelerator, see the Administer Test Acelerator topic. The following image shows the Home page of Test Accelerator:","title":"Overview of Test Accelerator"},{"location":"test/using/","text":"Run tests \u00b6 Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a Continuous Testing (CT) platform that increases the delivery speed from Development to QA to Operations, while adopting DevOps best practices. Test Accelerator enables you to test your web applications with a wide range of browsers and provides state-of-the-art test result data analytics. This topic describes how you can use Test Accelerator to run different types of tests on your web application, run infrastructure and scale tests, view test results, and schedule test jobs. Contents \u00b6 Get started Overview of tests Accessing Test Accelerator Run tests Customizing your automation code Running URL tests Running manual tests Running security tests Running API tests Running automated tests Running scale tests Running infrastructure tests Running codeless infrastructure tests View test results Viewing and downloading test results Viewing and downloading logs Manage test jobs Viewing jobs list Stopping test jobs Rerunning test jobs Scheduling test jobs Managing job schedules Overview of tests \u00b6 Test Accelerator provides the ability to automate the creation of test setups and perform different types of tests on your web application. This enables you to concentrate on the actual use cases of your application and reduce the time required to deliver the application. Test Accelerator enables you to perform the following different types of tests: URL test -- This test can instantly check the availability of a web application on different browsers. Security test -- This test performs a security check of your web application by running different types of security packs on the application. API test -- This test covers the testing of the application programming interfaces (APIs) of your applications to determine if they meet the functionality, reliability, performance, and security expectations. Automated (Cross Browser) test -- This test automatically provisions machines with browsers of your choice and then runs your existing automation test suites in the selected browsers. Scale test -- This test performs the scale testing of your web application across a range of browsers. It enables you to configure multiple browsers, the browser count to test the load, the automation test suite, and the run time in hours. Manual test -- This test quickly provisions Kubernetes pods with the selected browsers and reduces the effort to create test setups. You can concentrate on testing the actual use cases. Infrastructure test -- This test covers the testing of your IT infrastructure, such as servers, network switches, and routers. Test Accelerator supports the Serverspec, Inspec, Azure Spec, AWS Spec, and GCP Spec specification types. Codeless Infra test -- This test covers the testing of your AWS, Azure, or Google Cloud Platform (GCP) resources by using the default spec code that is provided by Test Accelerator. It saves time and resources spent on writing your own custom code. Note: The URL, manual, security, and codeless infra tests do not have a dependency on any automation test suite. Accessing Test Accelerator \u00b6 Sign in to Hitachi Cloud Accelerator Platform . For information about creating a Cloud Accelerator Platform account, see Create & access account . Click the Accelerator icon ( ) in the top-left corner. From the list of accelerators, select Hitachi Cloud Accelerator - Test . The Home page of Test Accelerator appears. Note: You can access Test Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions. Customizing your automation code \u00b6 Before you run the automated (cross-browser) tests , scale (load) tests , or API tests with your own automation code, perform the following procedures to customize your automation code: Configure environment variables in your automation code Configure the use of unique data for each test run Configure environment variables in your automation code \u00b6 To use Test Accelerator effectively, you must customize your automation code to use the environment variables that are exposed by Test Accelerator. Using these environment variables makes your automation code URL- and browser-independent and enables it to run on any environment without requiring changes to your code. BROWSER This environment variable enables you to run your automation code on the browsers that are selected while creating automated (cross-browser) or scale test jobs in Test Accelerator. When the automated (cross-browser) or scale test job starts, the Browsers field value is exposed as the BROWSER environment variable, as shown below: BROWSER=firefox BROWSER=chrome To configure the browsers correctly, you must use this environment variable in your automation code, as shown in the following Java code example: browser = System.getenv(Constants.BROWSER); TEST_URL Each web application has a URL on which the automated (cross browser), scale test, or API test is run. This URL can change with time, environment, and many other factors. Each time the URL changes, you also need to update your automation code. This environment variables enables you to run your automation code on the web application URL that is provided while creating automated (cross-browser), scale test, or API test jobs in Test Accelerator. When the automated (cross-browser), scale test, or API test job starts, the Application URL field value is exposed as the TEST_URL environment variable, as shown below: TEST_URL=http://domain-name.com To configure the web application URL correctly, you must use this environment variable in your automation code, as shown in the following Java code example: test_url = System.getenv(Constants.TEST_URL); Using the TEST_URL environment variable makes your automation code URL-independent and enables it to run on any environment without requiring any changes to your code. Note: If you do not configure web drivers correctly, your tests might fail and you might not be able to use all features of Test Accelerator. Configure the use of unique data for each test run \u00b6 In the case of automated (cross-browser) or scale tests, if you choose to run your automation code on multiple browsers, either sequentially or in parallel, you might want to use unique data for each execution. Test Accelerator exposes the RUN_INDEX environment variable, which has a unique incremental value for each browser. For example, if you are running tests on five browsers in parallel, the RUN_INDEX environment variable increments from 0 to 4 respectively for each browser: RUN_INDEX=0 # Browser 1 RUN_INDEX=1 # Browser 2 RUN_INDEX=2 # Browser 3 ... ... ... RUN_INDEX=n-1 # Browser n The following table lists a few scenarios in which you might need to use unique data for each execution. The table also describes how the RUN_INDEX variable might provide a solution in these scenarios. Scenario Possible solution Use a different login credential set for each execution when running 10 browsers in parallel Maintain a CSV file of multiple user-password pairs and use the RUN_INDEX value to fetch unique rows of credential pairs for each instance of your browser. Select a different drop-down value for each execution Pass the RUN_INDEX value to the Select Drop-down method, which in turn selects a value based on the drop-down index. Buy a unique product for every execution on an e-commerce website Use the RUN_INDEX value as the locator index to buy different products in every instance of the browser. Running URL tests \u00b6 The URL test enables you to quickly test the availability of a web application by testing URL access to that application across various browsers. You must provide the application URL and the text that Test Accelerator must search for on the Home page of that application. The test passes only if Test Accelerator finds that text on the Home page. On the Home page, click New Test . In the Select Test window, click URL Test . The URL Test page appears, as shown in the following image. Under Select Browser , click the browser on which you want to run the URL test and then select the appropriate versions. Repeat this step for each browser on which you want to run the URL test. In Job Name , enter a unique name for your test job. In Application URL , enter the Home page of the web application whose availability you want to test. In Search Text , enter the text that Test Accelerator must search for on the Home page of the web application. Test Accelerator considers the URL test to be successful if this specified text is available on the page. In Timeout (secs) , define the duration, in seconds, for which Test Accelerator should wait to access the URL. (Optional) To check all the links that are available on the specified page, select Crawl URL . (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . The tests in Test Accelerator run on Kubernetes cluster. Test Accelerator creates separate Kubernetes pods using a Kubernetes job to run the test in parallel on all selected browsers. View and download test results . Running manual tests \u00b6 While automation testing significantly reduces the time and effort required to test a web application, you might still need to run manual tests for a few specific scenarios. Also, if an automated test fails for any browser, you might need to create a setup with that browser and then manually run the tests. The Manual test enables you to quickly launch a Kubernetes pod with the appropriate browser version. On the Home page, click New Test . In the Select Test window, click Manual . The Manual Test page appears, as shown in the following image. Under Select Browser (s) , click the browser on which you want to run the manual test and select appropriate versions. Repeat this step for each browser on which you want to run the manual test. You can select multiple versions for each browser. In Name , enter a unique name for your test job. Click SUBMIT . A Kubernetes job is launched for each browser version for running the manual test. View and download test results . Running security tests \u00b6 The Security test enables you to perform a security check of your web application by running different types of security packs on the application. You can also view a detailed analysis of the test packs that are run on the web application. You can also enter the security credentials required for logging into your web application, if the web application requires a login. On the Home page, click New Test . In the Select Test window, click Security . The Security Test page appears, as shown in the following image. In Job Name , enter a unique name for your test job. In Application URL , enter the URL on which you want to run the security test. In Spider-Depth , enter the depth of the spider crawl. Select Use Ajax Spider , to use the Ajax spider add-on. Select Login , to enter the user credentials for the application you want to test. To use the Login feature, make sure that the credentials tab (username and password), and the login button are on the same page. a. Enter Username and Password for the web application. b. In the Enter login field and button details field , enter the following parameters: username_field_xpath : The xpath for the username field on the web application page. password_field_xpath : The xpath for the password field on the web application page. submit_button_xpath : The xpath for the submit button on the web application page. login_url : The URL for logging in to the the web application page. logout_url : The URL for logging out of the the web application page. Under Select security Packs , select the appropriate check boxes: App Scan -- This security pack runs automated application-level tests against the web application using OWASP Zed Attack Proxy (ZAP). Http Header -- This security pack verifies that HTTP headers adequately protect data from attackers. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results . Running API tests \u00b6 The API test covers the testing of the application programming interfaces (APIs) of your applications to determine if they meet the functionality, reliability, performance, and security expectations. ( Optional ) Customize your automation code to use the TEST_URL environment variable that is exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click API . The API Test page appears, as shown in the following image. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. In Application URL , enter the API URL of the web application for testing. Click Next . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test results and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. If Test Accelerator is not able to parse the report that the automation code generates, you must write code to parse the report file and generate the data that Test Accelerator supports. In the right panel, verify that the details you have specified are correct. To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results . Running automated tests \u00b6 The cross-browsers automated test covers the complete automation testing of your web application across a range of browsers. Test Accelerator supports the Selenium automation tool. You can select browsers and specify your test cases from a GitHub repository or upload a ZIP file. Test Accelerator runs all your test cases against the web application on all selected browsers. Cross-browser testing significantly reduces the effort to create the setups and overall testing, allowing you to concentrate on the actual use case. Run an automated test with custom code \u00b6 ( Optional ) Customize your automation code to use the environment variables that are exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click Cross Browser . The Automated Test page appears, as shown in the following image. The procedure to configure an automated test consists of three steps \u2013 select the browsers, provide the codebase, and define execution details. In the BASICS step, perform the following actions: Under Select Browser(s) , click the browser on which you want to run the test suite and select the appropriate versions. Repeat this step for each browser on which you want to run the automated test suite. In Job Name , enter a unique name for your automation job. In Application URL , enter the application URL on which Test Accelerator must run the automation test suite. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test results and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. If Test Accelerator is not able to parse the report that the automation code generates, you must write code to parse the report file and generate the data that Test Accelerator supports. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . Test Accelerator provisions the selected browser versions, downloads and runs your automation test suite, and captures screenshots for each test that is run. View and download test results . Running scale tests \u00b6 The ScaleNow test covers the scale testing of your web application across a range of browsers. You can select multiple browsers and their versions, the browser count to test the load, the automation test suite, and the run time in hours. ScaleNow significantly reduces your effort to create setups and simultaneously test on multiple machines, enabling you to concentrate on the actual use case. Test Accelerator runs all your test cases against your web application on all the selected browsers simultaneously and for the specified time. The ScaleNow test also has an incremental load feature that enables you to add users incrementally and create a real-time scenario for load testing. A new test job is created for each incremental load. To scale test your web application, perform the following actions: ( Optional ) Customize your automation code to use the environment variables that are exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click Scale Now . The procedure to configure a scale test consists of three steps \u2013 select the browsers, provide the codebase, and define execution details. On the ScaleNow page, in the BASICS step, perform the following actions: Under Select Browser(s) , click the browser on which you want to run the test suite and select the appropriate versions. Repeat this step for each browser on which you want to run the automation test suite. You can select multiple versions of each browser. In Job Name , enter a unique name for your test job. In Application URL , enter the application URL on which Test Accelerator must run the automation test suite. In Parallel Users Count , select the number of browsers that Test Accelerator must create in parallel to put load on your web application. In Hours to Run , select the duration, in hours, for which the test suite must run continuously and in the repeat mode. (Optional) To incrementally increase the users that Test Accelerator creates to put load on your web application, select Incremental Load . If you select this check box, you must also perform the following actions: In Increase Users with , enter the number of users that Test Accelerator must create in parallel after the specified interval. In Interval (min) , enter the duration, in minutes, after which Test Accelerator must create the specified number of users. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test result and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . Test Accelerator provisions the selected browsers on Kubernetes pods, downloads and runs your automation test suite. View and download test results . Running infrastructure tests \u00b6 The infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, routers, and firewalls. Test Accelerator supports the specification types listed in the following table. Specification type Details Serverspec Serverspec allows you to check if your servers are configured correctly and adhere to your policy requirements. To test your servers, you can provide your own ServerSpec code . Inspec InSpec allows you to check if your servers are configured correctly and adhere to your policy requirements. To test your servers, you can provide your own Inspec code . Azure Spec Azure Spec allows you to check if Azure resources have been deployed with the appropriate attribute values. To test your Azure infrastructure, run an infrastructure test with custom Azure Spec code . AWS Spec AWS Spec allows you to check if AWS resources have been deployed with the appropriate attribute values. For example, you can check that the appropriate AMI has been used to deploy an EC2 instance. To test your AWS infrastructure, run an infrastructure test with custom AWS Spec code . GCP Spec GCP Spec allows you to check if GCP resources have been deployed with the appropriate attribute values. To test your GCP infrastructure, run an infrastructure test with custom GCP Spec code . Run an infrastructure test with ServerSpec or InSpec code \u00b6 On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the appropriate specification type -- Server Spec or InSpec . Enter the IP address, user name, and password or key of the server that you want to test. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the infrastructure test code. In Test Run Command , enter the command for running the infrastructure test code. In Post-script , enter any Shell script that Test Accelerator must run after running the infrastructure test code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your infrastructure test code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the infrastructure test code. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View the infrastructure test results . Run an infrastructure test with custom Azure Spec code \u00b6 On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the Azure Spec specification type. Enter the following provider details to access your Azure account. Attribute Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the Azure Spec code. In Test Run Command , enter the command for running the Azure Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the Azure Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your Azure Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the Azure Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your Azure Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an Azure Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the Azure account for validating the infrastructure, and runs the test. View and download test results . Run an infrastructure test with custom AWS Spec code \u00b6 On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the Aws Spec specification type. For Credentials type , perform the following actions: Select Basic Credentials . (Optional) To use a more secure way of cross-account access, select Assume Role . Based on the selected credential type, specify the authentication details to access the AWS account in which you want to validate the infrastructure. Basic Credentials with Assume Role You can use this credential type when the AWS account for validating infrastructure is different from the AWS account in which Test Accelerator launches test instances. Required actions and permissions : To use this credential type, perform the following actions: In the AWS account for validating infrastructure, create a role that has readonly access to resources that need to be validated. This role must define as a trusted entity the AWS account for launching test instances. In the AWS account in which the Test Accelerator is deployed, create an IAM user and attach a policy with STS permission to assume the role that you have created in the AWS account for validating infrastructure. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the access key and secret key of the IAM user that you have created in the AWS account. In \u201cassume_role\u201d , enter ARN of the role that you have created in the AWS account for validating infrastructure. You must also specify the session name and external ID. The specified IAM user assumes this role to validate the infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. JSON format with sample values : { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECRET-KEY\", \"region\": \"us-east-1\", \"assume_role\": { \"role_arn\": \" arn:aws:iam::XXXXXXXXXXXX:role/readonly-role\", \"session_name\": \"\", \"external_id\": \"\" } Basic Credentials You can use this credential type in both scenarios: when the AWS account for validating infrastructure and the AWS account for launching instances is the same, or when they are two different accounts. Required actions and permissions : To use this credential type, create an IAM user in the AWS account for validating infrastructure and attach a policy with readonly access to resources that need to be validated. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the access key and secret key of the IAM user that you have created in the AWS account for validating infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. JSON format with sample values : { \"access_key\": \"XXXX\", \"secret_key\": \"XXX\", \"region\": \"XXXX\", } Basic Credentials (using temporary credentials) You can generate and use temporary credentials to access the AWS account for validating infrastructure. The temporary credentials that you specify can be for an IAM user or Assume Role. Required actions and permissions : To use temporary credentials for an IAM user, perform the following actions: In the AWS account for validating infrastructure, create an IAM user and attach a policy with readonly access to resources that need to be validated. Generate temporary credentials for the IAM user. To use temporary credentials for an Assume Role, perform the following actions: In the AWS account for validating infrastructure, create a role that has readonly access to resources that need to be validated. This role must define as a trusted entity the AWS account for launching test instances. Generate temporary credentials for the role that you have created in the AWS account for validating infrastructure. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the temporary credentials that you have generated for the IAM user or Assume Role in the AWS account for validating infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. In \"aws_session_token\" , enter the session token for the temporary credentials that you have generated. JSON format with sample values { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECREt-KEY\", \"region\": \"us-east-1\", \"aws_session_token\": \"SESSION-TOKEN\" } Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the AWS Spec code. In Test Run Command , enter the command for running the AWS Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the AWS Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your AWS Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the AWS Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your AWS Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an AWS Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the AWS account for validating the infrastructure, and runs the test. View and download test results . Run an infrastructure test with custom GCP Spec code \u00b6 On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the GCP Spec specification type. Enter the following provider details to access your GCP account. Attribute Details project The project for managing GCP resources. credentials The credentials can either be the path to, or the content of the GCP service account key file in JSON format. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the GCP Spec code. In Test Run Command , enter the command for running the GCP Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the GCP Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your GCP Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the GCP Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your GCP Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an GCP Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the GCP account for validating the infrastructure, and runs the test. View and download test results . Running codeless infra tests \u00b6 The codeless infra test covers the testing of your AWS, Azure or GCP resources by using the default spec code that is provided by Test Accelerator. It saves time and resources spent on writing your own custom code. Create an infrastructure test data JSON file. For more information, see input and expected output JSON parameters for Azure Spec , AWS Spec , and GCP Spec On the Home page, click New Test . In the Select Test window, click Codeless Infra . The Codeless Infra test page appears, as shown in the following image. In the Spec Type , select AWS Spec , Azure Spec , GCP Spec . In Name , enter a unique name for your test job. In the Provider Details section, specify the details provider details: For AWS Spec, enter basic credentials , basic Credentials (using temporary credentials) , or basic credentials with assume role . For Azure Spec, enter the Azure provider details . For GCP Spec, enter the GCP provider details . In the Infrastructure Details section, perform one of the following actions: Select JSON , and enter the Input and Expected Output JSON parameters. Select Upload Input File , and upload a zip file of the Input and Expected Output JSON parameters. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results . Create the infrastructure test data file for Azure Spec \u00b6 To run an infrastructure test with Default Azure Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported Azure resources and Azure spec versions , Infrastructure JSON file \u00b6 The following details are required to create the JSON file: Input: This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. Example JSON \"input\" : { \"azurerm_snapshot\" : { \"testAzureSnapshot\" : { \"name\" : \"sample_snapshot\" , \"location\" : \"East US\" , \"resource_group_name\" : \"sample_resources\" , \"create_option\" : \"Empty\" , \"disk_size_gb\" : 10 , \"source_resource_id\" : null , \"tags\" : {} } } }, Expected Output: This section contains the names or IDs of all Azure resources that need to be validated. Example JSON \"output\" : { \"azurerm_snapshot\" : { \"testAzureSnapshot\" : \"sample_snapshot\" } } Supported versions for Azure Spec \u00b6 The following table lists the Azure Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default Azure Spec code 2.22.3 and later 1.3 -- 2.22.2 -- 2.22.1 -- 2.21.0 -- 2.20.0 -- 2.19.0 1.1 -- 2.18.0 -- 2.17.0 1.0 Supported resources for Azure Spec \u00b6 The following table lists the resources that the Default Azure Spec code supports. The naming convention used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version azurerm_resource_group 1.0 and later azurerm_virtual_machine 1.0 and later azurerm_virtual_network 1.0 and later azurerm_virtual_machine_extension 1.0 and later azurerm_network_interface 1.0 and later azurerm_availability_set 1.0 and later azurerm_public_ip 1.0 and later azurerm_subnet 1.0 and later azurerm_subnet_network_security_group_association 1.0 and later azurerm_subnet_route_table_association 1.0 and later azurerm_image 1.0 and later azurerm_shared_image 1.0 and later azurerm_shared_image_gallery 1.0 and later azurerm_shared_image_version 1.0 and later azurerm_lb 1.0 and later azurerm_snapshot 1.0 and later azurerm_local_network_gateway 1.0 and later azurerm_lb_rule 1.0 and later azurerm_lb_backend_address_pool 1.0 and later azurerm_lb_outbound_rule 1.0 and later azurerm_lb_probe 1.0 and later azurerm_lb_nat_rule 1.0 and later azurerm_lb_nat_pool 1.0 and later azurerm_key_vault 1.0 and later azurerm_mysql_database 1.0 and later azurerm_mysql_server 1.0 and later azurerm_network_security_group 1.0 and later azurerm_network_security_rule 1.0 and later azurerm_network_profile 1.0 and later azurerm_mysql_firewall_rule 1.0 and later azurerm_mysql_configuration 1.0 and later azurerm_virtual_network_peering 1.0 and later azurerm_mysql_virtual_network_rule 1.0 and later azurerm_virtual_network_gateway 1.0 and later azurerm_virtual_network_gateway_connection 1.0 and later azurerm_user_assigned_identity 1.0 and later azurerm_api_management 1.0 and later azurerm_api_management_user 1.0 and later azurerm_api_management_group 1.0 and later azurerm_api_management_product 1.0 and later azurerm_api_management_group_user 1.0 and later azurerm_api_management_api_operation 1.0 and later azurerm_api_management_property 1.0 and later azurerm_api_management_api_operation_policy 1.0 and later azurerm_api_management_version_set 1.0 and later azurerm_api_management_api 1.0 and later azurerm_route_table 1.0 and later azurerm_route 1.0 and later azurerm_managed_disk 1.0 and later azurerm_cdn_endpoint 1.0 and later azurerm_cdn_profile 1.0 and later azurerm_container_group 1.0 and later azurerm_kubernetes_cluster 1.0 and later azurerm_role_definition 1.0 and later azurerm_role_assignment 1.0 and later azurerm_postgresql_configuration 1.0 and later azurerm_postgresql_database 1.0 and later azurerm_postgresql_firewall_rules 1.0 and later azurerm_batch_account 1.0 and later azurerm_batch_pool 1.0 and later azurerm_batch_certificate 1.0 and later azurerm_storage_account 1.0 and later azurerm_postgresql_server 1.0 and later azurerm_postgresql_virtual_network_rule 1.0 and later azurerm_sql_database 1.0 and later azurerm_sql_firewall_rule 1.0 and later azurerm_sql_server 1.0 and later azurerm_sql_virtual_network_rule 1.0 and later azurerm_key_vault_access_policy 1.0 and later azurerm_api_management_api_policy 1.0 and later azurerm_api_management_api_schema 1.0 and later azurerm_virtual_machine_data_disk_attachment 1.0 and later azurerm_redis_cache 1.1 and later azurerm_app_service_plan 1.1 and later azurerm_app_service 1.1 and later azurerm_application_insights 1.1 and later azurerm_cosmosdb_account 1.1 and later azurerm_container_group 1.1 and later azurerm_storage_container 1.3 and later azurerm_cosmosdb_sql_database 1.3 and later azurerm_key_vault_secret 1.3 and later azurerm_cosmosdb_sql_container 1.3 and later azurerm_logic_app_trigger_recurrence 1.3 and later azurerm_logic_app_trigger_custom 1.3 and later azurerm_logic_app_trigger_http_request 1.3 and later azurerm_log_analytics_workspace 1.3 and later azurerm_template_deployment 1.3 and later azurerm_logic_app_workflow 1.3 and later azurerm_logic_app_integration_account 1.3 and later azurerm_logic_app_action_http 1.3 and later azurerm_logic_app_action_custom 1.3 and later Create the infrastructure test data file for AWS Spec \u00b6 To run an infrastructure test with Default AWS Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported AWS resources and AWS spec versions . Infrastructure JSON file \u00b6 The following details are required to create the JSON file: Input This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. A corresponding entry with the unique name specified in the Output section is configured in the Input section under the same resource type. Only the attributes defined in the Input section are compared with the actual state. The resource type values in the Input section differ based on the version of the Default AWS Spec code. You can select one of the following options. Default AWS Spec code 1.3 and later In version 1.3, the resource type values are based on the Terraform resource names. For example, aws_vpc is the resource type under which VPC resources are defined. For details about the resource types that you must specify, see Supported resources . Example JSON \"input\": { \"aws_vpc\": { \"devVPC\": { \"cidr_block\": \"172.16.0.0/16\", \"tags\": { \"CreatedBy\": \"Deploy\", \"Name\": \"dev-datalake-quickstart-vpc\", \"Environment\": \"dev\", \"Owner\": \"a.g\", \"Product\": \"datalake-quickstart\" }, \"enable_dns_support\": true, \"enable_dns_hostnames\": true } } } Default AWS Spec code 1.2 In version 1.2, the resource type values are based on custom types defined in Test Accelerator. For example, VPC is the resource type under which VPC resources are defined. For details about the resource types that you must specify, see Resource types in Default AWS Spec code 1.2 . Example JSON ``` { \"VPC\":{ \"vpc\": { \"cidr_block\": \"10.0.0.0/16\" } }, \"EC2\": { \"Demo_EC2_Instance\": { \"instance_type\": \"t2.micro\", \"tags\": { \"Name\": \"Platform-bastion-us-east-1b\" } } } } ``` _**Note:** While Default AWS Spec code 1.3 is backward compatible and supports the older format of Input JSON, it is recommended that you switch to the latest format of Input JSON._ Expected Output This section contains the names or IDs of all AWS resources that need to be validated. Each key-value pair under each resource type represents the unique name of the resource and the corresponding ID of that resource. This ID is used to retrieve the attributes of the resource and validate them with the expected values defined in the Input section. Example JSON \"output\": { \"aws_vpc\": { \"devVPC\": \"vpc-XXXXXXX\", \"prdVPC\": \"vpc-XXXXXXX\" } } Note: The resource type values in the Output section are based on the Terraform resource names. For example, in the above example, aws_vpc is used for the VPC resources. Supported versions for AWS Spec \u00b6 The following table lists the Default AWS Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default AWS Spec code 2.22.1 and later 1.5 -- 2.20.0 -- 2.19.0 -- 2.18.0 -- 2.17.0 -- 2.16.0 -- 2.15.0 1.4 2.14.0 1.3 2.13.0 1.3 2.12.0 1.2 Supported resources for AWS Spec \u00b6 The following table lists the resources that the Default AWS Spec code supports. The naming convention that is used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version aws_instance 1.2 and later aws_vpc 1.2 and later aws_vpc_peering_connection 1.2 and later aws_subnet 1.2 and later aws_route_table 1.2 and later aws_security_group 1.2 and later aws_network_acl 1.2 and later aws_autoscaling_group 1.2 and later aws_launch_configuration 1.2 and later aws_elb 1.2 and later aws_nat_gateway 1.2 and later aws_internet_gateway 1.2 and later aws_eip 1.2 and later aws_launch_template 1.2 and later aws_efs_file_system 1.2 and later aws_efs_mount_target 1.2 and later aws_iam_group 1.2 and later aws_iam_role 1.2 and later aws_iam_policy 1.2 and later aws_iam_instance_profile 1.2 and later aws_iam_user 1.2 and later aws_flow_log 1.2 and later aws_vpc_endpoint 1.2 and later aws_cloudwatch_log_group 1.2 and later aws_route_table_association 1.2 and later aws_sns_topic 1.2 and later aws_sns_topic_policy 1.2 and later aws_sns_topic_subscription 1.2 and later aws_redshift_cluster 1.2 and later aws_redshift_subnet_group 1.2 and later aws_elasticsearch_domain 1.2 and later aws_lb_target_group 1.2 and later aws_alb 1.2 and later aws_customer_gateway 1.2 and later aws_db_option_group 1.2 and later aws_db_parameter_group 1.2 and later aws_db_subnet_group 1.2 and later aws_db_instance 1.2 and later aws_rds_cluster 1.2 and later aws_sqs_queue 1.2 and later aws_sqs_queue_policy 1.2 and later aws_vpn_gateway 1.2 and later aws_vpn_connection 1.2 and later aws_dynamodb_table 1.2 and later aws_ses_active_receipt_rule_set 1.2 and later aws_ses_receipt_rule 1.2 and later aws_ses_receipt_rule_set 1.2 and later aws_ses_receipt_filter 1.2 and later aws_ses_domain_identity 1.2 and later aws_ses_configuration_set 1.2 and later aws_ses_event_destination 1.2 and later aws_alb_listener 1.2 and later aws_alb_listener_rule 1.2 and later aws_elb_load_balancer_listener_policy 1.2 and later aws_route53_health_check 1.2 and later aws_route53_zone 1.2 and later aws_route53_record 1.2 and later aws_kms_key 1.4 and later aws_kms_alias 1.4 and later aws_s3_bucket 1.4 and later aws_iam_role_policy 1.4 and later aws_iam_group_policy 1.4 and later aws_kinesis_stream 1.4 and later aws_cloudtrail 1.4 and later aws_codecommit_repository 1.4 and later aws_security_group_rule 1.4 and later aws_volume_attachment 1.4 and later aws_swf_domain 1.4 and later aws_key_pair 1.4 and later aws_ami 1.4 and later aws_lambda_function 1.4 and later aws_s3_bucket_object 1.4 and later aws_s3_bucket_policy 1.4 and later aws_ebs_volume 1.4 and later aws_route 1.4 and later aws_alb_target_group 1.4 and later aws_vpc_peering_connection_accepter 1.4 and later aws_ecs_service 1.4 and later aws_ecr_lifecycle_policy 1.4 and later aws_ecr_repository_policy 1.4 and later aws_alb_target_group 1.4 and later aws_iam_policy_attachment 1.4 and later aws_vpc_dhcp_options_association 1.4 and later aws_iam_group_policy_attachment 1.4 and later aws_iam_role_policy_attachment 1.4 and later aws_ecs_cluster 1.4 and later aws_appautoscaling_target 1.4 and later aws_alb_listener_certificate 1.4 and later aws_vpc_dhcp_options 1.4 and later aws_alb_target_group_attachment 1.4 and later aws_spot_instance_request 1.4 and later aws_iam_server_certificate 1.4 and later aws_network_interface 1.4 and later aws_ecr_repository 1.4 and later aws_eip_association 1.4 and later aws_elasticache_security_group 1.4 and later aws_elasticache_subnet_group 1.4 and later aws_lb_cookie_stickiness_policy 1.4 and later aws_s3_bucket_notification 1.4 and later aws_elasticache_replication_group 1.4 and later aws_autoscaling_policy 1.4 and later aws_autoscaling_notification 1.4 and later aws_autoscaling_attachment 1.4 and later aws_autoscaling_lifecycle_hook 1.4 and later aws_elasticache_parameter_group 1.4 and later aws_placement_group 1.4 and later aws_api_gateway_api_key 1.4 and later aws_ebs_snapshot 1.4 and later aws_autoscaling_schedule 1.4 and later Create the infrastructure test data file for GCP Spec \u00b6 To run an infrastructure test with Default GCP Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported GCP resources and GCP Spec versions supported by Test Accelerator. Infrastructure JSON file \u00b6 The following details are required to create the JSON file: Input: This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. Example JSON \"input\" : { \"google_compute_address\" : { \"address\" : { \"name\" : \"ip-bastion-host\" , \"description\" : \"test gcpspec\" , \"labels\" :{ \"owner\" : \"hcap-test\" , \"environment\" : \"development\" }, \"address\" : \"10.0.0.26\" , \"address_type\" : \"INTERNAL\" , \"purpose\" : \"GCE_ENDPOINT\" , \"network_tier\" : \"PREMIUM\" , \"subnetwork\" : \"subnet-hv-hcap-dev-central\" , \"region\" : \"us-central1\" , \"project\" : \"hv-hcap-development\" } } }, Expected Output: This section contains the names or IDs of all Azure resources that need to be validated. Example JSON \"output\" : { \"google_compute_address\" : { \"address\" : \"hcap-disk\" } } Supported versions for GCP Spec \u00b6 The following table lists the GCP Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default GCP Spec code 3.2.0 1.1 Supported resources for GCP Spec \u00b6 The following table lists the resources that the Default GCP Spec code supports. The naming convention used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version google_compute_instance 1.1 and later google_compute_network 1.1 and later google_compute_subnetwork 1.1 and later google_compute_disk 1.1 and later google_compute_machine_image 1.1 and later google_compute_firewall 1.1 and later google_compute_router 1.1 and later google_compute_instance_template 1.1 and later google_compute_image 1.1 and later google_compute_address 1.1 and later google_compute_router_nat 1.1 and later google_compute_autoscaler 1.1 and later google_compute_instance_group_manager 1.1 and later google_storage_bucket_acl 1.1 and later google_storage_bucket 1.1 and later google_storage_default_object_acl 1.1 and later google_compute_vpn_gateway 1.1 and later google_compute_target_instance 1.1 and later google_compute_route 1.1 and later google_compute_instance_group 1.1 and later google_compute_target_pool 1.1 and later google_compute_node_group 1.1 and later google_compute_network_endpoint_group 1.1 and later google_container_node_pool 1.1 and later google_compute_ssl_policy 1.1 and later google_compute_ssl_certificate 1.1 and later google_compute_snapshot 1.1 and later Viewing and downloading test results \u00b6 The Home page of Test Accelerator displays a summary of the total number of test cases that have passed and the total number of builds that are ready for release, as shown in the following image. To view and download the test results of a specific job, perform the following actions: On the Home page of Test Accelerator, from the left panel, select the test job whose details you want to view. In the menu bar, you can view details such as when the job was last run and the total number of browsers that are in the Configuring , Running , Success , and Failed status. In the right panel, each bubble represents a browser in the selected test job. The size of the bubble represents the time taken by the test run and the color represents the state (Configuring, Running, Success, and Failed). If you hover over a bubble, you can view details about the browser it represents. (Optional) To filter the list of test jobs, perform the following actions: To filter the job list based on keywords or the job ID, use the search box. To view only scheduled jobs, select the Scheduled check box. To view only your own jobs, select the Owned by me check box. This check box is helpful when you are part of a shared group and can view the jobs created by other users in that group. To view only your own scheduled jobs, select the Owned by me and Scheduled check boxes. (Optional) To view the actual tests that are running on a browser, click the bubble for that browser when it is in the Running state. To view and download the report of a test run, perform the following actions: To view the report of a test run for a specific browser, click the bubble for that browser when it is in the Success or Failed state. The HTML report displays the result of the automation suite for the selected browser. The test results include a screenshot for each test case that is run on the browser. To download the report for a specific browser, right-click the bubble for that browser, and select Download Reports . To download the report of a test run for all selected browsers, click the Download Report icon ( ). A ZIP file is downloaded to your local computer. When you extract the contents of the ZIP file, you can see a folder for each browser version. To view the HTML report for a browser version, click the folder for that browser version and extract the contents of the reports.zip file. To view and download a summary report of the test run for all browsers, perform the following actions: Click the Full Report icon ( ) in the menu bar. The consolidated report enables you to analyze the test results at any time while a job is in the Running state. To download the summary report, click ExportToExcel or ExportToCSV . (Only for infrastructure test jobs) To view the infrastructure test report, perform the following actions: From the left panel, select the infrastructure test job whose details you want to view. To view detailed reports for the selected infrastructure job, in the Reports panel, select the appropriate Report link. Note: The Infra Dashboard displays a consolidated report for each resource type in the selected infrastructure job. (Optional) To download a report for the selected infrastructure job, perform the following actions: To download an Excel report that contains the test summary and detailed test analytics, click ExportToExcel . To download a ZIP file that contains detailed HTML reports for each resource type, click Download Report . Important: To use the reporting feature in Test Accelerator, your automation suite must have a valid reporting framework that enables creation of reports in HTML and JSON formats. Test Accelerator does not create reports but instead, displays reports that are created by your automation suite. While creating a test job, you must specify the relative path to the home directory of the automation suite and enter the JSON format file. Viewing and downloading logs \u00b6 From the Test accelerator home page, you can download logs for the ongoing and completed tests. To view and download logs for a test run, perform the following actions: To view the logs, right-click the bubble, and select Get Logs . To download the logs, right-click the bubble, and select Download Logs . Viewing jobs list \u00b6 On the Home page of Test Accelerator, click the More options icon ( ) in the top-right corner. Click Job List . On the Test Job List page, you can view the list of all test jobs that you have created and the test jobs that are shared with you. You can also view the job type, creation date, and status. (Optional) To filter the list of test jobs, enter appropriate keywords in the Search box. Stopping test jobs \u00b6 On the Home page of Test Accelerator, from the left panel, select the test job that you want to stop. In the menu bar, click the Abort icon ( ). In the confirmation message box, click YES . Rerunning test jobs \u00b6 On the Home page of Test Accelerator, from the left panel, select the test job that you want to rerun. In the menu bar, click the Re-run icon ( ). (Optional) Update the job details based on your requirements. For more information, see Running URL tests , Running manual tests , Running security tests , Running automated tests , Running scale tests , Running infrastructure tests , Running codeless infrastructure tests , and Running API tests . Note: The password for Git repository is not stored in the test job. You must re-enter the Git password in the CODEBASE step of the test jobs. Submit the test job. A new test job appears in the jobs list. Scheduling test jobs \u00b6 On the Home page of Test Accelerator, from the left panel, select the test job that you want to run multiple times based on a configured schedule. In the menu bar, click the Schedule icon ( ). In the Schedule Job window, enter the schedule name. To define how frequently the job must be run, for Repeats and Repeat every , select the appropriate options. To define when the scheduled jobs must start and end, for Starts and Ends , select the appropriate options. Click SCHEDULE . Test Accelerator creates and displays the first test job in the job list. Additional test jobs are added to this list based on the schedule that you have configured. (Optional) To view the scheduled test jobs, in the left panel on the Home page, select Scheduled . The list displays the name that you provided while scheduling the test jobs. You can search for a specific schedule name to view all test jobs that have been created based on that schedule. Managing job schedules \u00b6 Each time you schedule a test job, Test Accelerator creates a corresponding job schedule. You can view the list of all job schedules that have been created and the status of each schedule (Scheduled, Running, Completed, Failed, and Aborted). On the Home page of Test Accelerator, click the More options icon ( ) icon in the top-right corner. Click Schedule . The Schedule List page displays all the job schedules that have been created. The Name column displays the name that was provided while scheduling test jobs. To view details about a job schedule, click that schedule in the Schedule list. The right panel displays a summary about the selected job schedule. You can also view the date and time when the next test job is scheduled to run. To abort a job schedule, click the Actions icon ( ) for that job schedule, and click Abort . The Abort option is available only for job schedules with the Scheduled status. When you abort a schedule, corresponding future scheduled jobs are no longer created. To pause a job schedule, click the Actions icon ( ) for that job schedule, and click Pause . The Pause option is available only for job schedules with the Scheduled status. When you pause a schedule, corresponding future scheduled jobs are not created until you resume the job schedule. To resume a paused job schedule, click the Actions icon ( ) for that job schedule, and click Resume . To delete a job schedule from the list, click the Actions icon ( ) for that job schedule, and click Remove . When you delete a job schedule, corresponding future scheduled jobs are no longer created.","title":"Run tests"},{"location":"test/using/#run-tests","text":"Hitachi Cloud Accelerator Platform - Test (Test Accelerator) is a Continuous Testing (CT) platform that increases the delivery speed from Development to QA to Operations, while adopting DevOps best practices. Test Accelerator enables you to test your web applications with a wide range of browsers and provides state-of-the-art test result data analytics. This topic describes how you can use Test Accelerator to run different types of tests on your web application, run infrastructure and scale tests, view test results, and schedule test jobs.","title":"Run tests"},{"location":"test/using/#contents","text":"Get started Overview of tests Accessing Test Accelerator Run tests Customizing your automation code Running URL tests Running manual tests Running security tests Running API tests Running automated tests Running scale tests Running infrastructure tests Running codeless infrastructure tests View test results Viewing and downloading test results Viewing and downloading logs Manage test jobs Viewing jobs list Stopping test jobs Rerunning test jobs Scheduling test jobs Managing job schedules","title":"Contents"},{"location":"test/using/#overview-of-tests","text":"Test Accelerator provides the ability to automate the creation of test setups and perform different types of tests on your web application. This enables you to concentrate on the actual use cases of your application and reduce the time required to deliver the application. Test Accelerator enables you to perform the following different types of tests: URL test -- This test can instantly check the availability of a web application on different browsers. Security test -- This test performs a security check of your web application by running different types of security packs on the application. API test -- This test covers the testing of the application programming interfaces (APIs) of your applications to determine if they meet the functionality, reliability, performance, and security expectations. Automated (Cross Browser) test -- This test automatically provisions machines with browsers of your choice and then runs your existing automation test suites in the selected browsers. Scale test -- This test performs the scale testing of your web application across a range of browsers. It enables you to configure multiple browsers, the browser count to test the load, the automation test suite, and the run time in hours. Manual test -- This test quickly provisions Kubernetes pods with the selected browsers and reduces the effort to create test setups. You can concentrate on testing the actual use cases. Infrastructure test -- This test covers the testing of your IT infrastructure, such as servers, network switches, and routers. Test Accelerator supports the Serverspec, Inspec, Azure Spec, AWS Spec, and GCP Spec specification types. Codeless Infra test -- This test covers the testing of your AWS, Azure, or Google Cloud Platform (GCP) resources by using the default spec code that is provided by Test Accelerator. It saves time and resources spent on writing your own custom code. Note: The URL, manual, security, and codeless infra tests do not have a dependency on any automation test suite.","title":"Overview of tests"},{"location":"test/using/#accessing-test-accelerator","text":"Sign in to Hitachi Cloud Accelerator Platform . For information about creating a Cloud Accelerator Platform account, see Create & access account . Click the Accelerator icon ( ) in the top-left corner. From the list of accelerators, select Hitachi Cloud Accelerator - Test . The Home page of Test Accelerator appears. Note: You can access Test Accelerator and perform various actions only if your Cloud Accelerator Platform administrator has granted you the appropriate permissions.","title":"Accessing Test Accelerator"},{"location":"test/using/#customizing-your-automation-code","text":"Before you run the automated (cross-browser) tests , scale (load) tests , or API tests with your own automation code, perform the following procedures to customize your automation code: Configure environment variables in your automation code Configure the use of unique data for each test run","title":"Customizing your automation code"},{"location":"test/using/#configure-environment-variables-in-your-automation-code","text":"To use Test Accelerator effectively, you must customize your automation code to use the environment variables that are exposed by Test Accelerator. Using these environment variables makes your automation code URL- and browser-independent and enables it to run on any environment without requiring changes to your code. BROWSER This environment variable enables you to run your automation code on the browsers that are selected while creating automated (cross-browser) or scale test jobs in Test Accelerator. When the automated (cross-browser) or scale test job starts, the Browsers field value is exposed as the BROWSER environment variable, as shown below: BROWSER=firefox BROWSER=chrome To configure the browsers correctly, you must use this environment variable in your automation code, as shown in the following Java code example: browser = System.getenv(Constants.BROWSER); TEST_URL Each web application has a URL on which the automated (cross browser), scale test, or API test is run. This URL can change with time, environment, and many other factors. Each time the URL changes, you also need to update your automation code. This environment variables enables you to run your automation code on the web application URL that is provided while creating automated (cross-browser), scale test, or API test jobs in Test Accelerator. When the automated (cross-browser), scale test, or API test job starts, the Application URL field value is exposed as the TEST_URL environment variable, as shown below: TEST_URL=http://domain-name.com To configure the web application URL correctly, you must use this environment variable in your automation code, as shown in the following Java code example: test_url = System.getenv(Constants.TEST_URL); Using the TEST_URL environment variable makes your automation code URL-independent and enables it to run on any environment without requiring any changes to your code. Note: If you do not configure web drivers correctly, your tests might fail and you might not be able to use all features of Test Accelerator.","title":"Configure environment variables in your automation code"},{"location":"test/using/#configure-the-use-of-unique-data-for-each-test-run","text":"In the case of automated (cross-browser) or scale tests, if you choose to run your automation code on multiple browsers, either sequentially or in parallel, you might want to use unique data for each execution. Test Accelerator exposes the RUN_INDEX environment variable, which has a unique incremental value for each browser. For example, if you are running tests on five browsers in parallel, the RUN_INDEX environment variable increments from 0 to 4 respectively for each browser: RUN_INDEX=0 # Browser 1 RUN_INDEX=1 # Browser 2 RUN_INDEX=2 # Browser 3 ... ... ... RUN_INDEX=n-1 # Browser n The following table lists a few scenarios in which you might need to use unique data for each execution. The table also describes how the RUN_INDEX variable might provide a solution in these scenarios. Scenario Possible solution Use a different login credential set for each execution when running 10 browsers in parallel Maintain a CSV file of multiple user-password pairs and use the RUN_INDEX value to fetch unique rows of credential pairs for each instance of your browser. Select a different drop-down value for each execution Pass the RUN_INDEX value to the Select Drop-down method, which in turn selects a value based on the drop-down index. Buy a unique product for every execution on an e-commerce website Use the RUN_INDEX value as the locator index to buy different products in every instance of the browser.","title":"Configure the use of unique data for each test run"},{"location":"test/using/#running-url-tests","text":"The URL test enables you to quickly test the availability of a web application by testing URL access to that application across various browsers. You must provide the application URL and the text that Test Accelerator must search for on the Home page of that application. The test passes only if Test Accelerator finds that text on the Home page. On the Home page, click New Test . In the Select Test window, click URL Test . The URL Test page appears, as shown in the following image. Under Select Browser , click the browser on which you want to run the URL test and then select the appropriate versions. Repeat this step for each browser on which you want to run the URL test. In Job Name , enter a unique name for your test job. In Application URL , enter the Home page of the web application whose availability you want to test. In Search Text , enter the text that Test Accelerator must search for on the Home page of the web application. Test Accelerator considers the URL test to be successful if this specified text is available on the page. In Timeout (secs) , define the duration, in seconds, for which Test Accelerator should wait to access the URL. (Optional) To check all the links that are available on the specified page, select Crawl URL . (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . The tests in Test Accelerator run on Kubernetes cluster. Test Accelerator creates separate Kubernetes pods using a Kubernetes job to run the test in parallel on all selected browsers. View and download test results .","title":"Running URL tests"},{"location":"test/using/#running-manual-tests","text":"While automation testing significantly reduces the time and effort required to test a web application, you might still need to run manual tests for a few specific scenarios. Also, if an automated test fails for any browser, you might need to create a setup with that browser and then manually run the tests. The Manual test enables you to quickly launch a Kubernetes pod with the appropriate browser version. On the Home page, click New Test . In the Select Test window, click Manual . The Manual Test page appears, as shown in the following image. Under Select Browser (s) , click the browser on which you want to run the manual test and select appropriate versions. Repeat this step for each browser on which you want to run the manual test. You can select multiple versions for each browser. In Name , enter a unique name for your test job. Click SUBMIT . A Kubernetes job is launched for each browser version for running the manual test. View and download test results .","title":"Running manual tests"},{"location":"test/using/#running-security-tests","text":"The Security test enables you to perform a security check of your web application by running different types of security packs on the application. You can also view a detailed analysis of the test packs that are run on the web application. You can also enter the security credentials required for logging into your web application, if the web application requires a login. On the Home page, click New Test . In the Select Test window, click Security . The Security Test page appears, as shown in the following image. In Job Name , enter a unique name for your test job. In Application URL , enter the URL on which you want to run the security test. In Spider-Depth , enter the depth of the spider crawl. Select Use Ajax Spider , to use the Ajax spider add-on. Select Login , to enter the user credentials for the application you want to test. To use the Login feature, make sure that the credentials tab (username and password), and the login button are on the same page. a. Enter Username and Password for the web application. b. In the Enter login field and button details field , enter the following parameters: username_field_xpath : The xpath for the username field on the web application page. password_field_xpath : The xpath for the password field on the web application page. submit_button_xpath : The xpath for the submit button on the web application page. login_url : The URL for logging in to the the web application page. logout_url : The URL for logging out of the the web application page. Under Select security Packs , select the appropriate check boxes: App Scan -- This security pack runs automated application-level tests against the web application using OWASP Zed Attack Proxy (ZAP). Http Header -- This security pack verifies that HTTP headers adequately protect data from attackers. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results .","title":"Running security tests"},{"location":"test/using/#running-api-tests","text":"The API test covers the testing of the application programming interfaces (APIs) of your applications to determine if they meet the functionality, reliability, performance, and security expectations. ( Optional ) Customize your automation code to use the TEST_URL environment variable that is exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click API . The API Test page appears, as shown in the following image. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. In Application URL , enter the API URL of the web application for testing. Click Next . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test results and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. If Test Accelerator is not able to parse the report that the automation code generates, you must write code to parse the report file and generate the data that Test Accelerator supports. In the right panel, verify that the details you have specified are correct. To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results .","title":"Running API tests"},{"location":"test/using/#running-automated-tests","text":"The cross-browsers automated test covers the complete automation testing of your web application across a range of browsers. Test Accelerator supports the Selenium automation tool. You can select browsers and specify your test cases from a GitHub repository or upload a ZIP file. Test Accelerator runs all your test cases against the web application on all selected browsers. Cross-browser testing significantly reduces the effort to create the setups and overall testing, allowing you to concentrate on the actual use case.","title":"Running automated tests"},{"location":"test/using/#run-an-automated-test-with-custom-code","text":"( Optional ) Customize your automation code to use the environment variables that are exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click Cross Browser . The Automated Test page appears, as shown in the following image. The procedure to configure an automated test consists of three steps \u2013 select the browsers, provide the codebase, and define execution details. In the BASICS step, perform the following actions: Under Select Browser(s) , click the browser on which you want to run the test suite and select the appropriate versions. Repeat this step for each browser on which you want to run the automated test suite. In Job Name , enter a unique name for your automation job. In Application URL , enter the application URL on which Test Accelerator must run the automation test suite. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test results and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. If Test Accelerator is not able to parse the report that the automation code generates, you must write code to parse the report file and generate the data that Test Accelerator supports. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . Test Accelerator provisions the selected browser versions, downloads and runs your automation test suite, and captures screenshots for each test that is run. View and download test results .","title":"Run an automated test with custom code"},{"location":"test/using/#running-scale-tests","text":"The ScaleNow test covers the scale testing of your web application across a range of browsers. You can select multiple browsers and their versions, the browser count to test the load, the automation test suite, and the run time in hours. ScaleNow significantly reduces your effort to create setups and simultaneously test on multiple machines, enabling you to concentrate on the actual use case. Test Accelerator runs all your test cases against your web application on all the selected browsers simultaneously and for the specified time. The ScaleNow test also has an incremental load feature that enables you to add users incrementally and create a real-time scenario for load testing. A new test job is created for each incremental load. To scale test your web application, perform the following actions: ( Optional ) Customize your automation code to use the environment variables that are exposed by Test Accelerator. On the Home page, click New Test . In the Select Test window, click Scale Now . The procedure to configure a scale test consists of three steps \u2013 select the browsers, provide the codebase, and define execution details. On the ScaleNow page, in the BASICS step, perform the following actions: Under Select Browser(s) , click the browser on which you want to run the test suite and select the appropriate versions. Repeat this step for each browser on which you want to run the automation test suite. You can select multiple versions of each browser. In Job Name , enter a unique name for your test job. In Application URL , enter the application URL on which Test Accelerator must run the automation test suite. In Parallel Users Count , select the number of browsers that Test Accelerator must create in parallel to put load on your web application. In Hours to Run , select the duration, in hours, for which the test suite must run continuously and in the repeat mode. (Optional) To incrementally increase the users that Test Accelerator creates to put load on your web application, select Incremental Load . If you select this check box, you must also perform the following actions: In Increase Users with , enter the number of users that Test Accelerator must create in parallel after the specified interval. In Interval (min) , enter the duration, in minutes, after which Test Accelerator must create the specified number of users. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the automation test suite. In Test Run Command , enter the command for running the automation test suite. In Post-script , enter any Shell script that Test Accelerator must run after running the automation test suite. In Output Directory , specify the path to the Output directory where reports are generated after running your automation code. This path must be relative to the automation code directory. You can see the contents of this directory by clicking the bubble that is created for the browser. In Report File , specify the path of the file that has information about the test execution. This file path must be relative to the Output directory that you have specified. Test Accelerator uses this file to analyze the test result and generate a consolidated report. Test Accelerator supports the report file that is generated by some standard tools such as JUnit, Cucumber, and UFT. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . Test Accelerator provisions the selected browsers on Kubernetes pods, downloads and runs your automation test suite. View and download test results .","title":"Running scale tests"},{"location":"test/using/#running-infrastructure-tests","text":"The infrastructure test covers the testing of your IT infrastructure, such as servers, network switches, routers, and firewalls. Test Accelerator supports the specification types listed in the following table. Specification type Details Serverspec Serverspec allows you to check if your servers are configured correctly and adhere to your policy requirements. To test your servers, you can provide your own ServerSpec code . Inspec InSpec allows you to check if your servers are configured correctly and adhere to your policy requirements. To test your servers, you can provide your own Inspec code . Azure Spec Azure Spec allows you to check if Azure resources have been deployed with the appropriate attribute values. To test your Azure infrastructure, run an infrastructure test with custom Azure Spec code . AWS Spec AWS Spec allows you to check if AWS resources have been deployed with the appropriate attribute values. For example, you can check that the appropriate AMI has been used to deploy an EC2 instance. To test your AWS infrastructure, run an infrastructure test with custom AWS Spec code . GCP Spec GCP Spec allows you to check if GCP resources have been deployed with the appropriate attribute values. To test your GCP infrastructure, run an infrastructure test with custom GCP Spec code .","title":"Running infrastructure tests"},{"location":"test/using/#run-an-infrastructure-test-with-serverspec-or-inspec-code","text":"On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the appropriate specification type -- Server Spec or InSpec . Enter the IP address, user name, and password or key of the server that you want to test. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the infrastructure test code. In Test Run Command , enter the command for running the infrastructure test code. In Post-script , enter any Shell script that Test Accelerator must run after running the infrastructure test code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your infrastructure test code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the infrastructure test code. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View the infrastructure test results .","title":"Run an infrastructure test with ServerSpec or InSpec code"},{"location":"test/using/#run-an-infrastructure-test-with-custom-azure-spec-code","text":"On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the Azure Spec specification type. Enter the following provider details to access your Azure account. Attribute Details subscription_id A single Azure account can have multiple subscriptions. Enter the unique ID of your subscription to use Azure services. client_id Enter the ID of your application in Azure Active Directory. client_secret Enter the authentication key for the specified application. tenant_id Enter the ID of the Azure Active Directory tenant with which the specified subscription is associated. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the Azure Spec code. In Test Run Command , enter the command for running the Azure Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the Azure Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your Azure Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the Azure Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your Azure Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an Azure Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the Azure account for validating the infrastructure, and runs the test. View and download test results .","title":"Run an infrastructure test with custom Azure Spec code"},{"location":"test/using/#run-an-infrastructure-test-with-custom-aws-spec-code","text":"On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the Aws Spec specification type. For Credentials type , perform the following actions: Select Basic Credentials . (Optional) To use a more secure way of cross-account access, select Assume Role . Based on the selected credential type, specify the authentication details to access the AWS account in which you want to validate the infrastructure. Basic Credentials with Assume Role You can use this credential type when the AWS account for validating infrastructure is different from the AWS account in which Test Accelerator launches test instances. Required actions and permissions : To use this credential type, perform the following actions: In the AWS account for validating infrastructure, create a role that has readonly access to resources that need to be validated. This role must define as a trusted entity the AWS account for launching test instances. In the AWS account in which the Test Accelerator is deployed, create an IAM user and attach a policy with STS permission to assume the role that you have created in the AWS account for validating infrastructure. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the access key and secret key of the IAM user that you have created in the AWS account. In \u201cassume_role\u201d , enter ARN of the role that you have created in the AWS account for validating infrastructure. You must also specify the session name and external ID. The specified IAM user assumes this role to validate the infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. JSON format with sample values : { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECRET-KEY\", \"region\": \"us-east-1\", \"assume_role\": { \"role_arn\": \" arn:aws:iam::XXXXXXXXXXXX:role/readonly-role\", \"session_name\": \"\", \"external_id\": \"\" } Basic Credentials You can use this credential type in both scenarios: when the AWS account for validating infrastructure and the AWS account for launching instances is the same, or when they are two different accounts. Required actions and permissions : To use this credential type, create an IAM user in the AWS account for validating infrastructure and attach a policy with readonly access to resources that need to be validated. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the access key and secret key of the IAM user that you have created in the AWS account for validating infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. JSON format with sample values : { \"access_key\": \"XXXX\", \"secret_key\": \"XXX\", \"region\": \"XXXX\", } Basic Credentials (using temporary credentials) You can generate and use temporary credentials to access the AWS account for validating infrastructure. The temporary credentials that you specify can be for an IAM user or Assume Role. Required actions and permissions : To use temporary credentials for an IAM user, perform the following actions: In the AWS account for validating infrastructure, create an IAM user and attach a policy with readonly access to resources that need to be validated. Generate temporary credentials for the IAM user. To use temporary credentials for an Assume Role, perform the following actions: In the AWS account for validating infrastructure, create a role that has readonly access to resources that need to be validated. This role must define as a trusted entity the AWS account for launching test instances. Generate temporary credentials for the role that you have created in the AWS account for validating infrastructure. Provider details : In the Provider Details section, specify the following details: In \"access_key\" and \"secret_key\" , enter the temporary credentials that you have generated for the IAM user or Assume Role in the AWS account for validating infrastructure. In \u201cregion\u201d , enter the region in which the infrastructure must be validated. In \"aws_session_token\" , enter the session token for the temporary credentials that you have generated. JSON format with sample values { \"access_key\": \"ACCESS-KEY\", \"secret_key\": \"SECREt-KEY\", \"region\": \"us-east-1\", \"aws_session_token\": \"SESSION-TOKEN\" } Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the AWS Spec code. In Test Run Command , enter the command for running the AWS Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the AWS Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your AWS Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the AWS Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your AWS Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an AWS Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the AWS account for validating the infrastructure, and runs the test. View and download test results .","title":"Run an infrastructure test with custom AWS Spec code"},{"location":"test/using/#run-an-infrastructure-test-with-custom-gcp-spec-code","text":"On the Home page, click New Test . In the Select Test window, click Infra . The Infra Test page appears, as shown in the following image. The procedure to configure an infrastructure test consists of three steps -- select the spec details, provide the codebase, and define execution details. In the BASICS step, perform the following actions: In Job Name , enter a unique name for your test job. For Spec Type , select the GCP Spec specification type. Enter the following provider details to access your GCP account. Attribute Details project The project for managing GCP resources. credentials The credentials can either be the path to, or the content of the GCP service account key file in JSON format. Click NEXT . In the CODEBASE step, perform the following actions: For Automation Code , select one of the following options: If you want to upload your automation code as a ZIP file, select Upload Code and Click to upload . In the Upload File window, select the ZIP file and click Upload Zip File . If your automation code is available in a GitHub repository, select Git , and enter the following details about the GitHub repository: GitHub repository clone URL If your automation code is stored in a private repository, then select Private Repository , and enter your GitHub user ID and password. Branch name (If you do not enter the branch name, Test Accelerator downloads the automation code from the **master* branch by default.)* Click NEXT . In the EXECUTION step, perform the following actions: In Pre-script , enter any Shell script that Test Accelerator must run before running the GCP Spec code. In Test Run Command , enter the command for running the GCP Spec code. In Post-script , enter any Shell script that Test Accelerator must run after running the GCP Spec code. In Output Directory , enter the directory in which Test Accelerator must save the test run results. This path must be relative to the directory in which your GCP Spec code is stored. In Report File , enter the name of the report file that Test Accelerator must generate after running the GCP Spec code. If you want to upload a file that Test Accelerator must extract and provide while running your GCP Spec code, click Upload File . In the Upload Input Output File window, select the appropriate file as a ZIP file and click Upload File . Note: Test Accelerator extracts the contents of this ZIP file in the /home/seluser/hcaptest/code folder of the instance that it launches to run the infrastructure test. In the right panel, verify that the details you have specified are correct. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . When you start an GCP Spec infrastructure test job, Test Accelerator launches a test Kubernetes pod and downloads the infrastructure test code on the pod. It then connects to the GCP account for validating the infrastructure, and runs the test. View and download test results .","title":"Run an infrastructure test with custom GCP Spec code"},{"location":"test/using/#running-codeless-infra-tests","text":"The codeless infra test covers the testing of your AWS, Azure or GCP resources by using the default spec code that is provided by Test Accelerator. It saves time and resources spent on writing your own custom code. Create an infrastructure test data JSON file. For more information, see input and expected output JSON parameters for Azure Spec , AWS Spec , and GCP Spec On the Home page, click New Test . In the Select Test window, click Codeless Infra . The Codeless Infra test page appears, as shown in the following image. In the Spec Type , select AWS Spec , Azure Spec , GCP Spec . In Name , enter a unique name for your test job. In the Provider Details section, specify the details provider details: For AWS Spec, enter basic credentials , basic Credentials (using temporary credentials) , or basic credentials with assume role . For Azure Spec, enter the Azure provider details . For GCP Spec, enter the GCP provider details . In the Infrastructure Details section, perform one of the following actions: Select JSON , and enter the Input and Expected Output JSON parameters. Select Upload Input File , and upload a zip file of the Input and Expected Output JSON parameters. (Optional) To run this test job multiple times based on a configured schedule, click SCHEDULE . For information about scheduling tests, see Scheduling test jobs . Click SUBMIT . View and download test results .","title":"Running codeless infra tests"},{"location":"test/using/#create-the-infrastructure-test-data-file-for-azure-spec","text":"To run an infrastructure test with Default Azure Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported Azure resources and Azure spec versions ,","title":"Create the infrastructure test data file for Azure Spec"},{"location":"test/using/#infrastructure-json-file","text":"The following details are required to create the JSON file: Input: This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. Example JSON \"input\" : { \"azurerm_snapshot\" : { \"testAzureSnapshot\" : { \"name\" : \"sample_snapshot\" , \"location\" : \"East US\" , \"resource_group_name\" : \"sample_resources\" , \"create_option\" : \"Empty\" , \"disk_size_gb\" : 10 , \"source_resource_id\" : null , \"tags\" : {} } } }, Expected Output: This section contains the names or IDs of all Azure resources that need to be validated. Example JSON \"output\" : { \"azurerm_snapshot\" : { \"testAzureSnapshot\" : \"sample_snapshot\" } }","title":"Infrastructure JSON file"},{"location":"test/using/#supported-versions-for-azure-spec","text":"The following table lists the Azure Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default Azure Spec code 2.22.3 and later 1.3 -- 2.22.2 -- 2.22.1 -- 2.21.0 -- 2.20.0 -- 2.19.0 1.1 -- 2.18.0 -- 2.17.0 1.0","title":"Supported versions for Azure Spec"},{"location":"test/using/#supported-resources-for-azure-spec","text":"The following table lists the resources that the Default Azure Spec code supports. The naming convention used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version azurerm_resource_group 1.0 and later azurerm_virtual_machine 1.0 and later azurerm_virtual_network 1.0 and later azurerm_virtual_machine_extension 1.0 and later azurerm_network_interface 1.0 and later azurerm_availability_set 1.0 and later azurerm_public_ip 1.0 and later azurerm_subnet 1.0 and later azurerm_subnet_network_security_group_association 1.0 and later azurerm_subnet_route_table_association 1.0 and later azurerm_image 1.0 and later azurerm_shared_image 1.0 and later azurerm_shared_image_gallery 1.0 and later azurerm_shared_image_version 1.0 and later azurerm_lb 1.0 and later azurerm_snapshot 1.0 and later azurerm_local_network_gateway 1.0 and later azurerm_lb_rule 1.0 and later azurerm_lb_backend_address_pool 1.0 and later azurerm_lb_outbound_rule 1.0 and later azurerm_lb_probe 1.0 and later azurerm_lb_nat_rule 1.0 and later azurerm_lb_nat_pool 1.0 and later azurerm_key_vault 1.0 and later azurerm_mysql_database 1.0 and later azurerm_mysql_server 1.0 and later azurerm_network_security_group 1.0 and later azurerm_network_security_rule 1.0 and later azurerm_network_profile 1.0 and later azurerm_mysql_firewall_rule 1.0 and later azurerm_mysql_configuration 1.0 and later azurerm_virtual_network_peering 1.0 and later azurerm_mysql_virtual_network_rule 1.0 and later azurerm_virtual_network_gateway 1.0 and later azurerm_virtual_network_gateway_connection 1.0 and later azurerm_user_assigned_identity 1.0 and later azurerm_api_management 1.0 and later azurerm_api_management_user 1.0 and later azurerm_api_management_group 1.0 and later azurerm_api_management_product 1.0 and later azurerm_api_management_group_user 1.0 and later azurerm_api_management_api_operation 1.0 and later azurerm_api_management_property 1.0 and later azurerm_api_management_api_operation_policy 1.0 and later azurerm_api_management_version_set 1.0 and later azurerm_api_management_api 1.0 and later azurerm_route_table 1.0 and later azurerm_route 1.0 and later azurerm_managed_disk 1.0 and later azurerm_cdn_endpoint 1.0 and later azurerm_cdn_profile 1.0 and later azurerm_container_group 1.0 and later azurerm_kubernetes_cluster 1.0 and later azurerm_role_definition 1.0 and later azurerm_role_assignment 1.0 and later azurerm_postgresql_configuration 1.0 and later azurerm_postgresql_database 1.0 and later azurerm_postgresql_firewall_rules 1.0 and later azurerm_batch_account 1.0 and later azurerm_batch_pool 1.0 and later azurerm_batch_certificate 1.0 and later azurerm_storage_account 1.0 and later azurerm_postgresql_server 1.0 and later azurerm_postgresql_virtual_network_rule 1.0 and later azurerm_sql_database 1.0 and later azurerm_sql_firewall_rule 1.0 and later azurerm_sql_server 1.0 and later azurerm_sql_virtual_network_rule 1.0 and later azurerm_key_vault_access_policy 1.0 and later azurerm_api_management_api_policy 1.0 and later azurerm_api_management_api_schema 1.0 and later azurerm_virtual_machine_data_disk_attachment 1.0 and later azurerm_redis_cache 1.1 and later azurerm_app_service_plan 1.1 and later azurerm_app_service 1.1 and later azurerm_application_insights 1.1 and later azurerm_cosmosdb_account 1.1 and later azurerm_container_group 1.1 and later azurerm_storage_container 1.3 and later azurerm_cosmosdb_sql_database 1.3 and later azurerm_key_vault_secret 1.3 and later azurerm_cosmosdb_sql_container 1.3 and later azurerm_logic_app_trigger_recurrence 1.3 and later azurerm_logic_app_trigger_custom 1.3 and later azurerm_logic_app_trigger_http_request 1.3 and later azurerm_log_analytics_workspace 1.3 and later azurerm_template_deployment 1.3 and later azurerm_logic_app_workflow 1.3 and later azurerm_logic_app_integration_account 1.3 and later azurerm_logic_app_action_http 1.3 and later azurerm_logic_app_action_custom 1.3 and later","title":"Supported resources for Azure Spec"},{"location":"test/using/#create-the-infrastructure-test-data-file-for-aws-spec","text":"To run an infrastructure test with Default AWS Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported AWS resources and AWS spec versions .","title":"Create the infrastructure test data file for AWS Spec"},{"location":"test/using/#infrastructure-json-file_1","text":"The following details are required to create the JSON file: Input This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. A corresponding entry with the unique name specified in the Output section is configured in the Input section under the same resource type. Only the attributes defined in the Input section are compared with the actual state. The resource type values in the Input section differ based on the version of the Default AWS Spec code. You can select one of the following options. Default AWS Spec code 1.3 and later In version 1.3, the resource type values are based on the Terraform resource names. For example, aws_vpc is the resource type under which VPC resources are defined. For details about the resource types that you must specify, see Supported resources . Example JSON \"input\": { \"aws_vpc\": { \"devVPC\": { \"cidr_block\": \"172.16.0.0/16\", \"tags\": { \"CreatedBy\": \"Deploy\", \"Name\": \"dev-datalake-quickstart-vpc\", \"Environment\": \"dev\", \"Owner\": \"a.g\", \"Product\": \"datalake-quickstart\" }, \"enable_dns_support\": true, \"enable_dns_hostnames\": true } } } Default AWS Spec code 1.2 In version 1.2, the resource type values are based on custom types defined in Test Accelerator. For example, VPC is the resource type under which VPC resources are defined. For details about the resource types that you must specify, see Resource types in Default AWS Spec code 1.2 . Example JSON ``` { \"VPC\":{ \"vpc\": { \"cidr_block\": \"10.0.0.0/16\" } }, \"EC2\": { \"Demo_EC2_Instance\": { \"instance_type\": \"t2.micro\", \"tags\": { \"Name\": \"Platform-bastion-us-east-1b\" } } } } ``` _**Note:** While Default AWS Spec code 1.3 is backward compatible and supports the older format of Input JSON, it is recommended that you switch to the latest format of Input JSON._ Expected Output This section contains the names or IDs of all AWS resources that need to be validated. Each key-value pair under each resource type represents the unique name of the resource and the corresponding ID of that resource. This ID is used to retrieve the attributes of the resource and validate them with the expected values defined in the Input section. Example JSON \"output\": { \"aws_vpc\": { \"devVPC\": \"vpc-XXXXXXX\", \"prdVPC\": \"vpc-XXXXXXX\" } } Note: The resource type values in the Output section are based on the Terraform resource names. For example, in the above example, aws_vpc is used for the VPC resources.","title":"Infrastructure JSON file"},{"location":"test/using/#supported-versions-for-aws-spec","text":"The following table lists the Default AWS Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default AWS Spec code 2.22.1 and later 1.5 -- 2.20.0 -- 2.19.0 -- 2.18.0 -- 2.17.0 -- 2.16.0 -- 2.15.0 1.4 2.14.0 1.3 2.13.0 1.3 2.12.0 1.2","title":"Supported versions for AWS Spec"},{"location":"test/using/#supported-resources-for-aws-spec","text":"The following table lists the resources that the Default AWS Spec code supports. The naming convention that is used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version aws_instance 1.2 and later aws_vpc 1.2 and later aws_vpc_peering_connection 1.2 and later aws_subnet 1.2 and later aws_route_table 1.2 and later aws_security_group 1.2 and later aws_network_acl 1.2 and later aws_autoscaling_group 1.2 and later aws_launch_configuration 1.2 and later aws_elb 1.2 and later aws_nat_gateway 1.2 and later aws_internet_gateway 1.2 and later aws_eip 1.2 and later aws_launch_template 1.2 and later aws_efs_file_system 1.2 and later aws_efs_mount_target 1.2 and later aws_iam_group 1.2 and later aws_iam_role 1.2 and later aws_iam_policy 1.2 and later aws_iam_instance_profile 1.2 and later aws_iam_user 1.2 and later aws_flow_log 1.2 and later aws_vpc_endpoint 1.2 and later aws_cloudwatch_log_group 1.2 and later aws_route_table_association 1.2 and later aws_sns_topic 1.2 and later aws_sns_topic_policy 1.2 and later aws_sns_topic_subscription 1.2 and later aws_redshift_cluster 1.2 and later aws_redshift_subnet_group 1.2 and later aws_elasticsearch_domain 1.2 and later aws_lb_target_group 1.2 and later aws_alb 1.2 and later aws_customer_gateway 1.2 and later aws_db_option_group 1.2 and later aws_db_parameter_group 1.2 and later aws_db_subnet_group 1.2 and later aws_db_instance 1.2 and later aws_rds_cluster 1.2 and later aws_sqs_queue 1.2 and later aws_sqs_queue_policy 1.2 and later aws_vpn_gateway 1.2 and later aws_vpn_connection 1.2 and later aws_dynamodb_table 1.2 and later aws_ses_active_receipt_rule_set 1.2 and later aws_ses_receipt_rule 1.2 and later aws_ses_receipt_rule_set 1.2 and later aws_ses_receipt_filter 1.2 and later aws_ses_domain_identity 1.2 and later aws_ses_configuration_set 1.2 and later aws_ses_event_destination 1.2 and later aws_alb_listener 1.2 and later aws_alb_listener_rule 1.2 and later aws_elb_load_balancer_listener_policy 1.2 and later aws_route53_health_check 1.2 and later aws_route53_zone 1.2 and later aws_route53_record 1.2 and later aws_kms_key 1.4 and later aws_kms_alias 1.4 and later aws_s3_bucket 1.4 and later aws_iam_role_policy 1.4 and later aws_iam_group_policy 1.4 and later aws_kinesis_stream 1.4 and later aws_cloudtrail 1.4 and later aws_codecommit_repository 1.4 and later aws_security_group_rule 1.4 and later aws_volume_attachment 1.4 and later aws_swf_domain 1.4 and later aws_key_pair 1.4 and later aws_ami 1.4 and later aws_lambda_function 1.4 and later aws_s3_bucket_object 1.4 and later aws_s3_bucket_policy 1.4 and later aws_ebs_volume 1.4 and later aws_route 1.4 and later aws_alb_target_group 1.4 and later aws_vpc_peering_connection_accepter 1.4 and later aws_ecs_service 1.4 and later aws_ecr_lifecycle_policy 1.4 and later aws_ecr_repository_policy 1.4 and later aws_alb_target_group 1.4 and later aws_iam_policy_attachment 1.4 and later aws_vpc_dhcp_options_association 1.4 and later aws_iam_group_policy_attachment 1.4 and later aws_iam_role_policy_attachment 1.4 and later aws_ecs_cluster 1.4 and later aws_appautoscaling_target 1.4 and later aws_alb_listener_certificate 1.4 and later aws_vpc_dhcp_options 1.4 and later aws_alb_target_group_attachment 1.4 and later aws_spot_instance_request 1.4 and later aws_iam_server_certificate 1.4 and later aws_network_interface 1.4 and later aws_ecr_repository 1.4 and later aws_eip_association 1.4 and later aws_elasticache_security_group 1.4 and later aws_elasticache_subnet_group 1.4 and later aws_lb_cookie_stickiness_policy 1.4 and later aws_s3_bucket_notification 1.4 and later aws_elasticache_replication_group 1.4 and later aws_autoscaling_policy 1.4 and later aws_autoscaling_notification 1.4 and later aws_autoscaling_attachment 1.4 and later aws_autoscaling_lifecycle_hook 1.4 and later aws_elasticache_parameter_group 1.4 and later aws_placement_group 1.4 and later aws_api_gateway_api_key 1.4 and later aws_ebs_snapshot 1.4 and later aws_autoscaling_schedule 1.4 and later","title":"Supported resources for AWS Spec"},{"location":"test/using/#create-the-infrastructure-test-data-file-for-gcp-spec","text":"To run an infrastructure test with Default GCP Spec code, you must first create a JSON file that contains the test data. Before you create a JSON file, see the list of supported GCP resources and GCP Spec versions supported by Test Accelerator.","title":"Create the infrastructure test data file for GCP Spec"},{"location":"test/using/#infrastructure-json-file_2","text":"The following details are required to create the JSON file: Input: This section contains the expected values that must be compared with the actual values that are retrieved for the resources defined in the Output section. Example JSON \"input\" : { \"google_compute_address\" : { \"address\" : { \"name\" : \"ip-bastion-host\" , \"description\" : \"test gcpspec\" , \"labels\" :{ \"owner\" : \"hcap-test\" , \"environment\" : \"development\" }, \"address\" : \"10.0.0.26\" , \"address_type\" : \"INTERNAL\" , \"purpose\" : \"GCE_ENDPOINT\" , \"network_tier\" : \"PREMIUM\" , \"subnetwork\" : \"subnet-hv-hcap-dev-central\" , \"region\" : \"us-central1\" , \"project\" : \"hv-hcap-development\" } } }, Expected Output: This section contains the names or IDs of all Azure resources that need to be validated. Example JSON \"output\" : { \"google_compute_address\" : { \"address\" : \"hcap-disk\" } }","title":"Infrastructure JSON file"},{"location":"test/using/#supported-versions-for-gcp-spec","text":"The following table lists the GCP Spec code version that is available out of the box based on the version of Test Accelerator. Version of Test Accelerator Version of Default GCP Spec code 3.2.0 1.1","title":"Supported versions for GCP Spec"},{"location":"test/using/#supported-resources-for-gcp-spec","text":"The following table lists the resources that the Default GCP Spec code supports. The naming convention used for the supported resources is based on the HashiCorp Terraform resource names. Resource Name Available Release Version google_compute_instance 1.1 and later google_compute_network 1.1 and later google_compute_subnetwork 1.1 and later google_compute_disk 1.1 and later google_compute_machine_image 1.1 and later google_compute_firewall 1.1 and later google_compute_router 1.1 and later google_compute_instance_template 1.1 and later google_compute_image 1.1 and later google_compute_address 1.1 and later google_compute_router_nat 1.1 and later google_compute_autoscaler 1.1 and later google_compute_instance_group_manager 1.1 and later google_storage_bucket_acl 1.1 and later google_storage_bucket 1.1 and later google_storage_default_object_acl 1.1 and later google_compute_vpn_gateway 1.1 and later google_compute_target_instance 1.1 and later google_compute_route 1.1 and later google_compute_instance_group 1.1 and later google_compute_target_pool 1.1 and later google_compute_node_group 1.1 and later google_compute_network_endpoint_group 1.1 and later google_container_node_pool 1.1 and later google_compute_ssl_policy 1.1 and later google_compute_ssl_certificate 1.1 and later google_compute_snapshot 1.1 and later","title":"Supported resources for GCP Spec"},{"location":"test/using/#viewing-and-downloading-test-results","text":"The Home page of Test Accelerator displays a summary of the total number of test cases that have passed and the total number of builds that are ready for release, as shown in the following image. To view and download the test results of a specific job, perform the following actions: On the Home page of Test Accelerator, from the left panel, select the test job whose details you want to view. In the menu bar, you can view details such as when the job was last run and the total number of browsers that are in the Configuring , Running , Success , and Failed status. In the right panel, each bubble represents a browser in the selected test job. The size of the bubble represents the time taken by the test run and the color represents the state (Configuring, Running, Success, and Failed). If you hover over a bubble, you can view details about the browser it represents. (Optional) To filter the list of test jobs, perform the following actions: To filter the job list based on keywords or the job ID, use the search box. To view only scheduled jobs, select the Scheduled check box. To view only your own jobs, select the Owned by me check box. This check box is helpful when you are part of a shared group and can view the jobs created by other users in that group. To view only your own scheduled jobs, select the Owned by me and Scheduled check boxes. (Optional) To view the actual tests that are running on a browser, click the bubble for that browser when it is in the Running state. To view and download the report of a test run, perform the following actions: To view the report of a test run for a specific browser, click the bubble for that browser when it is in the Success or Failed state. The HTML report displays the result of the automation suite for the selected browser. The test results include a screenshot for each test case that is run on the browser. To download the report for a specific browser, right-click the bubble for that browser, and select Download Reports . To download the report of a test run for all selected browsers, click the Download Report icon ( ). A ZIP file is downloaded to your local computer. When you extract the contents of the ZIP file, you can see a folder for each browser version. To view the HTML report for a browser version, click the folder for that browser version and extract the contents of the reports.zip file. To view and download a summary report of the test run for all browsers, perform the following actions: Click the Full Report icon ( ) in the menu bar. The consolidated report enables you to analyze the test results at any time while a job is in the Running state. To download the summary report, click ExportToExcel or ExportToCSV . (Only for infrastructure test jobs) To view the infrastructure test report, perform the following actions: From the left panel, select the infrastructure test job whose details you want to view. To view detailed reports for the selected infrastructure job, in the Reports panel, select the appropriate Report link. Note: The Infra Dashboard displays a consolidated report for each resource type in the selected infrastructure job. (Optional) To download a report for the selected infrastructure job, perform the following actions: To download an Excel report that contains the test summary and detailed test analytics, click ExportToExcel . To download a ZIP file that contains detailed HTML reports for each resource type, click Download Report . Important: To use the reporting feature in Test Accelerator, your automation suite must have a valid reporting framework that enables creation of reports in HTML and JSON formats. Test Accelerator does not create reports but instead, displays reports that are created by your automation suite. While creating a test job, you must specify the relative path to the home directory of the automation suite and enter the JSON format file.","title":"Viewing and downloading test results"},{"location":"test/using/#viewing-and-downloading-logs","text":"From the Test accelerator home page, you can download logs for the ongoing and completed tests. To view and download logs for a test run, perform the following actions: To view the logs, right-click the bubble, and select Get Logs . To download the logs, right-click the bubble, and select Download Logs .","title":"Viewing and downloading logs"},{"location":"test/using/#viewing-jobs-list","text":"On the Home page of Test Accelerator, click the More options icon ( ) in the top-right corner. Click Job List . On the Test Job List page, you can view the list of all test jobs that you have created and the test jobs that are shared with you. You can also view the job type, creation date, and status. (Optional) To filter the list of test jobs, enter appropriate keywords in the Search box.","title":"Viewing jobs list"},{"location":"test/using/#stopping-test-jobs","text":"On the Home page of Test Accelerator, from the left panel, select the test job that you want to stop. In the menu bar, click the Abort icon ( ). In the confirmation message box, click YES .","title":"Stopping test jobs"},{"location":"test/using/#rerunning-test-jobs","text":"On the Home page of Test Accelerator, from the left panel, select the test job that you want to rerun. In the menu bar, click the Re-run icon ( ). (Optional) Update the job details based on your requirements. For more information, see Running URL tests , Running manual tests , Running security tests , Running automated tests , Running scale tests , Running infrastructure tests , Running codeless infrastructure tests , and Running API tests . Note: The password for Git repository is not stored in the test job. You must re-enter the Git password in the CODEBASE step of the test jobs. Submit the test job. A new test job appears in the jobs list.","title":"Rerunning test jobs"},{"location":"test/using/#scheduling-test-jobs","text":"On the Home page of Test Accelerator, from the left panel, select the test job that you want to run multiple times based on a configured schedule. In the menu bar, click the Schedule icon ( ). In the Schedule Job window, enter the schedule name. To define how frequently the job must be run, for Repeats and Repeat every , select the appropriate options. To define when the scheduled jobs must start and end, for Starts and Ends , select the appropriate options. Click SCHEDULE . Test Accelerator creates and displays the first test job in the job list. Additional test jobs are added to this list based on the schedule that you have configured. (Optional) To view the scheduled test jobs, in the left panel on the Home page, select Scheduled . The list displays the name that you provided while scheduling the test jobs. You can search for a specific schedule name to view all test jobs that have been created based on that schedule.","title":"Scheduling test jobs"},{"location":"test/using/#managing-job-schedules","text":"Each time you schedule a test job, Test Accelerator creates a corresponding job schedule. You can view the list of all job schedules that have been created and the status of each schedule (Scheduled, Running, Completed, Failed, and Aborted). On the Home page of Test Accelerator, click the More options icon ( ) icon in the top-right corner. Click Schedule . The Schedule List page displays all the job schedules that have been created. The Name column displays the name that was provided while scheduling test jobs. To view details about a job schedule, click that schedule in the Schedule list. The right panel displays a summary about the selected job schedule. You can also view the date and time when the next test job is scheduled to run. To abort a job schedule, click the Actions icon ( ) for that job schedule, and click Abort . The Abort option is available only for job schedules with the Scheduled status. When you abort a schedule, corresponding future scheduled jobs are no longer created. To pause a job schedule, click the Actions icon ( ) for that job schedule, and click Pause . The Pause option is available only for job schedules with the Scheduled status. When you pause a schedule, corresponding future scheduled jobs are not created until you resume the job schedule. To resume a paused job schedule, click the Actions icon ( ) for that job schedule, and click Resume . To delete a job schedule from the list, click the Actions icon ( ) for that job schedule, and click Remove . When you delete a job schedule, corresponding future scheduled jobs are no longer created.","title":"Managing job schedules"},{"location":"workflow/api-reference/","text":"Workflow Accelerator API Reference \u00b6 Hitachi Cloud Accelerator Platform - Workflow Accelerator provides a set of REST APIs (Application Programming Interface) for the Solution Package and Workflow Engine services. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API. Contents \u00b6 Accessing the Solution Package API documentation Accessing the Workflow Engine API documentation Using cURL to make API requests Accessing the Solution Package API documentation \u00b6 The Solution Package API documentation lists the APIs that you can use to register, update, and delete solution packages. It also includes APIs to get a list of registered solution packages or search for a solution package based on specific criteria such as name, version, and ID. To view the Solution Package API documentation for the version of Workflow Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/solutionpackagedoc/swagger-ui.html . To view the Solution Package API documentation for the latest version of Workflow Accelerator, see the Workflow Accelerator API Reference website . Note: The API documentation describes APIs for all supported schema versions of the solution package. Use the APIs for the schema version that you have used to create your solution package. The following image shows the Solution Package APIs for schema version 2. x . Accessing the Workflow Engine API documentation \u00b6 The Workflow Engine API documentation lists the APIs that you can use to deploy, redeploy, and destroy solution packages. It also includes APIs to get the deployment status of a solution package and get a list of deployed solution packages. To view the Workflow Engine API documentation for the version of Workflow Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/workflowenginedoc/swagger-ui.html . To view the Workflow Engine API documentation for the latest version of Workflow Accelerator, see the Workflow Accelerator API Reference website . Using cURL to make API requests \u00b6 The Workflow Accelerator solution packages can be deployed in many different ways. For example, you can build a UI to trigger the solution package deployment and internally call Workflow Accelerator REST APIs. Alternatively, you can use cURL commands or a REST API client (such as Postman) to directly run the API commands. This documentation provides examples of cURL commands for making API requests to Workflow Accelerator. For information about using these APIs to perform different actions in Workflow Accelerator, see Create and register solution package and Deploy solution packages . Set environment variables \u00b6 Before you run the API commands, set the following environment variables for Hitachi Cloud Accelerator Platform: HCAP_URL USERNAME PASSWORD Important: Ensure that the user account whose authentication details you specify, is a member of the required groups . Also, if your solution packages use pre-created providers in Deploy Accelerator, ensure that the user account can access the providers. Example export USERNAME=admin export PASSWORD=admin123 export HCAP_URL= https://hcap.hitachivantara.com cURL command syntax and options \u00b6 You can use the following cURL command syntax to make API requests to Workflow Accelerator. curl -X <Method> <URL> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ <Body> }' The cURL command includes the following sections: Method : This section contains the method used to call the API and the URL of the API. -X POST URL \\ Header : The values to be passed in the header of the command. The value for the authorization key must be your Cloud Accelerator Platform username and password, separated by a colon (:). The value for the content-type key must be application/json . -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ Body : The parameters to be passed in the body section of the URL. These values can be taken from the values defined in the solution package. -d '{ <data to be sent> }' Example curl -X PUT $HCAP_URL/api/hcapworkflowengine/solution-deploy/redeploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ solutionPackageId\": \"AXcdSDadssaUqCsd\", \"name\": \"DemoDeployment\", \"description\": \"This is a demo deployment\", \"inputParameters\": {\"vpc_cidr\": \"10.0.0.0/16\", \"zone\": \"us-east-1\"}, \"providers\":{ \"DemoProvider\": 8 } }'","title":"API reference"},{"location":"workflow/api-reference/#workflow-accelerator-api-reference","text":"Hitachi Cloud Accelerator Platform - Workflow Accelerator provides a set of REST APIs (Application Programming Interface) for the Solution Package and Workflow Engine services. The API Reference is organized by components and provides information about the method, syntax, parameters, and response code for each API.","title":"Workflow Accelerator API Reference"},{"location":"workflow/api-reference/#contents","text":"Accessing the Solution Package API documentation Accessing the Workflow Engine API documentation Using cURL to make API requests","title":"Contents"},{"location":"workflow/api-reference/#accessing-the-solution-package-api-documentation","text":"The Solution Package API documentation lists the APIs that you can use to register, update, and delete solution packages. It also includes APIs to get a list of registered solution packages or search for a solution package based on specific criteria such as name, version, and ID. To view the Solution Package API documentation for the version of Workflow Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/solutionpackagedoc/swagger-ui.html . To view the Solution Package API documentation for the latest version of Workflow Accelerator, see the Workflow Accelerator API Reference website . Note: The API documentation describes APIs for all supported schema versions of the solution package. Use the APIs for the schema version that you have used to create your solution package. The following image shows the Solution Package APIs for schema version 2. x .","title":"Accessing the Solution Package API documentation"},{"location":"workflow/api-reference/#accessing-the-workflow-engine-api-documentation","text":"The Workflow Engine API documentation lists the APIs that you can use to deploy, redeploy, and destroy solution packages. It also includes APIs to get the deployment status of a solution package and get a list of deployed solution packages. To view the Workflow Engine API documentation for the version of Workflow Accelerator that you have deployed, go to https:// YOUR_PLATFORM_BASE_URL /api-documentation/workflowenginedoc/swagger-ui.html . To view the Workflow Engine API documentation for the latest version of Workflow Accelerator, see the Workflow Accelerator API Reference website .","title":"Accessing the Workflow Engine API documentation"},{"location":"workflow/api-reference/#using-curl-to-make-api-requests","text":"The Workflow Accelerator solution packages can be deployed in many different ways. For example, you can build a UI to trigger the solution package deployment and internally call Workflow Accelerator REST APIs. Alternatively, you can use cURL commands or a REST API client (such as Postman) to directly run the API commands. This documentation provides examples of cURL commands for making API requests to Workflow Accelerator. For information about using these APIs to perform different actions in Workflow Accelerator, see Create and register solution package and Deploy solution packages .","title":"Using cURL to make API requests"},{"location":"workflow/api-reference/#set-environment-variables","text":"Before you run the API commands, set the following environment variables for Hitachi Cloud Accelerator Platform: HCAP_URL USERNAME PASSWORD Important: Ensure that the user account whose authentication details you specify, is a member of the required groups . Also, if your solution packages use pre-created providers in Deploy Accelerator, ensure that the user account can access the providers. Example export USERNAME=admin export PASSWORD=admin123 export HCAP_URL= https://hcap.hitachivantara.com","title":"Set environment variables"},{"location":"workflow/api-reference/#curl-command-syntax-and-options","text":"You can use the following cURL command syntax to make API requests to Workflow Accelerator. curl -X <Method> <URL> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ <Body> }' The cURL command includes the following sections: Method : This section contains the method used to call the API and the URL of the API. -X POST URL \\ Header : The values to be passed in the header of the command. The value for the authorization key must be your Cloud Accelerator Platform username and password, separated by a colon (:). The value for the content-type key must be application/json . -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ Body : The parameters to be passed in the body section of the URL. These values can be taken from the values defined in the solution package. -d '{ <data to be sent> }' Example curl -X PUT $HCAP_URL/api/hcapworkflowengine/solution-deploy/redeploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ solutionPackageId\": \"AXcdSDadssaUqCsd\", \"name\": \"DemoDeployment\", \"description\": \"This is a demo deployment\", \"inputParameters\": {\"vpc_cidr\": \"10.0.0.0/16\", \"zone\": \"us-east-1\"}, \"providers\":{ \"DemoProvider\": 8 } }'","title":"cURL command syntax and options"},{"location":"workflow/deploy-package/","text":"Deploy solution packages \u00b6 Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages across various cloud providers. This topic describes how to deploy the solution packages that you have registered. It also describes how to redeploy or destroy registered solution packages that you have already deployed. Contents \u00b6 Prerequisites Understanding the Solution Package Deployment Input Object structure Deploying a solution package Viewing the deployment status of a solution package Redeploying an existing deployment Destroying a deployment Viewing deployment logs Performing additional actions on deployments Prerequisites \u00b6 Before deploying a solution package, you must perform the following actions: Ensure that you have completed all prerequisites for Workflow Accelerator . Ensure that you have registered your solution package . Note the ID of the registered solution package. You will require this ID to deploy the solution package. Get IDs of the Deploy Accelerator providers that you have used in your solution package. The provider IDs are available on the List Providers page in Deploy Accelerator. Prepare the Solution Package Deployment Input Object. For more information, see Understanding the Solution Package Deployment Input Object structure . Understanding the Solution Package Deployment Input Object structure \u00b6 The Solution Package Deployment Input Object structure includes the following data objects: solutionPackageID name description inputParameters providers tags Based on your requirements, you must create the deployment input object for each solution package that you want to deploy. The following is an example of the Solution Package Deployment Input Object. { \"solutionPackageId\" : \"AW3URleQQYIgadxCC8hgJ\" , \"name\" : \"Staging_Env_Deploy\" , \"description\" : \"solution package developer demo\" , \"inputParameters\" : { \"name\" : \"demo\" , \"env\" : \"sample-demo\" , \"customer\" : \"hitachi\" , \"instance_type\" : \"t2.small\" , \"cidr\" : \"10.10.10.20/32\" , \"whitelisted_ip\" :[ \"71.124.103.45/32\" , \"76.212.44.133/16\" , \"210.19.95.45/32\" ], \"tagsTest\" : { \"project\" : \"Platform\" , \"buildUrl\" : \"true\" , \"environment\" : \"QA\" , \"product\" : \"Platform\" } }, \"providers\" : { \"aws-provider\" : 5 }, \"tags\" : { \"Environment\" : \"Sample\" } } solutionPackageID \u00b6 ID of the registered solution package. This ID is generated after registering the Solution Package. This is a mandatory parameter. name \u00b6 The name of the solution package deployment. The name has to be unique per solution package. This is a mandatory parameter. description \u00b6 Description of the solution package deployment. inputParameters \u00b6 Contains input parameters with its respective values required for the deployment. The name of the input parameter must be same as defined in the inputParameters object in the solution package JSON. The parameter in the object is defined in the following format: \"inputParameters\" : { \"Name\" : \"Value\" } Here, Name is the name of the parameter and Value is the value of the parameter. Consider the following points while defining the value of the parameter. If a parameter has predefined values, you must enter a value from the possibleValues defined in the inputParameters object in the Solution Package. If a parameter has a default value set in the Solution Package and it is not defined in deployment, the default value will be used. If a parameter does not have any value defined in the Solution Package, it is mandatory to define a value for the parameter during deployment. providers \u00b6 List of providers to be used for deployment. The parameter in the object is defined in the following format: \"providers\" : { \"Provider_name\" : <providerID> } Example \"providers\" : { \"aws-provider\" : 3 } The name of the provider must be same as defined in the providers section in the solution package JSON. The Provider_name parameter is the name of the provider in the Solution Package. Make sure that the Provider_name is same as defined in the Solution Package. The providerID value is the registered ID of the provider in Deploy Accelerator. Make sure that all the providerIDs defined in the Solution Package is entered in this object. For more information on registering providers in Deploy Accelerator using API, see Deploy Accelerator API documentation . You can override the default provider value at the time of Solution Package deployment by declaring a provider name and its provider ID. tags \u00b6 Additional tags for the solution. The tags must be in the following format: { \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag2 value\" } These tags can be used for creating a tag and searching a solution deployment with the tag name for a specific tag value. For example, you can create a tag called Environment that can have three values -- Testing , Staging , and Production . This will allow you to use the Environment parameter to search for a solution deployment that is in Production . Example { \"Environment\" : \"Testing\" } Deploying a solution package \u00b6 To deploy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X POST $HCAP_URL/api/hcapworkflowengine/solution-deploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ \"solutionPackageId\": \"SolutionPackageID\", \"name\": \"DeploymentName\", \"description\": \"Deployment description\", \"inputParameters\": { \"ParameterName\": \"value\", \"ParameterName\": \"Value\" }, \"providers\":{ \"ProviderName\": ProviderID, \"ProviderName\": ProviderID } }' Body JSON Example { \"solutionPackageId\" : \"AXcdSDadssaUqCsd\" , \"name\" : \"DemoDeployment\" , \"description\" : \"Deployment description\" , \"inputParameters\" : { \"vpc_cidr\" : \"10.0.0.0/16\" , \"zone\" : \"us-east-1\" }, \"providers\" :{ \"DemoProvider\" : 8 } } In the response section, confirm that the status of the solution package is DEPLOY_INITIATED . This status indicates that Workflow Accelerator has initiated the deployment of the workflow layers in the order in which they are listed in the solution package. For more information, see Viewing the deployment status of a solution package . Note: Make a note of the deployment ID as you will need it to view the deployment status of the solution package. You will also need this ID if you want to redeploy or destroy the deployed solution package. Redeploying an existing deployment \u00b6 To redeploy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X PUT $HCAP_URL/api/hcapworkflowengine/solution-deploy/redeploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ \"solutionPackageId\": \"SolutionPackageID\", \"name\": \"DeploymentName\", \"description\": \"Deployment description\", \"inputParameters\": {\"ParameterName\": \"value\", \"ParameterName\": \"Value\"}, \"providers\":{ \"ProviderName\": ProviderID, \"ProviderName\": ProviderID } }' Workflow Accelerator once again deploys the workflow layers in the order that is specified in the solution package. Destroying a deployment \u00b6 To destroy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X DELETE $HCAP_URL/api/hcapworkflowengine/solution-deploy/<SOLUTION_PACKAGE_DEPLOYMENT_ID> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' The order in which Workflow Accelerator destroys workflow layers in the solution package is the reverse of the deployment order. For example, the workflow layer that was deployed last is destroyed first. Viewing the deployment status of a solution package \u00b6 To view the deployment status of a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X GET $HCAP_URL/api/hcapworkflowengine/solution-deploy/<SOLUTION_PACKAGE_DEPLOYMENT_ID> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' In the response section, check the status attribute to view the deployment status of the solution package. The following table lists the status that is shown based on different scenarios. Scenario Status Deployment of a solution package has been initiated. DEPLOY_INITIATED Deployment of a solution package is in progress. DEPLOYING The solution package has been successfully deployed. Note: Make a note of the deployment ID as you will need it later if you want to redeploy or destroy this deployment of the solution package. DEPLOYED Deployment of any workflow layer in a solution package fails. FAILED Redeployment of an existing deployment of a solution package is in progress. DEPLOYING The Destroy action for an existing deployment of a solution package has been initiated DESTROY_INITIATED An existing deployment of a solution package is being destroyed. DESTROYING An existing deployment of a solution package is successfully destroyed. DESTROYED Note: To view the deployment status for a specific workflow layer in the solution package, check the workflowStatus attribute in the response section. Viewing deployment logs \u00b6 In the response section, check the recentError attribute to view any error messages related to the deployment of the solution package. You can also check the oldError attribute for viewing previous error messages. To view more detailed deployment logs of a failed environment in a solution package, you can sign in to Cloud Accelerator Platform and open the Deploy Accelerator environment whose deployment has failed. Ensure that the failed deployment is selected in the deployment list on the canvas and click the Logs icon. After you have fixed the issues with the environment, you can redeploy the existing deployment of the solution package . Performing additional actions on deployments \u00b6 The Workflow Engine REST APIs also allow you to perform a few additional actions on the deployments of solution packages. For example, you can get a list of all your deployed solution packages, get the solution package deployments by solution package ID, or get the solution package deployments by solution package ID and deployment name. For information about accessing the Workflow Engine API documentation, see Workflow Accelerator API Reference .","title":"Deploy solution package"},{"location":"workflow/deploy-package/#deploy-solution-packages","text":"Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages across various cloud providers. This topic describes how to deploy the solution packages that you have registered. It also describes how to redeploy or destroy registered solution packages that you have already deployed.","title":"Deploy solution packages"},{"location":"workflow/deploy-package/#contents","text":"Prerequisites Understanding the Solution Package Deployment Input Object structure Deploying a solution package Viewing the deployment status of a solution package Redeploying an existing deployment Destroying a deployment Viewing deployment logs Performing additional actions on deployments","title":"Contents"},{"location":"workflow/deploy-package/#prerequisites","text":"Before deploying a solution package, you must perform the following actions: Ensure that you have completed all prerequisites for Workflow Accelerator . Ensure that you have registered your solution package . Note the ID of the registered solution package. You will require this ID to deploy the solution package. Get IDs of the Deploy Accelerator providers that you have used in your solution package. The provider IDs are available on the List Providers page in Deploy Accelerator. Prepare the Solution Package Deployment Input Object. For more information, see Understanding the Solution Package Deployment Input Object structure .","title":"Prerequisites"},{"location":"workflow/deploy-package/#understanding-the-solution-package-deployment-input-object-structure","text":"The Solution Package Deployment Input Object structure includes the following data objects: solutionPackageID name description inputParameters providers tags Based on your requirements, you must create the deployment input object for each solution package that you want to deploy. The following is an example of the Solution Package Deployment Input Object. { \"solutionPackageId\" : \"AW3URleQQYIgadxCC8hgJ\" , \"name\" : \"Staging_Env_Deploy\" , \"description\" : \"solution package developer demo\" , \"inputParameters\" : { \"name\" : \"demo\" , \"env\" : \"sample-demo\" , \"customer\" : \"hitachi\" , \"instance_type\" : \"t2.small\" , \"cidr\" : \"10.10.10.20/32\" , \"whitelisted_ip\" :[ \"71.124.103.45/32\" , \"76.212.44.133/16\" , \"210.19.95.45/32\" ], \"tagsTest\" : { \"project\" : \"Platform\" , \"buildUrl\" : \"true\" , \"environment\" : \"QA\" , \"product\" : \"Platform\" } }, \"providers\" : { \"aws-provider\" : 5 }, \"tags\" : { \"Environment\" : \"Sample\" } }","title":"Understanding the Solution Package Deployment Input Object structure"},{"location":"workflow/deploy-package/#solutionpackageid","text":"ID of the registered solution package. This ID is generated after registering the Solution Package. This is a mandatory parameter.","title":"solutionPackageID"},{"location":"workflow/deploy-package/#name","text":"The name of the solution package deployment. The name has to be unique per solution package. This is a mandatory parameter.","title":"name"},{"location":"workflow/deploy-package/#description","text":"Description of the solution package deployment.","title":"description"},{"location":"workflow/deploy-package/#inputparameters","text":"Contains input parameters with its respective values required for the deployment. The name of the input parameter must be same as defined in the inputParameters object in the solution package JSON. The parameter in the object is defined in the following format: \"inputParameters\" : { \"Name\" : \"Value\" } Here, Name is the name of the parameter and Value is the value of the parameter. Consider the following points while defining the value of the parameter. If a parameter has predefined values, you must enter a value from the possibleValues defined in the inputParameters object in the Solution Package. If a parameter has a default value set in the Solution Package and it is not defined in deployment, the default value will be used. If a parameter does not have any value defined in the Solution Package, it is mandatory to define a value for the parameter during deployment.","title":"inputParameters"},{"location":"workflow/deploy-package/#providers","text":"List of providers to be used for deployment. The parameter in the object is defined in the following format: \"providers\" : { \"Provider_name\" : <providerID> } Example \"providers\" : { \"aws-provider\" : 3 } The name of the provider must be same as defined in the providers section in the solution package JSON. The Provider_name parameter is the name of the provider in the Solution Package. Make sure that the Provider_name is same as defined in the Solution Package. The providerID value is the registered ID of the provider in Deploy Accelerator. Make sure that all the providerIDs defined in the Solution Package is entered in this object. For more information on registering providers in Deploy Accelerator using API, see Deploy Accelerator API documentation . You can override the default provider value at the time of Solution Package deployment by declaring a provider name and its provider ID.","title":"providers"},{"location":"workflow/deploy-package/#tags","text":"Additional tags for the solution. The tags must be in the following format: { \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag2 value\" } These tags can be used for creating a tag and searching a solution deployment with the tag name for a specific tag value. For example, you can create a tag called Environment that can have three values -- Testing , Staging , and Production . This will allow you to use the Environment parameter to search for a solution deployment that is in Production . Example { \"Environment\" : \"Testing\" }","title":"tags"},{"location":"workflow/deploy-package/#deploying-a-solution-package","text":"To deploy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X POST $HCAP_URL/api/hcapworkflowengine/solution-deploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ \"solutionPackageId\": \"SolutionPackageID\", \"name\": \"DeploymentName\", \"description\": \"Deployment description\", \"inputParameters\": { \"ParameterName\": \"value\", \"ParameterName\": \"Value\" }, \"providers\":{ \"ProviderName\": ProviderID, \"ProviderName\": ProviderID } }' Body JSON Example { \"solutionPackageId\" : \"AXcdSDadssaUqCsd\" , \"name\" : \"DemoDeployment\" , \"description\" : \"Deployment description\" , \"inputParameters\" : { \"vpc_cidr\" : \"10.0.0.0/16\" , \"zone\" : \"us-east-1\" }, \"providers\" :{ \"DemoProvider\" : 8 } } In the response section, confirm that the status of the solution package is DEPLOY_INITIATED . This status indicates that Workflow Accelerator has initiated the deployment of the workflow layers in the order in which they are listed in the solution package. For more information, see Viewing the deployment status of a solution package . Note: Make a note of the deployment ID as you will need it to view the deployment status of the solution package. You will also need this ID if you want to redeploy or destroy the deployed solution package.","title":"Deploying a solution package"},{"location":"workflow/deploy-package/#redeploying-an-existing-deployment","text":"To redeploy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X PUT $HCAP_URL/api/hcapworkflowengine/solution-deploy/redeploy \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ \"solutionPackageId\": \"SolutionPackageID\", \"name\": \"DeploymentName\", \"description\": \"Deployment description\", \"inputParameters\": {\"ParameterName\": \"value\", \"ParameterName\": \"Value\"}, \"providers\":{ \"ProviderName\": ProviderID, \"ProviderName\": ProviderID } }' Workflow Accelerator once again deploys the workflow layers in the order that is specified in the solution package.","title":"Redeploying an existing deployment"},{"location":"workflow/deploy-package/#destroying-a-deployment","text":"To destroy a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X DELETE $HCAP_URL/api/hcapworkflowengine/solution-deploy/<SOLUTION_PACKAGE_DEPLOYMENT_ID> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' The order in which Workflow Accelerator destroys workflow layers in the solution package is the reverse of the deployment order. For example, the workflow layer that was deployed last is destroyed first.","title":"Destroying a deployment"},{"location":"workflow/deploy-package/#viewing-the-deployment-status-of-a-solution-package","text":"To view the deployment status of a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X GET $HCAP_URL/api/hcapworkflowengine/solution-deploy/<SOLUTION_PACKAGE_DEPLOYMENT_ID> \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' In the response section, check the status attribute to view the deployment status of the solution package. The following table lists the status that is shown based on different scenarios. Scenario Status Deployment of a solution package has been initiated. DEPLOY_INITIATED Deployment of a solution package is in progress. DEPLOYING The solution package has been successfully deployed. Note: Make a note of the deployment ID as you will need it later if you want to redeploy or destroy this deployment of the solution package. DEPLOYED Deployment of any workflow layer in a solution package fails. FAILED Redeployment of an existing deployment of a solution package is in progress. DEPLOYING The Destroy action for an existing deployment of a solution package has been initiated DESTROY_INITIATED An existing deployment of a solution package is being destroyed. DESTROYING An existing deployment of a solution package is successfully destroyed. DESTROYED Note: To view the deployment status for a specific workflow layer in the solution package, check the workflowStatus attribute in the response section.","title":"Viewing the deployment status of a solution package"},{"location":"workflow/deploy-package/#viewing-deployment-logs","text":"In the response section, check the recentError attribute to view any error messages related to the deployment of the solution package. You can also check the oldError attribute for viewing previous error messages. To view more detailed deployment logs of a failed environment in a solution package, you can sign in to Cloud Accelerator Platform and open the Deploy Accelerator environment whose deployment has failed. Ensure that the failed deployment is selected in the deployment list on the canvas and click the Logs icon. After you have fixed the issues with the environment, you can redeploy the existing deployment of the solution package .","title":"Viewing deployment logs"},{"location":"workflow/deploy-package/#performing-additional-actions-on-deployments","text":"The Workflow Engine REST APIs also allow you to perform a few additional actions on the deployments of solution packages. For example, you can get a list of all your deployed solution packages, get the solution package deployments by solution package ID, or get the solution package deployments by solution package ID and deployment name. For information about accessing the Workflow Engine API documentation, see Workflow Accelerator API Reference .","title":"Performing additional actions on deployments"},{"location":"workflow/getting-started/","text":"Overview of Workflow Accelerator \u00b6 Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages. It is an orchestration tool that can stitch together multiple workflow layers to deliver an end-to-end solution. You can use Workflow Accelerator to develop your solutions once and then deploy them multiple times by passing the appropriate set of inputs. Before you start creating solution packages, you must understand the following building blocks of Workflow Accelerator: Solution package Workflow actions Deployment tools Authentication mechanisms As the next step, you can view the prerequisites for using Workflow Accelerator and understand the solution package schema before you create and register and deploy solution packages. Solution package \u00b6 Basically, a solution package in Workflow Accelerator is a JSON file that contains information required for installing an end-to-end solution across various cloud platforms. It uses a predefined schema to define the different workflow layers of a solution, the deployment sequence of these layers, input and output parameters, deployment parameters, authentication details for deploying each layer, and other required information. For example, a solution package for an application can contain layers for networking infrastructure, a Kubernetes cluster, and the application itself. It can define that the networking infrastructure layer must be deployed first, followed by the Kubernetes cluster layer, and then finally the application layer. It can also use the output of the networking layer to deploy the Kubernetes cluster, and the output of the Kubernetes cluster layer to deploy the application layer. To register and deploy the solution packages that you have created, you have to use REST APIs . Workflow Accelerator currently does not have a user interface. Workflow actions \u00b6 Each workflow layer in a solution package specifies the type of action to be performed. Currently, Workflow Accelerator supports the following actions: Create providers in Deploy Accelerator Deploy environments through Deploy Accelerator Register Foundry solution packages Deploy Foundry solution packages Register Helm repositories with the Helm service in Workflow Accelerator Deploy Helm charts to a Kubernetes cluster from a Helm repository Deploy other solution packages Deployment tools \u00b6 The Workflow Accelerator uses the appropriate deployment tool based on the type of action to be performed by a workflow layer. For the latest supported workflow actions, it uses the deployment tools listed in the following table. Deployment tool Description Deploy Accelerator Deployment automation tool that enables you to deploy environments, which are a collection of resources that you need to deploy application stacks across various cloud platforms. Workflow Accelerator Helm service Inbuilt service available with the Workflow Accelerator that enables you to register Helm repositories and deploy Helm charts. Hitachi Foundry Framework for the rapid development, curation, and management of service-oriented software solutions. Enables you to deploy Foundry solution packages. Authentication mechanism \u00b6 Workflow Accelerator leverages providers in Deploy Accelerator to access the authentication details that are required to deploy environments. For example, Workflow Accelerator references the appropriate AWS provider in Deploy Accelerator to get authentication details for accessing the AWS account in which infrastructure needs to be deployed. To deploy Helm charts and Foundry solution packages, the authentication details for Helm repository and OCI Harbor repository have to be provided while deploying solution packages.","title":"Overview"},{"location":"workflow/getting-started/#overview-of-workflow-accelerator","text":"Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages. It is an orchestration tool that can stitch together multiple workflow layers to deliver an end-to-end solution. You can use Workflow Accelerator to develop your solutions once and then deploy them multiple times by passing the appropriate set of inputs. Before you start creating solution packages, you must understand the following building blocks of Workflow Accelerator: Solution package Workflow actions Deployment tools Authentication mechanisms As the next step, you can view the prerequisites for using Workflow Accelerator and understand the solution package schema before you create and register and deploy solution packages.","title":"Overview of Workflow Accelerator"},{"location":"workflow/getting-started/#solution-package","text":"Basically, a solution package in Workflow Accelerator is a JSON file that contains information required for installing an end-to-end solution across various cloud platforms. It uses a predefined schema to define the different workflow layers of a solution, the deployment sequence of these layers, input and output parameters, deployment parameters, authentication details for deploying each layer, and other required information. For example, a solution package for an application can contain layers for networking infrastructure, a Kubernetes cluster, and the application itself. It can define that the networking infrastructure layer must be deployed first, followed by the Kubernetes cluster layer, and then finally the application layer. It can also use the output of the networking layer to deploy the Kubernetes cluster, and the output of the Kubernetes cluster layer to deploy the application layer. To register and deploy the solution packages that you have created, you have to use REST APIs . Workflow Accelerator currently does not have a user interface.","title":"Solution package"},{"location":"workflow/getting-started/#workflow-actions","text":"Each workflow layer in a solution package specifies the type of action to be performed. Currently, Workflow Accelerator supports the following actions: Create providers in Deploy Accelerator Deploy environments through Deploy Accelerator Register Foundry solution packages Deploy Foundry solution packages Register Helm repositories with the Helm service in Workflow Accelerator Deploy Helm charts to a Kubernetes cluster from a Helm repository Deploy other solution packages","title":"Workflow actions"},{"location":"workflow/getting-started/#deployment-tools","text":"The Workflow Accelerator uses the appropriate deployment tool based on the type of action to be performed by a workflow layer. For the latest supported workflow actions, it uses the deployment tools listed in the following table. Deployment tool Description Deploy Accelerator Deployment automation tool that enables you to deploy environments, which are a collection of resources that you need to deploy application stacks across various cloud platforms. Workflow Accelerator Helm service Inbuilt service available with the Workflow Accelerator that enables you to register Helm repositories and deploy Helm charts. Hitachi Foundry Framework for the rapid development, curation, and management of service-oriented software solutions. Enables you to deploy Foundry solution packages.","title":"Deployment tools"},{"location":"workflow/getting-started/#authentication-mechanism","text":"Workflow Accelerator leverages providers in Deploy Accelerator to access the authentication details that are required to deploy environments. For example, Workflow Accelerator references the appropriate AWS provider in Deploy Accelerator to get authentication details for accessing the AWS account in which infrastructure needs to be deployed. To deploy Helm charts and Foundry solution packages, the authentication details for Helm repository and OCI Harbor repository have to be provided while deploying solution packages.","title":"Authentication mechanism"},{"location":"workflow/howto-guides/","text":"How-to guides for Workflow Accelerator \u00b6 The How-to guides for Workflow Accelerator focus on achieving specific goals, performing common tasks, or resolving specific issues. These guides include high-levels steps or detailed procedures with examples to help you to quickly accomplish your goals. Contents \u00b6","title":"How-to guides for Workflow Accelerator"},{"location":"workflow/howto-guides/#how-to-guides-for-workflow-accelerator","text":"The How-to guides for Workflow Accelerator focus on achieving specific goals, performing common tasks, or resolving specific issues. These guides include high-levels steps or detailed procedures with examples to help you to quickly accomplish your goals.","title":"How-to guides for Workflow Accelerator"},{"location":"workflow/howto-guides/#contents","text":"","title":"Contents"},{"location":"workflow/prereqs/","text":"Prerequisites for using Workflow Accelerator \u00b6 Before you start using Hitachi Cloud Accelerator Platform - Workflow Accelerator to create and deploy solution packages, you must perform the following prerequisite actions: Creating an account in Hitachi Cloud Accelerator Platform Creating a cloud provider Learning about Deploy Accelerator After completing the prerequisites the next steps are to understand the solution package schema and then create and register and deploy solution packages. Creating an account in Hitachi Cloud Accelerator Platform \u00b6 To perform actions in Workflow Accelerator, you require a user account in Hitachi Cloud Accelerator Platform. For more information about creating your account, see Creating a Cloud Accelerator Platform account. Contact your Cloud Accelerator Platform administrator to ensure that your user account is a member of the following groups: SOLUTION_PACKAGE_USER SOLUTION_PACKAGE_DEPLOYMENT_USER CLOUD_ARCHITECT HELM_USER For information about the access level granted by these groups, see Default groups . Creating a cloud account \u00b6 To deploy resources in the cloud using solution packages, create an account for the appropriate cloud service provider (for example, Amazon Web Services). Workflow Accelerator supports all cloud providers that Deploy Accelerator supports . Ensure that your cloud account has the access to deploy the required resources. Learning about Deploy Accelerator \u00b6 Workflow Accelerator works closely with Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). Therefore, it is recommended that you have a good knowledge of Deploy Accelerator, especially how to create environments , configure providers , export environments as blueprints , and start new deployments . For more information, see Overview of Deploy Accelerator .","title":"Prerequisites for using Workflow Accelerator"},{"location":"workflow/prereqs/#prerequisites-for-using-workflow-accelerator","text":"Before you start using Hitachi Cloud Accelerator Platform - Workflow Accelerator to create and deploy solution packages, you must perform the following prerequisite actions: Creating an account in Hitachi Cloud Accelerator Platform Creating a cloud provider Learning about Deploy Accelerator After completing the prerequisites the next steps are to understand the solution package schema and then create and register and deploy solution packages.","title":"Prerequisites for using Workflow Accelerator"},{"location":"workflow/prereqs/#creating-an-account-in-hitachi-cloud-accelerator-platform","text":"To perform actions in Workflow Accelerator, you require a user account in Hitachi Cloud Accelerator Platform. For more information about creating your account, see Creating a Cloud Accelerator Platform account. Contact your Cloud Accelerator Platform administrator to ensure that your user account is a member of the following groups: SOLUTION_PACKAGE_USER SOLUTION_PACKAGE_DEPLOYMENT_USER CLOUD_ARCHITECT HELM_USER For information about the access level granted by these groups, see Default groups .","title":"Creating an account in Hitachi Cloud Accelerator Platform"},{"location":"workflow/prereqs/#creating-a-cloud-account","text":"To deploy resources in the cloud using solution packages, create an account for the appropriate cloud service provider (for example, Amazon Web Services). Workflow Accelerator supports all cloud providers that Deploy Accelerator supports . Ensure that your cloud account has the access to deploy the required resources.","title":"Creating a cloud account"},{"location":"workflow/prereqs/#learning-about-deploy-accelerator","text":"Workflow Accelerator works closely with Hitachi Cloud Accelerator Platform - Deploy (Deploy Accelerator). Therefore, it is recommended that you have a good knowledge of Deploy Accelerator, especially how to create environments , configure providers , export environments as blueprints , and start new deployments . For more information, see Overview of Deploy Accelerator .","title":"Learning about Deploy Accelerator"},{"location":"workflow/solution-package-schema-v1/","text":"Solution package schema version 1.2 \u00b6 Solution package in Workflow Accelerator is a JSON file that allows you to define all the information that is required for installing end-to-end application across various cloud platforms. To register and deploy the solution packages that you create, you have to use REST APIs. This page describes the version 1.2 of the solution package schema. To view the most recent schema, see Understand the solution package schema. Solution package example \u00b6 Each solution package contains multiple JSON blocks for different data object types. The following is an example of the solution package for installing Magento E-Commerce application: { \"schemaVersion\" : \"1.2\" , \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@reancloud.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"license-321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } }, \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ], \"workflow\" : [ \"NwLayer:01.00.00\" , \"Magento:01.00.00\" ], \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" }, \"providers\" : [ { \"name\" : \"aws-provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ], \"input\" : { \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment\" }, { \"name\" : \"customer\" , \"type\" : \"String\" , \"description\" : \"Customer Name\" }, { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }, { \"name\" : \"db_name\" , \"type\" : \"String\" , \"description\" : \"db_name\" }, { \"name\" : \"db_password\" , \"type\" : \"String\" , \"description\" : \"db_password\" }, { \"name\" : \"admin_email\" , \"type\" : \"String\" , \"description\" : \"Email id for the admin\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ], \"inputMapping\" : { \"NwLayer\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" }, { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" , \"type\" : \"String\" }, { \"key\" : \"whitelisted_ip\" , \"value\" : \"${#input.whitelisted_ip}\" , \"type\" : \"List\" }, { \"key\" : \"tagsTest\" , \"value\" : \"${#input.tagsTest}\" , \"type\" : \"Map\" } ] }, \"MagentoApp\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" }, { \"key\" : \"instance_size\" , \"value\" : \"${#input.instance_size}\" }, { \"key\" : \"db_name\" , \"value\" : \"${#input.db_name}\" }, { \"key\" : \"db_password\" , \"value\" : \"${#input.db_password}\" }, { \"key\" : \"admin_email\" , \"value\" : \"${#input.admin_email}\" } { \"key\" : \"VPC_ID\" , \"value\" : \"${#output['NwLayer']['vpc_id']}\" } { \"key\" : \"Customer_full_ID\" , \"value\" : \"${#output['NwLayer']'customer_name']}-${#output['NwLayer']['env']}\" } ] } } } } Data objects in the solution package schema \u00b6 The solution package schema contains the following data objects: schemaVersion metadata blueprints providers workflow customData input inputParameters inputMapping schemaVersion \u00b6 The version of the solution package schema. The entry for this parameter must 1.0 by default. You can search a solution package using the schema version attribute. This is a mandatory parameter. metadata \u00b6 This data object contains the metadata about the Solution package and the application to be installed using Current Solution package. The information in the metadata section is used for displaying, accessing and searching the solution package by using parameters like name, version , language, cloud region, etc. The object must be in the following format: \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento e-commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@reancloud.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"License#321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } } Following are the attributes of the metadata object: name (mandatory parameter) \u00b6 The name of the solution package. The solution name and version must be unique per user version (mandatory parameter) \u00b6 The version of the solution package. You can create multiple versions for a single solution name. description \u00b6 Description of the solution. You can add a detailed information about the application in this parameter. icon \u00b6 URL to the location of the Application icon image. applicationVersion \u00b6 The version of the application which is getting deployed as a part of this solution package ownerName \u00b6 The name of the owner of the solution package. This is a mandatory parameter. ownerEmail \u00b6 The email ID of the owner of the solution package. This is a mandatory parameter. duration \u00b6 A user entry to define the minimum and maximum deployment time required by the solution package. The object must be in the following format: \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" } The correct format for entry of time 1d 3h 20m for 1 day, 3 hours and 20 minutes. This format does not support time entry in seconds. language \u00b6 List of languages supported by the application to be deployed using solution package. The list must be the language code in ISO standard. For example, \"language\" : [ \"en_US\" ] For more information on the language codes, in the ISO website, see Codes for the Representation of Names of Languages . licenseName \u00b6 The license required for using the solution package. Before entering the license name here, make sure that the license is created for the solution package. For more information on the creation of the license key, see the license-controller API documentation in Solution Package. The license-control API documentation is available at following URL: https://{HCAP-base-URL}/api-documentation/solutionpackagedoc/swagger-ui.html#/license-controller cloudRegion (mandatory parameter) \u00b6 List of cloud region where the solution package is to be deployed. For example, \"cloudRegion\" : [ \"us-east-1\" ] vertical \u00b6 Vertical of the organization to which the application belongs. tags \u00b6 Additional properties for the solution. The tags must be in the following format: { \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag2 value\" } These tags can be used for creating a tag and searching a solution with the tag name for a specific tag value. For example, you can tag a solution package as { \"domain\" : \"commerce\" }, which can be used for searching all the applications in the Finance domain. \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } blueprints \u00b6 This object contains the list of blueprints that will get automatically imported from the artifactory before starting the deployment of the application. In the artifactory, these blueprints must be stored in the repository that is configured in Deploy Accelerator. Environments in the blueprint are imported into Deploy Accelerator with the naming convention _ . Note : Blueprint will get imported only once, the solution package will reuse the previously imported blueprint if it is already present in Deploy Accelerator. The blueprint must be present in the artifactory with the provided name and version, otherwise complete deployment of the solution package will fail. The object must be in the following format: \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ] name \u00b6 Name of the blueprint. The entry must be exactly same as the name of the blueprint in the artifactory. version \u00b6 Version of the blueprint for the specified name. Note: You can also choose to manually import blueprints into Deploy Accelerator. In this case, you must specify an empty section for the blueprints object, as shown below. \"blueprints\" : [] workflow \u00b6 The execution order of the Deploy Accelerator environments. The Environment Name and Version must be same as declared in the blueprints data object. Make sure that the environment is present in Deploy Accelerator before starting the deployment . The object must be in the following format: \"workflow\" : [ \"environment_name_1:environment_1_version_number\" , \"environment_name_2:environment_2_version_number\" ] Note: The naming convention used for environments in a blueprint that are imported from the artifactory is _ . However, while specifying these environments in the workflow object, make sure that you enter only the . Before starting the deployment, the _ is automatically prefixed to the environment name. For example: \"workflow\" : [ \"NwLayer:01.00.00\" , \"Magento:01.00.00\" ] customData \u00b6 This object allows you to define custom attributes for the solution package. These custom attributes can be used as a search parameter for the solution package. The object must be in the following format: \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" } providers \u00b6 The list of providers that the environments defined in the workflow will be using while deployment of the blueprint. The object must be in the following format: \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] Depending on the target cloud provider and account in the blueprint, you can define the providers list in the following ways: Case 1 : All the blueprints for the solution are to be deployed on the same Cloud service Provider account. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" } ] Case 2 : Blueprints in the solution package are to be deployed on two different accounts of the same cloud service provider. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"aws_account2\" , \"type\" : \"aws\" } ] Case 3 : Blueprints in the solution package targeting multiple cloud providers. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"k8s_provider\" , \"type\" : \"helm\" } ] Note: Make sure you have added all providers in the providers list that will be required for deploying the blueprints. name \u00b6 Name of the provider. The entry must be exactly same as the provider declared in the blueprint. type \u00b6 Cloud service provider. For example, AWS, Google Cloud, Microsoft Azure, etc. default_provider_name \u00b6 The default_provider_name parameter allows you to define a default provider for your solution package. The value of this parameter must be same as the name of the provider created in Deploy Accelerator. If required, you can override the default provider value at the time of deployment. You can define multiple default providers in solution package. This is an optional parameter. \"providers\" : [ { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] input \u00b6 The input data object contains 2 sub objects inputParameters and inputMapping . The object must be in the following format: \"input\" : { \"inputParmaters\" : { /*Parame ter de f i n i t io n */ }, \"inputMapping\" : { /* \"Environment name\" */ { /*Parame ter Mappi n g*/ } } } inputParameters \u00b6 The inputParameters object contains the declaration of unique parameters that are required for deploying the environment. This section consists of all unique sets of inputs that the solution owner wants the user to enter while deploying the solution package. For example, instance_size, which can be used in the one of the blueprint to set the ec2 instance size \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment\" }, { \"name\" : \"customer\" , \"type\" : \"String\" , \"description\" : \"Customer Name\" }, { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }, { \"name\" : \"db_name\" , \"type\" : \"String\" , \"description\" : \"db_name\" }, { \"name\" : \"db_password\" , \"type\" : \"String\" , \"description\" : \"db_password\" }, { \"name\" : \"admin_email\" , \"type\" : \"String\" , \"description\" : \"Email id for the admin\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ] name \u00b6 Name of the parameter. The name must be a single string without any space. description \u00b6 Description of the parameter. You can add a detailed information about the application in this parameter. It is a free text area. type \u00b6 The data type of the input parameter. Supported data types are: List, String, Boolean, Json, Integer The default value for datatype is String. defaultValue \u00b6 The default value of the parameter, if no value is entered while deployment. possibleValues \u00b6 The list of possible values from which the user must select a value. Example: inputParameter \u00b6 { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] } inputMapping \u00b6 The inputMapping object contains the declaration of input parameter that will be used for a specific environment with a specific provider. The mapping of input parameter is done for an environment. An environment can have multiple parameters under it. You can declare parameters for multiple environments in single inputMapping object. \"inputMapping\" : { \"NwLayer\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" }, { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" , \"type\" : \"String\" }, { \"key\" : \"whitelisted_ip\" , \"value\" : \"${#input.whitelisted_ip}\" , \"type\" : \"List\" }, { \"key\" : \"tagsTest\" , \"value\" : \"${#input.tagsTest}\" , \"type\" : \"Map\" } ] } } environment name (dynamic parameter) \u00b6 The environment name for which the following parameters are used for deployment. provider \u00b6 The provider to be used by the environment for deployment. deployInput \u00b6 The values for the parameters to be used while deploying the environment using the solution package. key \u00b6 The exact name as defined in the inputParameter object. value \u00b6 The value for the inputParameter key. There are three types of input mapping values: Case 1 : Mapping values for input parameters used in the solution package. { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"String\" } Case 2 : Mapping environment output parameter of earlier deployed environment with the input parameter. When solution package has two blueprints which are not connected using Depends On feature of Deploy Accelerator, the output parameter of the parent blueprint must be used in the child blueprint. { \"key\" : \"vpc_id\" , \"value\" : \"${#output['NwLayer']['vpc_id']}\" , \"type\" : \"String\" } In this case NWLayer is name of the environment from where the output parameter will be received. Case 3 : Mapping input parameter with multiple input parameters or environment output. { \"key\" : \"resource-name-prefix\" , \"value\" : \"${#output['NwLayer']['customer_name']}-${#output['NwLayer']['env']}\" , \"type\" : \"String\" } type \u00b6 The type of the inputParameter key. The supported types are are List , String , Boolean , JSON , and Integer . The value that you enter for this parameter is case sensitive. Therefore, enter the supported types as mentioned in this section. If you enter any other value (for example: map ), registration of the solution package will fail. The default value of the parameter is String , if no value is entered while creating the Solution Package. Note: The value of the inputParameter key is resolved based on the type of the inputParameter. If you specify an incorrect type, complete deployment of the solution package will fail.","title":"Solution package schema version 1.2"},{"location":"workflow/solution-package-schema-v1/#solution-package-schema-version-12","text":"Solution package in Workflow Accelerator is a JSON file that allows you to define all the information that is required for installing end-to-end application across various cloud platforms. To register and deploy the solution packages that you create, you have to use REST APIs. This page describes the version 1.2 of the solution package schema. To view the most recent schema, see Understand the solution package schema.","title":"Solution package schema version 1.2"},{"location":"workflow/solution-package-schema-v1/#solution-package-example","text":"Each solution package contains multiple JSON blocks for different data object types. The following is an example of the solution package for installing Magento E-Commerce application: { \"schemaVersion\" : \"1.2\" , \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@reancloud.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"license-321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } }, \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ], \"workflow\" : [ \"NwLayer:01.00.00\" , \"Magento:01.00.00\" ], \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" }, \"providers\" : [ { \"name\" : \"aws-provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ], \"input\" : { \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment\" }, { \"name\" : \"customer\" , \"type\" : \"String\" , \"description\" : \"Customer Name\" }, { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }, { \"name\" : \"db_name\" , \"type\" : \"String\" , \"description\" : \"db_name\" }, { \"name\" : \"db_password\" , \"type\" : \"String\" , \"description\" : \"db_password\" }, { \"name\" : \"admin_email\" , \"type\" : \"String\" , \"description\" : \"Email id for the admin\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ], \"inputMapping\" : { \"NwLayer\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" }, { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" , \"type\" : \"String\" }, { \"key\" : \"whitelisted_ip\" , \"value\" : \"${#input.whitelisted_ip}\" , \"type\" : \"List\" }, { \"key\" : \"tagsTest\" , \"value\" : \"${#input.tagsTest}\" , \"type\" : \"Map\" } ] }, \"MagentoApp\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" }, { \"key\" : \"instance_size\" , \"value\" : \"${#input.instance_size}\" }, { \"key\" : \"db_name\" , \"value\" : \"${#input.db_name}\" }, { \"key\" : \"db_password\" , \"value\" : \"${#input.db_password}\" }, { \"key\" : \"admin_email\" , \"value\" : \"${#input.admin_email}\" } { \"key\" : \"VPC_ID\" , \"value\" : \"${#output['NwLayer']['vpc_id']}\" } { \"key\" : \"Customer_full_ID\" , \"value\" : \"${#output['NwLayer']'customer_name']}-${#output['NwLayer']['env']}\" } ] } } } }","title":"Solution package example"},{"location":"workflow/solution-package-schema-v1/#data-objects-in-the-solution-package-schema","text":"The solution package schema contains the following data objects: schemaVersion metadata blueprints providers workflow customData input inputParameters inputMapping","title":"Data objects in the solution package schema"},{"location":"workflow/solution-package-schema-v1/#schemaversion","text":"The version of the solution package schema. The entry for this parameter must 1.0 by default. You can search a solution package using the schema version attribute. This is a mandatory parameter.","title":"schemaVersion"},{"location":"workflow/solution-package-schema-v1/#metadata","text":"This data object contains the metadata about the Solution package and the application to be installed using Current Solution package. The information in the metadata section is used for displaying, accessing and searching the solution package by using parameters like name, version , language, cloud region, etc. The object must be in the following format: \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento e-commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@reancloud.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"License#321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } } Following are the attributes of the metadata object:","title":"metadata"},{"location":"workflow/solution-package-schema-v1/#name-mandatory-parameter","text":"The name of the solution package. The solution name and version must be unique per user","title":"name (mandatory parameter)"},{"location":"workflow/solution-package-schema-v1/#version-mandatory-parameter","text":"The version of the solution package. You can create multiple versions for a single solution name.","title":"version (mandatory parameter)"},{"location":"workflow/solution-package-schema-v1/#description","text":"Description of the solution. You can add a detailed information about the application in this parameter.","title":"description"},{"location":"workflow/solution-package-schema-v1/#icon","text":"URL to the location of the Application icon image.","title":"icon"},{"location":"workflow/solution-package-schema-v1/#applicationversion","text":"The version of the application which is getting deployed as a part of this solution package","title":"applicationVersion"},{"location":"workflow/solution-package-schema-v1/#ownername","text":"The name of the owner of the solution package. This is a mandatory parameter.","title":"ownerName"},{"location":"workflow/solution-package-schema-v1/#owneremail","text":"The email ID of the owner of the solution package. This is a mandatory parameter.","title":"ownerEmail"},{"location":"workflow/solution-package-schema-v1/#duration","text":"A user entry to define the minimum and maximum deployment time required by the solution package. The object must be in the following format: \"duration\" : { \"minDeploymentTime\" : \"1d 3h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" } The correct format for entry of time 1d 3h 20m for 1 day, 3 hours and 20 minutes. This format does not support time entry in seconds.","title":"duration"},{"location":"workflow/solution-package-schema-v1/#language","text":"List of languages supported by the application to be deployed using solution package. The list must be the language code in ISO standard. For example, \"language\" : [ \"en_US\" ] For more information on the language codes, in the ISO website, see Codes for the Representation of Names of Languages .","title":"language"},{"location":"workflow/solution-package-schema-v1/#licensename","text":"The license required for using the solution package. Before entering the license name here, make sure that the license is created for the solution package. For more information on the creation of the license key, see the license-controller API documentation in Solution Package. The license-control API documentation is available at following URL: https://{HCAP-base-URL}/api-documentation/solutionpackagedoc/swagger-ui.html#/license-controller","title":"licenseName"},{"location":"workflow/solution-package-schema-v1/#cloudregion-mandatory-parameter","text":"List of cloud region where the solution package is to be deployed. For example, \"cloudRegion\" : [ \"us-east-1\" ]","title":"cloudRegion (mandatory parameter)"},{"location":"workflow/solution-package-schema-v1/#vertical","text":"Vertical of the organization to which the application belongs.","title":"vertical"},{"location":"workflow/solution-package-schema-v1/#tags","text":"Additional properties for the solution. The tags must be in the following format: { \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag2 value\" } These tags can be used for creating a tag and searching a solution with the tag name for a specific tag value. For example, you can tag a solution package as { \"domain\" : \"commerce\" }, which can be used for searching all the applications in the Finance domain. \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false }","title":"tags"},{"location":"workflow/solution-package-schema-v1/#blueprints","text":"This object contains the list of blueprints that will get automatically imported from the artifactory before starting the deployment of the application. In the artifactory, these blueprints must be stored in the repository that is configured in Deploy Accelerator. Environments in the blueprint are imported into Deploy Accelerator with the naming convention _ . Note : Blueprint will get imported only once, the solution package will reuse the previously imported blueprint if it is already present in Deploy Accelerator. The blueprint must be present in the artifactory with the provided name and version, otherwise complete deployment of the solution package will fail. The object must be in the following format: \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ]","title":"blueprints"},{"location":"workflow/solution-package-schema-v1/#name","text":"Name of the blueprint. The entry must be exactly same as the name of the blueprint in the artifactory.","title":"name"},{"location":"workflow/solution-package-schema-v1/#version","text":"Version of the blueprint for the specified name. Note: You can also choose to manually import blueprints into Deploy Accelerator. In this case, you must specify an empty section for the blueprints object, as shown below. \"blueprints\" : []","title":"version"},{"location":"workflow/solution-package-schema-v1/#workflow","text":"The execution order of the Deploy Accelerator environments. The Environment Name and Version must be same as declared in the blueprints data object. Make sure that the environment is present in Deploy Accelerator before starting the deployment . The object must be in the following format: \"workflow\" : [ \"environment_name_1:environment_1_version_number\" , \"environment_name_2:environment_2_version_number\" ] Note: The naming convention used for environments in a blueprint that are imported from the artifactory is _ . However, while specifying these environments in the workflow object, make sure that you enter only the . Before starting the deployment, the _ is automatically prefixed to the environment name. For example: \"workflow\" : [ \"NwLayer:01.00.00\" , \"Magento:01.00.00\" ]","title":"workflow"},{"location":"workflow/solution-package-schema-v1/#customdata","text":"This object allows you to define custom attributes for the solution package. These custom attributes can be used as a search parameter for the solution package. The object must be in the following format: \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" }","title":"customData"},{"location":"workflow/solution-package-schema-v1/#providers","text":"The list of providers that the environments defined in the workflow will be using while deployment of the blueprint. The object must be in the following format: \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] Depending on the target cloud provider and account in the blueprint, you can define the providers list in the following ways: Case 1 : All the blueprints for the solution are to be deployed on the same Cloud service Provider account. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" } ] Case 2 : Blueprints in the solution package are to be deployed on two different accounts of the same cloud service provider. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"aws_account2\" , \"type\" : \"aws\" } ] Case 3 : Blueprints in the solution package targeting multiple cloud providers. [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"k8s_provider\" , \"type\" : \"helm\" } ] Note: Make sure you have added all providers in the providers list that will be required for deploying the blueprints.","title":"providers"},{"location":"workflow/solution-package-schema-v1/#name_1","text":"Name of the provider. The entry must be exactly same as the provider declared in the blueprint.","title":"name"},{"location":"workflow/solution-package-schema-v1/#type","text":"Cloud service provider. For example, AWS, Google Cloud, Microsoft Azure, etc.","title":"type"},{"location":"workflow/solution-package-schema-v1/#default_provider_name","text":"The default_provider_name parameter allows you to define a default provider for your solution package. The value of this parameter must be same as the name of the provider created in Deploy Accelerator. If required, you can override the default provider value at the time of deployment. You can define multiple default providers in solution package. This is an optional parameter. \"providers\" : [ { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ]","title":"default_provider_name"},{"location":"workflow/solution-package-schema-v1/#input","text":"The input data object contains 2 sub objects inputParameters and inputMapping . The object must be in the following format: \"input\" : { \"inputParmaters\" : { /*Parame ter de f i n i t io n */ }, \"inputMapping\" : { /* \"Environment name\" */ { /*Parame ter Mappi n g*/ } } }","title":"input"},{"location":"workflow/solution-package-schema-v1/#inputparameters","text":"The inputParameters object contains the declaration of unique parameters that are required for deploying the environment. This section consists of all unique sets of inputs that the solution owner wants the user to enter while deploying the solution package. For example, instance_size, which can be used in the one of the blueprint to set the ec2 instance size \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment\" }, { \"name\" : \"customer\" , \"type\" : \"String\" , \"description\" : \"Customer Name\" }, { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }, { \"name\" : \"db_name\" , \"type\" : \"String\" , \"description\" : \"db_name\" }, { \"name\" : \"db_password\" , \"type\" : \"String\" , \"description\" : \"db_password\" }, { \"name\" : \"admin_email\" , \"type\" : \"String\" , \"description\" : \"Email id for the admin\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ]","title":"inputParameters"},{"location":"workflow/solution-package-schema-v1/#name_2","text":"Name of the parameter. The name must be a single string without any space.","title":"name"},{"location":"workflow/solution-package-schema-v1/#description_1","text":"Description of the parameter. You can add a detailed information about the application in this parameter. It is a free text area.","title":"description"},{"location":"workflow/solution-package-schema-v1/#type_1","text":"The data type of the input parameter. Supported data types are: List, String, Boolean, Json, Integer The default value for datatype is String.","title":"type"},{"location":"workflow/solution-package-schema-v1/#defaultvalue","text":"The default value of the parameter, if no value is entered while deployment.","title":"defaultValue"},{"location":"workflow/solution-package-schema-v1/#possiblevalues","text":"The list of possible values from which the user must select a value.","title":"possibleValues"},{"location":"workflow/solution-package-schema-v1/#example-inputparameter","text":"{ \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }","title":"Example: inputParameter"},{"location":"workflow/solution-package-schema-v1/#inputmapping","text":"The inputMapping object contains the declaration of input parameter that will be used for a specific environment with a specific provider. The mapping of input parameter is done for an environment. An environment can have multiple parameters under it. You can declare parameters for multiple environments in single inputMapping object. \"inputMapping\" : { \"NwLayer\" : { \"provider\" : \"aws-provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" }, { \"key\" : \"env\" , \"value\" : \"${#input.env}\" }, { \"key\" : \"customer\" , \"value\" : \"${#input.customer}\" , \"type\" : \"String\" }, { \"key\" : \"whitelisted_ip\" , \"value\" : \"${#input.whitelisted_ip}\" , \"type\" : \"List\" }, { \"key\" : \"tagsTest\" , \"value\" : \"${#input.tagsTest}\" , \"type\" : \"Map\" } ] } }","title":"inputMapping"},{"location":"workflow/solution-package-schema-v1/#environment-name-dynamic-parameter","text":"The environment name for which the following parameters are used for deployment.","title":"environment name (dynamic parameter)"},{"location":"workflow/solution-package-schema-v1/#provider","text":"The provider to be used by the environment for deployment.","title":"provider"},{"location":"workflow/solution-package-schema-v1/#deployinput","text":"The values for the parameters to be used while deploying the environment using the solution package.","title":"deployInput"},{"location":"workflow/solution-package-schema-v1/#key","text":"The exact name as defined in the inputParameter object.","title":"key"},{"location":"workflow/solution-package-schema-v1/#value","text":"The value for the inputParameter key. There are three types of input mapping values: Case 1 : Mapping values for input parameters used in the solution package. { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"String\" } Case 2 : Mapping environment output parameter of earlier deployed environment with the input parameter. When solution package has two blueprints which are not connected using Depends On feature of Deploy Accelerator, the output parameter of the parent blueprint must be used in the child blueprint. { \"key\" : \"vpc_id\" , \"value\" : \"${#output['NwLayer']['vpc_id']}\" , \"type\" : \"String\" } In this case NWLayer is name of the environment from where the output parameter will be received. Case 3 : Mapping input parameter with multiple input parameters or environment output. { \"key\" : \"resource-name-prefix\" , \"value\" : \"${#output['NwLayer']['customer_name']}-${#output['NwLayer']['env']}\" , \"type\" : \"String\" }","title":"value"},{"location":"workflow/solution-package-schema-v1/#type_2","text":"The type of the inputParameter key. The supported types are are List , String , Boolean , JSON , and Integer . The value that you enter for this parameter is case sensitive. Therefore, enter the supported types as mentioned in this section. If you enter any other value (for example: map ), registration of the solution package will fail. The default value of the parameter is String , if no value is entered while creating the Solution Package. Note: The value of the inputParameter key is resolved based on the type of the inputParameter. If you specify an incorrect type, complete deployment of the solution package will fail.","title":"type"},{"location":"workflow/understand-solution-package-schema/","text":"Understand the solution package schema \u00b6 Solution package in Workflow Accelerator is a JSON file that allows you to define all the information that is required for installing end-to-end solutions across various cloud platforms. To register and deploy the solution packages that you create, you have to use REST APIs. This page describes the most recent major version (for example: 2.x) of the solution package schema. To view the schema for older versions, see Solution Package schema 1.0 . Contents \u00b6 Sample solution package Data objects in solution package schema Supported workflow actions Sample solution package \u00b6 The following is an example of a solution package for installing an application. For information about the different JSON blocks in the solution package, see Data objects in solution package schema . { \"schemaVersion\" : \"2.1\" , \"metadata\" : { \"name\" : \"demo\" , \"version\" : \"00.00.01\" , \"description\" : \"demo\" , \"icon\" : \"https://google.com\" , \"applicationVersion\" : \"01.00.00\" , \"language\" : [ \"english\" , \"japanese\" ], \"ownerName\" : \"demo-user\" , \"ownerEmail\" : \"demo.user@companyname.com\" , \"cloudRegion\" : [ \"us-east-1\" , \"us-west-1\" ], \"vertical\" : [], \"tags\" : { \"tag1\" : \"value1\" , \"tag2\" : \"value2\" , \"app\" : \"SampleApp\" } }, \"input\" : { \"inputParameters\" : [ { \"name\" : \"cidr\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"10.0.0.0/16\" }, { \"name\" : \"isApp\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"isVpc\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"instance_type\" , \"type\" : \"String\" , \"default\" : \"t2.micro\" , \"description\" : \"instance type\" }, { \"name\" : \"customer_name\" , \"type\" : \"String\" , \"description\" : \"instance name\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment like prod, qa, etc\" }, { \"name\" : \"list-value\" , \"type\" : \"List\" , \"defaultValue\" : \"'demo-one','demo-two'\" }, { \"name\" : \"json-value\" , \"type\" : \"Json\" } ], \"inputMapping\" : { \"vpc\" : { \"provider\" : \"aws_provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.cidr}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#input['customer_name']}\" }, { \"key\" : \"env\" , \"value\" : \"${#input['env']}\" }, { \"key\" : \"pqr\" , \"value\" : \"${#input['list-value']}\" , \"type\" : \"List\" }, { \"key\" : \"p\" , \"value\" : \"${#input['json-value']}\" , \"type\" : \"Map\" } ] }, \"app\" : { \"deployInput\" : [ { \"key\" : \"instance_type\" , \"value\" : \"${#input['instance_type']}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#output['vpc']['customer']}\" }, { \"key\" : \"env1\" , \"value\" : \"{ '${#input['customer_name']}' : '${#input['customer_name']}' }\" , \"type\" : \"Map\" }, { \"key\" : \"vpc_id\" , \"value\" : \"${#output['vpc']['vpc_id']}\" } ], \"provider\" : \"dns\" } } }, \"blueprints\" : [ { \"name\" : \"Demo-BP\" , \"version\" : \"0.1.1\" } ], \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ], \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output sample example with default type and directly from inputs of SP\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output sample example from output of layer\" , \"type\" : \"String\" }, { \"name\" : \"output3\" , \"value\" : \"${#output['app']['app_url']}\" , \"description\" : \"Output sample example from layer 2\" , \"type\" : \"String\" }, { \"name\" : \"output4\" , \"value\" : \"${#output['vpc']['prq-value']}\" , \"description\" : \"Output sample example with list type\" , \"type\" : \"List\" }, { \"name\" : \"output5\" , \"value\" : \"${#output['vpc']['json-value']}\" , \"description\" : \"Output sample example with map type\" , \"type\" : \"Map\" } ], \"workflow\" : [ { \"action\" : \"deployEnv\" , \"name\" : \"vpc\" , \"condition\" : \"${#input.isVpc}\" , \"data\" : { \"envName\" : \"vpc-demo\" , \"envVersion\" : \"01.00.00\" } }, { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } } ] } Data objects in solution package schema \u00b6 The solution package schema contains multiple JSON blocks for the following data objects: schemaVersion metadata schedule blueprints workflow customData providers input output schemaVersion \u00b6 The version of the solution package schema. You can search a solution package using the schemaVersion attribute. This is a mandatory parameter. It is recommended that you use the latest schema version to create a new solution package. For example, the entry for this parameter can be 2.0 . However, older schema versions continue to be supported. If you are updating an existing solution package, specify the schema version that was used to create that solution package. Note*:** All solution packages created with schema version 2.x must be registered or edited using the REST API version v2, as shown below: ***https://{HCAP-base-URL}/api/reansolutionpackage/solution_package/v2 metadata \u00b6 This data object contains the metadata about the solution package and the application to be installed using the solution package. The information in the metadata section is used to display, access, and search the solution package by using parameters such as name, version, language, and cloud region. The object must be in the following format: \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@hitachivantara.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 1h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"license-321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } } Following are the attributes of the metadata object: name (mandatory parameter) version (mandatory parameter) description icon applicationVersion ownerName ownerEmail duration language licenseName cloudRegion (mandatory parameter) vertical tags name (mandatory parameter) \u00b6 The name of the solution package. The solution name and version must be unique per user version (mandatory parameter) \u00b6 The version of the solution package. You can create multiple versions for a single solution name. description \u00b6 Description of the solution. You can add a detailed information about the application in this parameter. icon \u00b6 URL to the location of the Application icon image. applicationVersion \u00b6 The version of the application which is getting deployed as a part of this solution package ownerName \u00b6 The name of the owner of the solution package. This is a mandatory parameter. ownerEmail \u00b6 The email ID of the owner of the solution package. This is a mandatory parameter. duration \u00b6 A user entry to define the minimum and maximum deployment time required by the solution package. The object must be in the following format: \"duration\" : { \"minDeploymentTime\" : \"1d 1h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" } The correct format for entry of time 1d 3h 20m for 1 day, 3 hours and 20 minutes. This format does not support time entry in seconds. language \u00b6 List of languages supported by the application to be deployed using solution package. The list must be the language code in ISO standard. For example: \"language\" : [ \"en_US\" ] For more information on the language codes, in the ISO website, see Codes for the Representation of Names of Languages . licenseName \u00b6 The license required for using the solution package. Before entering the license name here, make sure that the license is created for the solution package. For more information on the creation of the license key, see the license-controller API documentation in Solution Package. The license-control API documentation is available at following URL: https://{HCAP-base-URL}/api-documentation/solutionpackagedoc/swagger-ui.html#/license-controller cloudRegion (mandatory parameter) \u00b6 List of cloud regions where the solution package is to be deployed. For example: \"cloudRegion\" : [ \"us-east-1\" ] vertical \u00b6 Vertical of the organization to which the application belongs. tags \u00b6 Additional properties for the solution. The tags must be in the following format: \"tags\" :{ \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag value\" } These tags can be used for creating a tag and searching a solution with the tag name for a specific tag value. For example, you can tag a solution package as { \"domain\" : \"commerce\" }, which can be used for searching all the applications in the Finance domain. \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } schedule \u00b6 This data object allows you to set a schedule for deploying a solution package multiple times. When a solution package with a schedule parameter is deployed successfully, the subsequent deployments are scheduled and triggered based on the specified schedule. The schedule object is optional. The object must be in the following format: schedule\": { \"cron\": \"0 */2 * ? * *\" } Consider the following points about the deployment of solution packages with the schedule parameter: If a solution deployment is destroyed, the scheduling is stopped. If a solution deployment is in a running state, the scheduled deployment is skipped. The next scheduled deployment starts even if the previous deployment job fails. The scheduling of next deployment is automatically triggered after the Workflow Accelerator services are up from a downtime. For example, if a job is schedules at 10:00, 12:00, and 14:00, and the Workflow Accelerator services face a downtime from 11:00 to 13:00, then the next deployment scheduled at 14:00 is automatically triggered. Note: If required, you can update a solution package to add, update, or remove the schedule object and then redeploy the solution package deployment. blueprints \u00b6 This object contains the list of blueprints that will get automatically imported from the artifactory before starting the deployment of the solution package. In the artifactory, these blueprints must be stored in the repository that is configured in Deploy Accelerator. Environments in the blueprint are imported into Deploy Accelerator with the naming convention <solution_package_name>_<environment_name> . Note : Blueprints will get imported only once, the solution package will reuse the previously imported blueprints if they are already present in Deploy Accelerator. The blueprints must be present in the artifactory with the provided name and version, otherwise complete deployment of the solution package will fail. The object must be in the following format: \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ] Following are the attributes of the blueprints object: name version name \u00b6 Name of the blueprint. The entry must be exactly same as the name of the blueprint in the artifactory. version \u00b6 Version of the blueprint for the specified name. Note: You can also choose to manually import blueprints into Deploy Accelerator. In this case, you must specify an empty section for the blueprints object, as shown below. \"blueprints\": [] workflow \u00b6 This data object contains the list of actions that will get executed. The actions are executed in the same order in which they are listed. The workflow object is defined using the following format. \"workflow\" : [ { \"action\" : \"<WorkflowActionName>\" , \"name\" : \"sampleName\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { <da ta a ttr ibu tes based o n t he work fl ow ac t io n > } }, { \"action\" : \"<WorkflowActionName>\" , \"name\" : \"sampleName\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { <da ta a ttr ibu tes based o n t he work fl ow ac t io n > } } ] The following example uses the deploySolutionPackage action. \"workflow\" : [ { \"action\" : \"deploySolutionPackage\" , \"name\" : \"sc_dependency1\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { \"solution_package_name\" : \"SolutionPackage_A1\" , \"solution_package_version\" : \"00.00.100\" } } ] Following are the attributes of the workflow object: action name condition data action \u00b6 The action that must be executed by the workflow layer. For the list of supported workflow actions, along with the data attributes, input mapping parameters, and output for each action, see Supported workflow actions . name \u00b6 Name of the workflow layer. All other objects in the solution package must use this layer name to reference the details that are defined in the data attribute. condition \u00b6 Condition based on which the layer is either executed or skipped. It allows you to control whether or not an optional layer of the solution package will be executed. This attribute is a Spring expression that can be resolved to Boolean based on the value of the condition: If the value is true or resolves to true , the layer will be executed. If the value is false or resolves to false , the layer will be skipped. The Spring expression for the condition can use input parameters, combination of multiple conditions, or output of a previous layer. Example: \"condition\" : \"${#input[\u2018Is_subnet_deployment\u2019]}\" \"condition\" : \"${#output['network-layer']['vpc_cidr'] == '10.0.0.0/16'}\" Note: The condition attribute is optional. If you do not use the condition attribute, the layer will be executed. data \u00b6 Details that are required to execute the defined action. The data parameters that you can declare are based on the action that has been defined for the workflow layer. For more information, see Supported workflow actions . customData \u00b6 This object allows you to define custom attributes for the solution package. These custom attributes can be used as a search parameter for the solution package. The object must be in the following format: \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" } providers \u00b6 This data object lists all providers that are required to deploy the workflow layers in the solution package. The object must be in the following format: \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] Following are the attributes of the providers object: name type default_provider_name name \u00b6 Name of the provider. The entry must be exactly same as the provider declared in the blueprint. type \u00b6 Cloud service provider. For example, AWS, Google Cloud, and Microsoft Azure. Depending on the target cloud provider and account in the blueprint, you can define the providers list in the following ways: Case 1 : All the blueprints for the solution are to be deployed on the same Cloud service Provider account. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" } ] Case 2 : Blueprints in the solution package are to be deployed on two different accounts of the same cloud service provider. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"aws_account2\" , \"type\" : \"aws\" } ] Case 3 : Blueprints in the solution package targeting multiple cloud providers. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"k8s_provider\" , \"type\" : \"helm\" } ] default_provider_name \u00b6 The default_provider_name parameter allows you to define a default provider for your solution package. The value of this parameter must be same as the name of the provider created in Deploy Accelerator. If required, you can override the default provider value at the time of deployment. You can define multiple default providers in solution package. This is an optional parameter. \"providers\" : [ { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] input \u00b6 The input object contains 2 sub objects inputParameters and inputMapping . The object must be in the following format: \"input\" : { \"inputParameters\" : { /*Parame ter de f i n i t io n */ }, \"inputMapping\" : { /* \"Workflow layer name\" */ { /*Parame ter Mappi n g*/ } } } inputParameters \u00b6 The inputParameters object contains the declaration of unique parameters that are required for deploying the workflow layer. This section consists of all unique sets of inputs that the solution owner wants the user to enter while deploying the solution package. For example, instance_size, which can be used in the one of the blueprint to set the EC2 instance size. The object must be in the following format: \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ] Following are the attributes of the inputParameters object: name: Name of the parameter. The name must be a single string without any space. type: The data type of the input parameter. The supported types are List, String, Boolean, Map, and Integer. description: Description of the parameter. You can add a detailed information about the application in this parameter. It is a free text area. defaultValue: The default value of the parameter, if no value is entered while deployment. possibleValues: The list of possible values from which the user must select a value. Example { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] } inputMapping \u00b6 The inputMapping object contains the declaration of parameters that are used to deploy a workflow layer. The parameters that you can declare are based on the action that has been defined for the workflow layer. For more information, see Supported workflow actions . You can declare parameters for multiple workflow layers in a single inputMapping object. The object must be in the following format: \"inputMapping\" : { \"workflowLayerName\" : { <I n pu t Mappi n g parame ters f or t he work fl ow layer> }, \"workflowLayerName\" : { <I n pu t Mappi n g parame ters f or t he work fl ow layer> } } output \u00b6 This data object is used to declare the output parameters of a solution package. You can define multiple parameters in the output object. You can view the result of the parameters defined in the output object after the solution package is deployed. The output parameters can also be used as input parameters in another solution package. The object must be in the following format: { \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output example with default type and directly from inputs of the solution package\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output example from the output of a workflow layer\" , \"type\" : \"String\" } ] } Following are the attributes of the output object: name value description type name \u00b6 The name of the output parameter. value \u00b6 The value for the output parameter. There are multiple ways to define values for output parameters. Case 1 : Use the value of an input parameter that is defined in the solution package. { \"name\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"String\" } Case 2 : Use the value of an output parameter of any workflow layer defined in the solution package. { \"name\" : \"vpc_id\" , \"value\" : \"${#output['network-layer']['vpc_id']}\" , \"type\" : \"String\" } In this case, network-layer is name of the workflow layer from where the output parameter will be received. Case 3 : Use the combined value of multiple input parameters or output parameters of any workflow layer defined in the solution package. { \"name\" : \"resource-name-prefix\" , \"value\" : \"${#output['network-layer']['customer_name']}-${#output['network-layer']['env']}\" , \"type\" : \"String\" } description \u00b6 The description of the output parameter. This is a free text section. type \u00b6 The type of the output parameter. The supported types are List, String, Boolean, Map, and Integer. The default value of the parameter is String , if no value is specified while creating the solution package. Supported workflow actions \u00b6 The workflow data object contains the list of actions that a solution package must execute. For each workflow action, you must specify the appropriate data, input mapping, and output parameters. The Solution Package schema supports the following workflow actions: deployEnv createProvider createHelmRepo deployHelmChart createFoundrySolutionPackage deployFoundrySolutionPackage deploySolutionPackage Summary of workflow action details \u00b6 This section provides a quick overview of the data parameters, input mapping parameters, and output of each workflow action that the Solution Package schema supports. For more information, see the detailed section on each workflow action. Workflow action Data parameters Input Mapping parameters Output of the workflow action deployEnv Deploys the environment that is defined in the data parameters. envName envVersion provider deployInput All outputs from the environment's deployment createProvider Creates a provider in Deploy Accelerator. provider_name provider_type provider_json_string provider_json provider_id provider_name provider_type globalProvider createHelmRepo Registers a new Helm repository. You can use this action when helm charts to be installed by the deployHelmChart action are not present in the default repository. repo_name repo_url username password is_oci_registry caFile inSecureSkipTlsVerify repo_name repo_url deployHelmChart Deploys Helm charts to the Kubernetes cluster from the Helm repository that is defined in the Input Mapping parameters. chart_name chart_version providerId repoName namespace values helmStatus helmDeploymentName helmInstallLogs createFoundrySolutionPackage Registers the Foundry solution package that is defined in the data parameters. The Foundry solution package must be located in the configured Artifactory of Hitachi Cloud Accelerator Platform. This action initially pushes the Foundry solution package to the Harbor registry that is configured for the Foundry Control Plane. Next, it registers the Foundry solution package, which then starts showing up in the list of available Foundry solution packages in the Foundry Control Plane. solution_package_name solution_package_version provider oci_url oci_username oci_password None deployFoundrySolutionPackage Deploys the Foundry solution package that is defined in the data parameters. Only Foundry solution packages that are available in the Foundry Control Plane can be deployed. solution_package_name solution_package_version auth solution_config chartName chartVersion chartDescription applicationVersion modelVersion solutionName updateDate solutionResources status deploySolutionPackage Deploys the solution package that is defined in the data parameters. This solution package can be a prerequisite for the current solution package. solution_package_name solution_package_version providers deploySolutionPackageInput All outputs from the solution package's deployment deployEnv action \u00b6 The deployEnv workflow action deploys the environment that is defined in the data attribute. Example { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } } Data parameters \u00b6 Following are the data parameters for the deployEnv workflow action: envName envVersion You can define only one environment name and version per action. The environment name and version must be the same as used in the blueprint that is imported either automatically or manually. Make sure that the environment is present in Deploy Accelerator before starting the deployment. Note: The naming convention used for environments in a blueprint that are imported from the artifactory is solutionPackageName_environmentName . However, while specifying these environments in the workflow object, make sure that you enter only the environmentName . Before starting the deployment, the solutionPackageName is automatically prefixed to the environment name. Input Mapping parameters \u00b6 For the deployenv workflow action, you can declare the input mapping that will be used for the workflow layer with a specific provider. provider -- The provider to be used by the workflow layer for deployment. region -- The region in which the environment is to be deployed. This is an optional parameter. connection -- The connection ID that can be used to connect to the instances in the deployed environment. This is an optional parameter. chefEnvironment -- The environment that is available on the selected Chef Server. This is an optional parameter. deployInput -- The values to be used while deploying the workflow layer using the solution package. key -- The exact name as defined in the Input Variables resource of the Deploy Accelerator environment for which you are configuring this input mapping. value -- The value for the inputParameter key. There are three types of input mapping values, as described below. Case 1 : Mapping values for input parameters used in the solution package. { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"STRING\" } Case 2 : Mapping output parameter of another workflow layer with the input parameter. When a solution package has two blueprints that are not connected using the Depends On feature of Deploy Accelerator, the output parameter of the parent blueprint must be used in the child blueprint. { \"key\" : \"vpc_id\" , \"value\" : \"${#output['network-layer']['vpc_id']}\" , \"type\" : \"STRING\" } In this case network-layer is name of the workflow layer from where the output parameter will be received. Case 3 : Mapping input parameter with multiple input parameters or workflow layer output. { \"key\" : \"resource-name-prefix\" , \"value\" : \"${#output['network-layer']['customer_name']}-${#output['network-layer']['env']}\" , \"type\" : \"STRING\" } type -- The type of the inputParameter key. The supported types are List, String, Boolean, Map, and Integer. The default value of the parameter is String , if no value is entered while creating the Solution Package. Note: The value of the inputParameter key is resolved based on the type of the inputParameter. If you specify an incorrect type, complete deployment of the solution package will fail. Example { \"provider\" : \"AWS_1\" , \"region\" : \"${#input['region']}\" , \"connection\" : { \"default\" : \"${#input['default']}\" , \"some_resource\" : \"${#input['some_resource']}\" }, \"chefEnvironment\" : \"${#input['chef_environment']}\" , \"deployInput\" : [] } Output parameters \u00b6 The output parameters for the deployEnv workflow action include all of the environment's deployment outputs. createProvider \u00b6 The createProvider workflow action creates a provider in Deploy Accelerator. Workflow Accelerator supports all providers and authentication methods that are available in Deploy Accelerator. For more information, see Configuring providers . Example { \"action\" : \"createProvider\" , \"name\" : \"aws_provider_creation_helm\" , \"condition\" : \"${#input['provider_creation_helm']}\" , \"data\" : { \"provider_name\" : \"helm_provider\" , \"provider_type\" : \"helm3\" } } Data parameters \u00b6 Following are the data parameters for the createProvider action: provider_name provider_type Input Mapping parameters \u00b6 For the createProvider action, you can define the inputs for providers with dynamic configuration. The parameters in the createProvider object must be same as the parameters used in Deploy Accelerator. You can define parameters for the provider in this section, or you can define the provider in the Solution package deployment object. You must use one of the following definition type. provider_json_string : Used to pass the provider JSON at the time of deployment, and must be defined in the Solution Package Deployment Object. \"inputMapping\" :{ \"aws_provider_creation\" : { \"provider_json_string\" : { \"#input['provider-json']\" } } } provider_json: The parameters for defining a provider. The parameters are same as the parameters in the provider definition JSON in the Deploy Accelerator. You might need to modify the parameters as per the provider type. The provider parameters defined in this section are fixed. access_key : Access key to the AWS account. secret_key : Secret key to the AWS account. region : Region of the AWS account. \"inputMapping\" : { \"aws_provider_creation\" : { \"provider_json\" : { \"access_key\" : \"${#input.access_key}\" , \"secret_key\" : \"${#input.secret_key}\" , \"region\" : \"${#input.region}\" } } } Output parameters \u00b6 Following are the output parameters for the createProvider workflow action: provider_id provider_name provider_type globalProvider createHelmRepo action \u00b6 The createHelmRepo action registers a new Helm repository. You can use this action when the helm charts to be installed by the deployHelmChart action are not available in the default repository in artifactory. Example { \"action\" : \"createHelmRepo\" , \"name\" : \"repoSP\" , \"condition\" : \"${#input['repoSP']}\" , \"data\" : { \"repo_name\" : \"repo\" } } Data parameters \u00b6 The data parameter for the createHelmRepo workflow action is repo_name . A repository named default is already created in the Helm service. Therefore, ensure that you do not specify default as the repository name. Input Mapping parameters \u00b6 For the createHelmRepo workflow action, you can define the parameters to register the Helm repository with the URL provided in the parameters. repo_url -- URL for deployment of the Helm repository. username -- Username for accessing the Helm repository. password -- Password for accessing the Helm repository. is_oci_registry -- Enable or disable support for OCI registry. Enter false for using Artifactory, and enter true for using Harbor. caFile : The CA bundle to verify the certificates of the HTTPS enabled servers. inSecureSkipTlsVerify : Enable or disable the TLS certificate check for the Helm repository. Example \"inputMapping\" : { \"helmRepo\" : { \"repo_url\" : \"$(#input.repo_url)\" , \"username\" : \"$(#input.username)\" , \"password\" : \"$(#input.password)\" , \"is_oci_registry\" : false , \"caFile\" : \"$(#input.caFile)\" , \"inSecureSkipTlsVerify\" : false } } Output parameters \u00b6 Following are the output parameters for the createHelmRepo workflow action: repo_name repo_url deployHelmChart \u00b6 The deployHelmChart workflow deploys Helm charts to the Kubernetes cluster from the Helm repository deployed in the earlier action. If no repository is declared in the createHelmRepo action, it refers to the the default repository in artifactory. Example { \"action\" : \"deployHelmChart\" , \"name\" : \"deploy_helm\" , \"condition\" : \"${#input['deploy_helm']}\" , \"data\" : { \"chart_name\" : \"repo/platform-helm-sampleapp\" , \"chart_version\" : \"0.1.2\" } } Data parameters \u00b6 Following are the data parameters for the deployHelmChart workflow action: chart_name chart_version The chart_name must be in the format repo_name/chart_name . If you have not declared any repo_name , use default/chart_name . Input Mapping parameters \u00b6 For the deployHelmChart workflow action, you can deploy a Helm chart in the recently deployed Helm repository. namespace - The namespace in which the Helm Chart is deployed. If not defined, it is deployed in the default namespace. repository - Registered repository name with the service from where it will deploy the helm charts. (mandatory) provider - The provider for deploying Helm charts. It should be registered with Deploy Accelerator. (mandatory) values - JSON for overriding the default values of the Helm Chart from Solution Package. values_string - JSON for overriding the values from the Solution Package deployment object. The name must match the value defined in the solution package. Example \"deploy_helm\" : { \"provider\" : \"${#providers['provider_creation_helm']['provider_id']}\" , \"repository\" : \"${#output['helm_repo_creation']['repo_name']}\" , \"namespace\" : \"${#input['namespace']}\" , \"values\" : { \"docker\" : { \"image\" : \"reanplatform-sampleapp:0.0.1\" , \"registry\" : { \"username\" : \"${#input.username}\" , \"password\" : \"${#input.password}\" , \"server\" : \"${#input.server}\" } } } } Output parameters \u00b6 Following are the output parameters for the deployHelmChart workflow action: helmStatus helmDeploymentName helmInstallLogs createFoundrySolutionPackage action \u00b6 The createFoundrySolutionPackage workflow action registers the Foundry solution package that is defined in the data attribute. This action consists of the following steps: The Foundry solution package, which is located in the configured Artifactory of Hitachi Cloud Accelerator Platform, is pushed to the Harbor registry that is configured for the Foundry Control Plane. The Foundry solution package is registered and starts showing up in the list of available Foundry solution packages in the Foundry Control Plane. Example { \"action\" : \"createFoundrySolutionPackage\" , \"name\" : \"foundry_package\" , \"condition\" : \"${#input['foundry_package']}\" , \"data\" : { \"solution_package_name\" : \"hello-world-changed\" , \"solution_package_version\" : \"0.2.0\" } } Data parameters \u00b6 Following are the data parameters for the createFoundrySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Foundry solution package and version per action. The Foundry solution package name and version must be the same as the file name and version that is stored in the configured artifactory of Cloud Accelerator Platform. Before starting the deployment, make sure that the solution package is present in the configured repository of the artifactory. If the Foundry solution package is already registered in the Foundry Control Plane, this action checks if there is any change in the Foundry solution package that is present in the configured repository of the Artifactory. If a change is identified, the registered Foundry solution package is updated. Note: If you destroy a deployment of the Workflow Accelerator solution package, the Foundry solution package that has already been registered does not get deregistered. It continues to be available in the Foundry Control Plane. Input Mapping parameters \u00b6 For the createFoundrySolutionPackage workflow action, you can declare the credentials for accessing the Harbor registry in which the Foundry solution package will be pushed, along with the provider for the Kubernetes cluster in which the Foundry Control Plane is installed. This Kubernetes provider must exist in Deploy Accelerator. provider -- The Kubernetes provider ID (in Deploy Accelerator) to be used by the workflow layer to connect to the cluster in which Foundry Control Plane is installed. You can get this provider ID either from the output of the previous layer or from the input parameters. oci_url -- URL for accessing the Harbor registry that is configured for the Foundry Control Plane. oci_username -- User name to sign-in to the Harbor registry. oci_password -- Password to sign-in to the Harbor registry. For the OCI parameters, you can map values for input parameters used in the solution package. Example \"inputMapping\" : { \"foundry-package\" : { \"provider\" : \"${#input['providerID']}\" , \"oci_url\" : \"${#input['harborHost']}\" , \"oci_username\" : \"${#input['harborUsername']}\" , \"oci_password\" : \"${#input['harborPassword']}\" } } Output parameters \u00b6 The createFoundrySolutionPackage workflow action has no output parameters. deployFoundrySolutionPackage \u00b6 The deployFoundrySolutionPackage workflow action deploys the Foundry solution package that is defined in the data attribute. Only Foundry solution packages that are available in the Foundry Control Plane can be deployed. Example { \"action\" : \"deployFoundrySolutionPackage\" , \"name\" : \"foundry_package_deploy\" , \"condition\" : \"${#input['foundry_package']}\" , \"data\" : { \"solution_package_name\" : \"hello-world-changed\" , \"solution_package_version\" : \"0.2.0\" } } Data parameters \u00b6 Following are the data parameters for the deployFoundrySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Foundry solution package and version per action. The Foundry solution package name and version must be the same as the name and version that is registered in the Foundry control plane. Input Mapping parameters \u00b6 For the deployFoundrySolutionPackage workflow action, you can declare the credentials for accessing the Foundry Control Plane, along with optional configuration information about the Foundry Control Plane. auth -- The authentication details required to connect to the Foundry Control Plane and deploy the Foundry solution package. client_id -- Client ID to connect to the Foundry Control Plane. client_secret -- Client Secret to connect to the Foundry Control Plane. username -- Username to connect to the Foundry Control Plane. password -- Password to connect to the Foundry Control Plane. foundry_host -- Host on which the Foundry Control Plane is installed. namespace -- Kubernetes namespace in which the Foundry Control Plane is installed. solution_config -- Information about the Foundry solution package deployment. modelVersion -- Version of the Foundry Control Plane. values OR values_string -- You can either provide a fixed list of values or specify that the values will be taken from a JSON string that the user will provide as input while deploying the solution package. Format for values \"values\" : { \"image\" : { \"gatekeeperInitPullPolicy\" : \"IfNotPresent\" , \"pullPolicy\" : \"IfNotPresent\" , \"repository\" : \"harbor.prod.platfom/foundry/foundry-hello-world\" , \"tag\" : \"2.0.0.345\" } } Format for values_string \"values_string\" : \"${#input['values_from_input_param']}\" Example: \"inputMapping\" : { \"foundry-package-deploy\" : { \"auth\" : { \"client_id\" : \"${#input['clientId]}\" , \"client_secret\" : \"${#input['clientSecret]}\" , \"username\" : \"${#input['username]}\" , \"password\" : \"${#input['password]}\" , \"foundry_host\" : \"${#input['foundryHost]}\" , \"namespace\" : \"${#input['namespace]}\" }, \"solution_config\" : { \"modelVersion\" : \"${#input['modelversion']}\" , \"values_string\" : \"${#input['values']}\" } } } Output parameters \u00b6 Following are the output parameters for the deployFoundrySolutionPackage workflow action: chartName chartVersion chartDescription applicationVersion modelVersion solutionName updateDate solutionResources status deploySolutionPackage \u00b6 The deploySolutionPackage workflow action deploys the Workflow Accelerator solution package that is a prerequisite for the current solution package. Example { \"action\" : \"deploySolutionPackage\" , \"name\" : \"sc_dependency1\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { \"solution_package_name\" : \"SolutionPackage_A1\" , \"solution_package_version\" : \"00.00.100\" } } Data parameters \u00b6 Following are data parameters for the deploySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Workflow Accelerator solution package and version per action. The solution package name and version must be the same as defined in its solution package JSON. Input Mapping parameters \u00b6 For the deploySolutionPackage workflow action, you can declare the providers and the input parameters required for deploying all the layers from the solution package declared in the action. Layer name object (dynamic parameter): The name of the workflow layer to be deployed by the solution package defined in action. providers : List of the providers required for deploying all the layers from referenced solution package. deploySolutionPackageInput : List of all the input parameters required for deploying all the layers from referenced solution package. Example \"inputMapping\" : { \"sc_dependency1\" : { \"providers\" : { \"aws-provider\" : \"${#providers['aws-provider']}\" }, \"deploySolutionPackageInput\" : [ { \"name\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" , \"type\" : \"String\" }, { \"name\" : \"env\" , \"value\" : \"${#input.env}, \" t ype \" : \" S tr i n g \" }, { \" na me \" : \" cus t omer \", \" value \" : \" $ { #i n pu t .cus t omer } \", \" t ype : \"String\" } ] } } Output parameters \u00b6 The output parameters for the deploySolutionPackage workflow action include all of the solution package's deployment outputs.","title":"Understand the solution package schema"},{"location":"workflow/understand-solution-package-schema/#understand-the-solution-package-schema","text":"Solution package in Workflow Accelerator is a JSON file that allows you to define all the information that is required for installing end-to-end solutions across various cloud platforms. To register and deploy the solution packages that you create, you have to use REST APIs. This page describes the most recent major version (for example: 2.x) of the solution package schema. To view the schema for older versions, see Solution Package schema 1.0 .","title":"Understand the solution package schema"},{"location":"workflow/understand-solution-package-schema/#contents","text":"Sample solution package Data objects in solution package schema Supported workflow actions","title":"Contents"},{"location":"workflow/understand-solution-package-schema/#sample-solution-package","text":"The following is an example of a solution package for installing an application. For information about the different JSON blocks in the solution package, see Data objects in solution package schema . { \"schemaVersion\" : \"2.1\" , \"metadata\" : { \"name\" : \"demo\" , \"version\" : \"00.00.01\" , \"description\" : \"demo\" , \"icon\" : \"https://google.com\" , \"applicationVersion\" : \"01.00.00\" , \"language\" : [ \"english\" , \"japanese\" ], \"ownerName\" : \"demo-user\" , \"ownerEmail\" : \"demo.user@companyname.com\" , \"cloudRegion\" : [ \"us-east-1\" , \"us-west-1\" ], \"vertical\" : [], \"tags\" : { \"tag1\" : \"value1\" , \"tag2\" : \"value2\" , \"app\" : \"SampleApp\" } }, \"input\" : { \"inputParameters\" : [ { \"name\" : \"cidr\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"10.0.0.0/16\" }, { \"name\" : \"isApp\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"isVpc\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"instance_type\" , \"type\" : \"String\" , \"default\" : \"t2.micro\" , \"description\" : \"instance type\" }, { \"name\" : \"customer_name\" , \"type\" : \"String\" , \"description\" : \"instance name\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment like prod, qa, etc\" }, { \"name\" : \"list-value\" , \"type\" : \"List\" , \"defaultValue\" : \"'demo-one','demo-two'\" }, { \"name\" : \"json-value\" , \"type\" : \"Json\" } ], \"inputMapping\" : { \"vpc\" : { \"provider\" : \"aws_provider\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.cidr}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#input['customer_name']}\" }, { \"key\" : \"env\" , \"value\" : \"${#input['env']}\" }, { \"key\" : \"pqr\" , \"value\" : \"${#input['list-value']}\" , \"type\" : \"List\" }, { \"key\" : \"p\" , \"value\" : \"${#input['json-value']}\" , \"type\" : \"Map\" } ] }, \"app\" : { \"deployInput\" : [ { \"key\" : \"instance_type\" , \"value\" : \"${#input['instance_type']}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#output['vpc']['customer']}\" }, { \"key\" : \"env1\" , \"value\" : \"{ '${#input['customer_name']}' : '${#input['customer_name']}' }\" , \"type\" : \"Map\" }, { \"key\" : \"vpc_id\" , \"value\" : \"${#output['vpc']['vpc_id']}\" } ], \"provider\" : \"dns\" } } }, \"blueprints\" : [ { \"name\" : \"Demo-BP\" , \"version\" : \"0.1.1\" } ], \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ], \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output sample example with default type and directly from inputs of SP\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output sample example from output of layer\" , \"type\" : \"String\" }, { \"name\" : \"output3\" , \"value\" : \"${#output['app']['app_url']}\" , \"description\" : \"Output sample example from layer 2\" , \"type\" : \"String\" }, { \"name\" : \"output4\" , \"value\" : \"${#output['vpc']['prq-value']}\" , \"description\" : \"Output sample example with list type\" , \"type\" : \"List\" }, { \"name\" : \"output5\" , \"value\" : \"${#output['vpc']['json-value']}\" , \"description\" : \"Output sample example with map type\" , \"type\" : \"Map\" } ], \"workflow\" : [ { \"action\" : \"deployEnv\" , \"name\" : \"vpc\" , \"condition\" : \"${#input.isVpc}\" , \"data\" : { \"envName\" : \"vpc-demo\" , \"envVersion\" : \"01.00.00\" } }, { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } } ] }","title":"Sample solution package"},{"location":"workflow/understand-solution-package-schema/#data-objects-in-solution-package-schema","text":"The solution package schema contains multiple JSON blocks for the following data objects: schemaVersion metadata schedule blueprints workflow customData providers input output","title":"Data objects in solution package schema"},{"location":"workflow/understand-solution-package-schema/#schemaversion","text":"The version of the solution package schema. You can search a solution package using the schemaVersion attribute. This is a mandatory parameter. It is recommended that you use the latest schema version to create a new solution package. For example, the entry for this parameter can be 2.0 . However, older schema versions continue to be supported. If you are updating an existing solution package, specify the schema version that was used to create that solution package. Note*:** All solution packages created with schema version 2.x must be registered or edited using the REST API version v2, as shown below: ***https://{HCAP-base-URL}/api/reansolutionpackage/solution_package/v2","title":"schemaVersion"},{"location":"workflow/understand-solution-package-schema/#metadata","text":"This data object contains the metadata about the solution package and the application to be installed using the solution package. The information in the metadata section is used to display, access, and search the solution package by using parameters such as name, version, language, and cloud region. The object must be in the following format: \"metadata\" : { \"name\" : \"magento\" , \"version\" : \"00.00.01\" , \"description\" : \"This solution package is for installing Magento commerce application.\" , \"icon\" : \"http://https://magento.com/log.png\" , \"applicationVersion\" : \"01.00.00\" , \"ownerName\" : \"MFI\" , \"ownerEmail\" : \"customer.mfi@hitachivantara.com\" , \"duration\" : { \"minDeploymentTime\" : \"1d 1h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" }, \"language\" : [ \"en_US\" ], \"licenseName\" : \"license-321\" , \"cloudRegion\" : [ \"us-east-1\" ], \"vertical\" : [], \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false } } Following are the attributes of the metadata object: name (mandatory parameter) version (mandatory parameter) description icon applicationVersion ownerName ownerEmail duration language licenseName cloudRegion (mandatory parameter) vertical tags","title":"metadata"},{"location":"workflow/understand-solution-package-schema/#name-mandatory-parameter","text":"The name of the solution package. The solution name and version must be unique per user","title":"name (mandatory parameter)"},{"location":"workflow/understand-solution-package-schema/#version-mandatory-parameter","text":"The version of the solution package. You can create multiple versions for a single solution name.","title":"version (mandatory parameter)"},{"location":"workflow/understand-solution-package-schema/#description","text":"Description of the solution. You can add a detailed information about the application in this parameter.","title":"description"},{"location":"workflow/understand-solution-package-schema/#icon","text":"URL to the location of the Application icon image.","title":"icon"},{"location":"workflow/understand-solution-package-schema/#applicationversion","text":"The version of the application which is getting deployed as a part of this solution package","title":"applicationVersion"},{"location":"workflow/understand-solution-package-schema/#ownername","text":"The name of the owner of the solution package. This is a mandatory parameter.","title":"ownerName"},{"location":"workflow/understand-solution-package-schema/#owneremail","text":"The email ID of the owner of the solution package. This is a mandatory parameter.","title":"ownerEmail"},{"location":"workflow/understand-solution-package-schema/#duration","text":"A user entry to define the minimum and maximum deployment time required by the solution package. The object must be in the following format: \"duration\" : { \"minDeploymentTime\" : \"1d 1h 20m\" , \"maxDeploymentTime\" : \"1d 2h 30m\" } The correct format for entry of time 1d 3h 20m for 1 day, 3 hours and 20 minutes. This format does not support time entry in seconds.","title":"duration"},{"location":"workflow/understand-solution-package-schema/#language","text":"List of languages supported by the application to be deployed using solution package. The list must be the language code in ISO standard. For example: \"language\" : [ \"en_US\" ] For more information on the language codes, in the ISO website, see Codes for the Representation of Names of Languages .","title":"language"},{"location":"workflow/understand-solution-package-schema/#licensename","text":"The license required for using the solution package. Before entering the license name here, make sure that the license is created for the solution package. For more information on the creation of the license key, see the license-controller API documentation in Solution Package. The license-control API documentation is available at following URL: https://{HCAP-base-URL}/api-documentation/solutionpackagedoc/swagger-ui.html#/license-controller","title":"licenseName"},{"location":"workflow/understand-solution-package-schema/#cloudregion-mandatory-parameter","text":"List of cloud regions where the solution package is to be deployed. For example: \"cloudRegion\" : [ \"us-east-1\" ]","title":"cloudRegion (mandatory parameter)"},{"location":"workflow/understand-solution-package-schema/#vertical","text":"Vertical of the organization to which the application belongs.","title":"vertical"},{"location":"workflow/understand-solution-package-schema/#tags","text":"Additional properties for the solution. The tags must be in the following format: \"tags\" :{ \"tagname1\" : \"tag1 value\" , \"tagname2\" : \"tag value\" } These tags can be used for creating a tag and searching a solution with the tag name for a specific tag value. For example, you can tag a solution package as { \"domain\" : \"commerce\" }, which can be used for searching all the applications in the Finance domain. \"tags\" : { \"domain\" : \"commerce\" , \"has_free_trials\" : false }","title":"tags"},{"location":"workflow/understand-solution-package-schema/#schedule","text":"This data object allows you to set a schedule for deploying a solution package multiple times. When a solution package with a schedule parameter is deployed successfully, the subsequent deployments are scheduled and triggered based on the specified schedule. The schedule object is optional. The object must be in the following format: schedule\": { \"cron\": \"0 */2 * ? * *\" } Consider the following points about the deployment of solution packages with the schedule parameter: If a solution deployment is destroyed, the scheduling is stopped. If a solution deployment is in a running state, the scheduled deployment is skipped. The next scheduled deployment starts even if the previous deployment job fails. The scheduling of next deployment is automatically triggered after the Workflow Accelerator services are up from a downtime. For example, if a job is schedules at 10:00, 12:00, and 14:00, and the Workflow Accelerator services face a downtime from 11:00 to 13:00, then the next deployment scheduled at 14:00 is automatically triggered. Note: If required, you can update a solution package to add, update, or remove the schedule object and then redeploy the solution package deployment.","title":"schedule"},{"location":"workflow/understand-solution-package-schema/#blueprints","text":"This object contains the list of blueprints that will get automatically imported from the artifactory before starting the deployment of the solution package. In the artifactory, these blueprints must be stored in the repository that is configured in Deploy Accelerator. Environments in the blueprint are imported into Deploy Accelerator with the naming convention <solution_package_name>_<environment_name> . Note : Blueprints will get imported only once, the solution package will reuse the previously imported blueprints if they are already present in Deploy Accelerator. The blueprints must be present in the artifactory with the provided name and version, otherwise complete deployment of the solution package will fail. The object must be in the following format: \"blueprints\" : [ { \"name\" : \"VPC\" , \"version\" : \"0.1.1\" }, { \"name\" : \"Magento\" , \"version\" : \"1.0.1\" } ] Following are the attributes of the blueprints object: name version","title":"blueprints"},{"location":"workflow/understand-solution-package-schema/#name","text":"Name of the blueprint. The entry must be exactly same as the name of the blueprint in the artifactory.","title":"name"},{"location":"workflow/understand-solution-package-schema/#version","text":"Version of the blueprint for the specified name. Note: You can also choose to manually import blueprints into Deploy Accelerator. In this case, you must specify an empty section for the blueprints object, as shown below. \"blueprints\": []","title":"version"},{"location":"workflow/understand-solution-package-schema/#workflow","text":"This data object contains the list of actions that will get executed. The actions are executed in the same order in which they are listed. The workflow object is defined using the following format. \"workflow\" : [ { \"action\" : \"<WorkflowActionName>\" , \"name\" : \"sampleName\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { <da ta a ttr ibu tes based o n t he work fl ow ac t io n > } }, { \"action\" : \"<WorkflowActionName>\" , \"name\" : \"sampleName\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { <da ta a ttr ibu tes based o n t he work fl ow ac t io n > } } ] The following example uses the deploySolutionPackage action. \"workflow\" : [ { \"action\" : \"deploySolutionPackage\" , \"name\" : \"sc_dependency1\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { \"solution_package_name\" : \"SolutionPackage_A1\" , \"solution_package_version\" : \"00.00.100\" } } ] Following are the attributes of the workflow object: action name condition data","title":"workflow"},{"location":"workflow/understand-solution-package-schema/#action","text":"The action that must be executed by the workflow layer. For the list of supported workflow actions, along with the data attributes, input mapping parameters, and output for each action, see Supported workflow actions .","title":"action"},{"location":"workflow/understand-solution-package-schema/#name_1","text":"Name of the workflow layer. All other objects in the solution package must use this layer name to reference the details that are defined in the data attribute.","title":"name"},{"location":"workflow/understand-solution-package-schema/#condition","text":"Condition based on which the layer is either executed or skipped. It allows you to control whether or not an optional layer of the solution package will be executed. This attribute is a Spring expression that can be resolved to Boolean based on the value of the condition: If the value is true or resolves to true , the layer will be executed. If the value is false or resolves to false , the layer will be skipped. The Spring expression for the condition can use input parameters, combination of multiple conditions, or output of a previous layer. Example: \"condition\" : \"${#input[\u2018Is_subnet_deployment\u2019]}\" \"condition\" : \"${#output['network-layer']['vpc_cidr'] == '10.0.0.0/16'}\" Note: The condition attribute is optional. If you do not use the condition attribute, the layer will be executed.","title":"condition"},{"location":"workflow/understand-solution-package-schema/#data","text":"Details that are required to execute the defined action. The data parameters that you can declare are based on the action that has been defined for the workflow layer. For more information, see Supported workflow actions .","title":"data"},{"location":"workflow/understand-solution-package-schema/#customdata","text":"This object allows you to define custom attributes for the solution package. These custom attributes can be used as a search parameter for the solution package. The object must be in the following format: \"customData\" : { \"name\" : \"SAP Hana\" , \"version\" : \"2.0.0\" }","title":"customData"},{"location":"workflow/understand-solution-package-schema/#providers","text":"This data object lists all providers that are required to deploy the workflow layers in the solution package. The object must be in the following format: \"providers\" : [ { \"name\" : \"aws_provider\" , \"type\" : \"aws\" }, { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ] Following are the attributes of the providers object: name type default_provider_name","title":"providers"},{"location":"workflow/understand-solution-package-schema/#name_2","text":"Name of the provider. The entry must be exactly same as the provider declared in the blueprint.","title":"name"},{"location":"workflow/understand-solution-package-schema/#type","text":"Cloud service provider. For example, AWS, Google Cloud, and Microsoft Azure. Depending on the target cloud provider and account in the blueprint, you can define the providers list in the following ways: Case 1 : All the blueprints for the solution are to be deployed on the same Cloud service Provider account. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" } ] Case 2 : Blueprints in the solution package are to be deployed on two different accounts of the same cloud service provider. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"aws_account2\" , \"type\" : \"aws\" } ] Case 3 : Blueprints in the solution package targeting multiple cloud providers. \"providers\" : [ { \"name\" : \"aws_account1\" , \"type\" : \"aws\" }, { \"name\" : \"k8s_provider\" , \"type\" : \"helm\" } ]","title":"type"},{"location":"workflow/understand-solution-package-schema/#default_provider_name","text":"The default_provider_name parameter allows you to define a default provider for your solution package. The value of this parameter must be same as the name of the provider created in Deploy Accelerator. If required, you can override the default provider value at the time of deployment. You can define multiple default providers in solution package. This is an optional parameter. \"providers\" : [ { \"name\" : \"artifactory_repo\" , \"type\" : \"artifactory\" , \"default_provider_name\" : \"default_artifactory\" } ]","title":"default_provider_name"},{"location":"workflow/understand-solution-package-schema/#input","text":"The input object contains 2 sub objects inputParameters and inputMapping . The object must be in the following format: \"input\" : { \"inputParameters\" : { /*Parame ter de f i n i t io n */ }, \"inputMapping\" : { /* \"Workflow layer name\" */ { /*Parame ter Mappi n g*/ } } }","title":"input"},{"location":"workflow/understand-solution-package-schema/#inputparameters","text":"The inputParameters object contains the declaration of unique parameters that are required for deploying the workflow layer. This section consists of all unique sets of inputs that the solution owner wants the user to enter while deploying the solution package. For example, instance_size, which can be used in the one of the blueprint to set the EC2 instance size. The object must be in the following format: \"inputParameters\" : [ { \"name\" : \"vpc_cidr\" , \"type\" : \"String\" , \"description\" : \"VPC CIDR\" }, { \"name\" : \"whitelisted_ip\" , \"type\" : \"List\" , \"description\" : \"List of IP's to whitelist\" }, { \"name\" : \"tagsTest\" , \"type\" : \"Map\" , \"description\" : \"Resource tags\" } ] Following are the attributes of the inputParameters object: name: Name of the parameter. The name must be a single string without any space. type: The data type of the input parameter. The supported types are List, String, Boolean, Map, and Integer. description: Description of the parameter. You can add a detailed information about the application in this parameter. It is a free text area. defaultValue: The default value of the parameter, if no value is entered while deployment. possibleValues: The list of possible values from which the user must select a value. Example { \"name\" : \"instance_size\" , \"type\" : \"String\" , \"description\" : \"instance_size\" , \"possibleValues\" : [ \"t2.large\" , \"t2.xlarge\" ] }","title":"inputParameters"},{"location":"workflow/understand-solution-package-schema/#inputmapping","text":"The inputMapping object contains the declaration of parameters that are used to deploy a workflow layer. The parameters that you can declare are based on the action that has been defined for the workflow layer. For more information, see Supported workflow actions . You can declare parameters for multiple workflow layers in a single inputMapping object. The object must be in the following format: \"inputMapping\" : { \"workflowLayerName\" : { <I n pu t Mappi n g parame ters f or t he work fl ow layer> }, \"workflowLayerName\" : { <I n pu t Mappi n g parame ters f or t he work fl ow layer> } }","title":"inputMapping"},{"location":"workflow/understand-solution-package-schema/#output","text":"This data object is used to declare the output parameters of a solution package. You can define multiple parameters in the output object. You can view the result of the parameters defined in the output object after the solution package is deployed. The output parameters can also be used as input parameters in another solution package. The object must be in the following format: { \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output example with default type and directly from inputs of the solution package\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output example from the output of a workflow layer\" , \"type\" : \"String\" } ] } Following are the attributes of the output object: name value description type","title":"output"},{"location":"workflow/understand-solution-package-schema/#name_3","text":"The name of the output parameter.","title":"name"},{"location":"workflow/understand-solution-package-schema/#value","text":"The value for the output parameter. There are multiple ways to define values for output parameters. Case 1 : Use the value of an input parameter that is defined in the solution package. { \"name\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"String\" } Case 2 : Use the value of an output parameter of any workflow layer defined in the solution package. { \"name\" : \"vpc_id\" , \"value\" : \"${#output['network-layer']['vpc_id']}\" , \"type\" : \"String\" } In this case, network-layer is name of the workflow layer from where the output parameter will be received. Case 3 : Use the combined value of multiple input parameters or output parameters of any workflow layer defined in the solution package. { \"name\" : \"resource-name-prefix\" , \"value\" : \"${#output['network-layer']['customer_name']}-${#output['network-layer']['env']}\" , \"type\" : \"String\" }","title":"value"},{"location":"workflow/understand-solution-package-schema/#description_1","text":"The description of the output parameter. This is a free text section.","title":"description"},{"location":"workflow/understand-solution-package-schema/#type_1","text":"The type of the output parameter. The supported types are List, String, Boolean, Map, and Integer. The default value of the parameter is String , if no value is specified while creating the solution package.","title":"type"},{"location":"workflow/understand-solution-package-schema/#supported-workflow-actions","text":"The workflow data object contains the list of actions that a solution package must execute. For each workflow action, you must specify the appropriate data, input mapping, and output parameters. The Solution Package schema supports the following workflow actions: deployEnv createProvider createHelmRepo deployHelmChart createFoundrySolutionPackage deployFoundrySolutionPackage deploySolutionPackage","title":"Supported workflow actions"},{"location":"workflow/understand-solution-package-schema/#summary-of-workflow-action-details","text":"This section provides a quick overview of the data parameters, input mapping parameters, and output of each workflow action that the Solution Package schema supports. For more information, see the detailed section on each workflow action. Workflow action Data parameters Input Mapping parameters Output of the workflow action deployEnv Deploys the environment that is defined in the data parameters. envName envVersion provider deployInput All outputs from the environment's deployment createProvider Creates a provider in Deploy Accelerator. provider_name provider_type provider_json_string provider_json provider_id provider_name provider_type globalProvider createHelmRepo Registers a new Helm repository. You can use this action when helm charts to be installed by the deployHelmChart action are not present in the default repository. repo_name repo_url username password is_oci_registry caFile inSecureSkipTlsVerify repo_name repo_url deployHelmChart Deploys Helm charts to the Kubernetes cluster from the Helm repository that is defined in the Input Mapping parameters. chart_name chart_version providerId repoName namespace values helmStatus helmDeploymentName helmInstallLogs createFoundrySolutionPackage Registers the Foundry solution package that is defined in the data parameters. The Foundry solution package must be located in the configured Artifactory of Hitachi Cloud Accelerator Platform. This action initially pushes the Foundry solution package to the Harbor registry that is configured for the Foundry Control Plane. Next, it registers the Foundry solution package, which then starts showing up in the list of available Foundry solution packages in the Foundry Control Plane. solution_package_name solution_package_version provider oci_url oci_username oci_password None deployFoundrySolutionPackage Deploys the Foundry solution package that is defined in the data parameters. Only Foundry solution packages that are available in the Foundry Control Plane can be deployed. solution_package_name solution_package_version auth solution_config chartName chartVersion chartDescription applicationVersion modelVersion solutionName updateDate solutionResources status deploySolutionPackage Deploys the solution package that is defined in the data parameters. This solution package can be a prerequisite for the current solution package. solution_package_name solution_package_version providers deploySolutionPackageInput All outputs from the solution package's deployment","title":"Summary of workflow action details"},{"location":"workflow/understand-solution-package-schema/#deployenv-action","text":"The deployEnv workflow action deploys the environment that is defined in the data attribute. Example { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } }","title":"deployEnv action"},{"location":"workflow/understand-solution-package-schema/#data-parameters","text":"Following are the data parameters for the deployEnv workflow action: envName envVersion You can define only one environment name and version per action. The environment name and version must be the same as used in the blueprint that is imported either automatically or manually. Make sure that the environment is present in Deploy Accelerator before starting the deployment. Note: The naming convention used for environments in a blueprint that are imported from the artifactory is solutionPackageName_environmentName . However, while specifying these environments in the workflow object, make sure that you enter only the environmentName . Before starting the deployment, the solutionPackageName is automatically prefixed to the environment name.","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters","text":"For the deployenv workflow action, you can declare the input mapping that will be used for the workflow layer with a specific provider. provider -- The provider to be used by the workflow layer for deployment. region -- The region in which the environment is to be deployed. This is an optional parameter. connection -- The connection ID that can be used to connect to the instances in the deployed environment. This is an optional parameter. chefEnvironment -- The environment that is available on the selected Chef Server. This is an optional parameter. deployInput -- The values to be used while deploying the workflow layer using the solution package. key -- The exact name as defined in the Input Variables resource of the Deploy Accelerator environment for which you are configuring this input mapping. value -- The value for the inputParameter key. There are three types of input mapping values, as described below. Case 1 : Mapping values for input parameters used in the solution package. { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input['vpc_cidr']}\" , \"type\" : \"STRING\" } Case 2 : Mapping output parameter of another workflow layer with the input parameter. When a solution package has two blueprints that are not connected using the Depends On feature of Deploy Accelerator, the output parameter of the parent blueprint must be used in the child blueprint. { \"key\" : \"vpc_id\" , \"value\" : \"${#output['network-layer']['vpc_id']}\" , \"type\" : \"STRING\" } In this case network-layer is name of the workflow layer from where the output parameter will be received. Case 3 : Mapping input parameter with multiple input parameters or workflow layer output. { \"key\" : \"resource-name-prefix\" , \"value\" : \"${#output['network-layer']['customer_name']}-${#output['network-layer']['env']}\" , \"type\" : \"STRING\" } type -- The type of the inputParameter key. The supported types are List, String, Boolean, Map, and Integer. The default value of the parameter is String , if no value is entered while creating the Solution Package. Note: The value of the inputParameter key is resolved based on the type of the inputParameter. If you specify an incorrect type, complete deployment of the solution package will fail. Example { \"provider\" : \"AWS_1\" , \"region\" : \"${#input['region']}\" , \"connection\" : { \"default\" : \"${#input['default']}\" , \"some_resource\" : \"${#input['some_resource']}\" }, \"chefEnvironment\" : \"${#input['chef_environment']}\" , \"deployInput\" : [] }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters","text":"The output parameters for the deployEnv workflow action include all of the environment's deployment outputs.","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#createprovider","text":"The createProvider workflow action creates a provider in Deploy Accelerator. Workflow Accelerator supports all providers and authentication methods that are available in Deploy Accelerator. For more information, see Configuring providers . Example { \"action\" : \"createProvider\" , \"name\" : \"aws_provider_creation_helm\" , \"condition\" : \"${#input['provider_creation_helm']}\" , \"data\" : { \"provider_name\" : \"helm_provider\" , \"provider_type\" : \"helm3\" } }","title":"createProvider"},{"location":"workflow/understand-solution-package-schema/#data-parameters_1","text":"Following are the data parameters for the createProvider action: provider_name provider_type","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_1","text":"For the createProvider action, you can define the inputs for providers with dynamic configuration. The parameters in the createProvider object must be same as the parameters used in Deploy Accelerator. You can define parameters for the provider in this section, or you can define the provider in the Solution package deployment object. You must use one of the following definition type. provider_json_string : Used to pass the provider JSON at the time of deployment, and must be defined in the Solution Package Deployment Object. \"inputMapping\" :{ \"aws_provider_creation\" : { \"provider_json_string\" : { \"#input['provider-json']\" } } } provider_json: The parameters for defining a provider. The parameters are same as the parameters in the provider definition JSON in the Deploy Accelerator. You might need to modify the parameters as per the provider type. The provider parameters defined in this section are fixed. access_key : Access key to the AWS account. secret_key : Secret key to the AWS account. region : Region of the AWS account. \"inputMapping\" : { \"aws_provider_creation\" : { \"provider_json\" : { \"access_key\" : \"${#input.access_key}\" , \"secret_key\" : \"${#input.secret_key}\" , \"region\" : \"${#input.region}\" } } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_1","text":"Following are the output parameters for the createProvider workflow action: provider_id provider_name provider_type globalProvider","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#createhelmrepo-action","text":"The createHelmRepo action registers a new Helm repository. You can use this action when the helm charts to be installed by the deployHelmChart action are not available in the default repository in artifactory. Example { \"action\" : \"createHelmRepo\" , \"name\" : \"repoSP\" , \"condition\" : \"${#input['repoSP']}\" , \"data\" : { \"repo_name\" : \"repo\" } }","title":"createHelmRepo action"},{"location":"workflow/understand-solution-package-schema/#data-parameters_2","text":"The data parameter for the createHelmRepo workflow action is repo_name . A repository named default is already created in the Helm service. Therefore, ensure that you do not specify default as the repository name.","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_2","text":"For the createHelmRepo workflow action, you can define the parameters to register the Helm repository with the URL provided in the parameters. repo_url -- URL for deployment of the Helm repository. username -- Username for accessing the Helm repository. password -- Password for accessing the Helm repository. is_oci_registry -- Enable or disable support for OCI registry. Enter false for using Artifactory, and enter true for using Harbor. caFile : The CA bundle to verify the certificates of the HTTPS enabled servers. inSecureSkipTlsVerify : Enable or disable the TLS certificate check for the Helm repository. Example \"inputMapping\" : { \"helmRepo\" : { \"repo_url\" : \"$(#input.repo_url)\" , \"username\" : \"$(#input.username)\" , \"password\" : \"$(#input.password)\" , \"is_oci_registry\" : false , \"caFile\" : \"$(#input.caFile)\" , \"inSecureSkipTlsVerify\" : false } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_2","text":"Following are the output parameters for the createHelmRepo workflow action: repo_name repo_url","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#deployhelmchart","text":"The deployHelmChart workflow deploys Helm charts to the Kubernetes cluster from the Helm repository deployed in the earlier action. If no repository is declared in the createHelmRepo action, it refers to the the default repository in artifactory. Example { \"action\" : \"deployHelmChart\" , \"name\" : \"deploy_helm\" , \"condition\" : \"${#input['deploy_helm']}\" , \"data\" : { \"chart_name\" : \"repo/platform-helm-sampleapp\" , \"chart_version\" : \"0.1.2\" } }","title":"deployHelmChart"},{"location":"workflow/understand-solution-package-schema/#data-parameters_3","text":"Following are the data parameters for the deployHelmChart workflow action: chart_name chart_version The chart_name must be in the format repo_name/chart_name . If you have not declared any repo_name , use default/chart_name .","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_3","text":"For the deployHelmChart workflow action, you can deploy a Helm chart in the recently deployed Helm repository. namespace - The namespace in which the Helm Chart is deployed. If not defined, it is deployed in the default namespace. repository - Registered repository name with the service from where it will deploy the helm charts. (mandatory) provider - The provider for deploying Helm charts. It should be registered with Deploy Accelerator. (mandatory) values - JSON for overriding the default values of the Helm Chart from Solution Package. values_string - JSON for overriding the values from the Solution Package deployment object. The name must match the value defined in the solution package. Example \"deploy_helm\" : { \"provider\" : \"${#providers['provider_creation_helm']['provider_id']}\" , \"repository\" : \"${#output['helm_repo_creation']['repo_name']}\" , \"namespace\" : \"${#input['namespace']}\" , \"values\" : { \"docker\" : { \"image\" : \"reanplatform-sampleapp:0.0.1\" , \"registry\" : { \"username\" : \"${#input.username}\" , \"password\" : \"${#input.password}\" , \"server\" : \"${#input.server}\" } } } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_3","text":"Following are the output parameters for the deployHelmChart workflow action: helmStatus helmDeploymentName helmInstallLogs","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#createfoundrysolutionpackage-action","text":"The createFoundrySolutionPackage workflow action registers the Foundry solution package that is defined in the data attribute. This action consists of the following steps: The Foundry solution package, which is located in the configured Artifactory of Hitachi Cloud Accelerator Platform, is pushed to the Harbor registry that is configured for the Foundry Control Plane. The Foundry solution package is registered and starts showing up in the list of available Foundry solution packages in the Foundry Control Plane. Example { \"action\" : \"createFoundrySolutionPackage\" , \"name\" : \"foundry_package\" , \"condition\" : \"${#input['foundry_package']}\" , \"data\" : { \"solution_package_name\" : \"hello-world-changed\" , \"solution_package_version\" : \"0.2.0\" } }","title":"createFoundrySolutionPackage action"},{"location":"workflow/understand-solution-package-schema/#data-parameters_4","text":"Following are the data parameters for the createFoundrySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Foundry solution package and version per action. The Foundry solution package name and version must be the same as the file name and version that is stored in the configured artifactory of Cloud Accelerator Platform. Before starting the deployment, make sure that the solution package is present in the configured repository of the artifactory. If the Foundry solution package is already registered in the Foundry Control Plane, this action checks if there is any change in the Foundry solution package that is present in the configured repository of the Artifactory. If a change is identified, the registered Foundry solution package is updated. Note: If you destroy a deployment of the Workflow Accelerator solution package, the Foundry solution package that has already been registered does not get deregistered. It continues to be available in the Foundry Control Plane.","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_4","text":"For the createFoundrySolutionPackage workflow action, you can declare the credentials for accessing the Harbor registry in which the Foundry solution package will be pushed, along with the provider for the Kubernetes cluster in which the Foundry Control Plane is installed. This Kubernetes provider must exist in Deploy Accelerator. provider -- The Kubernetes provider ID (in Deploy Accelerator) to be used by the workflow layer to connect to the cluster in which Foundry Control Plane is installed. You can get this provider ID either from the output of the previous layer or from the input parameters. oci_url -- URL for accessing the Harbor registry that is configured for the Foundry Control Plane. oci_username -- User name to sign-in to the Harbor registry. oci_password -- Password to sign-in to the Harbor registry. For the OCI parameters, you can map values for input parameters used in the solution package. Example \"inputMapping\" : { \"foundry-package\" : { \"provider\" : \"${#input['providerID']}\" , \"oci_url\" : \"${#input['harborHost']}\" , \"oci_username\" : \"${#input['harborUsername']}\" , \"oci_password\" : \"${#input['harborPassword']}\" } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_4","text":"The createFoundrySolutionPackage workflow action has no output parameters.","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#deployfoundrysolutionpackage","text":"The deployFoundrySolutionPackage workflow action deploys the Foundry solution package that is defined in the data attribute. Only Foundry solution packages that are available in the Foundry Control Plane can be deployed. Example { \"action\" : \"deployFoundrySolutionPackage\" , \"name\" : \"foundry_package_deploy\" , \"condition\" : \"${#input['foundry_package']}\" , \"data\" : { \"solution_package_name\" : \"hello-world-changed\" , \"solution_package_version\" : \"0.2.0\" } }","title":"deployFoundrySolutionPackage"},{"location":"workflow/understand-solution-package-schema/#data-parameters_5","text":"Following are the data parameters for the deployFoundrySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Foundry solution package and version per action. The Foundry solution package name and version must be the same as the name and version that is registered in the Foundry control plane.","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_5","text":"For the deployFoundrySolutionPackage workflow action, you can declare the credentials for accessing the Foundry Control Plane, along with optional configuration information about the Foundry Control Plane. auth -- The authentication details required to connect to the Foundry Control Plane and deploy the Foundry solution package. client_id -- Client ID to connect to the Foundry Control Plane. client_secret -- Client Secret to connect to the Foundry Control Plane. username -- Username to connect to the Foundry Control Plane. password -- Password to connect to the Foundry Control Plane. foundry_host -- Host on which the Foundry Control Plane is installed. namespace -- Kubernetes namespace in which the Foundry Control Plane is installed. solution_config -- Information about the Foundry solution package deployment. modelVersion -- Version of the Foundry Control Plane. values OR values_string -- You can either provide a fixed list of values or specify that the values will be taken from a JSON string that the user will provide as input while deploying the solution package. Format for values \"values\" : { \"image\" : { \"gatekeeperInitPullPolicy\" : \"IfNotPresent\" , \"pullPolicy\" : \"IfNotPresent\" , \"repository\" : \"harbor.prod.platfom/foundry/foundry-hello-world\" , \"tag\" : \"2.0.0.345\" } } Format for values_string \"values_string\" : \"${#input['values_from_input_param']}\" Example: \"inputMapping\" : { \"foundry-package-deploy\" : { \"auth\" : { \"client_id\" : \"${#input['clientId]}\" , \"client_secret\" : \"${#input['clientSecret]}\" , \"username\" : \"${#input['username]}\" , \"password\" : \"${#input['password]}\" , \"foundry_host\" : \"${#input['foundryHost]}\" , \"namespace\" : \"${#input['namespace]}\" }, \"solution_config\" : { \"modelVersion\" : \"${#input['modelversion']}\" , \"values_string\" : \"${#input['values']}\" } } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_5","text":"Following are the output parameters for the deployFoundrySolutionPackage workflow action: chartName chartVersion chartDescription applicationVersion modelVersion solutionName updateDate solutionResources status","title":"Output parameters"},{"location":"workflow/understand-solution-package-schema/#deploysolutionpackage","text":"The deploySolutionPackage workflow action deploys the Workflow Accelerator solution package that is a prerequisite for the current solution package. Example { \"action\" : \"deploySolutionPackage\" , \"name\" : \"sc_dependency1\" , \"condition\" : \"${#input['Is_solution_package_deployment']}\" , \"data\" : { \"solution_package_name\" : \"SolutionPackage_A1\" , \"solution_package_version\" : \"00.00.100\" } }","title":"deploySolutionPackage"},{"location":"workflow/understand-solution-package-schema/#data-parameters_6","text":"Following are data parameters for the deploySolutionPackage workflow action: solution_package_name solution_package_version You can define only one Workflow Accelerator solution package and version per action. The solution package name and version must be the same as defined in its solution package JSON.","title":"Data parameters"},{"location":"workflow/understand-solution-package-schema/#input-mapping-parameters_6","text":"For the deploySolutionPackage workflow action, you can declare the providers and the input parameters required for deploying all the layers from the solution package declared in the action. Layer name object (dynamic parameter): The name of the workflow layer to be deployed by the solution package defined in action. providers : List of the providers required for deploying all the layers from referenced solution package. deploySolutionPackageInput : List of all the input parameters required for deploying all the layers from referenced solution package. Example \"inputMapping\" : { \"sc_dependency1\" : { \"providers\" : { \"aws-provider\" : \"${#providers['aws-provider']}\" }, \"deploySolutionPackageInput\" : [ { \"name\" : \"vpc_cidr\" , \"value\" : \"${#input.vpc_cidr}\" , \"type\" : \"String\" }, { \"name\" : \"env\" , \"value\" : \"${#input.env}, \" t ype \" : \" S tr i n g \" }, { \" na me \" : \" cus t omer \", \" value \" : \" $ { #i n pu t .cus t omer } \", \" t ype : \"String\" } ] } }","title":"Input Mapping parameters"},{"location":"workflow/understand-solution-package-schema/#output-parameters_6","text":"The output parameters for the deploySolutionPackage workflow action include all of the solution package's deployment outputs.","title":"Output parameters"},{"location":"workflow/using/","text":"Create and register solution packages \u00b6 Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages. This topic describes how to create and register solution packages. Contents \u00b6 Prerequisites Creating a solution package Registering a solution package Performing additional actions on a solution package Prerequisites \u00b6 Before creating a solution package, you must perform the following actions: Ensure that you have completed all prerequisites for Workflow Accelerator . Plan the end-to-end solution for which you want to create the solution package. For example, identify the cloud service providers for the solution, determine the number of required workflow layers, and outline the dependency between the layers. For each workflow layer in the solution package, ensure that you have the details that need to be specified for the action. For more information, see Workflow actions . Make a note of the user inputs required to deploy each workflow layer in the solution package. Create the required providers in Deploy Accelerator and make a note of the provider details. For more information, see Configuring providers . For example, to deploy infrastructure in AWS, create an AWS provider. Similarly, to deploy a Helm chart in a Kubernetes cluster, create a Helm provider. Note: Workflow Accelerator supports all providers and authentication methods that are available in Deploy Accelerator. Creating a solution package \u00b6 Ensure that you have completed all the prerequisites for creating a solution package. Open a text editor and create a new file. Save the file in the JSON format ( .json ). In the JSON file, enter the schema structure of the solution package, as shown in the example below. { \"schemaVersion\" : \"X.X\" , \"scheduler\" :{}, \"metadata\" :{}, \"input\" :{}, \"blueprints\" :[], \"providers\" :[], \"outputs\" :{}, \"workflow\" :[] } Enter the appropriate data within each data object of the schema structure. For information about the data objects, see the latest solution package schema . Save your updates to the file. After you have created your solution package, you must register the solution package . You can deploy only registered solution packages. Registering a solution package \u00b6 To register a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X POST ${HCAP_URL}/api/solution_package/v2 \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ <Solution Package JSON> }' Body JSON Example { \"schemaVersion\" : \"2.1\" , \"metadata\" : { \"name\" : \"demo\" , \"version\" : \"00.00.01\" , \"description\" : \"demo\" , \"icon\" : \"https://google.com\" , \"applicationVersion\" : \"01.00.00\" , \"language\" : [ \"english\" , \"japanese\" ], \"ownerName\" : \"demo-user\" , \"ownerEmail\" : \"demo.user@companyname.com\" , \"cloudRegion\" : [ \"us-east-1\" , \"us-west-1\" ], \"vertical\" : [], \"tags\" : { \"tag1\" : \"value1\" , \"tag2\" : \"value2\" , \"app\" : \"SampleApp\" } }, \"input\" : { \"inputParameters\" : [ { \"name\" : \"cidr\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"10.0.0.0/16\" }, { \"name\" : \"isApp\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"isVpc\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"instance_type\" , \"type\" : \"String\" , \"default\" : \"t2.micro\" , \"description\" : \"instance type\" }, { \"name\" : \"customer_name\" , \"type\" : \"String\" , \"description\" : \"instance name\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment like prod, qa, etc\" }, { \"name\" : \"list-value\" , \"type\" : \"List\" , \"defaultValue\" : \"'demo-one','demo-two'\" }, { \"name\" : \"json-value\" , \"type\" : \"Json\" } ], \"inputMapping\" : { \"vpc\" : { \"provider\" : \"dns\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.cidr}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#input['customer_name']}\" }, { \"key\" : \"env\" , \"value\" : \"${#input['env']}\" }, { \"key\" : \"pqr\" , \"value\" : \"${#input['list-value']}\" , \"type\" : \"List\" }, { \"key\" : \"p\" , \"value\" : \"${#input['json-value']}\" , \"type\" : \"Map\" } ] }, \"app\" : { \"deployInput\" : [ { \"key\" : \"instance_type\" , \"value\" : \"${#input['instance_type']}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#output['vpc']['customer']}\" }, { \"key\" : \"env1\" , \"value\" : \"{ '${#input['customer_name']}' : '${#input['customer_name']}' }\" , \"type\" : \"Map\" }, { \"key\" : \"vpc_id\" , \"value\" : \"${#output['vpc']['vpc_id']}\" } ], \"provider\" : \"dns\" } } }, \"blueprints\" : [ ], \"providers\" : [ { \"name\" : \"dns\" , \"type\" : \"dns\" } ], \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output sample example with default type and directly from inputs of SP\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output sample example from output of layer\" , \"type\" : \"String\" }, { \"name\" : \"output3\" , \"value\" : \"${#output['app']['app_url']}\" , \"description\" : \"Output sample example from layer 2\" , \"type\" : \"String\" }, { \"name\" : \"output4\" , \"value\" : \"${#output['vpc']['prq-value']}\" , \"description\" : \"Output sample example with list type\" , \"type\" : \"List\" }, { \"name\" : \"output5\" , \"value\" : \"${#output['vpc']['json-value']}\" , \"description\" : \"Output sample example with map type\" , \"type\" : \"Map\" } ], \"workflow\" : [ { \"action\" : \"deployEnv\" , \"name\" : \"vpc\" , \"condition\" : \"${#input.isVpc}\" , \"data\" : { \"envName\" : \"vpc-demo\" , \"envVersion\" : \"01.00.00\" } }, { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } } ] } Performing additional actions on solution package \u00b6 The Solution Package REST APIs also allow you to perform a few additional actions on solution packages. For example, you can get a list of all your solution packages, search for a solution package, update a solution package, and delete a solution package. For information about accessing the Solution Package API documentation, see Workflow Accelerator API Reference .","title":"Create and register solution package"},{"location":"workflow/using/#create-and-register-solution-packages","text":"Hitachi Cloud Accelerator Platform - Workflow Accelerator allows you to create and maintain a catalog of solution packages and easily deploy these solution packages. This topic describes how to create and register solution packages.","title":"Create and register solution packages"},{"location":"workflow/using/#contents","text":"Prerequisites Creating a solution package Registering a solution package Performing additional actions on a solution package","title":"Contents"},{"location":"workflow/using/#prerequisites","text":"Before creating a solution package, you must perform the following actions: Ensure that you have completed all prerequisites for Workflow Accelerator . Plan the end-to-end solution for which you want to create the solution package. For example, identify the cloud service providers for the solution, determine the number of required workflow layers, and outline the dependency between the layers. For each workflow layer in the solution package, ensure that you have the details that need to be specified for the action. For more information, see Workflow actions . Make a note of the user inputs required to deploy each workflow layer in the solution package. Create the required providers in Deploy Accelerator and make a note of the provider details. For more information, see Configuring providers . For example, to deploy infrastructure in AWS, create an AWS provider. Similarly, to deploy a Helm chart in a Kubernetes cluster, create a Helm provider. Note: Workflow Accelerator supports all providers and authentication methods that are available in Deploy Accelerator.","title":"Prerequisites"},{"location":"workflow/using/#creating-a-solution-package","text":"Ensure that you have completed all the prerequisites for creating a solution package. Open a text editor and create a new file. Save the file in the JSON format ( .json ). In the JSON file, enter the schema structure of the solution package, as shown in the example below. { \"schemaVersion\" : \"X.X\" , \"scheduler\" :{}, \"metadata\" :{}, \"input\" :{}, \"blueprints\" :[], \"providers\" :[], \"outputs\" :{}, \"workflow\" :[] } Enter the appropriate data within each data object of the schema structure. For information about the data objects, see the latest solution package schema . Save your updates to the file. After you have created your solution package, you must register the solution package . You can deploy only registered solution packages.","title":"Creating a solution package"},{"location":"workflow/using/#registering-a-solution-package","text":"To register a solution package, use the following cURL command. For information about the cURL command syntax and options, see Using cURL to make API requests . curl -X POST ${HCAP_URL}/api/solution_package/v2 \\ -H 'authorization: $USERNAME:$PASSWORD' \\ -H 'content-type: application/json' \\ -d '{ <Solution Package JSON> }' Body JSON Example { \"schemaVersion\" : \"2.1\" , \"metadata\" : { \"name\" : \"demo\" , \"version\" : \"00.00.01\" , \"description\" : \"demo\" , \"icon\" : \"https://google.com\" , \"applicationVersion\" : \"01.00.00\" , \"language\" : [ \"english\" , \"japanese\" ], \"ownerName\" : \"demo-user\" , \"ownerEmail\" : \"demo.user@companyname.com\" , \"cloudRegion\" : [ \"us-east-1\" , \"us-west-1\" ], \"vertical\" : [], \"tags\" : { \"tag1\" : \"value1\" , \"tag2\" : \"value2\" , \"app\" : \"SampleApp\" } }, \"input\" : { \"inputParameters\" : [ { \"name\" : \"cidr\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"10.0.0.0/16\" }, { \"name\" : \"isApp\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"isVpc\" , \"type\" : \"String\" , \"description\" : \"AMI ID\" , \"defaultValue\" : \"true\" }, { \"name\" : \"instance_type\" , \"type\" : \"String\" , \"default\" : \"t2.micro\" , \"description\" : \"instance type\" }, { \"name\" : \"customer_name\" , \"type\" : \"String\" , \"description\" : \"instance name\" }, { \"name\" : \"env\" , \"type\" : \"String\" , \"description\" : \"Environment like prod, qa, etc\" }, { \"name\" : \"list-value\" , \"type\" : \"List\" , \"defaultValue\" : \"'demo-one','demo-two'\" }, { \"name\" : \"json-value\" , \"type\" : \"Json\" } ], \"inputMapping\" : { \"vpc\" : { \"provider\" : \"dns\" , \"deployInput\" : [ { \"key\" : \"vpc_cidr\" , \"value\" : \"${#input.cidr}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#input['customer_name']}\" }, { \"key\" : \"env\" , \"value\" : \"${#input['env']}\" }, { \"key\" : \"pqr\" , \"value\" : \"${#input['list-value']}\" , \"type\" : \"List\" }, { \"key\" : \"p\" , \"value\" : \"${#input['json-value']}\" , \"type\" : \"Map\" } ] }, \"app\" : { \"deployInput\" : [ { \"key\" : \"instance_type\" , \"value\" : \"${#input['instance_type']}\" }, { \"key\" : \"customer_name\" , \"value\" : \"${#output['vpc']['customer']}\" }, { \"key\" : \"env1\" , \"value\" : \"{ '${#input['customer_name']}' : '${#input['customer_name']}' }\" , \"type\" : \"Map\" }, { \"key\" : \"vpc_id\" , \"value\" : \"${#output['vpc']['vpc_id']}\" } ], \"provider\" : \"dns\" } } }, \"blueprints\" : [ ], \"providers\" : [ { \"name\" : \"dns\" , \"type\" : \"dns\" } ], \"outputs\" : [ { \"name\" : \"output1\" , \"value\" : \"${#input.cidr}\" , \"description\" : \"Output sample example with default type and directly from inputs of SP\" }, { \"name\" : \"output2\" , \"value\" : \"${#output['vpc']['customer']}\" , \"description\" : \"Output sample example from output of layer\" , \"type\" : \"String\" }, { \"name\" : \"output3\" , \"value\" : \"${#output['app']['app_url']}\" , \"description\" : \"Output sample example from layer 2\" , \"type\" : \"String\" }, { \"name\" : \"output4\" , \"value\" : \"${#output['vpc']['prq-value']}\" , \"description\" : \"Output sample example with list type\" , \"type\" : \"List\" }, { \"name\" : \"output5\" , \"value\" : \"${#output['vpc']['json-value']}\" , \"description\" : \"Output sample example with map type\" , \"type\" : \"Map\" } ], \"workflow\" : [ { \"action\" : \"deployEnv\" , \"name\" : \"vpc\" , \"condition\" : \"${#input.isVpc}\" , \"data\" : { \"envName\" : \"vpc-demo\" , \"envVersion\" : \"01.00.00\" } }, { \"action\" : \"deployEnv\" , \"name\" : \"app\" , \"condition\" : \"${#input.isApp}\" , \"data\" : { \"envName\" : \"app_demo\" , \"envVersion\" : \"01.00.00\" } } ] }","title":"Registering a solution package"},{"location":"workflow/using/#performing-additional-actions-on-solution-package","text":"The Solution Package REST APIs also allow you to perform a few additional actions on solution packages. For example, you can get a list of all your solution packages, search for a solution package, update a solution package, and delete a solution package. For information about accessing the Solution Package API documentation, see Workflow Accelerator API Reference .","title":"Performing additional actions on solution package"}]}